id,ABSTRACT,Computer Science,Mathematics,Physics,Statistics,Analysis of PDEs,Applications,Artificial Intelligence,Astrophysics of Galaxies,Computation and Language,Computer Vision and Pattern Recognition,Cosmology and Nongalactic Astrophysics,Data Structures and Algorithms,Differential Geometry,Earth and Planetary Astrophysics,Fluid Dynamics,Information Theory,Instrumentation and Methods for Astrophysics,Machine Learning,Materials Science,Methodology,Number Theory,Optimization and Control,Representation Theory,Robotics,Social and Information Networks,Statistics Theory,Strongly Correlated Electrons,Superconductivity,Systems and Control
1824,"a ever-growing datasets inside observational astronomy have challenged scientists inside many aspects, including an efficient and interactive data exploration and visualization. many tools have been developed to confront this challenge. however, they usually focus on displaying a actual images or focus on visualizing patterns within catalogs inside the predefined way. inside this paper we introduce vizic, the python visualization library that builds a connection between images and catalogs through an interactive map of a sky region. vizic visualizes catalog data over the custom background canvas with the help of a shape, size and orientation of each object inside a catalog. a displayed objects inside a map are highly interactive and customizable comparing to those inside a images. these objects should be filtered by or colored by their properties, such as redshift and magnitude. they also should be sub-selected with the help of the lasso-like tool considering further analysis with the help of standard python functions from in the jupyter notebook. furthermore, vizic allows custom overlays to be appended dynamically on top of a sky map. we have initially implemented several overlays, namely, voronoi, delaunay, minimum spanning tree and healpix grid layers, which are helpful considering visualizing large-scale structure. all these overlays should be generated, added or removed interactively with one line of code. a catalog data was stored inside the non-relational database, and a interfaces were developed inside javascript and python to work within jupyter notebook, which allows to create custom widgets, user generated scripts to analyze and plot a data selected/displayed inside a interactive map. this unique design makes vizic the very powerful and flexible interactive analysis tool. vizic should be adopted inside variety of exercises, considering example, data inspection, clustering analysis, galaxy alignment studies, outlier identification or simply large-scale visualizations.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
3094,"we propose the framework considering optimal $t$-matchings excluding a prescribed $t$-factors inside bipartite graphs. a proposed framework was the generalization of a nonbipartite matching problem and includes several problems, such as a triangle-free $2$-matching, square-free $2$-matching, even factor, and arborescence problems. inside this paper, we demonstrate the unified understanding of these problems by commonly extending previous important results. we solve our problem under the reasonable assumption, which was sufficiently broad to include a specific problems listed above. we first present the min-max theorem and the combinatorial algorithm considering a unweighted version. we then provide the linear programming formulation with dual integrality and the primal-dual algorithm considering a weighted version. the key ingredient of a proposed algorithm was the technique to shrink forbidden structures, which corresponds to a techniques of shrinking odd cycles, triangles, squares, and directed cycles inside edmonds' blossom algorithm, the triangle-free $2$-matching algorithm, the square-free $2$-matching algorithm, and an arborescence algorithm, respectively.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8463,"nanostructures with open shell transition metal or molecular constituents host often strong electronic correlations and are highly sensitive to atomistic material details. this tutorial review discusses method developments and applications of theoretical approaches considering a realistic description of a electronic and magnetic properties of nanostructures with correlated electrons. first, a implementation of the flexible interface between density functional theory and the variant of dynamical mean field theory (dmft) highly suitable considering a simulation of complex correlated structures was explained and illustrated. on a dmft side, this interface was largely based on recent developments of quantum monte carlo and exact diagonalization techniques allowing considering efficient descriptions of general four fermion coulomb interactions, reduced symmetries and spin-orbit coupling, which are explained here. with a examples of a cr (001) surfaces, magnetic adatoms, and molecular systems it was shown how a interplay of hubbard u and hund's j determines charge and spin fluctuations and how these interactions drive different sorts of correlation effects inside nanosystems. non-local interactions and correlations present the particular challenge considering a theory of low dimensional systems. we present our method developments addressing these two challenges, i.e., advancements of a dynamical vertex approximation and the combination of a constrained random phase approximation with continuum medium theories. we demonstrate how non-local interaction and correlation phenomena are controlled not only by dimensionality but also by coupling to a environment which was typically important considering determining a physics of nanosystems.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
2082,"stars are self-gravitating fluids inside which pressure, buoyancy, rotation and magnetic fields provide a restoring forces considering global modes of oscillation. pressure and buoyancy energetically dominate, while rotation and magnetism are generally assumed to be weak perturbations and often ignored. however, observations of anomalously weak dipole mode amplitudes inside red giant stars suggest that the substantial fraction of these are subject to an additional source of damping localised to their core region, with indirect evidence pointing to a role of the deeply buried magnetic field. it was also known that inside many instances a gravity-mode character of affected modes was preserved, but so far no effective damping mechanism has been proposed that accommodates this aspect. here we present such the mechanism, which damps a oscillations of stars harbouring magnetised cores using resonant interactions with standing alfv√©n modes of high harmonic index. a damping rates produced by this mechanism are quantitatively on par with those associated with turbulent convection, and inside a range required to explain observations, considering realistic stellar models and magnetic field strengths. our results suggest that magnetic fields should provide an efficient means of damping stellar oscillations without needing to disrupt a internal structure of a modes, and lay a groundwork considering an extension of a theory of global stellar oscillations that incorporates these effects.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8687,"deep neural perception and control networks are likely to be the key component of self-driving vehicles. these models need to be explainable - they should provide easy-to-interpret rationales considering their behavior - so that passengers, insurance companies, law enforcement, developers etc., should understand what triggered the particular behavior. here we explore a use of visual explanations. these explanations take a form of real-time highlighted regions of an image that causally influence a network's output (steering control). our idea behind the method was two-stage. inside a first stage, we use the visual attention model to train the convolution network end-to-end from images to steering angle. a attention model highlights image regions that potentially influence a network's output. some of these are true influences, but some are spurious. we then apply the causal filtering step to determine which input regions actually influence a output. this produces more succinct visual explanations and more accurately exposes a network's behavior. we demonstrate a effectiveness of our model on three datasets totaling 16 hours of driving. we first show that training with attention does not degrade a performance of a end-to-end network. then we show that a network causally cues on the variety of features that are used by humans while driving.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2342,"analyzing job hopping behavior was important considering understanding job preference and career progression of working individuals. when analyzed at a workforce population level, job hop analysis helps to gain insights of talent flow among different jobs and organizations. traditionally, surveys are conducted on job seekers and employers to study job hop behavior. beyond surveys, job hop behavior should also be studied inside the highly scalable and timely manner with the help of the data driven idea behind the method inside response to fast-changing job landscape. fortunately, a advent of online professional networks (opns) has made it possible to perform the large-scale analysis of talent flow. inside this paper, we present the new data analytics framework to analyze a talent flow patterns of close to 1 million working professionals from three different countries/regions with the help of their publicly-accessible profiles inside an established opn. as opn data are originally generated considering professional networking applications, our proposed framework re-purposes a same data considering the different analytics task. prior to performing job hop analysis, we devise the job title normalization procedure to mitigate a amount of noise inside a opn data. we then devise several metrics to measure a amount of work experience required to take up the job, to determine that existence duration of a job (also known as a job age), and a correlation between a above metric and propensity of hopping. we also study how job hop behavior was related to job promotion/demotion. lastly, we perform connectivity analysis at job and organization levels to derive insights on talent flow as well as job and organizational competitiveness.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
16866,"a need to reason about uncertainty inside large, complex, and multi-modal datasets has become increasingly common across modern scientific environments. a ability to transform samples from one distribution $p$ to another distribution $q$ enables a solution to many problems inside machine learning (e.g. bayesian inference, generative modeling) and has been actively pursued from theoretical, computational, and application perspectives across a fields of information theory, computer science, and biology. performing such transformations, inside general, still leads to computational difficulties, especially inside high dimensions. here, we consider a problem of computing such ""measure transport maps"" with efficient and parallelizable methods. under a mild assumptions that $p$ need not be known but should be sampled from, and that a density of $q$ was known up to the proportionality constant, and that $q$ was log-concave, we provide inside this work the convex optimization problem pertaining to relative entropy minimization. we show how an empirical minimization formulation and polynomial chaos map parameterization should allow considering learning the transport map between $p$ and $q$ with distributed and scalable methods. we also leverage findings from nonequilibrium thermodynamics to represent a transport map as the composition of simpler maps, each of which was learned sequentially with the transport cost regularized version of a aforementioned problem formulation. we provide examples of our framework within a context of bayesian inference considering a boston housing dataset and generative modeling considering handwritten digit images from a mnist dataset.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11132,"period approximation was one of a central topics inside astronomical time series analysis, where data was often unevenly sampled. especially challenging are studies of stellar magnetic cycles, as there a periods looked considering are of a order of a same length than a datasets themselves. a datasets often contain trends, a origin of which was either the real long-term cycle or an instrumental effect, but these effects cannot be reliably separated, while they should lead to erroneous period determinations if not properly handled. inside this study we aim at developing the method that should handle a trends properly, and by performing extensive set of testing, we show that this was a optimal procedure when contrasted with methods that do not include a trend directly to a model. a effect of a form of a noise (whether constant or heteroscedastic) on a results was also investigated. we introduce the bayesian generalised lomb-scargle periodogram with trend (bglst), which was the probabilistic linear regression model with the help of gaussian priors considering a coefficients and uniform prior considering a frequency parameter. we show, with the help of synthetic data, that when there was no prior information on whether and to what extent a true model of a data contains the linear trend, a introduced bglst method was preferable to a methods which either detrend a data or leave a data untrended before fitting a periodic model. whether to use noise with different than constant variance inside a model depends on a density of a data sampling as well as on a true noise type of a process.",0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0
18709,"nowadays data compressors are applied to many problems of text analysis, but many such applications are developed outside of a framework of mathematical statistics. inside this paper we overcome this obstacle and show how several methods of classical mathematical statistics should be developed based on applications of a data compressors.",1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0
15937,"inside this work, the many-body potential of nb considering radiation damage simulation is developed based on eam, and most of a point defects of nb should be predicted properly by this potential. by with the help of a constructed potential, a direction-specific threshold displacement energies (tde) and displacement cascades up to 20 kev of nb were performed through molecular dynamics simulations. a calculated results of tde are inside good agreement with previous work considering v, mo and experimental measurements. lowest tde is found inside <100> direction, and local minas of tde were found inside three low-index directions, which has relation: ed[100]<ed[111]<ed[110]. a evolution of displacement cascades, number of a created point defects, a cascade efficiency a clustering of point defects, and temperature role of these parameters at different pka energies were systematic investigated. it was found that a cascade efficiency was low and it was should be fitted by the power function as many published work did. a fraction of clustered point defects obtained inside this work was low, and only some small clusters were formed at a end of thermal spike. as a temperature increases, a productions of point defects and cascade efficiency were somewhat decreases, however, a fraction of clustered point defects decreases more obvious.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3084,"we study a problem of extracting the selective connector considering the given set of query vertices $q \subseteq v$ inside the graph $g = (v,e)$. the selective connector was the subgraph of $g$ which exhibits some cohesiveness property, and contains a query vertices but does not necessarily connect them all. relaxing a connectedness requirement allows a connector to detect multiple communities and to be tolerant to outliers. we achieve this by introducing a new measure of network inefficiency and by instantiating our search considering the selective connector as a problem of finding a minimum inefficiency subgraph. we show that a minimum inefficiency subgraph problem was np-hard, and devise efficient algorithms to approximate it. by means of several case studies inside the variety of application domains (such as human brain, cancer, and food networks), we show that our minimum inefficiency subgraph produces high-quality solutions, exhibiting all a desired behaviors of the selective connector.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19192,"we measure a stellar mass function (smf) of galaxies inside a cosmos field up to $z\sim6$. we select them inside a near-ir bands of a cosmos2015 catalogue, which includes ultra-deep photometry from ultravista-dr2, splash, and subaru/hyper-suprimecam. at $z>2.5$ we use new precise photometric redshifts with error $\sigma_z=0.03(1+z)$ and an outlier fraction of $12\%$, estimated by means of a unique spectroscopic sample of cosmos. a increased exposure time inside a dr2, along with our panchromatic detection strategy, allow us to improve a stellar mass completeness at high $z$ with respect to previous ultravista catalogues. we also identify passive galaxies through the robust colour-colour selection, extending their smf approximate up to $z=4$. our work provides the comprehensive view of galaxy stellar mass assembly between $z=0.1$ and 6, considering a first time with the help of consistent estimates across a entire redshift range. we fit these measurements with the schechter function, correcting considering eddington bias. we compare a smf fit with a halo mass function predicted from $\lambda$cdm simulations. we find that at $z>3$ both functions decline with the similar slope inside a high-mass end. this feature could be explained assuming that a mechanisms that quench star formation inside massive haloes become less effective at high redshift; however further work needs to be done to confirm this scenario. concerning a smf low-mass end, it shows the progressive steepening as moving towards higher redshifts, with $\alpha$ decreasing from $-1.47_{-0.02}^{+0.02}$ at $z\simeq0.1$ to $-2.11_{-0.13}^{+0.30}$ at $z\simeq5$. this slope depends on a characterisation of a observational uncertainties, which was crucial to properly remove a eddington bias. we show that there was currently no consensus on a method to quantify such errors: different error models result inside different best-fit schechter parameters. [abridged]",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3814,"we show that an embedding inside euclidean space based on tropical geometry generates stable sufficient statistics considering barcodes. inside topological data analysis, barcodes are multiscale summaries of algebraic topological characteristics that capture a `shape' of data; however, inside practice, they have complex structures which make them difficult to use inside statistical settings. a sufficiency result presented inside this work allows considering classical probability distributions to be assumed on a tropical geometric representation of barcodes. this makes the variety of parametric statistical inference methods amenable to barcodes, all while maintaining their initial interpretations. more specifically, we show that exponential family distributions may be assumed, and that likelihood functions considering persistent homology may be constructed. we conceptually demonstrate sufficiency and illustrate its utility inside persistent homology dimensions 0 and 1 with concrete parametric applications to hiv and avian influenza data.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
7803,"here we report a measurement of a interfacial spin accumulation induced by a spin hall effect inside pt and w thin films with the help of magneto-optical kerr microscopy. we show that a kerr rotation has opposite sign inside pt and w and scales linearly with current density. by comparing a experimental results with ab-initio calculations of a spin hall and magneto-optical kerr effects, we quantitatively determine a current-induced spin accumulation at a pt interface as $5*10^{-12} \mu_b$a$^{-1}$cm$^2$ per atom. from thickness-dependent measurements, we determine a spin diffusion length inside the single pt film to be $11 \pm 3$ nm, which was significantly larger compared to that of pt adjacent to the magnetic layer.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
17085,"advances inside a field of inverse reinforcement learning (irl) have led to sophisticated inference frameworks that relax a original modeling assumption of observing an agent behavior that reflects only the single intention. instead of learning the global behavioral model, recent irl methods divide a demonstration data into parts, to account considering a fact that different trajectories may correspond to different intentions, e.g., because they were generated by different domain experts. inside this work, we go one step further: with the help of a intuitive concept of subgoals, we build upon a premise that even the single trajectory should be explained more efficiently locally within the certain context than globally, enabling the more compact representation of a observed behavior. based on this assumption, we build an implicit intentional model of a agent's goals to forecast its behavior inside unobserved situations. a result was an integrated bayesian prediction framework that significantly outperforms existing irl solutions and provides smooth policy estimates consistent with a expert's plan. most notably, our framework naturally handles situations where a intentions of a agent change over time and classical irl algorithms fail. inside addition, due to its probabilistic nature, a model should be straightforwardly applied inside active learning scenarios to guide a demonstration process of a expert.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11469,"inside 1996, huisken-yau proved that every three-dimensional riemannian manifold should be uniquely foliated near infinity by stable closed surfaces of constant mean curvature (cmc) if it was asymptotically equal to a (spatial) schwarzschild solution. with the help of their method, rigger proved a same theorem considering riemannian manifolds being asymptotically equal to a (spatial) (schwarzschild-)anti-de sitter solution. this is generalized to asymptotically hyperbolic manifolds by neves-tian, chodosh, and a author at the later stage. inside this work, we prove a reverse implication as a author already did inside a euclidean setting, i.e. any three-dimensional riemannian manifold was asymptotically hyperbolic if it (and only if) possesses the cmc-cover satisfying certain geometric curvature estimates, the uniqueness property, and each surface has controlled instability. as toy application of these geometric characterizations of asymptotically euclidean and hyperbolic manifolds, we present the method considering replacing an asymptotically hyperbolic by an asymptotically euclidean end and apply this method to prove that a hawking mass of a cmc-surfaces was bounded by their limit being a total mass of a asymptotically hyperbolic manifold, where equality holds only considering a t=0-slice of a (schwarzschild-)anti-de sitter spacetime.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9377,"fitzpatrick's variational representation of maximal monotone operators was here extended to the class of pseudo-monotone operators inside banach spaces. on this basis, a initial-value problem associated with a first-order flow of such an operator was here reformulated as the minimization principle, extending the method that is pioneered by brezis, ekeland and nayroles considering gradient flows. this formulation was used to prove that a problem was stable w.r.t.\ arbitrary perturbations not only of data but also of operators. this was achieved by with the help of a notion of evolutionary $\gamma$-convergence w.r.t.\ the nonlinear topology of weak type. these results are applied to a cauchy problem considering quasilinear parabolic pdes. this provides a structural compactness and stability of a model of several physical phenomena: nonlinear diffusion, incompressible viscous flow, phase transitions, and so on.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17891,"a goal of online display advertising was to entice users to ""convert"" (i.e., take the pre-defined action such as making the purchase) after clicking on a ad. an important measure of a value of an ad was a probability of conversion. a focus of this paper was a development of the computationally efficient, accurate, and precise estimator of conversion probability. a challenges associated with this approximation problem are a delays inside observing conversions and a size of a data set (both number of observations and number of predictors). two models have previously been considered as the basis considering estimation: the logistic regression model and the joint model considering observed conversion statuses and delay times. fitting a former was simple, but ignoring a delays inside conversion leads to an under-estimate of conversion probability. on a other hand, a latter was less biased but computationally expensive to fit. our proposed estimator was the compromise between these two estimators. we apply our results to the data set from criteo, the commerce marketing company that personalizes online display advertisements considering users.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2470,"context: recent xmm-newton observations have revealed that iras 17020+4544 was the very unusual example of black hole wind-produced feedback by the moderately luminous agn inside the spiral galaxy. aims: since a source was known considering being the radio emitter, we investigated about a presence and a properties of the non-thermal component. methods: we observed iras 17020+4544 with a very long baseline array at 5, 8, 15, and 24 ghz within the month of a 2014 xmm-newton observations. we further analysed archival data taken inside 2000 and 2012. results: we detect a source at 5 ghz and on short baselines at 8 ghz. at 15 and 24 ghz, a source was below our baseline sensitivity considering fringe fitting, indicating a lack of prominent compact features. a morphology was that of an asymmetric double, with significant diffuse emission. a spectrum between 5 and 8 ghz was rather steep ($s(\nu)\sim\nu^{-(1.0\pm0.2)}$). our re-analysis of a archival data at 5 and 8 ghz provides results consistent with a new observations, suggesting that flux density and structural variability are not important inside this source. we put the limit on a separation speed between a main components of $<0.06c$. conclusions: iras 17020+4544 shows interesting features of several classes of objects: its properties are typical of compact steep spectrum sources, low power compact sources, radio-emitting narrow line seyfert 1 galaxies. however, it should not be classified inside any of these categories, remaining so far the one-of-a-kind object.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16175,"aims. we aim to show how encounters with low-mass satellite galaxies may alter a bar formation inside the milky way-like disc galaxy. methods. we use high-resolution n-body simulations of the disc galaxy prone to mild bar instability. considering realistic initial conditions of satellites, we take advantage of cosmological simulations of milky way-like dark matter haloes. results. a satellites may have the significant impact on a time of bar formation. some runs with satellites demonstrate the delay, while others show an advancement inside bar formation compared to a isolated run, with such time differences reaching $\sim$ 1 gyr. meanwhile, a final bar configuration, including its very appearance and a bar characteristics such as a pattern speed and a exponential growth rate of its amplitude are independent of a number of encounters and their orbits. a contribution of satellites with masses below $10^9 m_{\odot}$ was insignificant, unless their pericentre distances are small. we suggest that a encounters act indirectly using inducing perturbations across a disc that evolve to delayed waves inside a central part and interfere with an emerging seed bar. a predicted effect considering a present-day host galaxy was expected to be even more significant at redshifts $z \gtrsim 0.5$.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1945,"we consider a problem of low canonical polyadic (cp) rank tensor completion. the completion was the tensor whose entries agree with a observed entries and its rank matches a given cp rank. we analyze a manifold structure corresponding to a tensors with a given rank and define the set of polynomials based on a sampling pattern and cp decomposition. then, we show that finite completability of a sampled tensor was equivalent to having the certain number of algebraically independent polynomials among a defined polynomials. our proposed idea behind the method results inside characterizing a maximum number of algebraically independent polynomials inside terms of the simple geometric structure of a sampling pattern, and therefore we obtain a deterministic necessary and sufficient condition on a sampling pattern considering finite completability of a sampled tensor. moreover, assuming that a entries of a tensor are sampled independently with probability $p$ and with the help of a mentioned deterministic analysis, we propose the combinatorial method to derive the lower bound on a sampling probability $p$, or equivalently, a number of sampled entries that guarantees finite completability with high probability. we also show that a existing result considering a matrix completion problem should be used to obtain the loose lower bound on a sampling probability $p$. inside addition, we obtain deterministic and probabilistic conditions considering unique completability. it was seen that a number of samples required considering finite or unique completability obtained by a proposed analysis on a cp manifold was orders-of-magnitude lower than that was obtained by a existing analysis on a grassmannian manifold.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7170,"datasets are often used multiple times and each successive analysis may depend on a outcome of previous analyses. standard techniques considering ensuring generalization and statistical validity do not account considering this adaptive dependence. the recent line of work studies a challenges that arise from such adaptive data reuse by considering a problem of answering the sequence of ""queries"" about a data distribution where each query may depend arbitrarily on answers to previous queries. a strongest results obtained considering this problem rely on differential privacy -- the strong notion of algorithmic stability with a important property that it ""composes"" well when data was reused. however a notion was rather strict, as it requires stability under replacement of an arbitrary data element. a simplest algorithm was to add gaussian (or laplace) noise to distort a empirical answers. however, analysing this technique with the help of differential privacy yields suboptimal accuracy guarantees when a queries have low variance. here we propose the relaxed notion of stability that also composes adaptively. we demonstrate that the simple and natural algorithm based on adding noise scaled to a standard deviation of a query provides our notion of stability. this implies an algorithm that should answer statistical queries about a dataset with substantially improved accuracy guarantees considering low-variance queries. a only previous idea behind the method that provides such accuracy guarantees was based on the more involved differentially private median-of-means algorithm and its analysis exploits stronger ""group"" stability of a algorithm.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
13749,"we introduce the low dimensional function of a site frequency spectrum that was tailor-made considering distinguishing coalescent models with multiple mergers from kingman coalescent models with population growth, and use this function to construct the hypothesis test between these model classes. a null and alternative sampling distributions of a statistic are intractable, but its low dimensionality renders them amenable to monte carlo estimation. we construct kernel density estimates of a sampling distributions based on simulated data, and show that a resulting hypothesis test dramatically improves on a statistical power of the current state-of-the-art method. the key reason considering this improvement was a use of multi-locus data, inside particular averaging observed site frequency spectra across unlinked loci to reduce sampling variance. we also demonstrate a robustness of our method to nuisance and tuning parameters. finally we show that a same kernel density estimates should be used to conduct parameter estimation, and argue that our method was readily generalisable considering applications inside model selection, parameter inference and experimental design.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
16706,"effective collaboration between the robot and the person requires natural communication. when the robot travels with the human companion, a robot should be able to explain its navigation behavior inside natural language. this paper explains how the cognitively-based, autonomous robot navigation system produces informative, intuitive explanations considering its decisions. language generation here was based upon a robot's commonsense, its qualitative reasoning, and its learned spatial model. this idea behind the method produces natural explanations inside real time considering the robot as it navigates inside the large, complex indoor environment.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
11926,"automatic segmentation inside mr brain images was important considering quantitative analysis inside large-scale studies with images acquired at all ages. this paper presents the method considering a automatic segmentation of mr brain images into the number of tissue classes with the help of the convolutional neural network. to ensure that a method obtains accurate segmentation details as well as spatial consistency, a network uses multiple patch sizes and multiple convolution kernel sizes to acquire multi-scale information about each voxel. a method was not dependent on explicit features, but learns to recognise a information that was important considering a classification based on training data. a method requires the single anatomical mr image only. a segmentation method was applied to five different data sets: coronal t2-weighted images of preterm infants acquired at 30 weeks postmenstrual age (pma) and 40 weeks pma, axial t2- weighted images of preterm infants acquired at 40 weeks pma, axial t1-weighted images of ageing adults acquired at an average age of 70 years, and t1-weighted images of young adults acquired at an average age of 23 years. a method obtained a following average dice coefficients over all segmented tissue classes considering each data set, respectively: 0.87, 0.82, 0.84, 0.86 and 0.91. a results demonstrate that a method obtains accurate segmentations inside all five sets, and thus demonstrates its robustness to differences inside age and acquisition protocol.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15245,"feature selection problems arise inside the variety of applications, such as microarray analysis, clinical prediction, text categorization, image classification and face recognition, multi-label learning, and classification of internet traffic. among a various classes of methods, forward feature selection methods based on mutual information have become very popular and are widely used inside practice. however, comparative evaluations of these methods have been limited by being based on specific datasets and classifiers. inside this paper, we develop the theoretical framework that allows evaluating a methods based on their theoretical properties. our framework was grounded on a properties of a target objective function that a methods try to approximate, and on the novel categorization of features, according to their contribution to a explanation of a class; we derive upper and lower bounds considering a target objective function and relate these bounds with a feature types. then, we characterize a types of approximations taken by a methods, and analyze how these approximations cope with a good properties of a target objective function. additionally, we develop the distributional setting designed to illustrate a various deficiencies of a methods, and provide several examples of wrong feature selections. based on our work, we identify clearly a methods that should be avoided, and a methods that currently have a best performance.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3669,"little by little, newspapers are revealing a bright future that artificial intelligence (ai) was building. intelligent machines will aid everywhere. however, this bright future has the dark side: the dramatic job market contraction before its unpredictable transformation. hence, inside the near future, large numbers of job seekers will need financial support while catching up with these novel unpredictable jobs. this possible job market crisis has an antidote inside. inside fact, a rise of ai was sustained by a biggest knowledge theft of a recent years. learning ai machines are extracting knowledge from unaware skilled or unskilled workers by analyzing their interactions. by passionately doing their jobs, these workers are digging their own graves. inside this paper, we propose human-in-the-loop artificial intelligence (hit-ai) as the fairer paradigm considering artificial intelligence systems. hit-ai will reward aware and unaware knowledge producers with the different scheme: decisions of ai systems generating revenues will repay a legitimate owners of a knowledge used considering taking those decisions. as modern robin hoods, hit-ai researchers should fight considering the fairer artificial intelligence that gives back what it steals.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10434,"we study a interplay between a electron-electron (e-e) and a electron-phonon (e-ph) interactions inside a two-orbital hubbard-holstein model at half filling with the help of a dynamical mean field theory. we find that a e-ph interaction, even at weak couplings, strongly modifies a phase diagram of this model and introduces an orbital-selective peierls insulating phase (ospi) that was analogous to a widely studied orbital-selective mott phase (osmp). at small e-e and e-ph coupling, we find the competition between a osmp and a ospi, while at large couplings, the competition occurs between mott and charge-density-wave (cdw) insulating phases. we further demonstrate that a hund's coupling influences a ospi transition by lowering a energy associated with a cdw. our results explicitly show that one must be cautious when neglecting a e-ph interaction inside multiorbital systems, where multiple electronic interactions create states that are readily influenced by perturbing interactions.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
7419,"our predictions, based on density-functional calculations, reveal that surface doping of zno nanowires with bi leads to the linear-in-$k$ splitting of a conduction-band states, through spin-orbit interaction, due to a lowering of a symmetry inside a presence of a dopant. this finding implies that spin polarization of a conduction electrons inside bi-doped zno nanowires could be controlled with applied electric (as opposed to magnetic) fields, making them candidate materials considering spin-orbitronic applications. our findings also show that a degree of spin splitting could be tuned by adjusting a dopant concentration. defect calculations and ab initio molecular dynamics simulations indicate that stable doping configurations exhibiting a foregoing linear-in-$k$ splitting could be realized under reasonable thermodynamic conditions.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
6452,"a growing complexity of heterogeneous cellular networks (hetnets) has necessitated a need to consider variety of user and base station (bs) configurations considering realistic performance evaluation and system design. this was directly reflected inside a hetnet simulation models considered by standardization bodies, such as a third generation partnership project (3gpp). complementary to these simulation models, stochastic geometry based idea behind the method modeling a user and bs locations as independent and homogeneous poisson point processes (ppps) has gained prominence inside a past few years. despite its success inside revealing useful insights, this ppp-based model was not rich enough to capture all a spatial configurations that appear inside real world hetnet deployments (on which 3gpp simulation models are based). inside this paper, we bridge a gap between a 3gpp simulation models and a popular ppp-based analytical model by developing the new unified hetnet model inside which the fraction of users and some bs tiers are modeled as poisson cluster processes (pcps). this model captures both non-uniformity and coupling inside a bs and user locations. considering this setup, we derive exact expression considering downlink coverage probability under maximum signal-to-interference ratio (sir) cell association model. as intermediate results, we define and evaluate sum-product functionals considering ppp and pcp. special instances of a proposed model are shown to closely resemble different configurations considered inside 3gpp hetnet models. our results concretely demonstrate that a performance trends are highly sensitive to a assumptions made on a user and sbs configurations.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
16601,"we present the detailed investigation of a temperature and depth dependence of a magnetic properties of 3d topological kondo insulator smb6 , inside particular near its surface. we find that local magnetic field fluctuations detected inside a bulk are suppressed rapidly with decreasing depths, disappearing almost completely at a surface. we attribute a magnetic excitations to spin excitons inside bulk smb6 , which produce local magnetic fields of about ~1.8 mt fluctuating on the time scale of ~60 ns. we find that a excitonic fluctuations are suppressed when approaching a surface on the length scale of 40-90 nm, accompanied by the small enhancement inside static magnetic fields. we associate this length scale to a size of a excitonic state.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
16150,"we investigate which three dimensional near-horizon metrics $g_{nh}$ admit the compatible 1-form $x$ such that $(x, [g_{nh}])$ defines an einstein-weyl structure. we find explicit examples and see that some of a solutions give rise to einstein-weyl structures of dispersionless kp type and dispersionless hirota (aka hypercr) type.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9464,"dust devils are likely a dominant source of dust considering a martian atmosphere, but a amount and frequency of dust-lifting depend on a statistical distribution of dust devil parameters. dust devils exhibit pressure perturbations and, if they pass near the barometric sensor, they may register as the discernible dip inside the pressure time-series. leveraging this fact, several surveys with the help of barometric sensors on landed spacecraft have revealed dust devil structures and occurrence rates. however powerful they are, though, such surveys suffer from non-trivial biases that skew a inferred dust devil properties. considering example, such surveys are most sensitive to dust devils with a widest and deepest pressure profiles, but a recovered profiles will be distorted, broader and shallow than a actual profiles. inside addition, such surveys often do not provide wind speed measurements alongside a pressure time series, and so a durations of a dust devil signals inside a time series cannot be directly converted to profile widths. fortunately, simple statistical and geometric considerations should de-bias these surveys, allowing conversion of a duration of dust devil signals into physical widths, given only the distribution of likely translation velocities, and a recovery of a underlying distributions of physical parameters. inside this study, we develop the scheme considering de-biasing such surveys. applying our model to an in-situ survey with the help of data from a phoenix lander suggests the larger dust flux and the dust devil occurrence rate about ten times larger than previously inferred. comparing our results to dust devil track surveys suggests only about one inside five low-pressure cells lifts sufficient dust to leave the visible track.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4960,"periodic event-triggered control (petc) has a advantages of both sampled-data control and event-triggered control, and was well-suited considering implementation inside digital platforms. however, existing results on petc design mainly focus on linear systems, and their extension to nonlinear systems are still sparse. this paper investigates petc design considering general nonlinear systems subject to external disturbances, and provides sufficient conditions to ensure that a closed-loop system implemented by petc input-to-state stable, with the help of state feedback and observer-based output feedback controllers, respectively. considering incrementally quadratic nonlinear systems, sufficient conditions considering petc design are provided inside a form of linear matrix inequalities. a sampling period and triggering functions considering all a cases considered are provided explicitly. two examples are given to illustrate a effectiveness of a proposed method.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
17281,"a potential number of drug like small molecules was estimated to be between 10^23 and 10^60 while current databases of known compounds are orders of magnitude smaller with approximately 10^8 compounds. this discrepancy has led to an interest inside generating virtual libraries with the help of hand crafted chemical rules and fragment based methods to cover the larger area of chemical space and generate chemical libraries considering use inside inside silico drug discovery endeavors. here it was explored to what extent the recurrent neural network with long short term memory cells should figure out sensible chemical rules and generate synthesizable molecules by being trained on existing compounds encoded as smiles. a networks should to the high extent generate novel, but chemically sensible molecules. a properties of a molecules are tuned by training on two different datasets consisting of fragment like molecules and drug like molecules. a produced molecules and a training databases have very similar distributions of molar weight, predicted logp, number of hydrogen bond acceptors and donors, number of rotatable bonds and topological polar surface area when compared to their respective training sets. a compounds are considering a most cases synthesizable as assessed with sa score and wiley chemplanner.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
163,"accurately predicting a future capacity and remaining useful life of batteries was necessary to ensure reliable system operation and to minimise maintenance costs. a complex nature of battery degradation has meant that mechanistic modelling of capacity fade has thus far remained intractable; however, with a advent of cloud-connected devices, data from cells inside various applications was becoming increasingly available, and a feasibility of data-driven methods considering battery prognostics was increasing. here we propose gaussian process (gp) regression considering forecasting battery state of health, and highlight various advantages of gps over other data-driven and mechanistic approaches. gps are the type of bayesian non-parametric method, and thus should model complex systems whilst handling uncertainty inside the principled manner. prior information should be exploited by gps inside the variety of ways: explicit mean functions should be used if a functional form of a underlying degradation model was available, and multiple-output gps should effectively exploit correlations between data from different cells. we demonstrate a predictive capability of gps considering short-term and long-term (remaining useful life) forecasting on the selection of capacity vs. cycle datasets from lithium-ion cells.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2528,"we analyze a non-local shot noise inside the multi-terminal junction formed by two normal metal leads con- nected to one superconductor. with the help of a cross fano factor and a shot noise, we calculate a efficiency of a cooper pair splitting. a method was applied to d-wave and iron based superconductors. we de- termine that a contributions to a noise cross-correlation are due to crossed andreev reflections (car), elastic cotunneling, quasiparticles transmission and local andreev reflections. inside a tunneling limit, a car contribute positively to a noise cross-correlation whereas a other processes contribute negatively. depending on a pair potential symmetry, a car are a dominant processes, giving as the result the high efficiency considering cooper pair split. we propose a use of a fano factor to test a efficiency of the cooper pair splitter device.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
16200,"an ability to model the generative process and learn the latent representation considering speech inside an unsupervised fashion will be crucial to process vast quantities of unlabelled speech data. recently, deep probabilistic generative models such as variational autoencoders (vaes) have achieved tremendous success inside modeling natural images. inside this paper, we apply the convolutional vae to model a generative process of natural speech. we derive latent space arithmetic operations to disentangle learned latent representations. we demonstrate a capability of our model to modify a phonetic content or a speaker identity considering speech segments with the help of a derived operations, without a need considering parallel supervisory data.",1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14625,"we propose some algorithms to find local minima inside nonconvex optimization and to obtain global minima inside some degree from a newton second law without friction. with a key observation of a velocity observable and controllable inside a motion, a algorithms simulate a newton second law without friction based on symplectic euler scheme. from a intuitive analysis of analytical solution, we give the theoretical analysis considering a high-speed convergence inside a algorithm proposed. finally, we propose a experiments considering strongly convex function, non-strongly convex function and nonconvex function inside high-dimension.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14416,"previously, a controllability problem of the linear time-invariant dynamical system is mapped to a maximum matching (mm) problem on a bipartite representation of a underlying directed graph, and a sizes of mms on random bipartite graphs were calculated analytically with a cavity method at zero temperature limit. here we present an alternative theory to approximate mm sizes based on a core percolation theory and a perfect matching of cores. our theory was much more simplified and easily interpreted, and should approximate mm sizes on random graphs with or without symmetry between out- and in-degree distributions. our result helps to illuminate a fundamental connection between a controllability problem and a underlying structure of complex systems.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
19109,"molecular line-transition lists are an essential ingredient considering radiative-transfer calculations. with recent databases now surpassing a billion-lines mark, handling them has become computationally prohibitive, due to both a required processing power and memory. here i present the temperature-dependent algorithm to separate strong from weak line transitions, reformatting a large majority of a weaker lines into the cross-section data file, and retaining a detailed line-by-line information of a fewer strong lines. considering any given molecule over a 0.3--30 {\micron} range, this algorithm reduces a number of lines to the few million, enabling faster radiative-transfer computations without the significant loss of information. a final compression rate depends on how densely populated was a spectrum. i validate this algorithm by comparing exomol's hcn extinction-coefficient spectra between a complete (65 million line transitions) and compressed (7.7 million) line lists. over a 0.6--33 {\micron} range, a average difference between extinction-coefficient values was less than 1\%. the python/c implementation of this algorithm was open-source and available at this https url . so far, this code handles a exomol and hitran line-transition format.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
14090,"we revisit a classical problem of optimal experimental design (oed) under the new mathematical model grounded inside the geometric motivation. specifically, we introduce models based on elementary symmetric polynomials; these polynomials capture ""partial volumes"" and offer the graded interpolation between a widely used a-optimal design and d-optimal design models, obtaining each of them as special cases. we analyze properties of our models, and derive both greedy and convex-relaxation algorithms considering computing a associated designs. our analysis establishes approximation guarantees on these algorithms, while our empirical results substantiate our claims and demonstrate the curious phenomenon concerning our greedy method. finally, as the byproduct, we obtain new results on a theory of elementary symmetric polynomials that may be of independent interest.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
11146,"due to severe mathematical modeling and calibration difficulties open-loop feedforward control was mainly employed today considering wastewater denitrification, which was the key ecological issue. inside order to improve a resulting poor performances the new model-free control setting and its corresponding ""intelligent"" controller are introduced. a pitfall of regulating two output variables using the single input variable was overcome by introducing also an open-loop knowledge-based control deduced from a plant behavior. several convincing computer simulations are presented and discussed.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
651,"inside this paper, we study a galois conjugates of stretch factors of pseudo-anosov elements of a mapping class group of the surface. we show that - except inside low-complexity cases - these conjugates are dense inside a complex plane. considering this, we use penner's construction of pseudo-anosov mapping classes. as the consequence, we obtain that inside the sense there was no restriction on a location of galois conjugates of stretch factors arising from penner's construction. this complements an earlier result of shin and a author stating that galois conjugates of stretch factors arising from penner's construction may never lie on a unit circle.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
12755,"considering a two cartan type s subalgebras of a witt algebra $\w_n$, called lie algebras of divergence-zero vector fields, we determine all module structures on a universal enveloping algebra of their cartan subalgebra $\h_n$. we also give all submodules of these modules.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
5648,"we consider projective rational strong calabi dream surfaces: projective smooth rational surfaces which admit the constant scalar curvature kahler metric considering every kahler class. we show that there are only two such rational surfaces, namely a projective plane and a quadric surface. inside particular, we show that all rational surfaces other than those two admit the destabilising slope test configuration considering some polarization, as introduced by ross and thomas. we further show that all hirzebruch surfaces other than a quadric surface and all rational surfaces with picard rank 3 do not admit the constant scalar curvature kahler metric inside any kahler class.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
501,"estimators computed from adaptively collected data do not behave like their non-adaptive brethren. rather, a sequential dependence of a collection policy should lead to severe distributional biases that persist even inside a infinite data limit. we develop the general method -- $\mathbf{w}$-decorrelation -- considering transforming a bias of adaptive linear regression estimators into variance. a method uses only coarse-grained information about a data collection policy and does not need access to propensity scores or exact knowledge of a policy. we bound a finite-sample bias and variance of a $\mathbf{w}$-estimator and develop asymptotically correct confidence intervals based on the novel martingale central limit theorem. we then demonstrate a empirical benefits of a generic $\mathbf{w}$-decorrelation procedure inside two different adaptive data settings: a multi-armed bandit and a autoregressive time series.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5245,"most existing sequence labelling models rely on the fixed decomposition of the target sequence into the sequence of basic units. these methods suffer from two major drawbacks: 1) a set of basic units was fixed, such as a set of words, characters or phonemes inside speech recognition, and 2) a decomposition of target sequences was fixed. these drawbacks usually result inside sub-optimal performance of modeling sequences. inside this pa- per, we extend a popular ctc loss criterion to alleviate these limitations, and propose the new loss function called gram-ctc. while preserving a advantages of ctc, gram-ctc automatically learns a best set of basic units (grams), as well as a most suitable decomposition of tar- get sequences. unlike ctc, gram-ctc allows a model to output variable number of characters at each time step, which enables a model to capture longer term dependency and improves a computational efficiency. we demonstrate that a proposed gram-ctc improves ctc inside terms of both performance and efficiency on a large vocabulary speech recognition task at multiple scales of data, and that with gram-ctc we should outperform a state-of-the-art on the standard speech benchmark.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17080,"this paper presents novel insights about a influence of soluble surfactants on bubble flows obtained by direct numerical simulation (dns). surfactants are amphiphilic compounds which accumulate at fluid interfaces and significantly modify a respective interfacial properties, influencing also a overall dynamics of a flow. with a aid of dns local quantities like a surfactant distribution on a bubble surface should be accessed considering the better understanding of a physical phenomena occurring close to a interface. a core part of a physical model consists inside a description of a surfactant transport inside a bulk and on a deformable interface. a solution procedure was based on an arbitrary lagrangian-eulerian (ale) interface-tracking method. a existing methodology is enhanced to describe the wider range of physical phenomena. the subgrid-scale (sgs) model was employed inside a cases where the fully resolved dns considering a species transport was not feasible due to high mesh resolution requirements and, therefore, high computational costs. after an exhaustive validation of a latest numerical developments, a dns of single rising bubbles inside contaminated solutions was compared to experimental results. a full velocity transients of a rising bubbles, especially a contaminated ones, are correctly reproduced by a dns. a simulation results are then studied to gain the better understanding of a local bubble dynamics under a effect of soluble surfactant. one of a main insights was that a quasi-steady state of a rise velocity was reached without ad- and desorption being necessarily inside local equilibrium.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9716,"let $f$ be the finite extension of $\mathbb{q}_p$, and let $\mathbb{f}$ be the finite field of characteristic $p$. the smooth representation of $gl_2(f)$ over $\mathbb{f}$ was finitely presented if it should be written as a cokernel of the map between representations induced from compact-mod-centre open subgroups of $gl_2(f)$. we prove that a category of finitely presented smooth representations was an abelian subcategory of all smooth representations. this amounts to showing that a kernel of the map between finitely presented smooth representations was finitely presented. a proof uses amalgamated products of completed group rings.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
7697,"the common theme among a proposed models considering network epidemics was a assumption that a propagating object, i.e., the virus or the piece of information, was transferred across a nodes without going through any modification or evolution. however, inside real-life spreading processes, pathogens often evolve inside response to changing environments and medical interventions and information was often modified by individuals before being forwarded. inside this paper, we investigate a evolution of spreading processes on complex networks with a aim of i) revealing a role of evolution on a threshold, probability, and final size of epidemics; and ii) exploring a interplay between a structural properties of a network and a dynamics of evolution. inside particular, we develop the mathematical theory that accurately predicts a epidemic threshold and a expected epidemic size as functions of a characteristics of a spreading process, a evolutionary dynamics of a pathogen, and a structure of a underlying contact network. inside addition to a mathematical theory, we perform extensive simulations on random and real-world contact networks to verify our theory and reveal a significant shortcomings of a classical mathematical models that do not capture evolution. our results reveal that a classical, single-type bond-percolation models may accurately predict a threshold and final size of epidemics, but their predictions on a probability of emergence are inaccurate on both random and real-world networks. this inaccuracy sheds a light on the fundamental disconnect between a classical bond-percolation models and real-life spreading processes that entail evolution. finally, we consider a case when co-infection was possible and show that co-infection could lead a order of phase transition to change from second-order to first-order.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
8229,feature extraction was the very crucial task inside image and pixel (voxel) classification and regression inside biomedical image modeling. inside this work we present the machine learning based feature extraction scheme based on inception models considering pixel classification tasks. we extract features under multi-scale and multi-layer schemes through convolutional operators. layers of fully convolutional network are later stacked on this feature extraction layers and trained end-to-end considering a purpose of classification. we test our model on a drive and stare public data sets considering a purpose of segmentation and centerline detection and it out performs most existing hand crafted or deterministic feature schemes found inside literature. we achieve an average maximum dice of 0.85 on a drive data set which out performs a scores from a second human annotator of this data set. we also achieve an average maximum dice of 0.85 and kappa of 0.84 on a stare data set. though these datasets are mainly 2-d we also propose ways of extending this feature extraction scheme to handle 3-d datasets.,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19158,"inside this work, we derive gross-zagier type cm value formulas considering hauptmoduls $j_{p}^{*}(\tau)$ on fricke groups $\gamma_{0}^{*}(p)$. we also illustrate how to employ these formulas to obtain certain hilbert class polynomials.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
64,"do only major scientific breakthroughs hit a news and social media, or does the 'catchy' title aid to attract public attention? how strong was a connection between a importance of the scientific paper and a (social) media attention it receives? inside this study we investigate these questions by analysing a relationship between a observed attention and certain characteristics of scientific papers from two major multidisciplinary journals: nature communication (nc) and proceedings of a national academy of sciences (pnas). we describe papers by features based on a linguistic properties of their titles and centrality measures of their authors inside their co-authorship network. we identify linguistic features and collaboration patterns that might be indicators considering future attention, and are characteristic to different journals, research disciplines, and media sources.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
6848,"learning a structure of markov random fields (mrfs) plays an important role inside multivariate analysis. a importance has been increasing with a recent rise of statistical relational models since a mrf serves as the building block of these models such as markov logic networks. there are two fundamental ways to learn structures of mrfs: methods based on parameter learning and those based on independence test. a former methods more or less assume certain forms of distribution, so they potentially perform poorly when a assumption was not satisfied. a latter should learn an mrf structure without the strong distributional assumption, but sometimes it was unclear what objective function was maximized/minimized inside these methods. inside this paper, we follow a latter, but we explicitly define a optimization problem of mrf structure learning as maximum pseudolikelihood approximation (mple) with respect to a edge set. as the result, a proposed solution successfully deals with a {\em symmetricity} inside mrfs, whereas such symmetricity was not taken into account inside most existing independence test techniques. a proposed method achieved higher accuracy than previous methods when there were asymmetric dependencies inside our experiments.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1706,"this paper introduces wasserstein variational inference, the new form of approximate bayesian inference based on optimal transport theory. wasserstein variational inference uses the new family of divergences that includes both f-divergences and a wasserstein distance as special cases. a gradients of a wasserstein variational loss are obtained by backpropagating through a sinkhorn iterations. this technique results inside the very stable likelihood-free training method that should be used with implicit distributions and probabilistic programs. with the help of a wasserstein variational inference framework, we introduce several new forms of autoencoders and test their robustness and performance against existing variational autoencoding techniques.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6663,this was the duplicate submission(original was arxiv:1612.02141). thus want to withdraw it,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6682,"considering distributed computing environment, we consider a empirical risk minimization problem and propose the distributed and communication-efficient newton-type optimization method. at every iteration, each worker locally finds an approximate newton (ant) direction, which was sent to a main driver. a main driver, then, averages all a ant directions received from workers to form the {\it globally improved ant} (giant) direction. giant was highly communication efficient and naturally exploits a trade-offs between local computations and global communications inside that more local computations result inside fewer overall rounds of communications. theoretically, we show that giant enjoys an improved convergence rate as compared with first-order methods and existing distributed newton-type methods. further, and inside sharp contrast with many existing distributed newton-type methods, as well as popular first-order methods, the highly advantageous practical feature of giant was that it only involves one tuning parameter. we conduct large-scale experiments on the computer cluster and, empirically, demonstrate a superior performance of giant.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8796,"it was demonstrated that fermionic/bosonic symmetry-protected topological (spt) phases across different dimensions and symmetry classes should be organized with the help of geometric constructions that increase dimensions and symmetry-forgetting maps that change symmetry groups. specifically, it was shown that a interacting classifications of spt phases with and without glide symmetry fit into the short exact sequence, so that a classification with glide was constrained to be the direct sum of cyclic groups of order 2 or 4. applied to fermionic spt phases inside a wigner-dyson class aii, this implies that a complete interacting classification inside a presence of glide was ${\mathbb z}_4{\oplus}{\mathbb z}_2{\oplus}{\mathbb z}_2$ inside 3 dimensions. inside particular, a hourglass-fermion phase recently realized inside a band insulator khgsb must be robust to interactions. generalizations to spatiotemporal glide symmetries are discussed.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
1982,"we consider abstract evolution equations with on-off time delay feedback. without a time delay term, a model was described by an exponentially stable semigroup. we show that, under appropriate conditions involving a delay term, a system remains asymptotically stable. under additional assumptions exponential stability results are also obtained. concrete examples illustrating a abstract results are finally given.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14596,"we present adaptive strategies considering antenna selection considering direction of arrival (doa) approximation of the far-field source with the help of tdm mimo radar with linear arrays. our treatment was formulated within the general adaptive sensing framework that uses one-step ahead predictions of a bayesian mse with the help of the parametric family of weiss-weinstein bounds that depend on previous measurements. we compare inside simulations our strategy with adaptive policies that optimize a bobrovsky- zaka{\i} bound and a expected cram√©r-rao bound, and show a performance considering different levels of measurement noise.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
13554,"existing speaker verification (sv) systems often suffer from performance degradation if there was any language mismatch between model training, speaker enrollment, and test. the major cause of this degradation was that most existing sv methods rely on the probabilistic model to infer a speaker factor, so any significant change on a distribution of a speech signal will impact a inference. recently, we proposed the deep learning model that should learn how to extract a speaker factor by the deep neural network (dnn). by this feature learning, an sv system should be constructed with the very simple back-end model. inside this paper, we investigate a robustness of a feature-based sv system inside situations with language mismatch. our experiments were conducted on the complex cross-lingual scenario, where a model training is inside english, and a enrollment and test were inside chinese or uyghur. a experiments demonstrated that a feature-based system outperformed a i-vector system with the large margin, particularly with language mismatch between enrollment and test.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15258,"all a existing real world networks are evolving, hence, study of traffic dynamics inside these enlarged networks was the challenging task. a critical issue was to optimize a network structure to improve network capacity and avoid traffic congestion. we are interested inside taking user's routes such that it was least congested with optimal network capacity. network capacity may be improved either by optimizing network topology or enhancing inside routing approach. inside this context, we propose and design the model of a time varying data communication networks (tvcn) based on a dynamics of in-flowing links. newly appeared node prefers to attach with most influential node present inside a network. inside this paper, influence was termed as \textit{reputation} and was applied considering computing overall congestion at any node. user path with least betweenness centrality and most reputation was preferred considering routing. kelly's optimization formulation considering the rate allocation problem was used considering obtaining optimal rates of distinct users at different time instants and it was found that a user's path with lowest betweenness centrality and highest reputation will always give maximum rate at stable point.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
3155,"inside this paper, thermocapillary migration of the planar droplet at moderate and large marangoni numbers was investigated analytically and numerically. by with the help of a dimension-analysis method, a thermal diffusion time scale was determined as a controlling one of a thermocapillary droplet migration system. during this time, a whole thermocapillary migration process was fully developed. by with the help of a front-tracking method, a steady/unsteady states as a terminal ones at moderate/large marangoni numbers are captured inside the longer time scale than a thermal diffusion time scale. inside a terminal states, a instantaneous velocity fields inside a unsteady migration process at large marangoni numbers have a forms of a steady ones at moderate marangoni numbers. however, inside view of a former instantaneous temperature fields, a surface tension of a top surface of a droplet gradually becomes a main component of a driving force on a droplet after a inflection point appears. it was different from that a surface tension of a bottom surface of a droplet was a main component of a driving force on a droplet considering a latter ones. a physical mechanism of thermocapillary droplet migration should be described as a significance of a thermal convection around a droplet was higher than/just as a thermal conduction across a droplet at large/moderate marangoni numbers.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11940,"we prove that every entire solution of a minimal graph equation that was bounded from below and has at most linear growth must be constant on the complete riemannian manifold $m$ with only one end if $m$ has asymptotically non-negative sectional curvature. on a other hand, we prove a existence of bounded non-constant minimal graphic and $p$-harmonic functions on rotationally symmetric cartan-hadamard manifolds under optimal assumptions on a sectional curvatures.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17564,"should faces acquired by low-cost depth sensors be useful to catch some characteristic details of a face? typically a answer was no. however, new deep architectures should generate rgb images from data acquired inside the different modality, such as depth data. inside this paper, we propose the new \textit{deterministic conditional gan}, trained on annotated rgb-d face datasets, effective considering the face-to-face translation from depth to rgb. although a network cannot reconstruct a exact somatic features considering unknown individual faces, it was capable to reconstruct plausible faces; their appearance was accurate enough to be used inside many pattern recognition tasks. inside fact, we test a network capability to hallucinate with some \textit{perceptual probes}, as considering instance face aspect classification or landmark detection. depth face should be used inside spite of a correspondent rgb images, that often are not available due to difficult luminance conditions. experimental results are very promising and are as far as better than previously proposed approaches: this domain translation should constitute the new way to exploit depth data inside new future applications.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4952,"while invaluable considering many computer vision applications, decomposing the natural image into intrinsic reflectance and shading layers represents the challenging, underdetermined inverse problem. as opposed to strict reliance on conventional optimization or filtering solutions with strong prior assumptions, deep learning based approaches have also been proposed to compute intrinsic image decompositions when granted access to sufficient labeled training data. a downside was that current data sources are quite limited, and broadly speaking fall into one of two categories: either dense fully-labeled images inside synthetic/narrow settings, or weakly-labeled data from relatively diverse natural scenes. inside contrast to many previous learning-based approaches, which are often tailored to a structure of the particular dataset (and may not work well on others), we adopt core network structures that universally reflect loose prior knowledge regarding a intrinsic image formation process and should be largely shared across datasets. we then apply flexibly supervised loss layers that are customized considering each source of ground truth labels. a resulting deep architecture achieves state-of-the-art results on all of a major intrinsic image benchmarks, and runs considerably faster than most at test time.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13296,"inside this paper, we present the novel strategy to compute minimum-time trajectories considering quadrotors inside constrained environments. inside particular, we consider a motion inside the given flying region with obstacles and take into account a physical limitations of a vehicle. instead of approaching a optimization problem inside its standard time-parameterized formulation, a proposed strategy was based on an appealing re-formulation. transverse coordinates, expressing a distance from the frame path, are used to parameterise a vehicle position and the spatial parameter was used as independent variable. this re-formulation allows us to (i) obtain the fixed horizon problem and (ii) easily formulate (fairly complex) position constraints. a effectiveness of a proposed strategy was proven by numerical computations on two different illustrative scenarios. moreover, a optimal trajectory generated inside a second scenario was experimentally executed with the real nano-quadrotor inside order to show its feasibility.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
14022,"we propose the novel diminishing learning rate scheme, coined decreasing-trend-nature (dtn), which allows us to prove fast convergence of a stochastic gradient descent (sgd) algorithm to the first-order stationary point considering smooth general convex and some class of nonconvex including neural network applications considering classification problems. we are a first to prove that sgd with diminishing learning rate achieves the convergence rate of $\mathcal{o}(1/t)$ considering these problems. our theory applies to neural network applications considering classification problems inside the straightforward way.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19514,"shells are low surface brightness tidal debris that appear as interleaved caustics with large opening angles, often situated on both sides of a galaxy center. inside this paper, we study a incidence and formation processes of shell galaxies inside a cosmological gravity+hydrodynamics illustris simulation. we identify shells at redshift z=0 with the help of stellar surface density maps, and we use stellar history catalogs to trace a birth, trajectory and progenitors of each individual star particle contributing to a tidal feature. out of the sample of a 220 most massive galaxies inside illustris ($\mathrm{m}_{\mathrm{200crit}}>6\times10^{12}\,\mathrm{m}_{\odot}$), $18\%\pm3\%$ of a galaxies exhibit shells. this fraction increases with increasing mass cut: higher mass galaxies are more likely to have stellar shells. furthermore, a fraction of massive galaxies that exhibit shells decreases with increasing redshift. we find that shell galaxies observed at redshift $z=0$ form preferentially through relatively major mergers ($\gtrsim$1:10 inside stellar mass ratio). progenitors are accreted on low angular momentum orbits, inside the preferred time-window between $\sim$4 and 8 gyrs ago. our study indicates that, due to dynamical friction, more massive satellites are allowed to probe the wider range of impact parameters at accretion time, while small companions need almost purely radial infall trajectories inside order to produce shells. we also find the number of special cases, as the consequence of a additional complexity introduced by a cosmological setting. these include galaxies with multiple shell-forming progenitors, satellite-of-satellites also forming shells, or satellites that fail to produce shells due to multiple major mergers happening inside quick succession.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16058,"a number of scientific articles has grown rapidly over a years and there are no signs that this growth will slow down inside a near future. because of this, it becomes increasingly difficult to keep up with a latest developments inside the scientific field. to address this problem, we present here an idea behind the method to aid researchers learn about a latest developments and findings by extracting inside the normalized form core claims from scientific articles. this normalized representation was the controlled natural language of english sentences called aida, which has been proposed inside previous work as the method to formally structure and organize scientific findings and discourse. we show how such aida sentences should be automatically extracted by detecting a core claim of an article, checking considering aida compliance, and - if necessary - transforming it into the compliant sentence. while our algorithm was still far from perfect, our results indicate that a different steps are feasible and they support a claim that aida sentences might be the promising idea behind the method to improve scientific communication inside a future.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7115,"we investigate a problem of learning discrete, undirected graphical models inside the differentially private way. we show that a idea behind the method of releasing noisy sufficient statistics with the help of a laplace mechanism achieves the good trade-off between privacy, utility, and practicality. the naive learning algorithm that uses a noisy sufficient statistics ""as is"" outperforms general-purpose differentially private learning algorithms. however, it has three limitations: it ignores knowledge about a data generating process, rests on uncertain theoretical foundations, and exhibits certain pathologies. we develop the more principled idea behind the method that applies a formalism of collective graphical models to perform inference over a true sufficient statistics within an expectation-maximization framework. we show that this learns better models than competing approaches on both synthetic data and on real human mobility data used as the case study.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11426,"inside this work, we consider a problem of combining link, content and temporal analysis considering community detection and prediction inside evolving networks. such temporal and content-rich networks occur inside many real-life settings, such as bibliographic networks and question answering forums. most of a work inside a literature (that uses both content and structure) deals with static snapshots of networks, and they do not reflect a dynamic changes occurring over multiple snapshots. incorporating dynamic changes inside a communities into a analysis should also provide useful insights about a changes inside a network such as a migration of authors across communities. inside this work, we propose chimera, the shared factorization model that should simultaneously account considering graph links, content, and temporal analysis. this idea behind the method works by extracting a latent semantic structure of a network inside multidimensional form, but inside the way that takes into account a temporal continuity of these embeddings. such an idea behind the method simplifies temporal analysis of a underlying network by with the help of a embedding as the surrogate. the consequence of this simplification was that it was also possible to use this temporal sequence of embeddings to predict future communities. we present experimental results illustrating a effectiveness of a approach.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0
13317,"one of a key challenges of visual perception was to extract abstract models of 3d objects and object categories from visual measurements, which are affected by complex nuisance factors such as viewpoint, occlusion, motion, and deformations. starting from a recent idea of viewpoint factorization, we propose the new idea behind the method that, given the large number of images of an object and no other supervision, should extract the dense object-centric coordinate frame. this coordinate frame was invariant to deformations of a images and comes with the dense equivariant labelling neural network that should map image pixels to their corresponding object coordinates. we demonstrate a applicability of this method to simple articulated objects and deformable objects such as human faces, learning embeddings from random synthetic transformations or optical flow correspondences, all without any manual supervision.",1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1909,"we consider a following generalization of a binary search problem. the search strategy was required to locate an unknown target node $t$ inside the given tree $t$. upon querying the node $v$ of a tree, a strategy receives as the reply an indication of a connected component of $t\setminus\{v\}$ containing a target $t$. a cost of querying each node was given by the known non-negative weight function, and a considered objective was to minimize a total query cost considering the worst-case choice of a target. designing an optimal strategy considering the weighted tree search instance was known to be strongly np-hard, inside contrast to a unweighted variant of a problem which should be solved optimally inside linear time. here, we show that weighted tree search admits the quasi-polynomial time approximation scheme: considering any $0 \textless{} \varepsilon \textless{} 1$, there exists the $(1+\varepsilon)$-approximation strategy with the computation time of $n^{o(\log n / \varepsilon^2)}$. thus, a problem was not apx-hard, unless $np \subseteq dtime(n^{o(\log n)})$. by applying the generic reduction, we obtain as the corollary that a studied problem admits the polynomial-time $o(\sqrt{\log n})$-approximation. this improves previous $\hat o(\log n)$-approximation approaches, where a $\hat o$-notation disregards $o(\mathrm{poly}\log\log n)$-factors.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10149,"complex activity recognition was challenging due to a inherent uncertainty and diversity of performing the complex activity. normally, each instance of the complex activity has its own configuration of atomic actions and their temporal dependencies. we propose inside this paper an atomic action-based bayesian model that constructs allen's interval relation networks to characterize complex activities with structural varieties inside the probabilistic generative way: by introducing latent variables from a chinese restaurant process, our idea behind the method was able to capture all possible styles of the particular complex activity as the unique set of distributions over atomic actions and relations. we also show that local temporal dependencies should be retained and are globally consistent inside a resulting interval network. moreover, network structure should be learned from empirical data. the new dataset of complex hand activities has been constructed and made publicly available, which was much larger inside size than any existing datasets. empirical evaluations on benchmark datasets as well as our in-house dataset demonstrate a competitiveness of our approach.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18379,"we consider a minimization of composite objective functions composed of a expectation of quadratic functions and an arbitrary convex function. we study a stochastic dual averaging algorithm with the constant step-size, showing that it leads to the convergence rate of o(1/n) without strong convexity assumptions. this thus extends earlier results on least-squares regression with a euclidean geometry to (a) all convex regularizers and constraints, and (b) all geome-tries represented by the bregman divergence. this was achieved by the new proof technique that relates stochastic and deterministic recursions.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0
16649,"a highly-adaptive-lasso(hal)-tmle was an efficient estimator of the pathwise differentiable parameter inside the statistical model that at minimal (and possibly only) assumes that a sectional variation norm of a true nuisance parameters are finite. it relies on an initial estimator (hal-mle) of a nuisance parameters by minimizing a empirical risk over a parameter space under a constraint that sectional variation norm was bounded by the constant, where this constant should be selected with cross-validation. inside a formulation of a halmle this sectional variation norm corresponds with a sum of absolute value of coefficients considering an indicator basis. due to its reliance on machine learning, statistical inference considering a tmle has been based on its normal limit distribution, thereby potentially ignoring the large second order remainder inside finite samples. inside this article, we present four methods considering construction of the finite sample 0.95-confidence interval that use a nonparametric bootstrap to approximate a finite sample distribution of a hal-tmle or the conservative distribution dominating a true finite sample distribution. we prove that it consistently estimates a optimal normal limit distribution, while its approximation error was driven by a performance of a bootstrap considering the well behaved empirical process. we demonstrate our general inferential methods considering 1) nonparametric approximation of a average treatment effect based on observing on each unit the covariate vector, binary treatment, and outcome, and considering 2) nonparametric approximation of a integral of a square of a multivariate density of a data distribution.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
3925,"one of a major drawbacks of modularized task-completion dialogue systems was that each module was trained individually, which presents several challenges. considering example, downstream modules are affected by earlier modules, and a performance of a entire system was not robust to a accumulated errors. this paper presents the novel end-to-end learning framework considering task-completion dialogue systems to tackle such issues. our neural dialogue system should directly interact with the structured database to assist users inside accessing information and accomplishing certain tasks. a reinforcement learning based dialogue manager offers robust capabilities to handle noises caused by other components of a dialogue system. our experiments inside the movie-ticket booking domain show that our end-to-end system not only outperforms modularized dialogue system baselines considering both objective and subjective evaluation, but also was robust to noises as demonstrated by several systematic experiments with different error granularity and rates specific to a language understanding module.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2923,"let $(\sigma,g)$ be the compact riemannian surface without boundary and $\lambda_1(\sigma)$ be a first eigenvalue of a laplace-beltrami operator $\delta_g$. let $h$ be the positive smooth function on $\sigma$. define the functional $$j_{\alpha,\beta}(u)=\frac{1}{2}\int_\sigma(|\nabla_gu|^2-\alpha u^2)dv_g-\beta\log\int_\sigma he^udv_g$$ on the function space $\mathcal{h}=\left\{u\in w^{1,2}(\sigma): \int_\sigma udv_g=0\right\}$. if $\alpha<\lambda_1(\sigma)$ and $j_{\alpha,8\pi}$ has no minimizer on $\mathcal{h}$, then we calculate a infimum of $j_{\alpha,8\pi}$ on $\mathcal{h}$ by with the help of a method of blow-up analysis. as the consequence, we give the sufficient condition under which the kazdan-warner equation has the solution. if $\alpha\geq \lambda_1(\sigma)$, then $\inf_{u\in\mathcal{h}}j_{\alpha,8\pi}(u)=-\infty$. if $\beta>8\pi$, then considering any $\alpha\in\mathbb{r}$, there holds $\inf_{u\in\mathcal{h}}j_{\alpha,\beta}(u)=-\infty$. moreover, we consider a same problem inside a case that $\alpha$ was large, where higher order eigenvalues are involved.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8239,"a electric field gradient (efg) tensor at a $^{75}$as site couples to a orbital occupations of a as p-orbitals and was the sensitive probe of local nematicity inside bafe$_2$as$_2$. we use nuclear magnetic resonance to measure a nuclear quadrupolar splittings and find that a efg asymmetry responds linearly to a presence of the strain field inside a paramagnetic phase. we extract a nematic susceptibility from a slope of this linear response as the function of temperature and find that it diverges near a structural transition inside agreement with other measures of a bulk nematic susceptibility. our work establishes an alternative method to extract a nematic susceptibility which, inside contrast to transport methods, should be extended in a superconducting state.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
13768,"confidence was the fundamental concept inside statistics, but there was the tendency to misinterpret it as probability. inside this paper, i argue that an intuitively and mathematically more appropriate interpretation of confidence was through belief/plausibility functions, inside particular, those that satisfy the certain validity property. given their close connection with confidence, it was natural to ask how the valid belief/plausibility function should be constructed directly. a inferential model (im) framework provides such the construction, and here i prove the complete-class theorem stating that, considering every nominal confidence region, there exists the valid im whose plausibility regions are contained by a given confidence region. this characterization has implications considering statistics understanding and communication, and highlights a importance of belief functions and a im framework.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
6931,"star formation inside galaxies relies on a availability of cold, dense gas, which, inside turn, relies on factors internal and external to a galaxies. inside order to provide the simple model considering how star formation was regulated by various physical processes inside galaxies, we analyse data at redshift $z=0$ from the hydrodynamical cosmological simulation that includes prescriptions considering star formation and stellar evolution, active galactic nuclei (agn), and their associated feedback processes. this model should determine a star formation rate (sfr) as the function of galaxy stellar mass, gas mass, black hole mass, and environment. we find that gas mass was a most important quantity controlling star formation inside low-mass galaxies, and star-forming galaxies inside dense environments have higher sfr than their counterparts inside a field. inside high-mass galaxies, we find that black holes more massive than $\sim10^{7.5}$ m$_\odot$ should be triggered to quench star formation inside their host; this mass scale was emergent inside our simulations. furthermore, this black hole mass corresponds to the galaxy bulge mass $\sim2\times10^{10}$ m$_\odot$, consistent with a mass at which galaxies start to become dominated by early types ($\sim3\times10^{10}$ m$_\odot$, as previously shown inside observations by kauffmann et al.). finally, we demonstrate that our model should reproduce well a sfr measured from observations of galaxies inside a gama and alfalfa surveys.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19809,"microwave plasma discharges working at low pressure are nowadays the well-developed technique mainly used to provide radiations at different wavelengths. a aim of this work was to show that those discharges are an efficient windowless vuv photon source considering planetary atmospheric photochemistry experiments. to do this, we use the surfatron-type discharge with the neon gas flow inside a mbar pressure range coupled to the photochemical reactor. working inside a vuv range allows to focus on nitrogen-dominated atmospheres ({\lambda}<100nm). a experimental setup makes sure that no other energy sources (electrons, metastable atoms) than a vuv photons interact with a reactive medium. neon owns two resonance lines at 73.6 and 74.3 nm which behave differently regarding a pressure or power conditions. inside parallel, a vuv photon flux emitted at 73.6 nm has been experimentally estimated inside different conditions of pressure and power and varies inside the large range between 2x1013 ph.s-1.cm-2 and 4x1014 ph.s-1.cm-2 which was comparable to the vuv synchrotron photon flux. our first case study was a atmosphere of titan and its n2-ch4 atmosphere. with this vuv source, a production of hcn and c2n2, two major titan compounds, was detected, ensuring a suitability of a source considering atmospheric photochemistry experiments.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18526,"we study detection methods considering multivariable signals under dependent noise. a main focus was on three-dimensional signals, i.e. on signals inside a space-time domain. examples considering such signals are multifaceted. they include geographic and climatic data as well as image data, that are observed over the fixed time horizon. we assume that a signal was observed as the finite block of noisy samples whereby we are interested inside detecting changes from the given reference signal. our detector statistic was based on the sequential partial sum process, related to classical signal decomposition and reconstruction approaches applied to a sampled signal. we show that this detector process converges weakly under a no change null hypothesis that a signal coincides with a reference signal, provided that a spatial-temporal partial sum process associated to a random field of a noise terms disturbing a sampled signal con- verges to the brownian motion. more generally, we also establish a limiting distribution under the wide class of local alternatives that allows considering smooth as well as discontinuous changes. our results also cover extensions to a case that a reference signal was unknown. we conclude with an extensive simulation study of a detection algorithm.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
6745,"domain adaptation refers to a problem of leveraging labeled data inside the source domain to learn an accurate model inside the target domain where labels are scarce or unavailable. the recent idea behind the method considering finding the common representation of a two domains was using domain adversarial training (ganin & lempitsky, 2015), which attempts to induce the feature extractor that matches a source and target feature distributions inside some feature space. however, domain adversarial training faces two critical limitations: 1) if a feature extraction function has high-capacity, then feature distribution matching was the weak constraint, 2) inside non-conservative domain adaptation (where no single classifier should perform well inside both a source and target domains), training a model to do well on a source domain hurts performance on a target domain. inside this paper, we address these issues through a lens of a cluster assumption, i.e., decision boundaries should not cross high-density data regions. we propose two novel and related models: 1) a virtual adversarial domain adaptation (vada) model, which combines domain adversarial training with the penalty term that punishes a violation a cluster assumption; 2) a decision-boundary iterative refinement training with the teacher (dirt-t) model, which takes a vada model as initialization and employs natural gradient steps to further minimize a cluster assumption violation. extensive empirical results demonstrate that a combination of these two models significantly improve a state-of-the-art performance on a digit, traffic sign, and wi-fi recognition domain adaptation benchmarks.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4239,"probabilistic integration of the continuous dynamical system was the way of systematically introducing model error, at scales no larger than errors introduced by standard numerical discretisation, inside order to enable thorough exploration of possible responses of a system to inputs. it was thus the potentially useful idea behind the method inside the number of applications such as forward uncertainty quantification, inverse problems, and data assimilation. we extend a convergence analysis of probabilistic integrators considering deterministic ordinary differential equations, as proposed by conrad et al.\ (\textit{stat.\ comput.}, 2016), to establish mean-square convergence inside a uniform norm on discrete- or continuous-time solutions under relaxed regularity assumptions on a driving vector fields and their induced flows. specifically, we show that randomised high-order integrators considering globally lipschitz flows and randomised euler integrators considering dissipative vector fields with polynomially-bounded local lipschitz constants all have a same mean-square convergence rate as their deterministic counterparts, provided that a variance of a integration noise was not of higher order than a corresponding deterministic integrator. these and similar results are proven considering probabilistic integrators where a random perturbations may be state-dependent, non-gaussian, or non-centred random variables.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
18657,"we theoretically investigate pump-probe optical responses inside a two-dimensional extended hubbard model describing cuprates by with the help of the time-dependent lanczos method. at half filling, pumping generates photoinduced absorptions in a mott gap. the part of low-energy absorptions was attributed to a independent propagation of photoinduced holons and doublons. a spectral weight just below a mott gap increases with decreasing a on-site coulomb interaction $u$. we find that a next-nearest-neighbor coulomb interaction $v_1$ enhances this $u$ dependence, indicating a presence of biexcitonic contributions formed by two holon-doublon pairs. photo-pumping inside hole-doped systems also induces spectral weights below remnant mott-gap excitations, being consistent with recent experiments. a induced weights are less sensitive to $v_1$ and may be related to a formation of the biexcitonic state inside a presence of hole carriers.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
2177,"let $(m,g)$ be the pseudo-riemannian manifold of signature $(p,q)$. we construct mutually quasi-inverse equivalences between a groupoid of bundles of weakly-faithful complex clifford modules on $(m,g)$ and a groupoid of reduced complex lipschitz structures on $(m,g)$. as an application, we show that $(m,g)$ admits the bundle of irreducible complex clifford modules if and only if it admits either the $spin^{c}(p,q)$ structure (when $p+q$ was odd) or the $pin^{c}(p,q)$ structure (when $p+q$ was even). when $p-q\equiv_8 3,4,6, 7$, we compare with a classification of bundles of irreducible real clifford modules which we obtained inside previous work. a results obtained inside this note form the counterpart of a classification of bundles of faithful complex clifford modules which is previously given by t. friedrich and a. trautman.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10516,"if the significant fraction of a dark matter inside a universe was made of an ultra-light scalar field, named fuzzy dark matter (fdm) with the mass $m_a$ of a order of $10^{-22}-10^{-21}$ ev, then its de broglie wavelength was large enough to impact a physics of large scale structure formation. inside particular, a associated cut-off inside a linear matter power spectrum modifies a structure of a intergalactic medium (igm) at a scales probed by a lyman-$\alpha$ forest of distant quasars. we study this effect by making use of dedicated cosmological simulations which take into account a hydrodynamics of a igm. we explore heuristically a amplitude of quantum pressure considering a fdm masses considered here and conclude that quantum effects should not modify significantly a non-linear evolution of matter density at a scales relevant to a measured lyman-$\alpha$ flux power, and considering $m_a \geq 10^{-22}$ ev. we derive the scaling law between $m_a$ and a mass of a well-studied thermal warm dark matter (wdm) model that was best adapted to a lyman-$\alpha$ forest data, and differs significantly from a one infered by the simple linear extrapolation. by comparing fdm simulations with a lyman-$\alpha$ flux power spectra determined from a boss survey, and marginalizing over relevant nuisance parameters, we exclude fdm masses inside a range $10^{-22} \leq m_a < 2.3\times 10^{-21}$ ev at 95 % cl. adding higher-resolution lyman-$\alpha$ spectra extends a exclusion range up to $2.9\times 10^{-21}$ ev. this provides the significant constraint on fdm models tailored to solve a ""small-scale problems"" of $\lambda$cdm.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12454,"we study quasar proximity zones inside a redshift range $5.77 \leq z \leq 6.54$ by homogeneously analyzing $34$ medium resolution spectra, encompassing both archival and newly obtained data, and exploiting recently updated systemic redshift and magnitude measurements. whereas previous studies found strong evolution of proximity zone sizes with redshift, and argued that this provides evidence considering the rapidly evolving intergalactic medium (igm) neutral fraction during reionization, we measure the much shallower trend $\propto(1+z)^{-1.44}$. we compare our measured proximity zone sizes to predictions from hydrodynamical simulations post-processed with one-dimensional radiative transfer, and find good agreement between observations and theory irrespective of a ionization state of a ambient igm. this insensitivity to igm ionization state has been previously noted, and results from a fact that a definition of proximity zone size as a first drop of a smoothed quasar spectrum below a $10\%$ flux transmission level probes locations where a ionizing radiation from a quasar was an order of magnitude larger than a expected ultraviolet ionizing background that sets a neutral fraction of a igm. our analysis also uncovered three objects with exceptionally small proximity zones (two have $r_p < 1$proper mpc), which constitute outliers from a observed distribution and are challenging to explain with our radiative transfer simulations. we consider various explanations considering their origin, such as strong absorption line systems associated with a quasar or patchy reionization, but find that a most compelling scenario was that these quasars have been shining considering $\lesssim 10^5$yr.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18355,"a electric coupling between surface ions and bulk ferroelectricity gives rise to the continuum of mixed states inside ferroelectric thin films, exquisitely sensitive to temperature and external factors, such as applied voltage and oxygen pressure. here we develop a comprehensive analytical description of these coupled ferroelectric and ionic (""ferroionic"") states by combining a ginzburg-landau-devonshire description of a ferroelectric properties of a film with langmuir adsorption model considering a electrochemical reaction at a film surface. we explore a thermodynamic and kinetic characteristics of a ferroionic states as the function of temperature, film thickness, and external electric potential. these studies provide the new insight into mesoscopic properties of ferroelectric thin films, whose surface was exposed to chemical environment as screening charges supplier.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
1863,"traditional image clustering methods take the two-step approach, feature learning and clustering, sequentially. however, recent research results demonstrated that combining a separated phases inside the unified framework and training them jointly should achieve the better performance. inside this paper, we first introduce fully convolutional auto-encoders considering image feature learning and then propose the unified clustering framework to learn image representations and cluster centers jointly based on the fully convolutional auto-encoder and soft $k$-means scores. at initial stages of a learning procedure, a representations extracted from a auto-encoder may not be very discriminative considering latter clustering. we address this issue by adopting the boosted discriminative distribution, where high score assignments are highlighted and low score ones are de-emphasized. with a gradually boosted discrimination, clustering assignment scores are discriminated and cluster purities are enlarged. experiments on several vision benchmark datasets show that our methods should achieve the state-of-the-art performance.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
12838,"this survey presents a main results achieved considering a influence maximization problem inside social networks. this problem was well studied inside a literature and, thanks to its recent applications, some of which currently deployed on a field, it was receiving more and more attention inside a scientific community. a problem should be formulated as follows: given the graph, with each node having the certain probability of influencing its neighbors, select the subset of vertices so that a number of nodes inside a network that are influenced was maximized. starting from this model, we introduce a main theoretical developments and computational results that have been achieved, taking into account different diffusion models describing how a information spreads throughout a network, various ways inside which a sources of information could be placed, and how to tackle a problem inside a presence of uncertainties affecting a network. finally, we present one of a main application that has been developed and deployed exploiting tools and techniques previously discussed.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
8773,"predictable feature analysis (pfa) (richthofer, wiskott, icmla 2015) was an algorithm that performs dimensionality reduction on high dimensional input signal. it extracts those subsignals that are most predictable according to the certain prediction model. we refer to these extracted signals as predictable features. inside this work we extend a notion of pfa to take supplementary information into account considering improving its predictions. such information should be the multidimensional signal like a main input to pfa, but was regarded external. that means it won't participate inside a feature extraction - no features get extracted or composed of it. features will be exclusively extracted from a main input such that they are most predictable based on themselves and a supplementary information. we refer to this enhanced pfa as pfax (pfa extended). even more important than improving prediction quality was to observe a effect of supplementary information on feature selection. pfax transparently provides insight how a supplementary information adds to prediction quality and whether it was valuable at all. finally we show how to invert that relation and should generate a supplementary information such that it would yield the certain desired outcome of a main signal. we apply this to the setting inspired by reinforcement learning and let a algorithm learn how to control an agent inside an environment. with this method it was feasible to locally optimize a agent's state, i.e. reach the certain goal that was near enough. we are preparing the follow-up paper that extends this method such that also global optimization was feasible.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
2555,"sufficient and necessary conditions considering a stability of positive feedback interconnections of negative imaginary systems are derived using an integral quadratic constraint (iqc) approach. a iqc framework accommodates distributed-parameter systems with irrational transfer function representations, while generalising existing results inside a literature and allowing exploitation of flexibility at zero and infinite frequencies to reduce conservatism inside a analysis. a main results manifest a important property that a negative imaginariness of systems gives rise to the certain form of iqcs on positive frequencies that are bounded away from zero and infinity. two additional sets of iqcs on a dc and instantaneous gains of a systems are shown to be sufficient and necessary considering closed-loop stability along the homotopy of systems.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
4376,when the flow was not allowed to be reoriented a maximum residual flow problem with $k$-arc destruction was known to be $np$-hard considering $k=2$. we show that when the flow was allowed to be adaptive a problem becomes polynomial considering every fixed $k$.,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15800,"machine learning (ml) plays an ever-increasing role inside advanced automotive functionality considering driver assistance and autonomous operation; however, its adequacy from a perspective of safety certification remains controversial. inside this paper, we analyze a impacts that a use of ml as an implementation idea behind the method has on iso 26262 safety lifecycle and ask what could be done to address them. we then provide the set of recommendations on how to adapt a standard to accommodate ml.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
8844,"dynamic epidemic models have proven valuable considering public health decision makers as they provide useful insights into a understanding and prevention of infectious diseases. however, inference considering these types of models should be difficult because a disease spread was typically only partially observed e.g. inside form of reported incidences inside given time periods. this chapter discusses how to perform likelihood-based inference considering partially observed markov epidemic models when it was relatively easy to generate samples from a markov transmission model while a likelihood function was intractable. a first part of a chapter reviews a theoretical background of inference considering partially observed markov processes (pomp) using iterated filtering. inside a second part of a chapter a performance of a method and associated practical difficulties are illustrated on two examples. inside a first example the simulated outbreak data set consisting of a number of newly reported cases aggregated by week was fitted to the pomp where a underlying disease transmission model was assumed to be the simple markovian sir model. a second example illustrates possible model extensions such as seasonal forcing and over-dispersion inside both, a transmission and observation model, which should be used, e.g., when analysing routinely collected rotavirus surveillance data. both examples are implemented with the help of a r-package pomp (king et al., 2016) and a code was made available online.",0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
11419,"motivated by a spin-triplet superconductor sr2ruo4, a thermal hall conductivity was investigated considering several pairing symmetries with broken time-reversal symmetry. inside a chiral p-wave phase with the fully opened quasiparticle excitation gap, a temperature dependence of a thermal hall conductivity has the temperature linear term associated with a topological property directly, and an exponential term, which shows the drastic change around a lifshitz transition. examining f-wave states as alternative candidates with $\bm d=\delta_0\hat{z}(k_x^2-k_y^2)(k_x\pm ik_y)$ and $\bm d=\delta_0\hat{z}k_xk_y(k_x\pm ik_y)$ with gapless quasiparticle excitations, we study a temperature dependence of a thermal hall conductivity, where considering a former state a thermal hall conductivity has the quadratic dependence on temperature, originating from a linear dispersions, inside addition to linear and exponential behavior. a obtained result may enable us to distinguish between a chiral p-wave and f-wave states inside sr2ruo4.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
11116,"the lack of understanding of human biology creates the hurdle considering a development of precision medicines. to overcome this hurdle we need to better understand a potential synergy between the given investigational treatment (vs. placebo or active control) and various demographic or genetic factors, disease history and severity, etc., with a goal of identifying those patients at increased risk of exhibiting clinically meaningful treatment benefit. considering this reason, we propose a vg method, which combines a idea of an individual treatment effect (ite) from virtual twins (foster, et al., 2011) with a unbiased variable selection and cutoff value determination algorithm from guide (loh, et al., 2015). simulation results show a vg method has less variable selection bias than virtual twins and higher statistical power than guide interaction inside a presence of prognostic variables with strong treatment effects. type i error and predictive performance of virtual twins, guide and vg are compared through a use of simulation studies. results obtained after retrospectively applying vg to data from the clinical trial also are discussed.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16947,"spectral clustering has found extensive use inside many areas. most traditional spectral clustering algorithms work inside three separate steps: similarity graph construction; continuous labels learning; discretizing a learned labels by k-means clustering. such common practice has two potential flaws, which may lead to severe information loss and performance degradation. first, predefined similarity graph might not be optimal considering subsequent clustering. it was well-accepted that similarity graph highly affects a clustering results. to this end, we propose to automatically learn similarity information from data and simultaneously consider a constraint that a similarity matrix has exact c connected components if there are c clusters. second, a discrete solution may deviate from a spectral solution since k-means method was well-known as sensitive to a initialization of cluster centers. inside this work, we transform a candidate solution into the new one that better approximates a discrete one. finally, those three subtasks are integrated into the unified framework, with each subtask iteratively boosted by with the help of a results of a others towards an overall optimal solution. it was known that a performance of the kernel method was largely determined by a choice of kernels. to tackle this practical problem of how to select a most suitable kernel considering the particular data set, we further extend our model to incorporate multiple kernel learning ability. extensive experiments demonstrate a superiority of our proposed method as compared to existing clustering approaches.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1047,"data available across a web was largely unstructured. offers published by multiple sources like banks, digital wallets, merchants, etc., are one of a most accessed advertising data inside today's world. this data gets accessed by millions of people on the daily basis and was easily interpreted by humans, but since it was largely unstructured and diverse, with the help of an algorithmic way to extract meaningful information out of these offers was hard. identifying a essential offer entities (for instance, its amount, a product on which a offer was applicable, a merchant providing a offer, etc.) from these offers plays the vital role inside targeting a right customers to improve sales. this work presents and evaluates various existing named entity recognizer (ner) models which should identify a required entities from offer feeds. we also propose the novel hybrid ner model constructed by two-level stacking of conditional random field, bidirectional lstm and spacy models at a first level and an svm classifier at a second. a proposed hybrid model has been tested on offer feeds collected from multiple sources and has shown better performance inside a offer domain when compared to a existing models.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
9157,"photometric stereo was the method considering estimating a normal vectors of an object from images of a object under varying lighting conditions. motivated by several recent works that extend photometric stereo to more general objects and lighting conditions, we study the new robust idea behind the method to photometric stereo that utilizes dictionary learning. specifically, we propose and analyze two approaches to adaptive dictionary regularization considering a photometric stereo problem. first, we propose an image preprocessing step that utilizes an adaptive dictionary learning model to remove noise and other non-idealities from a image dataset before estimating a normal vectors. we also propose an alternative model where we directly apply a adaptive dictionary regularization to a normal vectors themselves during estimation. we study a practical performance of both methods through extensive simulations, which demonstrate a state-of-the-art performance of both methods inside a presence of noise.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
443,"learning graphical models from data was an important problem with wide applications, ranging from genomics to a social sciences. nowadays datasets often have upwards of thousands---sometimes tens or hundreds of thousands---of variables and far fewer samples. to meet this challenge, we have developed the new r package called sparsebn considering learning a structure of large, sparse graphical models with the focus on bayesian networks. while there are many existing software packages considering this task, this package focuses on a unique setting of learning large networks from high-dimensional data, possibly with interventions. as such, a methods provided place the premium on scalability and consistency inside the high-dimensional setting. furthermore, inside a presence of interventions, a methods implemented here achieve a goal of learning the causal network from data. additionally, a sparsebn package was fully compatible with existing software packages considering network analysis.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0
2434,"this paper addresses a problem of output voltage regulation considering multiple dc/dc converters connected to the microgrid, and prescribes the scheme considering sharing power among different sources. this architecture was structured inside such the way that it admits quantifiable analysis of a closed-loop performance of a network of converters; a analysis simplifies to studying closed-loop performance of an equivalent {\em single-converter} system. a proposed architecture allows considering a proportion inside which a sources provide power to vary with time; thus overcoming limitations of our previous designs. additionally, a proposed control framework was suitable to both centralized and decentralized implementations, i.e., a same control architecture should be employed considering voltage regulation irrespective of a availability of common load-current (or power) measurement, without a need to modify controller parameters. a performance becomes quantifiably better with better communication of a demanded load to all a controllers at all a converters (in a centralized case); however guarantees viability when such communication was absent. case studies comprising of battery, pv and generic sources are presented and demonstrate a enhanced performance of prescribed optimal controllers considering voltage regulation and power sharing.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
16074,"critical node problems involve identifying the subset of critical nodes from an undirected graph whose removal results inside optimizing the pre-defined measure over a residual graph. as useful models considering the variety of practical applications, these problems are computational challenging. inside this paper, we study a classic critical node problem (cnp) and introduce an effective memetic algorithm considering solving cnp. a proposed algorithm combines the double backbone-based crossover operator (to generate promising offspring solutions), the component-based neighborhood search procedure (to find high-quality local optima) and the rank-based pool updating strategy (to guarantee the healthy population). specially, a component-based neighborhood search integrates two key techniques, i.e., two-phase node exchange strategy and node weighting scheme. a double backbone-based crossover extends a idea of general backbone-based crossovers. extensive evaluations on 42 synthetic and real-world benchmark instances show that a proposed algorithm discovers 21 new upper bounds and matches 18 previous best-known upper bounds. we also demonstrate a relevance of our algorithm considering effectively solving the variant of a classic cnp, called a cardinality-constrained critical node problem. finally, we investigate a usefulness of each key algorithmic component.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1867,"we view intersection handling on autonomous vehicles as the reinforcement learning problem, and study its behavior inside the transfer learning setting. we show that the network trained on one type of intersection generally was not able to generalize to other intersections. however, the network that was pre-trained on one intersection and fine-tuned on another performs better on a new task compared to training inside isolation. this network also retains knowledge of a prior task, even though some forgetting occurs. finally, we show that a benefits of fine-tuning hold when transferring simulated intersection handling knowledge to the real autonomous vehicle.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
4284,"we consider the probit model without covariates, but a latent gaussian variables having compound symmetry covariance structure with the single parameter characterizing a common correlation. we study a parameter approximation problem under such one-parameter probit models. as the surprise, we demonstrate that a likelihood function does not yield consistent estimates considering a correlation. we then formally prove a parameter's nonestimability by deriving the non-vanishing minimax lower bound. this counter-intuitive phenomenon provides an interesting insight that one bit information of a latent gaussian variables was not sufficient to consistently recover their correlation. on a other hand, we further show that trinary data generated from a gaussian variables should consistently approximate a correlation with parametric convergence rate. thus we reveal the phase transition phenomenon regarding a discretization of latent gaussian variables while preserving a estimability of a correlation.",0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0
4598,inside this paper we develop a notion of screen isoparametric hypersurface considering null hypersurfaces of robertson-walker spacetimes. with the help of this formalism we derive cartan identities considering a screen principal curvatures of null screen hypersurfaces inside lorentzian space forms and provide the local characterization of such hypersurfaces.,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17666,"autonomous unmanned aerial vehicles (uavs) that should execute aggressive (i.e., high-speed and high-acceleration) maneuvers have attracted significant attention inside a past few years. inside this paper, we propose the novel control law considering accurate tracking of aggressive quadcopter trajectories. a proposed method tracks position and yaw angle with their derivatives of up to fourth order, specifically, a position, velocity, acceleration, jerk, and snap along with a yaw angle, yaw rate and yaw acceleration. two key aspects of a proposed method are a following. first, a controller exploits a differential flatness of a quadcopter dynamics to generate feedforward inputs considering attitude rate and attitude acceleration inside order to track a jerk and snap references. a tracking was enabled by direct control of body torque with the help of closed-loop control of all four propeller speeds based on optical encoders attached to a motors. second, a controller utilizes a incremental nonlinear dynamic inversion (indi) method considering accurate tracking of linear and angular accelerations despite external disturbances. hence, no prior modeling of aerodynamic effects was required. we rigorously analyze a proposed controller through response analysis, and we demonstrate it inside experiments. a proposed control law enables the 1-kg quadcopter uav to track complex 3d trajectories, reaching speeds up to 8.2 m/s and accelerations up to 2g, while keeping a root-mean-square tracking error down to 4 cm, inside the flight volume that was roughly 6.5 m long, 6.5 m wide, and 1.5 m tall. we also demonstrate a robustness of a controller by attaching the drag plate to a uav inside flight tests and by pulling on a uav with the rope during hover.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1
16289,"a main objective of this thesis was a study of a evolution under a ricci flow of surfaces with singularities of cone type. the second objective, emerged from a techniques we use, was a study of families of ricci flow solitons inside dimension 2 and 3. a ricci flow was an evolution equation considering riemannian manifolds, introduced by r. hamilton inside 1982. it was from a achievements made by g. perelman with this technique inside 2002 when a ricci flow has been established inside the discipline itself, generating the great interest inside a community. this thesis contains four original results. first result was the complete classification of solitons inside smooth and cone surfaces. this classification completes a preceding results found by hamilton, chow and wu and others, and we obtain explicit descriptions of all solitons inside dimension 2. second result was the geometrization of cone surfaces by ricci flow. this result, which uses a aforementioned first result, extends a theory of hamilton to a singular case. this was a most comprehensive result inside a thesis, considering which we use and develop analysis and pde techniques, as well as comparison geometry techniques. third result was a existence of the ricci flow that removes cone singularities. this clearly exposes a non-uniqueness of solutions to a flow , inside analogy to a ricci flow with cusps of p. topping. a fourth result was a construction of the new expanding gradient ricci soliton inside dimension 3. just as we do with solitons on cone surfaces, we give an explicit construction with the help of techniques of phase portraits. we also prove that this was a only soliton with its topology and its lower bound of a curvature, and besides this was the critical case amongst all expanding solitons inside dimension 3 with curvature bounded below.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4982,"self-attentive feed-forward sequence models have been shown to achieve impressive results on sequence modeling tasks, thereby presenting the compelling alternative to recurrent neural networks (rnns) which has remained a de-facto standard architecture considering many sequence modeling problems to date. despite these successes, however, feed-forward sequence models like a transformer fail to generalize inside many tasks that recurrent models handle with ease (e.g. copying when a string lengths exceed those observed at training time). moreover, and inside contrast to rnns, a transformer model was not computationally universal, limiting its theoretical expressivity. inside this paper we propose a universal transformer which addresses these practical and theoretical shortcomings and we show that it leads to improved performance on several tasks. instead of recurring over a individual symbols of sequences like rnns, a universal transformer repeatedly revises its representations of all symbols inside a sequence with each recurrent step. inside order to combine information from different parts of the sequence, it employs the self-attention mechanism inside every recurrent step. assuming sufficient memory, its recurrence makes a universal transformer computationally universal. we further employ an adaptive computation time (act) mechanism to allow a model to dynamically adjust a number of times a representation of each position inside the sequence was revised. beyond saving computation, we show that act should improve a accuracy of a model. our experiments show that on various algorithmic tasks and the diverse set of large-scale language understanding tasks a universal transformer generalizes significantly better and outperforms both the vanilla transformer and an lstm inside machine translation, and achieves the new state of a art on a babi linguistic reasoning task and a challenging lambada language modeling task.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
13694,"the type iin supernova (sn) was dominated by a interaction of sn ejecta with a circumstellar medium (csm). some sne iin (e.g., sn 2006jd) have episodes of re-brightening (""bumps"") inside their light curves. we present iptf13z, the sn iin discovered by a intermediate palomar transient factory (iptf) and characterised by several bumps inside its light curve. we analyse this peculiar behaviour trying to infer a properties of a csm and of a sn explosion, as well as a nature of its progenitor star. we obtained multi-band optical photometry considering over 1000 days after discovery with a p48 and p60 telescopes at palomar observatory. we obtained low-resolution optical spectra inside a same period. we did an archival search considering progenitor outbursts. we analyse our photometry and spectra, and compare iptf13z to other sne iin. the simple analytical model was used to approximate properties of a csm. iptf13z is the sn iin showing the light curve with five bumps during its decline phase. a bumps had amplitudes between 0.4 and 0.9 mag and durations between 20 and 120 days. a most prominent bumps appeared inside all our different optical bands. a spectra showed typical sn iin characteristics, with emission lines of h$\alpha$ (with broad component fwhm ~$10^{3}-10^{4} ~{\rm ~km ~s^{-1}}$ and narrow component fwhm ~$10^2 \rm ~km ~s^{-1}$) and he i, but also with fe ii, ca ii, na i d and h$\beta$ p-cygni profiles (with velocities of ~$10^{3}$ ${\rm ~km ~s^{-1}}$). the pre-explosion outburst is identified lasting $\gtrsim 50$ days, with $m_r \approx -15$ mag around 210 days before discovery. large, variable progenitor mass-loss rates (~> 0.01 $m_{\odot} \rm ~yr^{-1}$) and csm densities (~> 10$^{-16}$ g cm$^{-3}$) are derived. we suggest that a light curve bumps of iptf13z arose from sn ejecta interacting with denser regions inside a csm, possibly produced by a eruptions of the luminous blue variable star.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15015,we give an introduction to a mckay correspondence and its connection to quotients of $\mathbb{c}^n$ by finite reflection groups. this yields the natural construction of noncommutative resolutions of a discriminants of these reflection groups. this paper was an extended version of e.f.'s talk with a same title delivered at a icra.,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
595,"we present an attention based visual analysis framework to compute grasp-relevant information inside order to guide grasp planning with the help of the multi-fingered robotic hand. our idea behind the method uses the computational visual attention model to locate regions of interest inside the scene, and uses the deep convolutional neural network to detect grasp type and point considering the sub-region of a object presented inside the region of interest. we demonstrate a proposed framework inside object grasping tasks, inside which a information generated from a proposed framework was used as prior information to guide a grasp planning. results show that a proposed framework should not only speed up grasp planning with more stable configurations, but also was able to handle unknown objects. furthermore, our framework should handle cluttered scenarios. the new grasp type dataset (gtd) that considers 6 commonly used grasp types and covers 12 household objects was also presented.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
8339,"community structure was an important area of research. it has received the considerable attention from a scientific community. despite its importance, one of a key problems inside locating information about community detection was a diverse spread of related articles across various disciplines. to a best of our knowledge, there was no current comprehensive review of recent literature which uses the scientometric analysis with the help of complex networks analysis covering all relevant articles from a web of science (wos). here we present the visual survey of key literature with the help of citespace. a idea was to identify emerging trends besides with the help of network techniques to examine a evolution of a domain. towards that end, we identify a most influential, central, as well as active nodes with the help of scientometric analyses. we examine authors, key articles, cited references, core subject categories, key journals, institutions, as well as countries. a exploration of a scientometric literature of a domain reveals that yong wang was the pivot node with a highest centrality. additionally, we have observed that mark newman was a most highly cited author inside a network. we have also identified that a journal, ""reviews of modern physics"" has a strongest citation burst. inside terms of cited documents, an article by andrea lancichinetti has a highest centrality score. we have also discovered that a origin of a key publications inside this domain was from a united states. whereas scotland has a strongest and longest citation burst. additionally, we have found that a categories of ""computer science"" and ""engineering"" lead other categories based on frequency and centrality respectively.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
17552,"the fundamental component of a game theoretic idea behind the method to distributed control was a design of local utility functions. inside part i of this work we showed how to systematically design local utilities so as to maximize a induced worst case performance. a purpose of a present manuscript was to specialize a general results obtained inside part i to the class of monotone submodular, supermodular and set covering problems. inside a case of set covering problems, we show how any distributed algorithm capable of computing the nash equilibrium inherits the performance certificate matching a well known 1-1/e approximation of nemhauser. relative to a class of submodular maximization problems considered here, we show how a performance offered by a game theoretic idea behind the method improves on existing approximation algorithms. we briefly discuss a algorithmic complexity of computing (pure) nash equilibria and show how our idea behind the method generalizes and subsumes previously fragmented results inside a area of optimal utility design. two applications and corresponding numerics are presented: a vehicle target assignment problem and the coverage problem arising inside distributed caching considering wireless networks.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
14641,"we classify and characterize three dimensional $u(1)$ quantum spin liquids (deconfined $u(1)$ gauge theories) with global symmetries. these spin liquids have an emergent gapless photon and emergent electric/magnetic excitations (which we assume are gapped). we first discuss inside great detail a case with time reversal and $so(3)$ spin rotational symmetries. we find there are 15 distinct such quantum spin liquids based on a properties of bulk excitations. we show how to interpret them as gauged symmetry-protected topological states (spts). some of these states possess fractional response to an external $so(3)$ gauge field, due to which we dub them ""fractional topological paramagnets"". we identify 11 other anomalous states that should be grouped into 3 anomaly classes. a classification was further refined by weakly coupling these quantum spin liquids to bosonic symmetry protected topological (spt) phases with a same symmetry. this refinement does not modify a bulk excitation structure but modifies universal surface properties. taking this refinement into account, we find there are 168 distinct such $u(1)$ quantum spin liquids. after this warm-up we provide the general framework to classify symmetry enriched $u(1)$ quantum spin liquids considering the large class of symmetries. as the more complex example, we discuss $u(1)$ quantum spin liquids with time reversal and $z_2$ symmetries inside detail. based on a properties of a bulk excitations, we find there are 38 distinct such spin liquids that are anomaly-free. there are also 37 anomalous $u(1)$ quantum spin liquids with this symmetry. finally, we briefly discuss a classification of $u(1)$ quantum spin liquids enriched by some other symmetries.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
11616,"we derive an explicit formula, as well as an efficient procedure, considering constructing the generalized jacobian considering a projector of the given square matrix onto a birkhoff polytope, i.e., a set of doubly stochastic matrices. to guarantee a high efficiency of our procedure, the semismooth newton method considering solving a dual of a projection problem was proposed and efficiently implemented. extensive numerical experiments are presented to demonstrate a merits and effectiveness of our method by comparing its performance against other powerful solvers such as a commercial software gurobi and a academic code pproj [{\sc hager and zhang}, siam journal on optimization, 26 (2016), pp.~1773--1798]. inside particular, our algorithm was able to solve a projection problem with over one billion variables and nonnegative constraints to the very high accuracy inside less than 15 minutes on the modest desktop computer. more importantly, based on our efficient computation of a projections and their generalized jacobians, we should design the highly efficient augmented lagrangian method (alm) considering solving the class of convex quadratic programming (qp) problems constrained by a birkhoff polytope. a resulted alm was demonstrated to be much more efficient than gurobi inside solving the collection of qp problems arising from a relaxation of quadratic assignment problems.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
17330,"determining wavelength-dependent exoplanet radii measurements was an excellent way to probe a composition of exoplanet atmospheres. inside light of this, borsa et al. (2016) sought to develop the technique to obtain such measurements by comparing ground-based transmission spectra to a expected brightness variations during an exoplanet transit. however, we demonstrate herein that this was not possible due to a transit light curve normalisation necessary to remove a effects of a earth's atmosphere on a ground-based observations. this was because a recoverable exoplanet radius was set by a planet-to-star radius ratio within a transit light curve; we demonstrate this both analytically and with simulated planet transits, as well as through the reanalysis of a hd 189733b data.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
19148,"experience inside exploring our own solar system has shown that direct investigation of planetary bodies with the help of space probes invariably yields scientific knowledge not otherwise obtainable. inside a case of exoplanets, such direct investigation may be required to confirm inferences made by astronomical observations, especially with regard to planetary interiors, surface processes, geological evolution, and possible biology. this will necessitate transporting sophisticated scientific instruments across interstellar space, and some proposed methods considering achieving this with flight-times measured inside decades are reviewed. it was concluded that, with a possible exception of very lightweight (and thus scientifically limited) probes accelerated to velocities of ~0.1c with powerful earth-based lasers, achieving such the capability may have to wait until a development of the space-based civilization capable of leveraging a material and energy resources of a solar system.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
4800,"influence maximization (im), which selects the set of $k$ users (called seeds) to maximize a influence spread over the social network, was the fundamental problem inside the wide range of applications such as viral marketing and network monitoring. existing im solutions fail to consider a highly dynamic nature of social influence, which results inside either poor seed qualities or long processing time when a network evolves. to address this problem, we define the novel im query named stream influence maximization (sim) on social streams. technically, sim adopts a sliding window model and maintains the set of $k$ seeds with a largest influence value over a most recent social actions. next, we propose a influential checkpoints (ic) framework to facilitate continuous sim query processing. a ic framework creates the checkpoint considering each window slide and ensures an $\varepsilon$-approximate solution. to improve its efficiency, we further devise the sparse influential checkpoints (sic) framework which selectively keeps $o(\frac{\log{n}}{\beta})$ checkpoints considering the sliding window of size $n$ and maintains an $\frac{\varepsilon(1-\beta)}{2}$-approximate solution. experimental results on both real-world and synthetic datasets confirm a effectiveness and efficiency of our proposed frameworks against a state-of-the-art im approaches.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
15347,"after a first direct detection of gravitational waves (gw), detection of stochastic background of gws was an important next step, and a first gw event suggests that it was within a reach of a second-generation ground-based gw detectors. such the gw signal was typically tiny, and should be detected by cross-correlating a data from two spatially separated detectors if a detector noise was uncorrelated. it has been advocated, however, that a global magnetic fields inside a earth-ionosphere cavity produce a environmental disturbances at low-frequency bands, known as schumann resonances, which potentially couple with gw detectors. inside this paper, we present the simple analytical model to approximate its impact on a detection of stochastic gws. a model crucially depends on a geometry of a detector pair through a directional coupling, and we investigate a basic properties of a correlated magnetic noise based on a analytic expressions. a model reproduces a major trend of a recently measured global correlation between a gw detectors using magnetometer. a estimated values of a impact of correlated noise also match those obtained from a measurement. finally, we give an implication to a detection of stochastic gws including upcoming detectors, kagra and ligo india. a model suggests that ligo hanford-virgo and virgo-kagra pairs are possibly less sensitive to a correlated noise, and should achieve the better sensitivity to a stochastic gw signal inside a most pessimistic case.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
6478,training the deep convolutional neural net typically starts with the random initialisation of all filters inside all layers which severely reduces a forward signal and back-propagated error and leads to slow and sub-optimal training. techniques that counter that focus on either increasing a signal or increasing a gradients adaptively but a model behaves very differently at a beginning of training compared to later when stable pathways through a net have been established. to compound this problem a effective minibatch size varies greatly between layers at different depths and between individual filters as activation sparsity typically increases with depth leading to the reduction inside effective learning rate since gradients may superpose rather than add and this further compounds a covariate shift problem as deeper neurons are less able to adapt to upstream shift. proposed here was the method of automatic gain control of a signal built into each convolutional neuron that achieves equivalent or superior performance than batch normalisation and was compatible with single sample or minibatch gradient descent. a same model was used both considering training and inference. a technique comprises the scaled per sample map mean subtraction from a raw convolutional filter output followed by scaling of a difference.,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15575,"reference was the crucial property of language that allows us to connect linguistic expressions to a world. modeling it requires handling both continuous and discrete aspects of meaning. data-driven models excel at a former, but struggle with a latter, and a reverse was true considering symbolic models. this paper (a) introduces the concrete referential task to test both aspects, called cross-modal entity tracking; (b) proposes the neural network architecture that uses external memory to build an entity library inspired inside a drss of drt, with the mechanism to dynamically introduce new referents or add information to referents that are already inside a library. our model shows promise: it beats traditional neural network architectures on a task. however, it was still outperformed by memory networks, another model with external memory.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8560,"we consider scattering of electromagnetic waves from the distant point source by a gravitational field of a sun, taking a field oblateness due to a quadrupole moment of a sun into account. effects of a field oblateness should play an important role inside a high resolution solar gravitational lens imaging inside a sub-micrometer wavelength range of a electromagnetic spectrum.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
19823,"we present results on a dust attenuation of galaxies at redshift $\sim3-6$ by studying a relationship between a uv spectral slope ($\beta_{\rm uv}$) and a infrared excess (irx; $l_{\rm ir}$/$l_{\rm uv}$) with the help of alma far-infrared continuum observations. our study was based on the sample of 67 massive, star-forming galaxies with the median mass of $m_{\ast}\sim 10^{10.7}\,m_{\rm \odot}$ spanning the redshift range $z=2.6-3.7$ (median $z=3.2$) that were observed with alma at $\lambda_{rest}=300\,{\rm \mu m}$. both a individual alma detections (41 sources) and stacks including all galaxies show a irx-$\beta_{\rm uv}$ relationship at $z\sim3$ was mostly consistent with that of local starburst galaxies on average. however, we find evidence considering the large dispersion around a mean relationship by up to $\pm0.5$ dex. nevertheless, a locally calibrated dust correction factors based on a irx-$\beta_{\rm uv}$ relation are on average applicable to main-sequence $z\sim3$ galaxies. this does not appear to be a case at even higher redshifts, however. with the help of public alma observations of $z\sim4-6$ galaxies we find evidence considering the significant evolution inside a irx-$\beta_{\rm uv}$ and a irx-$m_{\ast}$ relations beyond $z\sim3$ toward lower irx values. we discuss several caveats that could affect these results, including a assumed dust temperature. alma observations of larger $z>3$ galaxy samples will be required to confirm this intriguing redshift evolution.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13621,"we discuss a approximation of a interferometric visibility (fringe contrast) considering a exozodi survey conducted at a chara array with a jouflu beam combiner. we investigate a use of a statistical median to approximate a uncalibrated visibility from an ensemble of fringe exposures. under the broad range of operating conditions, numerical simulations indicate that this estimator has the smaller bias compared to other estimators. we also propose an improved method considering calibrating visibilities, which not only takes into account a time-interval between observations of calibrators and science targets, but also a uncertainties of a calibrators' raw visibilities. we test our methods with data corresponding to stars that do not display a exozodi phenomenon. a results of our tests show that a proposed method yields smaller biases and errors. a relative reduction inside bias and error was generally modest, but should be as high as $\sim 20-40\%$ considering a brightest stars of a chara data, and statistically significant at a $95\%$ confidence level (cl).",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
2744,"transient stability simulation of the large-scale and interconnected electric power system involves solving the large set of differential algebraic equations (daes) at every simulation time-step. with a ever-growing size and complexity of power grids, dynamic simulation becomes more time-consuming and computationally difficult with the help of conventional sequential simulation techniques. to cope with this challenge, this paper aims to develop the fully distributed idea behind the method intended considering implementation on high performance computer (hpc) clusters. the novel, relaxation-based domain decomposition algorithm known as parallel-general-norton with multiple-port equivalent (pgnme) was proposed as a core technique of the two-stage decomposition idea behind the method to divide a overall dynamic simulation problem into the set of subproblems that should be solved concurrently to exploit parallelism and scalability. while a convergence property has traditionally been the concern considering relaxation-based decomposition, an approximation mechanism based on multiple-port network equivalent was adopted as a preconditioner to enhance a convergence of a proposed algorithm. a proposed algorithm was illustrated with the help of rigorous mathematics and validated both inside terms of speed-up and capability. moreover, the complexity analysis was performed to support a observation that pgnme scales well when a size of a subproblems are sufficiently large.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
15069,"large scale, dynamical simulations have been performed considering a two dimensional octahedron model, describing a kardar-parisi-zhang (kpz) considering nonlinear, or a edwards-wilkinson (ew) class considering linear surface growth. a autocorrelation functions of a heights and a dimer lattice gas variables are determined with high precision. parallel random-sequential (rs) and two-sub-lattice stochastic dynamics (sca) have been compared. a latter causes the constant correlation inside a long time limit, but after subtracting it one should find a same height functions as inside case of rs. on a other hand a ordered update alters a dynamics of a lattice gas variables, by increasing (decreasing) a memory effects considering nonlinear (linear) models with respect to random-sequential. additionally, we support a kpz ansatz and a kallabis-krug conjecture inside $2+1$ dimensions and provide the precise growth exponent value $\beta=0.2414(2)$. we show a emergence of finite size corrections, which occur long before a steady state roughness was reached.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
2400,"the cm-order was the reduced order equipped with an involution that mimics complex conjugation. a witt-picard group of such an order was the certain group of ideal classes that was closely related to a ""minus part"" of a class group. we present the deterministic polynomial-time algorithm considering a following problem, which may be viewed as the special case of a principal ideal testing problem: given the cm-order, decide whether two given elements of its witt-picard group are equal. inside order to prevent coefficient blow-up, a algorithm operates with lattices rather than with ideals. an important ingredient was the technique introduced by gentry and szydlo inside the cryptographic context. our application of it to lattices over cm-orders hinges upon the novel existence theorem considering auxiliary ideals, which we deduce from the result of konyagin and pomerance inside elementary number theory.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
10785,"a kashiwara crystal $b(\infty)$ parametrizes the basis considering a verma module of the kac-moody algebra. it has the deep combinatorial structure which one seeks to understand. considering each sequence $j$ of reduced decompositions of elements of a weyl group $w$, it has the realization as the subset $b_j(\infty)$ of the crystal $b_j$ which as the set was just $j$ copies of a natural numbers. a goal was to determine $b_j(\infty)$ and inside particular to show that it was the polyhedral subset of $b_j$. inside earlier work this led to a notion of an $s$-graph associated to the given simple root $\alpha$. here a notion of the giant $s$-graph depending on the fixed simple root was introduced. it was essentially the union of $s$-graphs considering each simple root with one distinguished vertex depending on $\alpha$. its vertices, which forms the giant $s$-set, determine the set of dual kashiwara functions. these are linear functions on $b_j$, whose common maximum determines a dual kashiwara parameter with respect to $\alpha$. from these parameters one may compute $b_j(\infty)$ as an explicit polyhedral subset of $b_j$. considering $w$ finite, berenstein and zelevinsky had studied this problem by introducing a notion of the trail inside the fundamental module. a functions they define may also be viewed as the set of dual kashiwara functions. a goal was to relate these two approaches and without restriction on $w$. it was shown under a hypothesis that no ""false"" trails exist, that a set of trails determines a ""$z$-convex envelope"" of the giant $s$-set. a proof involves a study of identities inside demazure modules.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
8676,"we study offline data poisoning attacks inside contextual bandits, the class of reinforcement learning problems with important applications inside online recommendation and adaptive medical treatment, among others. we provide the general attack framework based on convex optimization and show that by slightly manipulating rewards inside a data, an attacker should force a bandit algorithm to pull the target arm considering the target contextual vector. a target arm and target contextual vector are both chosen by a attacker. that is, a attacker should hijack a behavior of the contextual bandit. we also investigate a feasibility and a side effects of such attacks, and identify future directions considering defense. experiments on both synthetic and real-world data demonstrate a efficiency of a attack algorithm.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11977,barkhausen current noise was used to probe a slow field-driven conversion of a glassy relaxor ferroelectric state to an ordered ferroelectric (fe) state. a frequent presence of distinct micron-scale barkhausen events well before a polarization current starts to speed up shows that a process was not the conventional nucleation-limited one. a prevalence of reverse switching events near a onset of a rapid part of a transition strongly indicates that electric dipole interactions play the key role. a combination of barkhausen noise changes and changes inside a complex dielectric response indicate that a process consists of an initial mixed-alignment domain formation stage followed by growth of a domains aligned with a applied field.,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
7481,"chiral superconductors support chiral edge modes and potentially spontaneous edge currents at their boundaries. motivated by a putative multiband chiral p-wave superconductor sr$_2$ruo$_4$, we study a influence of a interference between different bands at a edges, which may appear inside a presence of moderate edge disorder or inside edge tunneling measurements. we show that interband interference should strongly modify a measurable quantities at a edges when a order parameter exhibits phase difference between a bands. this was illustrated by investigating a edge dispersion and a edge current distribution inside a presence of interband mixing, as well as a conductance at the tunneling junction. a results are discussed inside connection with a putative chiral p-wave superconductor sr$_2$ruo$_4$. inside passing, we also discuss similar interference effects inside multiband models with other pairing symmetries.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
5980,"inside fluid dynamical simulations inside astrophysics, large deformations are common and surface tracking was sometimes necessary. smoothed particle hydrodynamics (sph) method has been used inside many of such simulations. recently, however, it has been shown that sph cannot handle contact discontinuities or free surfaces accurately. there are several reasons considering this problem. a first one was that sph requires that a density was continuous and differentiable. a second one was that sph does not have a consistency, and thus a accuracy was zeroth order inside space. inside addition, we cannot express accurate boundary conditions with sph. inside this paper, we propose the novel, high-order scheme considering particle-based hydrodynamics of compress- ible fluid. our method was based on kernel-weighted high-order fitting polynomial considering intensive variables. with this approach, we should construct the scheme which solves all of a three prob- lems described above. considering shock capturing, we use the tensor form of von-neumann-richtmyer artificial viscosity. we have applied our method to many test problems and obtained excel- lent result. our method was not conservative, since particles do not have mass or energy, but only their densities. however, because of a lagrangian nature of our scheme, a violation of a conservation laws turned out to be small. we name this method consistent particle hydrodynamics inside strong form (cphsf).",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
13850,"inside this paper we analyze a practical implications of szemer√©di's regularity lemma inside a preservation of metric information contained inside large graphs. to this end, we present the heuristic algorithm to find regular partitions. our experiments show that this method was quite robust to a natural sparsification of proximity graphs. inside addition, this robustness should be enforced by graph densification.",1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3796,"we analyse finite-time singularities of a teichm√ºller harmonic map flow -- the natural gradient flow of a harmonic map energy -- and find the canonical way of flowing beyond them inside order to construct global solutions inside full generality. moreover, we prove the no-loss-of-topology result at finite time, which completes a proof that this flow decomposes an arbitrary map into the collection of branched minimal immersions connected by curves.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
397,"a number of discovered tev sources populating a extragalactic sky inside 2017 was nearly 70, mostly blazars located up to the redshift ~1. ten years ago, inside 2007, less than 20 tev emitters were known, up to the maximum redshift of 0.2. this was the major achievement of current generation of cherenkov telescopes operating inside synergy with optical, x-ray, and gev gamma-ray telescopes. the review of selected results from a extragalactic tev sky was presented, with particular emphasis on recently detected distant sources.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
6353,"we present c and o abundances inside a magellanic clouds derived from deep spectra of hii regions. a data have been taken with a ultraviolet-visual echelle spectrograph at a 8.2-m vlt. a sample comprises 5 hii regions inside a large magellanic cloud (lmc) and 4 inside a small magellanic cloud (smc). we measure pure recombination lines (rls) of cii and oii inside all a objects, permitting to derive a abundance discrepancy factors (adfs) considering o^2+, as well as their o/h, c/h and c/o ratios. we compare a adfs with those of other hii regions inside different galaxies. a results suggest the possible metallicity dependence of a adf considering a low-metallicity objects, but more uncertain considering high-metallicity objects. we compare nebular and b-type stellar abundances and we find that a stellar abundances agree better with a nebular ones derived from collisionally excited lines (cels). comparing these results with other galaxies we observe that stellar abundances seem to agree better with a nebular ones derived from cels inside low-metallicity environments and from rls inside high-metallicity environments. a c/h, o/h and c/o ratios show almost flat radial gradients, inside contrast with a spiral galaxies where such gradients are negative. we explore a chemical evolution analysing c/o vs. o/h and comparing with a results of hii regions inside other galaxies. a lmc seems to show the similar chemical evolution to a external zones of small spiral galaxies and a smc behaves as the typical star-forming dwarf galaxy.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2086,"phylogenetic models have polynomial parametrization maps. considering symmetric group-based models, matsen studied a polynomial inequalities that characterize a joint probabilities inside a image of these parametrizations. we employ this description considering maximum likelihood approximation using numerical algebraic geometry. inside particular, we explore an example where a maximum likelihood approximate does not exist, which would be difficult to discover without with the help of algebraic methods. we also study a embedding problem considering symmetric group-based models, i.e. we identify which mutation matrices are matrix exponentials of rate matrices that are invariant under the group action.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
5287,"we study bivariate stochastic recurrence equations (sres) motivated by applications to garch(1,1) processes. if coefficient matrices of sres have strictly positive entries, then a kesten result applies and it gives solutions with regularly varying tails. moreover, a tail indices are a same considering all coordinates. however, considering applications, this framework was too restrictive. we study sres when coefficients are triangular matrices and prove that a coordinates of a solution may exhibit regularly varying tails with different indices. we also specify each tail index together with its constant. a results are used to characterize regular variations of bivariate stationary garch(1,1) processes.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
2350,"we consider the theory of the two-component dirac fermion localized on the (2+1) dimensional brane coupled to the (3+1) dimensional bulk. with the help of a fermionic particle-vortex duality, we show that a theory has the strong-weak duality that maps a coupling $e$ to $\tilde e=(8\pi)/e$. we explore a theory at $e^2=8\pi$ where it was self-dual. a electrical conductivity of a theory was the constant independent of frequency. when a system was at finite density and magnetic field at filling factor $\nu=\frac12$, a longitudinal and hall conductivity satisfies the semicircle law, and a ratio of a longitudinal and hall thermal electric coefficients was completely determined by a hall angle. a thermal hall conductivity was directly related to a thermal electric coefficients.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
11949,"inside settings where only unlabelled speech data was available, zero-resource speech technology needs to be developed without transcriptions, pronunciation dictionaries, or language modelling text. there are two central problems inside zero-resource speech processing: (i) finding frame-level feature representations which make it easier to discriminate between linguistic units (phones or words), and (ii) segmenting and clustering unlabelled speech into meaningful units. inside this thesis, we argue that the combination of top-down and bottom-up modelling was advantageous inside tackling these two problems. to address a problem of frame-level representation learning, we present a correspondence autoencoder (cae), the neural network trained with weak top-down supervision from an unsupervised term discovery system. by combining this top-down supervision with unsupervised bottom-up initialization, a cae yields much more discriminative features than previous approaches. we then present our unsupervised segmental bayesian model that segments and clusters unlabelled speech into hypothesized words. by imposing the consistent top-down segmentation while also with the help of bottom-up knowledge from detected syllable boundaries, our system outperforms several others on multi-speaker conversational english and xitsonga speech data. finally, we show that a clusters discovered by a segmental bayesian model should be made less speaker- and gender-specific by with the help of features from a cae instead of traditional acoustic features. inside summary, a different models and systems presented inside this thesis show that both top-down and bottom-up modelling should improve representation learning, segmentation and clustering of unlabelled speech data.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18795,"we give the simple, fast algorithm considering hyperparameter optimization inspired by techniques from a analysis of boolean functions. we focus on a high-dimensional regime where a canonical example was training the neural network with the large number of hyperparameters. a algorithm --- an iterative application of compressed sensing techniques considering orthogonal polynomials --- requires only uniform sampling of a hyperparameters and was thus easily parallelizable. experiments considering training deep neural networks on cifar-10 show that compared to state-of-the-art tools (e.g., hyperband and spearmint), our algorithm finds significantly improved solutions, inside some cases better than what was attainable by hand-tuning. inside terms of overall running time (i.e., time required to sample various settings of hyperparameters plus additional computation time), we are at least an order of magnitude faster than hyperband and bayesian optimization. we also outperform random search 8x. additionally, our method comes with provable guarantees and yields a first improvements on a sample complexity of learning decision trees inside over two decades. inside particular, we obtain a first quasi-polynomial time algorithm considering learning noisy decision trees with polynomial sample complexity.",1,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0
411,"adaptive regularization methods come inside diagonal and full-matrix variants. however, only a former have enjoyed widespread adoption inside training large-scale deep models. this was due to a computational overhead of manipulating the full matrix inside high dimension. inside this paper, we show how to make full-matrix adaptive regularization practical and useful. we present ggt, the truly scalable full-matrix adaptive optimizer. at a heart of our algorithm was an efficient method considering computing a inverse square root of the low-rank matrix. we show that ggt converges to first-order local minima, providing a first rigorous theoretical analysis of adaptive regularization inside non-convex optimization. inside preliminary experiments, ggt trains faster across the variety of synthetic tasks and standard deep learning benchmarks.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19725,"prediction suffix trees (pst) provide an effective tool considering sequence modelling and prediction. current prediction techniques considering psts rely on exact matching between a suffix of a current sequence and a previously observed sequence. we present the provably correct algorithm considering learning the pst with approximate suffix matching by relaxing a exact matching condition. we then present the self-bounded enhancement of our algorithm where a depth of suffix tree grows automatically inside response to a model performance on the training sequence. through experiments on synthetic datasets as well as three real-world datasets, we show that a approximate matching pst results inside better predictive performance than a other variants of pst.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15712,"recent learning-based super-resolution (sr) methods often focus on dictionary learning or network training. inside this paper, we discuss inside detail the new sr method based on local patch encoding (lpe) instead of traditional dictionary learning. a proposed method consists of the learning stage and the reconstructing stage. inside a learning stage, image patches are classified into different classes by means of a proposed lpe, and then the projection matrix was computed considering each class by utilizing the simple constraint. inside a reconstructing stage, an input lr patch should be simply reconstructed by computing its lpe code and then multiplying a corresponding projection matrix. furthermore, we discuss a relationship between a proposed method and a anchored neighborhood regression methods; we also analyze a extendibility of a proposed method. a experimental results on several image sets demonstrate a effectiveness of a lpe-based methods.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19857,"inside this paper, we introduce the novel community detection algorithm inside graphs, called scoda (streaming community detection algorithm), based on an edge streaming setting. this algorithm has an extremely low memory footprint and the lightning-fast execution time as it only stores two integers per node and processes each edge strictly once. a idea behind the method was based on a following simple observation: if we pick an edge uniformly at random inside a network, this edge was more likely to connect two nodes of a same community than two nodes of distinct communities. we exploit this idea to build communities by local changes at each edge arrival. with the help of theoretical arguments, we relate a ability of scoda to detect communities to usual quality metrics of these communities like a conductance. experimental results performed on massive real-life networks ranging from one million to more than one billion edges shows that scoda runs more than ten times faster than existing algorithms and leads to similar or better detection scores on a largest graphs.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
3358,"drug-target interaction (dti) prediction plays the very important role inside drug development and drug discovery. biochemical experiments or \textit{in vitro} methods are very expensive, laborious and time-consuming. therefore, \textit{in silico} approaches including docking simulation and machine learning have been proposed to solve this problem. inside particular, machine learning approaches have attracted increasing attentions recently. however, inside addition to a known drug-target interactions, most of a machine learning methods require extra characteristic information such as chemical structures, genome sequences, binding types and so on. whenever such information was not available, they may perform poor. very recently, a similarity-based link prediction methods were extended to bipartite networks, which should be applied to solve a dti prediction problem by with the help of topological information only. inside this work, we propose the method based on low-rank matrix projection to solve a dti prediction problem. on one hand, when there was no extra characteristic information of drugs or targets, a proposed method utilizes only a known interactions. on a other hand, a proposed method should also utilize a extra characteristic information when it was available and a performances will be remarkably improved. moreover, a proposed method should predict a interactions associated with new drugs or targets of which we know nothing about their associated interactions, but only some characteristic information. we compare a proposed method with ten baseline methods, e.g., six similarity-based methods that utilize only a known interactions and four methods that utilize a extra characteristic information. a datasets and codes implementing a simulations are available at this https url.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6100,"visual question answering (vqa) requires ai models to comprehend data inside two domains, vision and text. current state-of-the-art models use learned attention mechanisms to extract relevant information from a input domains to answer the certain question. thus, robust attention mechanisms are essential considering powerful vqa models. inside this paper, we propose the recurrent attention mechanism and show its benefits compared to a traditional convolutional approach. we introduce the baseline vqa model with visual attention and compare a performance difference between convolutional and recurrent attention on a vqa 2.0 dataset. furthermore, we experiment replacing attention mechanisms inside state-of-the-art models with our recurrent attention units (raus) and show increased performance. additionally, we design an architecture considering vqa which utilizes recurrent attention units to highlight a benefits of raus. our single model outperforms a first place winner on a vqa 2016 challenge and to a best of our knowledge, it was a second best performing single model on a vqa 1.0 dataset. furthermore, our model noticeably improves upon a winner of a vqa 2017 challenge.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3153,"this review paper summarizes a current state-of-art and challenges considering a future developments of fiber-reinforced composites considering structural applications with multifunctional capabilities. after the brief analysis of a reasons of a successful incorporation of fiber-reinforced composites inside many different industrial sectors, a review analyzes three critical factors that will define a future of composites. a first one was a application of novel fiber-deposition and preforming techniques together with innovative liquid moulding strategies, which will be combined by optimization tools based on novel multiscale modelling approaches, so fiber-reinforced composites with optimized properties should be designed and manufactured considering each application. inside addition, composite applications will be enhanced by a incorporation of multifunctional capabilities. among them, electrical conductivity, energy storage (structural supercapacitors and batteries) and energy harvesting (piezoelectric and solar energy) seem to be a most promising ones.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
9110,"we consider the general regularised interpolation problem considering learning the parameter vector from data. a well known representer theorem says that under certain conditions on a regulariser there exists the solution inside a linear span of a data points. this was a core of kernel methods inside machine learning as it makes a problem computationally tractable. necessary and sufficient conditions considering differentiable regularisers on hilbert spaces to admit the representer theorem have been proved. we extend those results to nondifferentiable regularisers on uniformly convex and uniformly smooth banach spaces. this gives the (more) complete answer to a question when there was the representer theorem. we then note that considering regularised interpolation inside fact a solution was determined by a function space alone and independent of a regulariser, making a extension to banach spaces even more valuable.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
9918,"traditional linear methods considering forecasting multivariate time series are not able to satisfactorily model a non-linear dependencies that may exist inside non-gaussian series. we build on a theory of learning vector-valued functions inside a reproducing kernel hilbert space and develop the method considering learning prediction functions that accommodate such non-linearities. a method not only learns a predictive function but also a matrix-valued kernel underlying a function search space directly from a data. our idea behind the method was based on learning multiple matrix-valued kernels, each of those composed of the set of input kernels and the set of output kernels learned inside a cone of positive semi-definite matrices. inside addition to superior predictive performance inside a presence of strong non-linearities, our method also recovers a hidden dynamic relationships between a series and thus was the new alternative to existing graphical granger techniques.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
13819,"we construct triply periodic zero mean curvature surfaces of mixed type inside a lorentz-minkowski 3-space, with a same topology as a triply periodic minimal surfaces inside a euclidean 3-space, called schwarz rpd surfaces.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6361,"wall-bounded flows experience the transition to turbulence characterized by a coexistence of laminar and turbulent domains inside some range of reynolds number r, a natural control parameter. this transitional regime takes place between an upper threshold rt above which turbulence was uniform (featureless) and the lower threshold rg below which any form of turbulence decays, possibly at a end of overlong chaotic transients. a most emblematic cases of flow along flat plates transiting to/from turbulence according to this scenario are reviewed. a coexistence was generally inside a form of bands, alternatively laminar and turbulent, and oriented obliquely with respect to a general flow direction. a final decay of a bands at rg points to a relevance of directed percolation and criticality inside a sense of statistical-physics phase transitions. a nature of a transition at rt where bands form was still somewhat mysterious and does not easily fit a scheme holding considering pattern-forming instabilities at increasing control parameter on the laminar background. inside contrast, a bands arise at rt out of the uniform turbulent background at the decreasing control parameter. ingredients of the possible theory of laminar-turbulent patterning are discussed.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18983,"we present the three-dimensional ising model where lines of equal spins are frozen inside such that they form an ordered framework structure. a frame spins impose an external field on a rest of a spins (active spins). we demonstrate that this ""porous ising model"" should be seen as the minimal model considering condensation transitions of gas molecules inside metal-organic frameworks. with the help of monte carlo simulation techniques, we compare a phase behavior of the porous ising model with that of the particle-based model considering a condensation of methane (ch$_4$) inside a isoreticular metal-organic framework irmof-16. considering both models, we find the line of first-order phase transitions that end inside the critical point. we show that a critical behavior inside both cases belongs to a 3d ising universality class, inside contrast to other phase transitions inside confinement such as capillary condensation.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
466,"we consider a problem of sampling from posterior distributions considering bayesian models where some parameters are restricted to be orthogonal matrices. such matrices are sometimes used inside neural networks models considering reasons of regularization and stabilization of training procedures, and also should parameterize matrices of bounded rank, positive-definite matrices and others. inside \citet{byrne2013geodesic} authors have already considered sampling from distributions over manifolds with the help of exact geodesic flows inside the scheme similar to hamiltonian monte carlo (hmc). we propose new sampling scheme considering the set of orthogonal matrices that was based on a same approach, uses ideas of riemannian optimization and does not require exact computation of geodesic flows. a method was theoretically justified by proof of symplecticity considering a proposed iteration. inside experiments we show that a new scheme was comparable or faster inside time per iteration and more sample-efficient comparing to conventional hmc with explicit orthogonal parameterization and geodesic monte-carlo. we also provide promising results of bayesian ensembling considering orthogonal neural networks and low-rank matrix factorization.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
13460,"pathfinding was the very popular area inside computer game development. while two-dimensional (2d) pathfinding was widely applied inside most of a popular game engines, little implementation of real three-dimensional (3d) pathfinding should be found. this research presents the dynamic search space optimization algorithm which should be applied to tessellate 3d search space unevenly, significantly reducing a total number of resulting nodes. a algorithm should be used with popular pathfinding algorithms inside 3d game engines. furthermore, the simplified standalone 3d pathfinding algorithm was proposed inside this paper. a proposed algorithm relies on ray-casting or line vision to generate the feasible path during runtime without requiring division of a search space into the 3d grid. both of a proposed algorithms are simulated on unreal engine to show innerworkings and resultant path comparison with a*. a advantages and shortcomings of a proposed algorithms are also discussed along with future directions.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2149,"we propose the weakly-supervised idea behind the method that takes image-sentence pairs as input and learns to visually ground (i.e., localize) arbitrary linguistic phrases, inside a form of spatial attention masks. specifically, a model was trained with images and their associated image-level captions, without any explicit region-to-phrase correspondence annotations. to this end, we introduce an end-to-end model which learns visual groundings of phrases with two types of carefully designed loss functions. inside addition to a standard discriminative loss, which enforces that attended image regions and phrases are consistently encoded, we propose the novel structural loss which makes use of a parse tree structures induced by a sentences. inside particular, we ensure complementarity among a attention masks that correspond to sibling noun phrases, and compositionality of attention masks among a children and parent phrases, as defined by a sentence parse tree. we validate a effectiveness of our idea behind the method on a microsoft coco and visual genome datasets.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1828,"this manuscript attempts to present the way inside which a classical construction of a dirac operator should be carried over to a setting of diffeology. the more specific aim was to describe the procedure considering gluing together two usual dirac operators and to explain inside what sense a result was again the dirac operator. since versions of cut-and-paste (surgery) operations have already appeared inside a context of atiyah-singer theory, we specify that our gluing procedure was designed to lead to spaces that are not smooth manifolds inside any ordinary sense, and since much attention has been paid inside recent years to dirac operators on spaces with singularities, we also specify that our idea behind the method was more of the piecewise-linear nature (although, hopefully, singular spaces inside the more analytic sense will enter a picture sooner or later; but this work was not yet about them). most of it was devoted to a diffeological versions of a components that go into a standard definition of the dirac operator as a composition of the clifford connection with clifford action by sections of a cotangent bundle; the diffeological dirac operator was then standardly defined.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5224,"a vanishing gradient problem is the major obstacle considering a success of deep learning. inside recent years it is gradually alleviated through multiple different techniques. however a problem is not really overcome inside the fundamental way, since it was inherent to neural networks with activation functions based on dot products. inside the series of papers, we are going to analyze alternative neural network structures which are not based on dot products. inside this first paper, we revisit neural networks built up of layers based on distance measures and gaussian activation functions. these kinds of networks were only sparsely used inside a past since they are hard to train when with the help of plain stochastic gradient descent methods. we show that by with the help of root mean square propagation (rmsprop) it was possible to efficiently learn multi-layer neural networks. furthermore we show that when appropriately initialized these kinds of neural networks suffer much less from a vanishing and exploding gradient problem than traditional neural networks even considering deep networks.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15278,"this work presents the study on a extraction and analysis of the set of 101 categories of eye movement features from three types of eye movement events: fixations, saccades, and post-saccadic oscillations. a eye movements were recorded during the reading task. considering a categories of features with multiple instances inside the recording we extract corresponding feature subtypes by calculating descriptive statistics on a distributions of these instances. the unified framework of detailed descriptions and mathematical formulas are provided considering a extraction of a feature set. a analysis of feature values was performed with the help of the large database of eye movement recordings from the normative population of 298 subjects. we demonstrate a central tendency and overall variability of feature values over a experimental population, and more importantly, we quantify a test-retest reliability (repeatability) of each separate feature. a described methods and analysis should provide valuable tools inside fields exploring a eye movements, such as inside behavioral studies, attention and cognition research, medical research, biometric recognition, and human-computer interaction.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15324,"we consider riemannian 4-manifolds that gromov-hausdorff converge to the lower dimensional limit space, with a ricci tensor going to zero. among other things, we show that if a limit space was two dimensional then under some mild assumptions, a limiting four dimensional geometry away from a curvature blowup region was semiflat kaehler.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3008,"we consider a cauchy problem considering a nonlinear schr√∂dinger equation on a whole space. after introducing the weaker concept of finite speed of propagation, we show that a concatenation of initial data gives rise to solutions whose time of existence increases as one translates one of a initial data. moreover, we show that, given global decaying solutions with initial data $u_0, v_0$, if $|y|$ was large, then a concatenated initial data $u_0+v_0(\cdot -y)$ gives rise to globally decaying solutions.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10266,"we consider a inverse problems of determining a potential or a damping coefficient appearing inside a wave equation. we will prove a unique determination of these coefficients from a one point measurement. since our problem was under-determined, so some extra assumption on a coefficients was required to prove a uniqueness.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9452,"we use the numerical solution of a deterministic tdgl equations to determine a response induced by the probe field inside the material quenched into the superconducting state. we characterize differences inside response according to whether a probe was applied before, during, or after a phase stiffness has built up to its final steady state value.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
114,"foreign policy analysis has been struggling to find ways to measure policy preferences and paradigm shifts inside international political systems. this paper presents the novel, potential solution to this challenge, through a application of the neural word embedding (word2vec) model on the dataset featuring speeches by heads of state or government inside a united nations general debate. a paper provides three key contributions based on a output of a word2vec model. first, it presents the set of policy attention indices, synthesizing a semantic proximity of political speeches to specific policy themes. second, it introduces country-specific semantic centrality indices, based on topological analyses of countries' semantic positions with respect to each other. third, it tests a hypothesis that there exists the statistical relation between a semantic content of political speeches and un voting behavior, falsifying it and suggesting that political speeches contain information of different nature then a one behind voting outcomes. a paper concludes with the discussion of a practical use of its results and consequences considering foreign policy analysis, public accountability, and transparency.",1,0,0,1,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19906,"the large fraction of giant planets have gaseous envelopes that are limited to about 10 % of their total mass budget. such planets are present inside a solar system (uranus, neptune) and are frequently observed inside short periods around other stars (the so-called super-earths). inside contrast to these observations, theoretical calculations based on a evolution of hydrostatic envelopes argue that such low mass envelopes cannot be maintained around cores exceeding five earth masses. instead, under nominal disc conditions, these planets would acquire massive envelopes through runaway gas accretion within a lifetime of a protoplanetary disc. inside this work, we show that planetary envelopes are not inside hydrostatic balance, which slows down envelope growth. the series of 3-dimensional, global, radiative hydrodynamical simulations reveal the steady state gas flow, which enters through a poles and exits inside a disc midplane. gas was pushed through a outer envelope inside about 10 orbital timescales. inside regions of a disc that are not significantly dust-depleted, envelope accretion onto cores of about five earth masses should get stalled as a gas flow enters a deep interior. accreted solids sublimate deep inside a convective interior, but small opacity-providing grains are trapped inside a flow and do not settle, which further prevents rapid envelope accretion. a transition to runaway gas accretion should however be reached when cores grow larger than typical super-earths, beyond 15 earth masses, and preferably when disc opacities are below kappa=1 cm^2/g. these findings offer an explanation considering a typical low-mass envelopes around a cores of super-earths.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7648,"we extend teichmueller dynamics to the flow on a total space of the flat bundle of deformation spaces of representations of a fundamental group of the fixed surface s inside the lie group g. a resulting dynamical system was the continuous version of a action of a mapping class group of s on a deformation space. we observe how ergodic properties of this action relate to this flow. when g was compact, this flow was strongly mixing over each component of a derormation space and of each stratum of a teichmueller unit sphere bundle over a riemann moduli space. we prove ergodicity considering a analogous lift of a weil-petersson geodesic local. flow.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15523,"we prove the new off-diagonal asymptotic of a bergman kernels associated to tensor powers of the positive line bundle on the compact k√§hler manifold. we show that if a k√§hler potential was real analytic, then a bergman kernel accepts the complete asymptotic expansion inside the neighborhood of a diagonal of shrinking size $k^{-\frac14}$. these improve a earlier results inside a subject considering smooth potentials, where an expansion exists inside the $k^{-\frac12}$ neighborhood of a diagonal. we obtain our results by finding upper bounds of a form $c^m m!^{2}$ considering a bergman coefficients $b_m(x, \bar y)$, which was an interesting problem on its own. we find such upper bounds with the help of a method of berman-berndtsson-sj√∂strand. we also show that sharpening these upper bounds would improve a rate of shrinking neighborhoods of a diagonal $x=y$ inside our results. inside a special case of metrics with local constant holomorphic sectional curvatures, we obtain off-diagonal asymptotic inside the fixed (as $k \to \infty$) neighborhood of a diagonal, which recovers the result of berman [ber] (see remark 3.5 of [ber] considering higher dimensions). inside this case, we also find an explicit formula considering a bergman kernel mod $o(e^{-k \delta} )$.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
166,"we consider 3+1 rotationally symmetric lorentzian einstein spacetime manifolds with $\lambda >0$ and reduce a equations to 2+1 einstein equations coupled to `shifted' wave maps. subsequently, we prove various (explicit) positive mass-energy theorems. no smallness was assumed.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12278,"inside this review article, we show our recent results relating to a undoped (ce-free) superconductivity inside a electron-doped high-tc cuprates with a so-called t' structure. considering an introduction, we briefly mention a characteristics of a electron-doped t'-cuprates, including a reduction annealing, conventional phase diagram and undoped superconductivity. then, our transport and magnetic results and results relating to a superconducting pairing symmetry of a undoped and underdoped t'-cuprates are shown. collaborating spectroscopic and nuclear magnetic resonance results are also shown briefly. it has been found that, through a reduction annealing, the strongly localized state of carriers accompanied by an antiferromagnetic pseudogap inside a as-grown samples changes to the metallic and superconducting state with the short-range magnetic order inside a reduced superconducting samples. a formation of a short-range magnetic order due to the very small amount of excess oxygen inside a reduced superconducting samples suggests that a t'-cuprates exhibiting a undoped superconductivity inside a parent compounds are regarded as strongly correlated electron systems, as well as a hole-doped high-tc cuprates. we show our proposed electronic structure model to understand a undoped superconductivity. finally, unsolved future issues of a t'-cuprates are discussed.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
12461,"this paper presents the convolutional neural network based idea behind the method considering estimating a relative pose between two cameras. a proposed network takes rgb images from both cameras as input and directly produces a relative rotation and translation as output. a system was trained inside an end-to-end manner utilising transfer learning from the large scale classification dataset. a introduced idea behind the method was compared with widely used local feature based methods (surf, orb) and a results indicate the clear improvement over a baseline. inside addition, the variant of a proposed architecture containing the spatial pyramid pooling (spp) layer was evaluated and shown to further improve a performance.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13577,"no man was an island, as individuals interact and influence one another daily inside our society. when social influence takes place inside experiments on the population of interconnected individuals, a treatment on the unit may affect a outcomes of other units, the phenomenon known as interference. this thesis develops the causal framework and inference methodology considering experiments where interference takes place on the network of influence (i.e. network interference). inside this framework, a network potential outcomes serve as a key quantity and flexible building blocks considering causal estimands that represent the variety of primary, peer, and total treatment effects. these causal estimands are estimated using principled bayesian imputation of missing outcomes. a theory on a unconfoundedness assumptions leading to simplified imputation highlights a importance of including relevant network covariates inside a potential outcome model. additionally, experimental designs that result inside balanced covariates and sizes across treatment exposure groups further improve a causal estimate, especially by mitigating potential outcome model mis-specification. a true potential outcome model was not typically known inside real-world experiments, so a best practice was to account considering interference and confounding network covariates through both balanced designs and model-based imputation. the full factorial simulated experiment was formulated to demonstrate this principle by comparing performance across different randomization schemes during a design phase and estimators during a analysis phase, under varying network topology and true potential outcome models. overall, this thesis asserts that interference was not just the nuisance considering analysis but rather an opportunity considering quantifying and leveraging peer effects inside real-world experiments.",0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
3619,"inside recent years, deep learning based on artificial neural network (ann) has achieved great success inside pattern recognition. however, there was no clear understanding of such neural computational models. inside this paper, we try to unravel ""black-box"" structure of ann model from network flow. specifically, we consider a feed forward ann as the network flow model, which consists of many directional class-pathways. each class-pathway encodes one class. a class-pathway of the class was obtained by connecting a activated neural nodes inside each layer from input to output, where activation value of neural node (node-value) was defined by a weights of each layer inside the trained ann-classifier. from a perspective of a class-pathway, training an ann-classifier should be regarded as a formulation process of class-pathways of different classes. by analyzing a a distances of each two class-pathways inside the trained ann-classifiers, we try to answer a questions, why a classifier performs so? at last, from a neural encodes view, we define a importance of each neural node through a class-pathways, which was helpful to optimize a structure of the classifier. experiments considering two types of ann model including multi-layer mlp and cnn verify that a network flow based on class-pathway was the reasonable explanation considering ann models.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7144,"monte carlo method was the broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. they are often used inside physical and mathematical problems and are most useful when it was difficult or impossible to use other mathematical methods. basically, many statisticians have been increasingly drawn to monte carlo method inside three distinct problem classes: optimization, numerical integration, and generating draws from the probability distribution. inside this paper, we will introduce a monte carlo method considering calculating coefficients inside generalized linear model(glm), especially considering logistic regression. our main methods are metropolis hastings(mh) algorithms and stochastic approximation inside monte carlo computation(samc). considering comparison, we also get results automatically with the help of mle method inside r software.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4908,"a classification of mri images according to a anatomical field of view was the necessary task to solve when faced with a increasing quantity of medical images. inside parallel, advances inside deep learning makes it the suitable tool considering computer vision problems. with the help of the common architecture (such as alexnet) provides quite good results, but not sufficient considering clinical use. improving a model was not an easy task, due to a large number of hyper-parameters governing both a architecture and a training of a network, and to a limited understanding of their relevance. since an exhaustive search was not tractable, we propose to optimize a network first by random search, and then by an adaptive search based on gaussian processes and probability of improvement. applying this method on the large and varied mri dataset, we show the substantial improvement between a baseline network and a final one (up to 20\% considering a most difficult classes).",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17639,"platooning allows vehicles to travel with small intervehicle distance inside the coordinated fashion thanks to vehicle-to-vehicle connectivity. when applied at the larger scale, platooning will create significant opportunities considering energy savings due to reduced aerodynamic drag, as well as increased road capacity and congestion reduction resulting from shorter vehicle headways. however, these potential savings are maximized if platooning-capable vehicles spend most of their travel time within platoons. ad hoc platoon formation may not ensure the high rate of platoon driving. inside this paper we consider a problem of central coordination of platooning-capable vehicles. by coordinating their routes and departure times, we should maximize a fuel savings afforded by platooning vehicles. a resulting problem was the combinatorial optimization problem that considers a platoon coordination and vehicle routing problems simultaneously. we demonstrate our methodology by evaluating a benefits of the coordinated solution and comparing it with a uncoordinated case when platoons form only inside an ad hoc manner. we compare a coordinated and uncoordinated scenarios on the grid network with different assumptions about demand and a time vehicles are willing to wait.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
18301,"humans should understand and produce new utterances effortlessly, thanks to their compositional skills. once the person learns a meaning of the new verb ""dax,"" he or she should immediately understand a meaning of ""dax twice"" or ""sing and dax."" inside this paper, we introduce a scan domain, consisting of the set of simple compositional navigation commands paired with a corresponding action sequences. we then test a zero-shot generalization capabilities of the variety of recurrent neural networks (rnns) trained on scan with sequence-to-sequence methods. we find that rnns should make successful zero-shot generalizations when a differences between training and test commands are small, so that they should apply ""mix-and-match"" strategies to solve a task. however, when generalization requires systematic compositional skills (as inside a ""dax"" example above), rnns fail spectacularly. we conclude with the proof-of-concept experiment inside neural machine translation, suggesting that lack of systematicity might be partially responsible considering neural networks' notorious training data thirst.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2984,"filamentary regions of high vorticity irregularly form and disappear inside a turbulent flows of classical fluids. we report an experimental comparative study of these so-called "" coherent structures "" inside the classical versus quantum fluid, with the help of liquid helium with the superfluid fraction varied from 0% up to 83%. a low pressure core of a vorticity filaments was detected by pressure probes located on a sidewall of the 78-cm-diameter von k√°rm√°n cell driven up to record turbulent intensity (r $\lambda$ $\sim$ $\sqrt$ re 10000). a statistics of occurrence, magnitude and relative distribution of a filaments inside the classical fluid are found indistinguishable from their superfluid counterpart, namely a bundles of quantized vortex lines. this suggest that a internal structure of vortex filaments, as well as their dissipative properties have the negligible impact on their macroscopic dynamics, such as lifetime and intermittent properties.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2346,"deep generative models trained with large amounts of unlabelled data have proven to be powerful within a domain of unsupervised learning. many real life data sets contain the small amount of labelled data points, that are typically disregarded when training generative models. we propose a cluster-aware generative model, that uses unlabelled information to infer the latent representation that models a natural clustering of a data, and additional labelled data points to refine this clustering. a generative performances of a model significantly improve when labelled information was exploited, obtaining the log-likelihood of -79.38 nats on permutation invariant mnist, while also achieving competitive semi-supervised classification accuracies. a model should also be trained fully unsupervised, and still improve a log-likelihood performance with respect to related methods.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17470,"many of a fascinating and unconventional properties of several transition-metal compounds with partially filled d-shells are due to strong electronic correlations. while local correlations are inside principle treated exactly within a frame of a dynamical mean-field theory, there are two major and interlinked routes considering important further methodical advances: on a one hand, there was the strong need considering methods being able to describe material-specific aspects, i.e., methods combining a dmft with modern band-structure theory, and, on a other hand, nonlocal correlations beyond a mean-field paradigm must be accounted for. referring to several concrete example systems, we argue why these two routes are worth pursuing and how they should be combined, we describe several related methodical developments and present respective results, and we discuss possible ways to overcome remaining obstacles.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
19684,"recent work on a representation of functions on sets has considered a use of summation inside the latent space to enforce permutation invariance. inside particular, it has been conjectured that a dimension of this latent space may remain fixed as a cardinality of a sets under consideration increases. however, we demonstrate that a analysis leading to this conjecture requires mappings which are highly discontinuous and argue that this was only of limited practical use. motivated by this observation, we prove that an implementation of this model using continuous mappings (as provided by e.g. neural networks or gaussian processes) actually imposes the constraint on a dimensionality of a latent space. practical universal function representation considering set inputs should only be achieved with the latent dimension at least a size of a maximum number of input elements.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
18483,inside this paper we develop methods to extend a minimal hypersurface idea behind the method to positive scalar curvature problems to all dimensions. this includes the proof of a positive mass theorem inside all dimensions without the spin assumption. it also includes statements about a structure of compact manifolds of positive scalar curvature extending a work of \cite{sy1} to all dimensions. a technical work inside this paper was to construct minimal slicings and associated weight functions inside a presence of small singular sets and to show that a singular sets do not become too large inside a lower dimensional slices. it was shown that a singular set inside any slice was the closed set with hausdorff codimension at least three. inside particular considering arguments which involve slicing down to dimension $1$ or $2$ a method was successful. a arguments should be viewed as an extension of a minimal hypersurface regularity theory to this setting of minimal slicings.,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17419,"we report a effects of heavy-ion irradiation on fese single crystals by irradiating uranium up to the dose equivalent matching field of $b_\phi$ = 16 t. almost continuous columnar defects along a $c$-axis with the diameter $\sim$10 nm are confirmed by high-resolution transmission electron microscopy. $t_c$ was found to be suppressed by introducing columnar defects at the rate of d$t_c$/d$b_\phi$ $\sim$ -0.29kt$^{-1}$, which was much larger than those observed inside iron pnictides. this unexpected large suppression of $t_c$ inside fese was discussed inside relation to a large diameter of a columnar defects as well as its unique band structure with the remarkably small fermi energy. a critical current density was first dramatically enhanced with irradiation reaching the value over $\sim$2$\times$10$^5$ a/cm$^2$ ($\sim$5 times larger than that of a pristine sample) at 2 k (self-field) with $b_\phi$ = 2 t, then gradually suppressed with increasing $b_\phi$. a $\delta$$l$-pinning associated with charge-carrier mean free path fluctuations, and a $\delta$$t_c$-pinning associated with spatial fluctuations of a transition temperature are found to coexist inside a pristine fese, while a irradiation increases a contribution from $\delta$$l$-pinning, and makes it dominant over $b_\phi$ = 4 t.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
13534,"online-learning research has mainly been focusing on minimizing one objective function. inside many real-world applications, however, several objective functions have to be considered simultaneously. recently, an algorithm considering dealing with several objective functions inside a i.i.d. case has been presented. inside this paper, we extend a multi-objective framework to a case of stationary and ergodic processes, thus allowing dependencies among observations. we first identify an asymptomatic lower bound considering any prediction strategy and then present an algorithm whose predictions achieve a optimal solution while fulfilling any continuous and convex constraining criterion.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11019,"the new understanding of a stability of self-interacting dark matter was pointed out, based on a simplest spontaneously broken abelian $u(1)$ gauge model with one complex scalar and one dirac fermion. a key was a imposition of dark charge conjugation symmetry. it allows a possible existence of two stable particles: a dirac fermion and a vector gauge boson which acts as the light mediator considering a former's self-interaction. since this light mediator does not decay, it avoids a strong cosmological constraints recently obtained considering all such models where a light mediator decays into standard-model particles.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9,"we present mcda, the modification of a corot detrend algorithm (cda) suitable to detrend chromatic light curves. by means of robust statistics and better handling of short term variability, a implementation decreases a systematic light curve variations and improves a detection of exoplanets when compared with a original algorithm. all corot chromatic light curves (a total of 65,655) were analysed with our algorithm. dozens of new transit candidates and all previously known corot exoplanets were rediscovered inside those light curves with the help of the box-fitting algorithm. considering three of a new cases spectroscopic measurements of a candidates' host stars were retrieved from a eso science archive facility and used to calculate stellar parameters and, inside a best cases, radial velocities. inside addition to our improved detrend technique we announce a discovery of the planet that orbits the $0.79_{-0.09}^{+0.08}\,r_\odot$ star with the period of $6.71837\pm0.00001$ days and has $0.57_{-0.05}^{+0.06}\,r_{\rm j}$ and $0.15\pm0.10\,m_{\rm j}$. we also present a analysis of two cases inside which parameters found suggest a existence of possible planetary companions.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8923,"smart cities must integrate the number of interdependent cyber-physical systems that operate inside the coordinated manner to improve a well-being of a city's residents. the cyber-physical system (cps) was the system of computational elements controlling physical entities. large-scale cpss are more vulnerable to attacks due to a cyber-physical interdependencies that should lead to cascading failures which should have the significant detrimental effect on the city. inside this paper, the novel idea behind the method was proposed considering analyzing a problem of allocating security resources, such as firewalls and anti-malware, over a various cyber components of an interdependent cps to protect a system against imminent attacks. a problem was formulated as the colonel blotto game inside which a attacker seeks to allocate its resources to compromise a cps, while a defender chooses how to distribute its resources to defend against potential attacks. to evaluate a effects of defense and attack, various cps factors are considered including human-cps interactions as well as physical and topological characteristics of the cps such as flow and capacity of interconnections and minimum path algorithms. results show that, considering a case inside which a attacker was not aware of a cps interdependencies, a defender should have the higher payoff, compared to a case inside which a attacker has complete information. a results also show that, inside a case of more symmetric nodes, due to interdependencies, a defender achieves its highest payoff at a equilibrium compared to a case with independent, asymmetric nodes.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
19679,"this work explores a trade-off between a number of samples required to accurately build models of dynamical systems and a degradation of performance inside various control objectives due to the coarse approximation. inside particular, we show that simple models should be easily fit from input/output data and are sufficient considering achieving various control objectives. we derive bounds on a number of noisy input/output samples from the stable linear time-invariant system that are sufficient to guarantee that a corresponding finite impulse response approximation was close to a true system inside a $\mathcal{h}_\infty$-norm. we demonstrate that these demands are lower than those derived inside prior art which aimed to accurately identify dynamical models. we also explore how different physical input constraints, such as power constraints, affect a sample complexity. finally, we show how our analysis fits within a established framework of robust control, by demonstrating how the controller designed considering an approximate system provably meets performance objectives on a true system.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0
17333,"human trafficking was one of a most atrocious crimes and among a challenging problems facing law enforcement which demands attention of global magnitude. inside this study, we leverage textual data from a website ""backpage""- used considering classified advertisement- to discern potential patterns of human trafficking activities which manifest online and identify advertisements of high interest to law enforcement. due to a lack of ground truth, we rely on the human analyst from law enforcement, considering hand-labeling the small portion of a crawled data. we extend a existing laplacian svm and present s3vm-r, by adding the regularization term to exploit exogenous information embedded inside our feature space inside favor of a task at hand. we train a proposed method with the help of labeled and unlabeled data and evaluate it on the fraction of a unlabeled data, herein referred to as unseen data, with our expert's further verification. results from comparisons between our method and other semi-supervised and supervised approaches on a labeled data demonstrate that our learner was effective inside identifying advertisements of high interest to law enforcement",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17323,"we develope the self-consistent description of a broad line region based on a concept of a failed wind powered by a radiation pressure acting on dusty accretion disk atmosphere inside keplerian motion. a material raised high above a disk was illuminated, dust evaportes, and a matter falls back towards a disk. this material was a source of emission lines. a model predicts a inner and outer radius of a region, a cloud dynamics under a dust radiation pressure and, subsequently, just a gravitational field of a central black hole, which results inside assymetry between a rise and fall. knowledge of a dynamics allows to predict a shapes of a emission lines as functions of a basic parameters of an active nucleus: black hole mass, accretion rate, black hole spin (or accretion efficiency) and a viewing angle with respect to a symmetry axis. here we show preliminary results based on analytical approximations to a cloud motion.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
650,"we establish the bijective correspondence between certain non-self-intersecting curves inside an $n$-punctured disc and positive ${\mathbf c}$-vectors of acyclic cluster algebras whose quivers have multiple arrows between every pair of vertices. as the corollary, we obtain the proof of the conjecture by k.-h. lee and k. lee (arxiv:1703.09113) on a combinatorial description of real schur roots considering acyclic quivers with multiple arrows, and give the combinatorial characterization of seeds inside terms of curves inside an $n$-punctured disc.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
10222,"machine learning algorithms based on deep neural networks have achieved remarkable results and are being extensively used inside different domains. however, a machine learning algorithms requires access to raw data which was often privacy sensitive. to address this issue, we develop new techniques to provide solutions considering running deep neural networks over encrypted data. inside this paper, we develop new techniques to adopt deep neural networks within a practical limitation of current homomorphic encryption schemes. more specifically, we focus on classification of a well-known convolutional neural networks (cnn). first, we design methods considering approximation of a activation functions commonly used inside cnns (i.e. relu, sigmoid, and tanh) with low degree polynomials which was essential considering efficient homomorphic encryption schemes. then, we train convolutional neural networks with a approximation polynomials instead of original activation functions and analyze a performance of a models. finally, we implement convolutional neural networks over encrypted data and measure performance of a models. our experimental results validate a soundness of our idea behind the method with several convolutional neural networks with varying number of layers and structures. when applied to a mnist optical character recognition tasks, our idea behind the method achieves 99.52\% accuracy which significantly outperforms a state-of-the-art solutions and was very close to a accuracy of a best non-private version, 99.77\%. also, it should make close to 164000 predictions per hour. we also applied our idea behind the method to cifar-10, which was much more complex compared to mnist, and were able to achieve 91.5\% accuracy with approximation polynomials used as activation functions. these results show that cryptodl provides efficient, accurate and scalable privacy-preserving predictions.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14995,"a internet and online forums such as reddit have become an increasingly popular medium considering citizens to engage inside political conversations. however, a online disinhibition effect resulting from a ability to use pseudonymous identities may manifest inside a form of offensive speech, consequently making political discussions more aggressive and polarizing than they already are. such environments may result inside harassment and self-censorship from its targets. inside this paper, we present preliminary results from the large-scale temporal measurement aimed at quantifying offensiveness inside online political discussions. to enable our measurements, we develop and evaluate an offensive speech classifier. we then use this classifier to quantify and compare offensiveness inside a political and general contexts. we perform our study with the help of the database of over 168m reddit comments made by over 7m pseudonyms between january 2015 and january 2017 -- the period covering several divisive political events including a 2016 us presidential elections.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
9836,"considering the particular disease there may be two diagnostic tests developed, where each of a tests was subject to several studies. the quadrivariate generalized linear mixed model (glmm) has been recently proposed to joint meta-analyse and compare two diagnostic tests. we propose the d-vine copula mixed model considering joint meta-analysis and comparison of two diagnostic tests. our general model includes a quadrivariate glmm as the special case and should also operate on a original scale of sensitivities and specificities. a method allows a direct calculation of sensitivity and specificity considering each test, as well as, a parameters of a summary receiver operator characteristic (sroc) curve, along with the comparison between a srocs of each test. our methodology was demonstrated with an extensive simulation study and illustrated by meta-analysing two examples where 2 tests considering a diagnosis of the particular disease are compared. our study suggests that there should be an improvement on glmm inside fit to data since our model should also provide tail dependencies and asymmetries.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14999,"we investigate a stability of the statistically stationary conductive state considering rayleigh-b√©nard convection between stress-free plates that arises due to the bulk stochastic internal heating. this setup may be seen as the generalization to the stochastic setting of a seminal 1916 study of lord rayleigh. our results indicate that stochastic forcing at small magnitude has the stabilizing effect, while strong stochastic forcing has the destabilizing effect. a methodology put forth inside this article, which combines rigorous analysis with careful computation, also provides an idea behind the method to hydrodynamic stability considering the variety of systems subject to the large scale stochastic forcing.",0,1,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12606,"inside a experimental electroluminescence (el) spectra of light-emitting diodes (leds) based on n-polar (in,ga)n/gan nanowires (nws), we observed the double peak structure. a relative intensity of a two peaks evolves inside the peculiar way with injected current. spatially and spectrally resolved el maps confirmed a presence of two main transitions inside a spectra, and suggested that they are emitted by a majority of single nano-leds. inside order to elucidate a physical origin of this effect, we performed theoretical calculations of a strain, electric field, and charge density distributions both considering planar leds and nw-leds. on this basis, we simulated also a el spectra of these devices, which exhibit the double peak structure considering n-polar heterostructures, both inside a nw and a planar case. inside contrast, this feature was not observed when ga-polar planar leds are simulated. we found that a physical origin of a double peak structure was the stronger quantum-confined stark effect occurring inside a first and last quantum well of a n-polar heterostructures. a peculiar evolution of a relative peak intensities with injected current, seen only inside a case of a nw-led, was attributed to a three-dimensional strain variation resulting from elastic relaxation at a free sidewalls of a nws. therefore, this study provides important insights on a working principle of n-polar leds based on both planar and nw heterostructures.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
14685,"a main result of this note was a existence of martingale solutions to a stochastic heat equation (she) inside the riemannian manifold by with the help of suitable dirichlet forms on a corresponding path/loop space. moreover, we present some characterizations of a lower bound of a ricci curvature by functional inequalities of various associated dirichlet forms.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13619,"conditional preference networks (cp-nets) are the graphical representation of the person's (conditional) preferences over the set of discrete variables. inside this paper, we introduce the novel method of quantifying preference considering any given outcome based on the cp-net representation of the user's preferences. we demonstrate that these values are useful considering reasoning about user preferences. inside particular, they allow us to order (any subset of) a possible outcomes inside accordance with a user's preferences. further, these values should be used to improve a efficiency of outcome dominance testing. that is, given the pair of outcomes, we should determine which a user prefers more efficiently. through experimental results, we show that this method was more effective than existing techniques considering improving dominance testing efficiency. we show that a above results also hold considering cp-nets that express indifference between variable values.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
16607,"internet social networks have become the ubiquitous application allowing people to easily share text, pictures, and audio and video files. popular networks include whatsapp, facebook, reddit and linkedin. we present an extensive study of a usage of a whatsapp social network, an internet messaging application that was quickly replacing sms messaging. inside order to better understand people's use of a network, we provide an analysis of over 6 million messages from over 100 users, with a objective of building demographic prediction models with the help of activity data. we performed extensive statistical and numerical analysis of a data and found significant differences inside whatsapp usage across people of different genders and ages. we also inputted a data into a weka data mining package and studied models created from decision tree and bayesian network algorithms. we found that different genders and age demographics had significantly different usage habits inside almost all message and group attributes. we also noted differences inside users' group behavior and created prediction models, including a likelihood the given group would have relatively more file attachments, if the group would contain the larger number of participants, the higher frequency of activity, quicker response times and shorter messages. we were successful inside quantifying and predicting the user's gender and age demographic. similarly, we were able to predict different types of group usage. all models were built without analyzing message content. we present the detailed discussion about a specific attributes that were contained inside all predictive models and suggest possible applications based on these results.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
7769,"most of researches on image forensics have been mainly focused on detection of artifacts introduced by the single processing tool. they lead inside a development of many specialized algorithms looking considering one or more particular footprints under specific settings. naturally, a performance of such algorithms are not perfect, and accordingly a provided output might be noisy, inaccurate and only partially correct. furthermore, the forged image inside practical scenarios was often a result of utilizing several tools available by image-processing software systems. therefore, reliable tamper detection requires developing more poweful tools to deal with various tempering scenarios. fusion of forgery detection tools based on fuzzy inference system has been used before considering addressing this problem. adjusting a membership functions and defining proper fuzzy rules considering attaining to better results are time-consuming processes. this should be accounted as main disadvantage of fuzzy inference systems. inside this paper, the neuro-fuzzy inference system considering fusion of forgery detection tools was developed. a neural network characteristic of these systems provides appropriate tool considering automatically adjusting a membership functions. moreover, initial fuzzy inference system was generated based on fuzzy clustering techniques. a proposed framework was implemented and validated on the benchmark image splicing data set inside which three forgery detection tools are fused based on adaptive neuro-fuzzy inference system. a outcome of a proposed method reveals that applying neuro fuzzy inference systems could be the better idea behind the method considering fusion of forgery detection tools.",1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7152,"considering applications as varied as bayesian neural networks, determinantal point processes, elliptical graphical models, and kernel learning considering gaussian processes (gps), one must compute the log determinant of an $n \times n$ positive definite matrix, and its derivatives - leading to prohibitive $\mathcal{o}(n^3)$ computations. we propose novel $\mathcal{o}(n)$ approaches to estimating these quantities from only fast matrix vector multiplications (mvms). these stochastic approximations are based on chebyshev, lanczos, and surrogate models, and converge quickly even considering kernel matrices that have challenging spectra. we leverage these approximations to develop the scalable gaussian process idea behind the method to kernel learning. we find that lanczos was generally superior to chebyshev considering kernel learning, and that the surrogate idea behind the method should be highly efficient and accurate with popular kernels.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8897,"let $\mathcal {w}^k(\mathfrak{sl}_4, f_{\text {subreg}})$ be a universal $\mathcal{w}$-algebra associated to $\mathfrak{sl}_4$ with its subregular nilpotent element, and let $\mathcal {w}_k(\mathfrak{sl}_4, f_{\text {subreg}})$ be its simple quotient. there was the heisenberg subalgebra $\mathcal{h}$, and we denote by $\mathcal{c}^k$ a coset $\text{com}(\mathcal{h}, \mathcal {w}^k(\mathfrak{sl}_4, f_{\text {subreg}}))$, and by $\mathcal{c}_k$ its simple quotient. we show that considering $k=-4+(m+4)/3$ where $m$ was an integer greater than $2$ and $m+1$ was coprime to $3$, $\mathcal{c}_k$ was isomorphic to the rational, regular $\mathcal w$-algebra $\mathcal{w}(\mathfrak{sl}_m, f_{\text{reg}})$. inside particular, $\mathcal{w}_k(\mathfrak{sl}_4, f_{\text {subreg}})$ was the simple current extension of a tensor product of $\mathcal{w}(\mathfrak{sl}_m, f_{\text{reg}})$ with the rank one lattice vertex operator algebra, and thus was rational.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
15117,"astrophysicists are interested inside recovering a 3d gas emissivity of the galaxy cluster from the 2d image taken by the telescope. the blurring phenomenon and presence of point sources make this inverse problem even harder to solve. a current state-of-the-art technique was two step: first identify a location of potential point sources, then mask these locations and deproject a data. we instead model a data as the poisson generalized linear model (involving blurring, abel and wavelets operators) regularized by two lasso penalties to induce sparse wavelet representation and sparse point sources. a amount of sparsity was controlled by two quantile universal thresholds. as the result, our method outperforms a existing one.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
13447,"we study a phase diagram of quantum hall bilayer systems with total filing $\nu_t=1/2+1/2$ of a lowest landau level as the function of layer distances $d$. based on numerical exact diagonalization calculations, we obtain three distinct phases, including an exciton superfluid phase with spontaneous interlayer coherence at small $d$, the composite fermi liquid at large $d$, and an intermediate phase considering $1.1<d/l_b<1.8$ ($l_b$ was a magnetic length). a transition from a exciton superfluid to a intermediate phase was identified by (i) the dramatic change inside a berry curvature of a ground state under twisted boundary conditions on a two layers; (ii) an energy level crossing of a first excited state. a transition from a intermediate phase to a composite fermi liquid was identified by a vanishing of a exciton superfluid stiffness. furthermore, from our finite-size study, a energy cost of transferring one electron between a layers shows an even-odd effect and possibly extrapolates to the finite value inside a thermodynamic limit, indicating a enhanced intralayer correlation. our identification of an intermediate phase and its distinctive features shed new light on a theoretical understanding of a quantum hall bilayer system at total filling $\nu_t=1$.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
1084,"natural language and symbols are intimately correlated. recent advances inside machine learning (ml) and inside natural language processing (nlp) seem to contradict a above intuition: symbols are fading away, erased by vectors or tensors called distributed and distributional representations. however, there was the strict link between distributed/distributional representations and symbols, being a first an approximation of a second. the clearer understanding of a strict link between distributed/distributional representations and symbols will certainly lead to radically new deep learning networks. inside this paper we make the survey that aims to draw a link between symbolic representations and distributed/distributional representations. this was a right time to revitalize a area of interpreting how symbols are represented in neural networks.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6798,"a interplay between spin-orbit coupling (soc) and electron correlation ($u$) was considered considering many exotic phenomena inside iridium oxides. we have investigated a evolution of structural, magnetic and electronic properties inside pyrochlore iridate y$_2$ir$_{2-x}$ru$_{x}$o$_7$ where a substitution of ru has been aimed to tune this interplay. a ru substitution does not introduce any structural phase transition, however, we do observe an evolution of lattice parameters with a doping level $x$. x-ray photoemission spectroscopy (xps) study indicates ru adopts charge state of ru$^{4+}$ and replaces a ir$^{4+}$ accordingly. magnetization data reveal both a onset of magnetic irreversibility and a magnetic moment decreases with progressive substitution of ru. these materials show non-equilibrium low temperature magnetic state as revealed by magnetic relaxation data. interestingly, we find magnetic relaxation rate increases with substitution of ru. a electrical resistivity shows an insulating behavior inside whole temperature range, however, resistivity decreases with substitution of ru. nature of electronic conduction has been found to follow power-law behavior considering all a materials.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
13210,"introducing inequality constraints inside gaussian process (gp) models should lead to more realistic uncertainties inside learning the great variety of real-world problems. we consider a finite-dimensional gaussian idea behind the method from maatouk and bay (2017) which should satisfy inequality conditions everywhere (either boundedness, monotonicity or convexity). our contributions are threefold. first, we extend their idea behind the method inside order to deal with general sets of linear inequalities. second, we explore several markov chain monte carlo (mcmc) techniques to approximate a posterior distribution. third, we investigate theoretical and numerical properties of a constrained likelihood considering covariance parameter estimation. according to experiments on both artificial and real data, our full framework together with the hamiltonian monte carlo-based sampler provides efficient results on both data fitting and uncertainty quantification.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
12258,"preprocessing tools considering automated text analysis have become more widely available inside major languages, but non-english tools are often still limited inside their functionality. when working with spanish-language text, researchers should easily find tools considering tokenization and stemming, but may not have a means to extract more complex word features like verb tense or mood. yet spanish was the morphologically rich language inside which such features are often identifiable from word form. conjugation rules are consistent, but many special verbs and nouns take on different rules. while building the complete dictionary of known words and their morphological rules would be labor intensive, resources to do so already exist, inside spell checkers designed to generate valid forms of known words. this paper introduces the set of tools considering spanish-language morphological analysis, built with the help of a coes spell checking tools, to label person, mood, tense, gender and number, derive the word's root noun or verb infinitive, and convert verbs to their nominal form.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10076,"studying complexity of various bribery problems has been one of a main research focus inside computational social choice. inside all a models of bribery studied so far, a briber has to pay every voter some amount of money depending on what a briber wants a voter to report and a briber has some budget at her disposal. although these models successfully capture many real world applications, inside many other scenarios, a voters may be unwilling to deviate too much from their true preferences. inside this paper, we study a computational complexity of a problem of finding the preference profile which was as close to a true preference profile as possible and still achieves a briber's goal subject to budget constraints. we call this problem optimal bribery. we consider three important measures of distances, namely, swap distance, footrule distance, and maximum displacement distance, and resolve a complexity of a optimal bribery problem considering many common voting rules. we show that a problem was polynomial time solvable considering a plurality and veto voting rules considering all a three measures of distance. on a other hand, we prove that a problem was np-complete considering the class of scoring rules which includes a borda voting rule, maximin, copeland$^\alpha$ considering any $\alpha\in[0,1]$, and bucklin voting rules considering all a three measures of distance even when a distance allowed per voter was $1$ considering a swap and maximum displacement distances and $2$ considering a footrule distance even without a budget constraints (which corresponds to having an infinite budget). considering a $k$-approval voting rule considering any constant $k>1$ and a simplified bucklin voting rule, we show that a problem was np-complete considering a swap distance even when a distance allowed was $2$ and considering a footrule distance even when a distance allowed was $4$ even without a budget constraints.",1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10329,"we study a multi-armed bandit problem with multiple plays and the budget constraint considering both a stochastic and a adversarial setting. at each round, exactly $k$ out of $n$ possible arms have to be played (with $1\leq k \leq n$). inside addition to observing a individual rewards considering each arm played, a player also learns the vector of costs which has to be covered with an a-priori defined budget $b$. a game ends when a sum of current costs associated with a played arms exceeds a remaining budget. firstly, we analyze this setting considering a stochastic case, considering which we assume each arm to have an underlying cost and reward distribution with support $[c_{\min}, 1]$ and $[0, 1]$, respectively. we derive an upper confidence bound (ucb) algorithm which achieves $o(nk^4 \log b)$ regret. secondly, considering a adversarial case inside which a entire sequence of rewards and costs was fixed inside advance, we derive an upper bound on a regret of order $o(\sqrt{nb\log(n/k)})$ utilizing an extension of a well-known $\texttt{exp3}$ algorithm. we also provide upper bounds that hold with high probability and the lower bound of order $\omega((1 - k/n)^2 \sqrt{nb/k})$.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18160,"it was important to study a risks of publishing privacy-sensitive data. even if sensitive identities (e.g., name, social security number) were removed and advanced data perturbation techniques were applied, several de-anonymization attacks have been proposed to re-identify individuals. however, existing attacks have some limitations: 1) they are limited inside de-anonymization accuracy; 2) they require prior seed knowledge and suffer from a imprecision of such seed information. we propose the novel structure-based de-anonymization attack, which does not require a attacker to have prior information (e.g., seeds). our attack was based on two key insights: with the help of multi-hop neighborhood information, and optimizing a process of de-anonymization by exploiting enhanced machine learning techniques. a experimental results demonstrate that our method was robust to data perturbations and significantly outperforms a state-of-the-art de-anonymization techniques by up to $10\times$ improvement.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
387,"this paper proposes the centralized and the distributed sub-optimal control strategy to maintain inside safe regions a real-time transient frequencies of the given collection of buses, and simultaneously preserve asymptotic stability of a entire network. inside the receding horizon fashion, a centralized control input was obtained by iteratively solving an open-loop optimization aiming to minimize a aggregate control effort over controllers regulated on individual buses with transient frequency and stability constraints. due to a non-convexity of a optimization, we propose the convexification technique by identifying the reference control input trajectory. we then extend a centralized control to the distributed scheme, where each subcontroller should only access a state information within the local region. simulations on the ieee-39 network illustrate our results.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
15783,"red clump stars are fundamental distance indicators inside astrophysics, although theoretical stellar models predict the dependence of absolute magnitudes with ages. this effect was particularly strong below 2 gyr, but even above this limit the mild age dependence was still expected. we use seismically identified red clump stars inside a kepler field considering which we have reliable distances, masses and ages from a saga survey to first explore this effect. by excluding red clump stars with masses larger than 1.6 msun (corresponding to ages younger than 2 gyr), we derive robust calibrations linking intrinsic colors to absolute magnitudes inside a following photometric systems: str√∂mgren $by$, johnson $bv$, sloan $griz$, 2mass $jhk_s$ and wise $w1w2w3$. with a precision achieved we also detect the slope of absolute magnitudes 0.020(0.003) mag per gyrin a infrared, implying that distance calibrations of clump stars should be off by up to 0.2 mag inside a infrared (over a range from 2 gyr to 12 gyr) if their ages are unknown. even larger uncertainties affect optical bands, because of a stronger interdependency of absolute magnitudes on colors and age. our distance calibrations are ultimately based on asteroseismology, and we show how a distance scale should be used to test a accuracy of seismic scaling relations. within a uncertainties our calibrations are inside agreement with those built upon local red clump with hipparcos} parallaxes, although we find the tension which if confirmed would imply that scaling relations overestimate radii of red clump stars by 2(+-20%. data-releases post gaia dr1 will provide an important testbed considering our results.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14813,"inside the recent paper, flandrin [2015] has proposed filtering based on a zeros of the spectrogram, with the help of a short-time fourier transform and the gaussian window. his results are based on empirical observations on a distribution of a zeros of a spectrogram of white gaussian noise. these zeros tend to be uniformly spread over a time-frequency plane, and not to clutter. our contributions are threefold: we rigorously define a zeros of a spectrogram of continuous white gaussian noise, we explicitly characterize their statistical distribution, and we investigate a computational and statistical underpinnings of a practical implementation of signal detection based on a statistics of spectrogram zeros. inside particular, we stress that a zeros of spectrograms of white gaussian noise correspond to zeros of gaussian analytic functions, the topic of recent independent mathematical interest [hough et al., 2009].",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
17438,"we study verdier quotients of diverse homotopy categories of the full additive subcategory $\mathcal e$ of an abelian category. inside particular, we consider a categories $k^{x,y}({\mathcal e})$ considering $x\in\{\infty, +,-,b\}$, and $y\in\{\emptyset,b,+,-,\infty\}$ a homotopy categories of left, right, unbounded complexes with homology being $0$, bounded, left or right bounded, or unbounded. inclusion of these categories give the partially ordered set, and we study localisation sequences or recollement diagrams between a verdier quotients, and prove that many quotients lead to equivalent categories.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
19089,"we study spin transfer torques induced by the spin-triplet supercurrent inside the magnet with a superconducting proximity effect. by the perturbative approach, we show that spin-triplet correlations realize new types of torques, which are analogous to a adiabatic and non-adiabatic ($\beta$) torques, without extrinsic spin-flip scattering. remarkable advantages compared to conventional spin-transfer torques are highlighted inside domain wall manipulation. oscillatory motions of the domain wall do not occur considering the small gilbert damping, and a threshold current density to drive its motion becomes zero inside a absence of extrinsic pinning potentials due to a nonadiabatic torque controlled by a triplet correlations.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
10198,"3d scene understanding was important considering robots to interact with a 3d world inside the meaningful way. most previous works on 3d scene understanding focus on recognizing geometrical or semantic properties of a scene independently. inside this work, we introduce data associated recurrent neural networks (da-rnns), the novel framework considering joint 3d scene mapping and semantic labeling. da-rnns use the new recurrent neural network architecture considering semantic labeling on rgb-d videos. a output of a network was integrated with mapping techniques such as kinectfusion inside order to inject semantic information into a reconstructed 3d scene. experiments conducted on the real world dataset and the synthetic dataset with rgb-d videos demonstrate a ability of our method inside semantic 3d scene mapping.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
13589,"diffusions and related random walk procedures are of central importance inside many areas of machine learning, data analysis, and applied mathematics. because they spread mass agnostically at each step inside an iterative manner, they should sometimes spread mass ""too aggressively,"" thereby failing to find a ""right"" clusters. we introduce the novel capacity releasing diffusion (crd) process, which was both faster and stays more local than a classical spectral diffusion process. as an application, we use our crd process to develop an improved local algorithm considering graph clustering. our local graph clustering method should find local clusters inside the model of clustering where one begins a crd process inside the cluster whose vertices are connected better internally than externally by an $o(\log^2 n)$ factor, where $n$ was a number of nodes inside a cluster. thus, our crd process was a first local graph clustering algorithm that was not subject to a well-known quadratic cheeger barrier. our result requires the certain smoothness condition, which we expect to be an artifact of our analysis. our empirical evaluation demonstrates improved results, inside particular considering realistic social graphs where there are moderately good---but not very good---clusters.",1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14191,"teleconference or telepresence based on virtual reality (vr) headmount display (hmd) device was the very interesting and promising application since hmd should provide immersive feelings considering users. however, inside order to facilitate face-to-face communications considering hmd users, real-time 3d facial performance capture of the person wearing hmd was needed, which was the very challenging task due to a large occlusion caused by hmd. a existing limited solutions are very complex either inside setting or inside idea behind the method as well as lacking a performance capture of 3d eye gaze movement. inside this paper, we propose the convolutional neural network (cnn) based solution considering real-time 3d face-eye performance capture of hmd users without complex modification to devices. to address a issue of lacking training data, we generate massive pairs of hmd face-label dataset by data synthesis as well as collecting vr-ir eye dataset from multiple subjects. then, we train the dense-fitting network considering facial region and an eye gaze network to regress 3d eye model parameters. extensive experimental results demonstrate that our system should efficiently and effectively produce inside real time the vivid personalized 3d avatar with a correct identity, pose, expression and eye motion corresponding to a hmd user.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
771,"we consider a problem of planning under observation and motion uncertainty considering nonlinear robotics systems. determining a optimal solution to this problem, generally formulated as the partially observed markov decision process (pomdp), was computationally intractable. we propose the trajectory-optimized linear quadratic gaussian (t-lqg) idea behind the method that leads to quantifiably near-optimal solutions considering a pomdp problem. we provide the novel ""separation principle"" considering a design of an optimal nominal open-loop trajectory followed by an optimal feedback control law, which provides the near-optimal feedback control policy considering belief space planning problems involving the polynomial order of calculations of minimum order.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1
8411,"we study a problem of resilient consensus of sampled-data multi-agent networks with double-integrator dynamics. a term resilient points to algorithms considering a presence of attacks by faulty/malicious agents inside a network. each normal agent updates its state based on the predetermined control law with the help of its neighbors' information which may be delayed while misbehaving agents make updates arbitrarily and might threaten a consensus within a network. assuming that a maximum number of malicious agents inside a system was known, we focus on algorithms where each normal agent ignores large and small position values among its neighbors to avoid being influenced by malicious agents. a malicious agents are assumed to be omniscient inside that they know a updating times and delays and should collude with each other. we deal with both synchronous and partially asynchronous cases with delayed information and derive topological conditions inside terms of graph robustness.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
8800,"classic topic models are built under a bag of words assumption, inside which word position was ignored considering simplicity. besides, symmetric priors are typically used inside most applications. inside order to easily learn topics with different properties among a same corpus, we propose the new line of work inside which a paragraph structure was exploited. our proposal was based on a following assumption: inside many text document corpora there are formal constraints shared across all a collection, e.g. sections. when this assumption was satisfied, some paragraphs may be related to general concepts shared by all documents inside a corpus, while others would contain a genuine description of documents. assuming each paragraph should be semantically more general, specific, or hybrid, we look considering ways to measure this, transferring this distinction to topics and being able to learn what we call specific and general topics. experiments show that this was the proper methodology to highlight certain paragraphs inside structured documents at a same time we learn interesting and more diverse topics.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18309,"shifted combinatorial optimization was the new nonlinear optimization framework which was the broad extension of standard combinatorial optimization, involving a choice of several feasible solutions at the time. this framework captures well studied and diverse problems ranging from so-called vulnerability problems to sharing and partitioning problems. inside particular, every standard combinatorial optimization problem has its shifted counterpart, which was typically much harder. already with explicitly given input set a shifted problem may be np-hard. inside this article we initiate the study of a parameterized complexity of this framework. first we show that shifting over an explicitly given set with its cardinality as a parameter may be inside xp, fpt or p, depending on a objective function. second, we study a shifted problem over sets definable inside mso logic (which includes, e.g., a well known mso partitioning problems). our main results here are that shifted combinatorial optimization over mso definable sets was inside xp with respect to a mso formula and a treewidth (or more generally clique-width) of a input graph, and was w[1]-hard even under further severe restrictions.",1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
10358,"let k be the finite base field. inside this note, making use of topological periodic cyclic homology and of a theory of noncommutative motives, we prove that a numerical grothendieck group of every smooth proper dg k-linear category was the finitely generated free abelian group. along a way, we prove moreover that a category of noncommutative numerical motives over k was abelian semi-simple, as conjectured by kontsevich. furthermore, we show that a zeta functions of endomorphisms of noncommutative chow motives are rational and satisfy the functional equation.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
16670,"we propose the new idea behind the method considering metric learning by framing it as learning the sparse combination of locally discriminative metrics that are inexpensive to generate from a training data. this flexible framework allows us to naturally derive formulations considering global, multi-task and local metric learning. a resulting algorithms have several advantages over existing methods inside a literature: the much smaller number of parameters to be estimated and the principled way to generalize learned metrics to new testing data points. to analyze a idea behind the method theoretically, we derive the generalization bound that justifies a sparse combination. empirically, we evaluate our algorithms on several datasets against state-of-the-art metric learning methods. a results are consistent with our theoretical findings and demonstrate a superiority of our idea behind the method inside terms of classification performance and scalability.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8307,"motivated by our conjecture of an earlier work predicting a degeneration at a second page of a fr√∂licher spectral sequence of any compact complex manifold supporting an skt metric $\omega$ (i.e. such that $\partial\bar\partial\omega=0$), we prove degeneration at $e_2$ whenever a manifold admits the hermitian metric whose torsion operator $\tau$ and its adjoint vanish on $\delta''$-harmonic forms of positive degrees up to $\mbox{dim}_\c x$. besides a pseudo-differential laplacian inducing the hodge theory considering $e_2$ that we constructed inside earlier work and demailly's bochner-kodaira-nakano formula considering hermitian metrics, the key ingredient was the general formula considering a dimensions of a vector spaces featuring inside a fr√∂licher spectral sequence inside terms of a asymptotics, as the positive constant $h$ decreases to zero, of a small eigenvalues of the rescaled laplacian $\delta_h$, introduced here inside a present form, that we adapt to a context of the complex structure from a well-known construction of a adiabatic limit and from a analogous result considering riemannian foliations of √°lvarez l√≥pez and kordyukov.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12952,"today's artificial assistants are typically prompted to perform tasks through direct, imperative commands such as \emph{set the timer} or \emph{pick up a box}. however, to progress toward more natural exchanges between humans and these assistants, it was important to understand a way non-imperative utterances should indirectly elicit action of an addressee. inside this paper, we investigate command types inside a setting of the grounded, collaborative game. we focus on the less understood family of utterances considering eliciting agent action, locatives like \emph{the chair was inside a other room}, and demonstrate how these utterances indirectly command inside specific game state contexts. our work shows that models with domain-specific grounding should effectively realize a pragmatic reasoning that was necessary considering more robust natural language interaction.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19675,"reusable model design becomes desirable with a rapid expansion of machine learning applications. inside this paper, we focus on a reusability of pre-trained deep convolutional models. specifically, different from treating pre-trained models as feature extractors, we reveal more treasures beneath convolutional layers, i.e., a convolutional activations could act as the detector considering a common object inside a image co-localization problem. we propose the simple but effective method, named deep descriptor transforming (ddt), considering evaluating a correlations of descriptors and then obtaining a category-consistent regions, which should accurately locate a common object inside the set of images. empirical studies validate a effectiveness of a proposed ddt method. on benchmark image co-localization datasets, ddt consistently outperforms existing state-of-the-art methods by the large margin. moreover, ddt also demonstrates good generalization ability considering unseen categories and robustness considering dealing with noisy data.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15091,"a millisecond-duration radio flashes known as fast radio bursts (frbs) represent an enigmatic astrophysical phenomenon. recently, a sub-arcsecond localization (~ 100mas precision) of frb121102 with the help of a vla has led to its unambiguous association with persistent radio and optical counterparts, and to a identification of its host galaxy. however, an even more precise localization was needed inside order to probe a direct physical relationship between a millisecond bursts themselves and a associated persistent emission. here we report very-long-baseline radio interferometric observations with the help of a european vlbi network and a 305-m arecibo telescope, which simultaneously detect both a bursts and a persistent radio emission at milliarcsecond angular scales and show that they are co-located to within the projected linear separation of < 40pc (< 12mas angular separation, at 95% confidence). we detect consistent angular broadening of a bursts and persistent radio source (~ 2-4mas at 1.7ghz), which are both similar to a expected milky way scattering contribution. a persistent radio source has the projected size constrained to be < 0.7pc (< 0.2mas angular extent at 5.0ghz) and the lower limit considering a brightness temperature of t_b > 5 x 10^7k. together, these observations provide strong evidence considering the direct physical link between frb121102 and a compact persistent radio source. we argue that the burst source associated with the low-luminosity active galactic nucleus or the young neutron star energizing the supernova remnant are a two scenarios considering frb121102 that best match a observed data.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
1916,"inside this article, as always we will start by deliberating at our project's historical general view and then we will try to construct the new poisson bracket on our simplest example $sl_2$ and then we will try to give the universal construction based on our universal variables and then will try to construct lattice $w_2$ algebras which will play the key role inside our other constructions on lattice $w_3$ algebras and finally we will try to find a only non trivial dependent generator of our lattice $w_4$ algebras and so on considering lattice $w_n$ algebras. and inside a late of this article we will have appendix a, which will contain some parts of a mathematica coding which we have used and have made considering to find our algebra structures.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
18836,"this paper studies the stylized, yet natural, learning-to-rank problem and points out a critical incorrectness of the widely used nearest neighbor algorithm. we consider the model with $n$ agents (users) $\{x_i\}_{i \in [n]}$ and $m$ alternatives (items) $\{y_j\}_{j \in [m]}$, each of which was associated with the latent feature vector. agents rank items nondeterministically according to a plackett-luce model, where a higher a utility of an item to a agent, a more likely this item will be ranked high by a agent. our goal was to find neighbors of an arbitrary agent or alternative inside a latent space. we first show that a kendall-tau distance based knn produces incorrect results inside our model. next, we fix a problem by introducing the new algorithm with features constructed from ""global information"" of a data matrix. our idea behind the method was inside sharp contrast to most existing feature engineering methods. finally, we design another new algorithm identifying similar alternatives. a construction of alternative features should be done with the help of ""local information,"" highlighting a algorithmic difference between finding similar agents and similar alternatives.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
10009,"with the help of combination of density functional theory and monte carlo simulation, we study a phase stability and electronic properties of two dimensional hexagonal composites of boron nitride and graphene, with the goal to uncover a role of a interface geometry formed between a two. our study highlights that preferential creation of extended armchair interfaces may facilitate formation of solid solution of boron nitride and graphene within the certain temperature range. we further find that considering band-gap engineering, armchair interfaces or patchy interfaces with mixed geometry are most suitable. extending a study to nanoribbon geometry shows that reduction of dimensionality makes a tendency to phase segregation of a two phases even stronger. our thorough study should form an useful database inside designing boron nitride-graphene composites with desired properties.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
18734,"inside this work, we perform an empirical comparison among a ctc, rnn-transducer, and attention-based seq2seq models considering end-to-end speech recognition. we show that, without any language model, seq2seq and rnn-transducer models both outperform a best reported ctc models with the language model, on a popular hub5'00 benchmark. on our internal diverse dataset, these trends continue - rnntransducer models rescored with the language model after beam search outperform our best ctc models. these results simplify a speech recognition pipeline so that decoding should now be expressed purely as neural network operations. we also study how a choice of encoder architecture affects a performance of a three models - when all encoder layers are forward only, and when encoders downsample a input representation aggressively.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12724,"we introduce an updated physical model to simulate a formation and evolution of galaxies inside cosmological, large-scale gravity+magnetohydrodynamical simulations with a moving mesh code arepo. a overall framework builds upon a successes of a illustris galaxy formation model, and includes prescriptions considering star formation, stellar evolution, chemical enrichment, primordial and metal-line cooling of a gas, stellar feedback with galactic outflows, and black hole formation, growth and multi-mode feedback. inside this paper we give the comprehensive description of a physical and numerical advances which form a core of a illustristng (the next generation) framework. we focus on a revised implementation of a galactic winds, of which we modify a directionality, velocity, thermal content, and energy scalings, and explore its effects on a galaxy population. as described inside earlier works, a model also includes the new black hole driven kinetic feedback at low accretion rates, magnetohydrodynamics, and improvements to a numerical scheme. with the help of the suite of (25 mpc $h^{-1}$)$^3$ cosmological boxes we assess a outcome of a new model at our fiducial resolution. a presence of the self-consistently amplified magnetic field was shown to have an important impact on a stellar content of $10^{12} m_{\rm sun}$ haloes and above. finally, we demonstrate that a new galactic winds promise to solve key problems identified inside illustris inside matching observational constraints and affecting a stellar content and sizes of a low mass end of a galaxy population.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16905,"evaluating similarity between graphs was of major importance inside several computer vision and pattern recognition problems, where graph representations are often used to model objects or interactions between elements. a choice of the distance or similarity metric is, however, not trivial and should be highly dependent on a application at hand. inside this work, we propose the novel metric learning method to evaluate distance between graphs that leverages a power of convolutional neural networks, while exploiting concepts from spectral graph theory to allow these operations on irregular graphs. we demonstrate a potential of our method inside a field of connectomics, where neuronal pathways or functional connections between brain regions are commonly modelled as graphs. inside this problem, a definition of an appropriate graph similarity function was critical to unveil patterns of disruptions associated with certain brain disorders. experimental results on a abide dataset show that our method should learn the graph similarity metric tailored considering the clinical application, improving a performance of the simple k-nn classifier by 11.9% compared to the traditional distance metric.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2685,"a in-plane infinitesimal deformations of graphene are well understood: they should be computed by solving a equilibrium problem considering the sheet of isotropic elastic material with suitable stretching stiffness and poisson coefficient $\nu^{(m)}$. here, we pose a following question: does a poisson coefficient $\nu^{(m)}$ affect a response to bending of graphene? despite what happens if one adopts classical structural models, it does not. inside this letter we show that the new material property, conceptually and quantitatively different from $\nu^{(m)}$, has to be introduced. we term this new parameter bending poisson coefficient; we propose considering it the physical interpretation inside terms of a atomic interactions and produce the quantitative evaluation.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
13942,"multi-label classification (mlc) was an important learning problem that expects a learning algorithm to take a hidden correlation of a labels into account. extracting a hidden correlation was generally the challenging task. inside this work, we propose the novel deep learning framework to better extract a hidden correlation with a aid of a memory structure within recurrent neural networks. a memory stores a temporary guesses on a labels and effectively allows a framework to rethink about a goodness and correlation of a guesses before making a final prediction. furthermore, a rethinking process makes it easy to adapt to different evaluation criterion to match real-world application needs. experimental results across many real-world data sets justify that a rethinking process indeed improves mlc performance across different evaluation criteria and leads to superior performance over state-of-the-art mlc algorithms.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5601,"probabilistic representations of movement primitives open important new possibilities considering machine learning inside robotics. these representations are able to capture a variability of a demonstrations from the teacher as the probability distribution over trajectories, providing the sensible region of exploration and a ability to adapt to changes inside a robot environment. however, to be able to capture variability and correlations between different joints, the probabilistic movement primitive requires a approximation of the larger number of parameters compared to their deterministic counterparts, that focus on modeling only a mean behavior. inside this paper, we make use of prior distributions over a parameters of the probabilistic movement primitive to make robust estimates of a parameters with few training instances. inside addition, we introduce general purpose operators to adapt movement primitives inside joint and task space. a proposed training method and adaptation operators are tested inside the coffee preparation and inside robot table tennis task. inside a coffee preparation task we evaluate a generalization performance to changes inside a location of a coffee grinder and brewing chamber inside the target area, achieving a desired behavior after only two demonstrations. inside a table tennis task we evaluate a hit and return rates, outperforming previous approaches while with the help of fewer task specific heuristics.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
8915,"we present the bayesian method considering feature selection inside a presence of grouping information with sparsity on a between- and within group level. instead of with the help of the stochastic algorithm considering parameter inference, we employ expectation propagation, which was the deterministic and fast algorithm. available methods considering feature selection inside a presence of grouping information have the number of short-comings: on one hand, lasso methods, while being fast, underestimate a regression coefficients and do not make good use of a grouping information, and on a other hand, bayesian approaches, while accurate inside parameter estimation, often rely on a stochastic and slow gibbs sampling procedure to recover a parameters, rendering them infeasible e.g. considering gene network reconstruction. our idea behind the method of the bayesian sparse-group framework with expectation propagation enables us to not only recover accurate parameter estimates inside signal recovery problems, but also makes it possible to apply this bayesian framework to large-scale network reconstruction problems. a presented method was generic but inside terms of application we focus on gene regulatory networks. we show on simulated and experimental data that a method constitutes the good choice considering network reconstruction regarding a number of correctly selected features, prediction on new data and reasonable computing time.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17888,"we present a results of cosmological hydrodynamic simulations with zoom-in initial conditions, and investigate a formation of a first galaxies and their evolution towards observable galaxies at $z \sim 6$. we focus on three different galaxies which end up inside halos with masses $m_{h} = 2.4 \times10^{10}~h^{-1}\; m_{\odot}$ (halo-10), $1.6 \times10^{11}~h^{-1}\; m_{\odot}$ (halo-11) and $0.7 \times10^{12}~h^{-1} m_{\odot}$ (halo-12) at z=6. our simulations also probe impacts of different sub-grid assumptions, i.e., sf efficiency and cosmic reionization, on sf histories inside a first galaxies. we find that star formation occurs intermittently due to supernova (sn) feedback at z > 10, and then it proceeds more smoothly as a halo mass grows at lower redshifts. galactic disks are destroyed due to sn feedback, while galaxies inside simulations with no-feedback or lower sf efficiency models should sustain galactic disk considering long periods > 10 myr. a expulsion of gas at a galactic center also affects a inner dark matter density profile. however, sn feedback does not seem to keep a shallow profile of dark matter considering the long period. our simulated galaxies inside halo-11 and halo-12 reproduce a star formation rates (sfr) and stellar masses of observed lyman-$\alpha$ emitters (laes) at z = 7-8 fairly well given observational uncertainties. inside addition, we investigate a effect of uv background radiation on star formation as an external feedback source, and find that earlier reionization extends a quenching time of star formation due to photo-ionization heating, but does not affect a stellar mass at z=6.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8145,"cross-validation of predictive models was a de-facto standard considering model selection and evaluation. inside proper use, it provides an unbiased approximate of the model's predictive performance. however, data sets often undergo the preliminary data-dependent transformation, such as feature rescaling or dimensionality reduction, prior to cross-validation. it was widely believed that such the preprocessing stage, if done inside an unsupervised manner that does not consider a class labels or response values, has no effect on a validity of cross-validation. inside this paper, we show that this belief was not true. preliminary preprocessing should introduce either the positive or negative bias into a estimates of model performance. thus, it may lead to sub-optimal choices of model parameters and invalid inference. inside light of this, a scientific community should re-examine a use of preliminary preprocessing prior to cross-validation across a various application domains. by default, all data transformations, including unsupervised preprocessing stages, should be learned only from a training samples, and then merely applied to a validation and testing samples.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0
18218,"inside typical neural machine translation~(nmt), a decoder generates the sentence word by word, packing all linguistic granularities inside a same time-scale of rnn. inside this paper, we propose the new type of decoder considering nmt, which splits a decode state into two parts and updates them inside two different time-scales. specifically, we first predict the chunk time-scale state considering phrasal modeling, on top of which multiple word time-scale states are generated. inside this way, a target sentence was translated hierarchically from chunks to words, with information inside different granularities being leveraged. experiments show that our proposed model significantly improves a translation performance over a state-of-the-art nmt model.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6962,this brief note highlights some basic concepts required toward understanding a evolution of machine learning and deep learning models. a note starts with an overview of artificial intelligence and its relationship to biological neuron that ultimately led to a evolution of todays intelligent models.,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8830,"unsupervised segmentation and clustering of unlabelled speech are core problems inside zero-resource speech processing. most approaches lie at methodological extremes: some use probabilistic bayesian models with convergence guarantees, while others opt considering more efficient heuristic techniques. despite competitive performance inside previous work, a full bayesian idea behind the method was difficult to scale to large speech corpora. we introduce an approximation to the recent bayesian model that still has the clear objective function but improves efficiency by with the help of hard clustering and segmentation rather than full bayesian inference. like its bayesian counterpart, this embedded segmental k-means model (es-kmeans) represents arbitrary-length word segments as fixed-dimensional acoustic word embeddings. we first compare es-kmeans to previous approaches on common english and xitsonga data sets (5 and 2.5 hours of speech): es-kmeans outperforms the leading heuristic method inside word segmentation, giving similar scores to a bayesian model while being 5 times faster with fewer hyperparameters. however, its clusters are less pure than those of a other models. we then show that es-kmeans scales to larger corpora by applying it to a 5 languages of a zero resource speech challenge 2017 (up to 45 hours), where it performs competitively compared to a challenge baseline.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18488,"inside our previous work we showed that considering an ancient solution to a ricci flow with nonnegative curvature operator, assuming bounded geometry on one time slice, bounded entropy implies noncollapsing on all scales. inside this paper we prove a implication inside a other direction, that considering an ancient solution with bounded nonnegative curvature operator, noncollapsing implies bounded entropy. thus we prove perelman's assertion under a assumption of bounded geometry on one time slice. inside particular, considering ancient solutions of dimension three, we need only to assume bounded curvature. we also establish an equality between a asymptotic entropy and a asymptotic reduce volume, which was the result similar to xu, where he assumes noncollapsing and a type i curvature bound.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
414,"inside our recent paper w.s. rossi, p. frasca and f. fagnani, ""average resistance of toroidal graphs"", siam journal on control and optimization, 53(4):2541--2557, 2015, we studied how a average resistances of $d$-dimensional toroidal grids depend on a graph topology and on a dimension of a graph. our results were based on a connection between resistance and laplacian eigenvalues. inside this note, we contextualize our work inside a body of literature about random walks on graphs. indeed, a average effective resistance of a $d$-dimensional toroidal grid was proportional to a mean hitting time of a simple random walk on that grid. if $d\geq3 $, then a average resistance should be bounded uniformly inside a number of nodes and its value was of order $1/d$ considering large $d$.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
1201,"deep reinforcement learning could be used to learn dexterous robotic policies but it was challenging to transfer them to new robots with vastly different hardware properties. it was also prohibitively expensive to learn the new policy from scratch considering each robot hardware due to a high sample complexity of modern state-of-the-art algorithms. we propose the novel idea behind the method called \textit{hardware conditioned policies} where we train the universal policy conditioned on the vector representation of robot hardware. we considered robots inside simulation with varied dynamics, kinematic structure, kinematic lengths and degrees-of-freedom. first, we use a kinematic structure directly as a hardware encoding and show great zero-shot transfer to completely novel robots not seen during training. considering robots with lower zero-shot success rate, we also demonstrate that fine-tuning a policy network was significantly more sample-efficient than training the model from scratch. inside tasks where knowing a agent dynamics was important considering success, we learn an embedding considering robot hardware and show that policies conditioned on a encoding of hardware tend to generalize and transfer well. a code and videos are available on a project webpage: this https url.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
13142,"beside a spin density wave (sdw) order in a superconducting phase inside $\mathrm{cecoin_5}$ at high magnetic fields, recent neutron scattering measurements have found bragg peaks inside $5\%$ nd doped $\mathrm{cecoin_5}$ at low fields. a intensity of bragg peaks inside low fields was suppressed by increasing field. based on a phenomenological and microscopic modeling, we show that considering a pauli limited $d$-wave superconductors inside a vicinity of sdw instability relevant considering $\mathrm{cecoin_5}$, magnetic impurities locally induce droplets of sdw order. because of a strong anisotropy inside a momentum space inside a spin fluctuations guaranteed by a $d$-wave pairing symmetry, sharp peaks inside spin structure factor at $\mathbf{q}$s are produced by a impurities, even when a droplets of sdw do not order. at zero field, a nd impurity spins are along a $c$ axis due to a coupling to a conduction electrons with an easy $c$ axis, besides their own crystal field effect. a in-plane magnetic field cants a impurity moments toward a $ab$ plane, which suppress a droplets of sdw order. at high fields, a long-range sdw in a superconducting phase was stabilized as the consequence of magnon condensation. our results are consistent with a recent neutron scattering and thermal conductivity measurements.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
15799,"instanton partition functions of $\mathcal{n}=1$ 5d super yang-mills reduced on $s^1$ should be engineered inside type iib string theory from a $(p,q)$-branes web diagram. to this diagram was superimposed the web of representations of a ding-iohara-miki (dim) algebra that acts on a partition function. inside this correspondence, each segment was associated to the representation, and a (topological string) vertex was identified with a intertwiner operator constructed by awata, feigin and shiraishi. we define the new intertwiner acting on a representation spaces of levels $(1,n)\otimes(0,m)\to(1,n+m)$, thereby generalizing to higher rank $m$ a original construction. it allows us to use the folded version of a usual $(p,q)$-web diagram, bringing great simplifications to actual computations. as the result, a characterization of gaiotto states and vertical intertwiners, previously obtained by some of a authors, was uplifted to operator relations acting inside a fock space of horizontal representations. we further develop the method to build qq-characters of linear quivers based on a horizontal action of dim elements. while fundamental qq-characters should be built with the help of a coproduct, higher ones require a introduction of the (quantum) weyl reflection acting on tensor products of dim generators.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
9227,"a physical properties of polycrystalline materials depend on their microstructure, which was a nano-to-centimeter-scale arrangement of phases and defects inside their interior. such microstructure depends on a shape, crystallographic phase and orientation, and interfacing of a grains constituting a material. this article presents the new non-destructive 3d technique to study bulk samples with sizes inside a cm range with the resolution of hundred micrometers: time-of-flight three-dimensional neutron diffraction (tof 3dnd). compared to existing analogous x-ray diffraction techniques, tof 3dnd enables studies of samples that should be both larger inside size and made of heavier elements. moreover, tof 3dnd facilitates a use of complicated sample environments. a basic tof 3dnd setup, utilizing an imaging detector with high spatial and temporal resolution, should easily be implemented at the time-of-flight neutron beamline. a technique is developed and tested with data collected at a materials and life science experimental facility of a japan proton accelerator complex (j-parc) considering an iron sample. we successfully reconstructed a shape of 108 grains and developed an indexing procedure. a reconstruction algorithms have been validated by reconstructing two stacked co-ni-ga single crystals and by comparison with the grain map obtained by post-mortem electron backscatter diffraction (ebsd).",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
863,electromagnetic properties of single crystal terbium gallium garnet (tgg) are characterised from room down to millikelvin temperatures with the help of a whispering gallery mode method. microwave spectroscopy was performed at low powers equivalent to the few photons inside energy and conducted as functions of a magnetic field and temperature. the phase transition was detected close to a temperature of 3.5 k. this was observed considering multiple whispering gallery modes causing an abrupt negative frequency shift and the change inside transmission due to extra losses inside a new phase caused by the change inside complex magnetic susceptibility.,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
6905,"a local environment and a charge state of the nickel impurity inside cubic ba$_{0.8}$sr$_{0.2}$tio$_3$ are studied by xafs spectroscopy. according to a xanes data, a mean ni charge state was $\sim$2.5+. an analysis of a exafs spectra and their comparison with a results of first-principle calculations of a defect geometry suggest that ni$^{2+}$ ions are inside the high-spin state at a $b$ sites of a perovskite structure and a difference of a ni$^{2+}$ and ti$^{4+}$ charges was mainly compensated by distant oxygen vacancies. inside addition, the considerable amount of nickel inside a sample was inside the second phase banio$_{3-\delta}$. a measurements of a lattice parameter show the decrease inside a unit cell volume upon doping, which should indicate a existence of the small amount of ni$^{4+}$ ions at a $b$ sites.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
2025,"the new obstacle detection algorithm considering unmanned surface vehicles (usvs) was presented. the state-of-the-art graphical model considering semantic segmentation was extended to incorporate boat pitch and roll measurements from a on-board inertial measurement unit (imu), and the stereo verification algorithm that consolidates tentative detections obtained from a segmentation was proposed. a imu readings are used to approximate a location of horizon line inside a image, which automatically adjusts a priors inside a probabilistic semantic segmentation model. we derive a equations considering projecting a horizon into images, propose an efficient optimization algorithm considering a extended graphical model, and offer the practical imu-camera-usv calibration procedure. with the help of an usv equipped with multiple synchronized sensors, we captured the new challenging multi-modal dataset, and annotated its images with water edge and obstacles. experimental results show that a proposed algorithm significantly outperforms a state of a art, with nearly 30% improvement inside water-edge detection accuracy, an over 21% reduction of false positive rate, an almost 60% reduction of false negative rate, and an over 65% increase of true positive rate, while its matlab implementation runs inside real-time.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
7799,"a physics potential of edelweiss detectors considering a search of low-mass weakly interacting massive particles (wimps) was studied. with the help of the data-driven background model, projected exclusion limits are computed with the help of frequentist and multivariate analysis approaches, namely profile likelihood and boosted decision tree. both current and achievable experimental performance are considered. a optimal strategy considering detector optimization depends critically on whether a emphasis was put on wimp masses below or above $\sim$ 5 gev/c$^2$. a projected sensitivity considering a next phase of a edelweiss-iii experiment at a modane underground laboratory (lsm) considering low-mass wimp search was presented. by 2018 an upper limit on a spin-independent wimp-nucleon cross-section of $\sigma_{si} = 7 \times 10^{-42}$ cm$^2$ was expected considering the wimp mass inside a range 2$-$5 gev/c$^2$. a requirements considering the future hundred-kilogram scale experiment designed to reach a bounds imposed by a coherent scattering of solar neutrinos are also described. by improving a ionization resolution down to 50 ev$_{ee}$, we show that such an experiment installed inside an even lower background environment (e.g. at snolab) should allow to observe about 80 $^8$b neutrino events after discrimination.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8576,"clustered quantum materials provide the new platform considering a experimental study of many-body entanglement. here we address the simple model of the single-molecule nano-magnet featuring n interacting spins inside the transverse field. a field should induce an entanglement transition (et). we calculate a magnetisation, low-energy gap and neutron-scattering cross-section and find that a et has distinct signatures, detectable at temperatures as high as 5% of a interaction strength. a signatures are stronger considering smaller clusters.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
1589,"we study performance limits of solutions to utility maximization problems (e.g., max-min problems) inside wireless networks as the function of a power budget $\bar{p}$ available to transmitters. special focus was devoted to a utility and a transmit energy efficiency (i.e., utility over transmit power) of a solution. briefly, we show tight bounds considering a general class of network utility optimization problems that should be solved by computing conditional eigenvalues of standard interference mappings. a proposed bounds, which are based on a concept of asymptotic functions, are simple to compute, provide us with good estimates of a performance of networks considering any value of $\bar{p}$ inside many real-world applications, and enable us to determine points inside which networks move from the noise limited regime to an interference limited regime. furthermore, they also show that a utility and a transmit energy efficiency scales as $\theta(1)$ and $\theta(1/\bar{p})$, respectively, as $\bar{p}\to\infty$.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
3428,"modern scientific instruments produce vast amounts of data, which should overwhelm a processing ability of computer systems. lossy compression of data was an intriguing solution, but comes with its own dangers, such as potential signal loss, and a need considering careful parameter optimization. inside this work, we focus on the setting where this problem was especially acute -compressive sensing frameworks considering radio astronomy- and ask: should a precision of a data representation be lowered considering all inputs, with both recovery guarantees and practical performance? our first contribution was the theoretical analysis of a iterative hard thresholding (iht) algorithm when all input data, that is, a measurement matrix and a observation, are quantized aggressively to as little as 2 bits per value. under reasonable constraints, we show that there exists the variant of low precision iht that should still provide recovery guarantees. a second contribution was an analysis of our general quantized framework tailored to radio astronomy, showing that its conditions are satisfied inside this case. we evaluate our idea behind the method with the help of cpu and fpga implementations, and show that it should achieve up to 9.19x speed up with negligible loss of recovery quality, on real telescope data.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19138,"a lasso has been studied extensively as the tool considering estimating a coefficient vector inside a high-dimensional linear model; however, considerably less was known about estimating a error variance. indeed, most well-known theoretical properties of a lasso, including recent advances inside selective inference with a lasso, are established under a assumption that a underlying error variance was known. yet a error variance inside practice is, of course, unknown. inside this paper, we propose a natural lasso estimator considering a error variance, which maximizes the penalized likelihood objective. the key aspect of a natural lasso was that a likelihood was expressed inside terms of a natural parameterization of a multiparameter exponential family of the gaussian with unknown mean and variance. a result was the remarkably simple estimator with provably good performance inside terms of mean squared error. these theoretical results do not require placing any assumptions on a design matrix or a true regression coefficients. we also propose the companion estimator, called a organic lasso, which theoretically does not require tuning of a regularization parameter. both estimators do well compared to preexisting methods, especially inside settings where successful recovery of a true support of a coefficient vector was hard.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0
5252,"inside this note, we study a problem of uniqueness of ricci flow on complete noncompact manifolds. we consider a class of solutions with curvature bounded above by c/t when t > 0. inside paricular, we proved uniqueness if inside addition a initial curvature was of polynomial growth and ricci curvature of a flow was relatively small.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14561,"the novel effective hamiltonian inside a subspace of singly occupied states was obtained by applying a gutzwiller projection idea behind the method to the generalized hubbard model with a interactions between two nearest- neighbor sites. this model provides the more complete description of a physics of strongly correlated electron systems. a system was not necessarily inside the ferromagnetic state as temperature approaches zero at any doping level. a system, however, must be inside an antiferromagnetic state at a origin of a doping-temperature plane. moreover, a model exhibits superconductivity inside the doped region at sufficiently low temperatures. we summarize a studies and provide the phase diagram of a antiferromagnetism and a superconductivity of a model inside a doping-temperature plane here. details will be presented inside subsequent papers.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
16148,"inside a recent years impressive advances were made considering single image super-resolution. deep learning was behind the big part of this success. deep(er) architecture design and external priors modeling are a key ingredients. a internal contents of a low resolution input image was neglected with deep modeling despite a earlier works showing a power of with the help of such internal priors. inside this paper we propose the novel deep convolutional neural network carefully designed considering robustness and efficiency at both learning and testing. moreover, we propose the couple of model adaptation strategies to a internal contents of a low resolution input image and analyze their strong points and weaknesses. by trading runtime and with the help of internal priors we achieve 0.1 up to 0.3db psnr improvements over best reported results on standard datasets. our adaptation especially favors images with repetitive structures or under large resolutions. moreover, it should be combined with other simple techniques, such as back-projection or enhanced prediction, considering further improvements.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19285,"suppose a data consist of the set $s$ of points $x_j, 1 \leq j \leq j$, distributed inside the bounded domain $d \subset r^n$, where $n$ and $j$ are large numbers. inside this paper an algorithm was proposed considering checking whether there exists the manifold $\mathbb{m}$ of low dimension near which many of a points of $s$ lie and finding such $\mathbb{m}$ if it exists. there are many dimension reduction algorithms, both linear and non-linear. our algorithm was simple to implement and has some advantages compared with a known algorithms. if there was the manifold of low dimension near which most of a data points lie, a proposed algorithm will find it. some numerical results are presented illustrating a algorithm and analyzing its performance compared to a classical pca (principal component analysis) and isomap.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0
10921,"inside this paper, we develop the computationally efficient discrete approximation to log-gaussian cox process (lgcp) models considering a analysis of spatially aggregated disease count data. our idea behind the method overcomes an inherent limitation of spatial models based on markov structures, namely that each such model was tied to the specific partition of a study area, and allows considering spatially continuous prediction. we compare a predictive performance of our modelling idea behind the method with lgcp through the simulation study and an application to primary biliary cirrhosis incidence data inside newcastle-upon-tyne, uk. our results suggest that when disease risk was assumed to be the spatially continuous process, a proposed approximation to lgcp provides reliable estimates of disease risk both on spatially continuous and aggregated scales. a proposed methodology was implemented inside a open-source r package sdalgcp.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
5263,"inside this paper, we study a efficiency of egoistic and altruistic strategies within a model of social dynamics determined by voting inside the stochastic environment (the vise model) with the help of two criteria: maximizing a average capital increment and minimizing a number of bankrupt participants. a proposals are generated stochastically; three families of a corresponding distributions are considered: normal distributions, symmetrized pareto distributions, and student's $t$-distributions. it was found that a ""pit of losses"" paradox described earlier does not occur inside a case of heavy-tailed distributions. a egoistic strategy better protects agents from extinction inside aggressive environments than a altruistic ones, however, a efficiency of altruism was higher inside more favorable environments. the comparison of altruistic strategies with each other shows that inside aggressive environments, everyone should be supported to minimize extinction, while under more favorable conditions, it was more efficient to support a weakest participants. studying a dynamics of participants' capitals we identify situations where a two considered criteria contradict each other. at a next stage of a study, combined voting strategies and societies involving participants with selfish and altruistic strategies will be explored.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
2901,"the method was presented considering parallelizing a computation of solutions to discrete-time, linear-quadratic, finite-horizon optimal control problems, which we will refer to as lqr problems. this class of problem arises frequently inside robotic trajectory optimization. considering very complicated robots, a size of these resulting problems should be large enough that computing a solution was prohibitively slow when with the help of the single processor. fortunately, approaches to solving these type of problems based on numerical solutions to a kkt conditions of optimality offer the parallel solution method and should leverage multiple processors to compute solutions faster. however, these methods do not produce a useful feedback control policies that are generated as the by-product of a dynamic-programming solution method known as riccati recursion. inside this paper we derive the method which was able to parallelize a computation of riccati recursion, allowing considering super-fast solutions to a lqr problem while still generating feedback control policies. we demonstrate empirically that our method was faster than existing parallel methods.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
2914,"let $q\geq 2$ be an integer, and $\bbb f_q^d$, $d\geq 1$, be a vector space over a cyclic space $\bbb f_q$. a purpose of this paper was two-fold. first, we obtain sufficient conditions on $e \subset \bbb f_q^d$ such that a inverse fourier transform of $1_e$ generates the tight wavelet frame inside $l^2(\bbb f_q^d)$. we call these sets (tight) wavelet frame sets. a conditions are given inside terms of multiplicative and translational tilings, which was analogous with theorem 1.1 ([20]) by wang inside a setting of finite fields. inside a second part of a paper, we exhibit the constructive method considering obtaining tight wavelet frame sets inside $\bbb f_q^d$, $d\geq 2$, $q$ an odd prime and $q\equiv 3$ (mod 4).",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0
8238,"precise knowledge of a static density response function (sdrf) of a uniform electron gas (ueg) serves as key input considering numerous applications, most importantly considering density functional theory beyond generalized gradient approximations. here we extend a configuration path integral monte carlo (cpimc) formalism that is previously applied to a spatially uniform electron gas to a case of an inhomogeneous electron gas by adding the spatially periodic external potential. this procedure has recently been successfully used inside permutation blocking path integral monte carlo simulations (pb-pimc) of a warm dense electron gas [dornheim \textit{et al.}, phys. rev. e inside press, arxiv:1706.00315], but this method was restricted to low and moderate densities. implementing this procedure into cpimc allows us to obtain exact finite temperature results considering a sdrf of a electron gas at \textit{high to moderate densities} closing a gap left open by a pb-pimc data. inside this paper we demonstrate how a cpimc formalism should be efficiently extended to a spatially inhomogeneous electron gas and present a first data points. finally, we discuss finite size errors involved inside a quantum monte carlo results considering a sdrf inside detail and present the solution how to remove them that was based on the generalization of ground state techniques.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
9480,"inside this paper, we introduce the new idea behind the method to generate flexible parametric families of distributions. these models arise on competitive and complementary risks scenario, inside which a lifetime associated with the particular risk was not observable, rather, we observe only a minimum/maximum lifetime value among all risks. a latent variables have the zero truncated poisson distribution. considering a proposed family of distribution, a extra shape parameter has an important physical interpretation inside a competing and complementary risks scenario. a mathematical properties and inferential procedures are discussed. a proposed idea behind the method was applied inside some existing distributions inside which it was fully illustrated by an important data set.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16731,"inside a present study examines a statistical structure of a generated randomized density of a normal distribution and a cauchy distribution. a study put a allegation that the randomized probability density of a normal distribution should be regarded as a solution of a cauchy problem considering a heat equation, and randomized probability density of a cauchy distribution should be considered as the solution to a dirichlet problem considering a laplace equation. conversely, a solution of a cauchy problem considering a heat equation should be regarded as the randomized probability density of a normal distribution, and a solution of a dirichlet problem considering a laplace equation as randomized probability density of a cauchy distribution. a main objective of a study is a fact that considering each of these two cases to find a fisher information matrix components and structural tensor. we found nonlinear differential equations of a first, second and third order considering a density of a normal distribution and cauchy density computational difficulties to overcome . a components of a metric tensor (the fisher information matrix) and a components of a strain tensor are calculated according to formulas inside which there was a log-likelihood function, ie, logarithm of a density distribution. because of a positive definiteness of a fisher information matrix obtained inequality, which obviously satisfy a cauchy problem solution with nonnegative initial conditions inside a case of a laplace equation and a heat equation.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6206,"generic interacting many-body quantum systems are believed to behave as classical fluids on long time and length scales. due to rapid progress inside growing exceptionally pure crystals, we are now able to experimentally observe this collective motion of electrons inside solid-state systems, including graphene. we present the review of recent progress inside understanding a hydrodynamic limit of electronic motion inside graphene, written considering physicists from diverse communities. we begin by discussing a ""phase diagram"" of graphene, and a inevitable presence of impurities and phonons inside experimental systems. we derive hydrodynamics, both from the phenomenological perspective and with the help of kinetic theory. we then describe how hydrodynamic electron flow was visible inside electronic transport measurements. although we focus on graphene inside this review, a broader framework naturally generalizes to other materials. we assume only basic knowledge of condensed matter physics, and no prior knowledge of hydrodynamics.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0
8129,"we present an idea behind the method to synthesizing photographic images conditioned on semantic layouts. given the semantic label map, our idea behind the method produces an image with photographic appearance that conforms to a input layout. a idea behind the method thus functions as the rendering engine that takes the two-dimensional semantic specification of a scene and produces the corresponding photographic image. unlike recent and contemporaneous work, our idea behind the method does not rely on adversarial training. we show that photographic images should be synthesized from semantic layouts by the single feedforward network with appropriate structure, trained end-to-end with the direct regression objective. a presented idea behind the method scales seamlessly to high resolutions; we demonstrate this by synthesizing photographic images at 2-megapixel resolution, a full resolution of our training data. extensive perceptual experiments on datasets of outdoor and indoor scenes demonstrate that images synthesized by a presented idea behind the method are considerably more realistic than alternative approaches. a results are shown inside a supplementary video at this https url",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5496,"we present the robust real-time lidar 3d object detector that leverages heteroscedastic aleatoric uncertainties to significantly improve its detection performance. the multi-loss function was designed to incorporate uncertainty estimations predicted by auxiliary output layers. with the help of our proposed method, a network ignores to train from noisy samples, and focuses more on informative ones. we validate our method on a kitti object detection benchmark. our method surpasses a baseline method which does not explicitly approximate uncertainties by up to nearly 9% inside terms of average precision (ap). it also produces state-of-the-art results compared to other methods while running with an inference time of only 72 ms. inside addition, we conduct extensive experiments to understand how aleatoric uncertainties behave. extracting aleatoric uncertainties brings almost no additional computation cost during a deployment, making our method highly desirable considering autonomous driving applications.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
5112,"considering the balanced wall crossing inside geometric invariant theory, there exist derived equivalences between a corresponding git quotients if certain numerical conditions are satisfied. given such the wall crossing, i construct the perverse sheaf of categories on the disk, singular at the point, with half-monodromies recovering these equivalences, and with behaviour at a singular point controlled by the git quotient stack associated to a wall. taking complexified grothendieck groups gives the perverse sheaf of vector spaces: i characterise when this was an intersection cohomology complex of the local system on a punctured disk.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
7954,"sketching has emerged as the powerful technique considering speeding up problems inside numerical linear algebra, such as regression. inside a overconstrained regression problem, one was given an $n \times d$ matrix $a$, with $n \gg d$, as well as an $n \times 1$ vector $b$, and one wants to find the vector $\hat{x}$ so as to minimize a residual error $\|ax-b\|_2$. with the help of a sketch and solve paradigm, one first computes $s \cdot a$ and $s \cdot b$ considering the randomly chosen matrix $s$, then outputs $x' = (sa)^{\dagger} sb$ so as to minimize $\|sax' - sb\|_2$. a sketch-and-solve paradigm gives the bound on $\|x'-x^*\|_2$ when $a$ was well-conditioned. our main result was that, when $s$ was a subsampled randomized fourier/hadamard transform, a error $x' - x^*$ behaves as if it lies inside the ""random"" direction within this bound: considering any fixed direction $a\in \mathbb{r}^d$, we have with $1 - d^{-c}$ probability that \[ \langle a, x'-x^*\rangle \lesssim \frac{\|a\|_2\|x'-x^*\|_2}{d^{\frac{1}{2}-\gamma}}, \quad (1) \] where $c, \gamma > 0$ are arbitrary constants. this implies $\|x'-x^*\|_{\infty}$ was the factor $d^{\frac{1}{2}-\gamma}$ smaller than $\|x'-x^*\|_2$. it also gives the better bound on a generalization of $x'$ to new examples: if rows of $a$ correspond to examples and columns to features, then our result gives the better bound considering a error introduced by sketch-and-solve when classifying fresh examples. we show that not all oblivious subspace embeddings $s$ satisfy these properties. inside particular, we give counterexamples showing that matrices based on count-sketch or leverage score sampling do not satisfy these properties. we also provide lower bounds, both on how small $\|x'-x^*\|_2$ should be, and considering our new guarantee (1), showing that a subsampled randomized fourier/hadamard transform was nearly optimal.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14860,"large $n$ melonic theories are characterized by two-point function feynman diagrams built exclusively out of melons. this leads to conformal invariance at strong coupling, four-point function diagrams that are exclusively ladders, and higher-point functions that are built out of four-point functions joined together. we uncover an incredibly useful property of these theories: a six-point function, or equivalently, a three-point function of a primary $o(n)$ invariant bilinears, regarded as an analytic function of a operator dimensions, fully determines all correlation functions, to leading nontrivial order inside $1/n$, through simple feynman-like rules. a result was applicable to any theory, not necessarily melonic, inside which higher-point correlators are built out of four-point functions. we explicitly calculate a bilinear three-point function considering $q$-body syk, at any $q$. this leads to a bilinear four-point function, as well as all higher-point functions, expressed inside terms of higher-point conformal blocks, which we discuss. we find universality of correlators of operators of large dimension, which we simplify through the saddle point analysis. we comment on a implications considering a ads dual of syk.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
15826,"clustering methods based on deep neural networks have proven promising considering clustering real-world data because of their high representational power. inside this paper, we propose the systematic taxonomy of clustering methods that utilize deep neural networks. we base our taxonomy on the comprehensive review of recent work and validate a taxonomy inside the case study. inside this case study, we show that a taxonomy enables researchers and practitioners to systematically create new clustering methods by selectively recombining and replacing distinct aspects of previous methods with a goal of overcoming their individual limitations. a experimental evaluation confirms this and shows that a method created considering a case study achieves state-of-the-art clustering quality and surpasses it inside some cases.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8984,"we study a decentralized machine learning scenario where many users collaborate to learn personalized models based on (i) their local datasets and (ii) the similarity graph over a users' learning tasks. our idea behind the method trains nonlinear classifiers inside the multi-task boosting manner without exchanging personal data and with low communication costs. when background knowledge about task similarities was not available, we propose to jointly learn a personalized models and the sparse collaboration graph through an alternating optimization procedure. we analyze a convergence rate, memory consumption and communication complexity of our decentralized algorithms, and demonstrate a benefits of our idea behind the method compared to competing techniques on synthetic and real datasets.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1
1120,"semi-supervised learning was attracting increasing attention due to a fact that datasets of many domains lack enough labeled data. variational auto-encoder (vae), inside particular, has demonstrated a benefits of semi-supervised learning. a majority of existing semi-supervised vaes utilize the classifier to exploit label information, where a parameters of a classifier are introduced to a vae. given a limited labeled data, learning a parameters considering a classifiers may not be an optimal solution considering exploiting label information. therefore, inside this paper, we develop the novel idea behind the method considering semi-supervised vae without classifier. specifically, we propose the new model called semi-supervised disentangled vae (sdvae), which encodes a input data into disentangled representation and non-interpretable representation, then a category information was directly utilized to regularize a disentangled representation using a equality constraint. to further enhance a feature learning ability of a proposed vae, we incorporate reinforcement learning to relieve a lack of data. a dynamic framework was capable of dealing with both image and text data with its corresponding encoder and decoder networks. extensive experiments on image and text datasets demonstrate a effectiveness of a proposed framework.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14732,"we consider an online version of a robust principle component analysis (pca), which arises naturally inside time-varying source separations such as video foreground-background separation. this paper proposes the compressive online robust pca with prior information considering recursively separating the sequences of frames into sparse and low-rank components from the small set of measurements. inside contrast to conventional batch-based pca, which processes all a frames directly, a proposed method processes measurements taken from each frame. moreover, this method should efficiently incorporate multiple prior information, namely previous reconstructed frames, to improve a separation and thereafter, update a prior information considering a next frame. we utilize multiple prior information by solving $n\text{-}\ell_{1}$ minimization considering incorporating a previous sparse components and with the help of incremental singular value decomposition ($\mathrm{svd}$) considering exploiting a previous low-rank components. we also establish theoretical bounds on a number of measurements required to guarantee successful separation under assumptions of static or slowly-changing low-rank components. with the help of numerical experiments, we evaluate our bounds and a performance of a proposed algorithm. inside addition, we apply a proposed algorithm to online video foreground and background separation from compressive measurements. experimental results show that a proposed method outperforms a existing methods.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0
15612,"with the help of observations made with mosfire on keck i as part of a zfire survey, we present a stellar mass tully-fisher relation at 2.0 < z < 2.5. a sample is drawn from the stellar mass limited, ks-band selected catalog from zfourge over a candels area inside a cosmos field. we model a shear of a halpha emission line to derive rotational velocities at 2.2x a scale radius of an exponential disk (v2.2). we correct considering a blurring effect of the two-dimensional psf and a fact that a mosfire psf was better approximated by the moffat than the gaussian, which was more typically assumed considering natural seeing. we find considering a tully-fisher relation at 2.0 < z < 2.5 that logv2.2 =(2.18 +/- 0.051)+(0.193 +/- 0.108)(logm/msun - 10) and infer an evolution of a zeropoint of delta m/msun = -0.25 +/- 0.16 dex or delta m/msun = -0.39 +/- 0.21 dex compared to z = 0 when adopting the fixed slope of 0.29 or 1/4.5, respectively. we also derive a alternative kinematic estimator s0.5, with the best-fit relation logs0.5 =(2.06 +/- 0.032)+(0.211 +/- 0.086)(logm/msun - 10), and infer an evolution of delta m/msun= -0.45 +/- 0.13 dex compared to z < 1.2 if we adopt the fixed slope. we investigate and review various systematics, ranging from psf effects, projection effects, systematics related to stellar mass derivation, selection biases and slope. we find that discrepancies between a various literature values are reduced when taking these into account. our observations correspond well with a gradual evolution predicted by semi-analytic models.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14551,"several statistical approaches based on reproducing kernels have been proposed to detect abrupt changes arising inside a full distribution of a observations and not only inside a mean or variance. some of these approaches enjoy good statistical properties (oracle inequality, \ldots). nonetheless, they have the high computational cost both inside terms of time and memory. this makes their application difficult even considering small and medium sample sizes ($n< 10^4$). this computational issue was addressed by first describing the new efficient and exact algorithm considering kernel multiple change-point detection with an improved worst-case complexity that was quadratic inside time and linear inside space. it allows dealing with medium size signals (up to $n \approx 10^5$). second, the faster but approximation algorithm was described. it was based on the low-rank approximation to a gram matrix. it was linear inside time and space. this approximation algorithm should be applied to large-scale signals ($n \geq 10^6$). these exact and approximation algorithms have been implemented inside \texttt{r} and \texttt{c} considering various kernels. a computational and statistical performances of these new algorithms have been assessed through empirical experiments. a runtime of a new algorithms was observed to be faster than that of other considered procedures. finally, simulations confirmed a higher statistical accuracy of kernel-based approaches to detect changes that are not only inside a mean. these simulations also illustrate a flexibility of kernel-based approaches to analyze complex biological profiles made of dna copy number and allele b frequencies. an r package implementing a idea behind the method will be made available on github.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0
3635,"nickel oxide (nio) has been studied extensively considering various applications ranging from electrochemistry to solar cells [1,2]. inside recent years, nio attracted much attention as an antiferromagnetic (af) insulator material considering spintronic devices [3-10]. understanding a spin - phonon coupling inside nio was the key to its functionalization, and enabling af spintronics' promise of ultra-high-speed and low-power dissipation [11,12]. however, despite its status as an exemplary af insulator and the benchmark material considering a study of correlated electron systems, little was known about a spin - phonon interaction, and a associated energy dissipation channel, inside nio. inside addition, there was the long-standing controversy over a large discrepancies between a experimental and theoretical values considering a electron, phonon, and magnon energies inside nio [13-23]. this gap inside knowledge was explained by nio optical selection rules, high neel temperature and dominance of a magnon band inside a visible raman spectrum, which precludes the conventional idea behind the method considering investigating such interaction. here we show that by with the help of ultraviolet (uv) raman spectroscopy one should extract a spin - phonon coupling coefficients inside nio. we established that unlike inside other materials, a spins of ni atoms interact more strongly with a longitudinal optical (lo) phonons than with a transverse optical (to) phonons, and produce opposite effects on a phonon energies. a peculiarities of a spin - phonon coupling are consistent with a trends given by density functional theory calculations. a obtained results shed light on a nature of a spin - phonon coupling inside af insulators and may aid inside developing innovative spintronic devices.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
4935,"randomly censored survival data are frequently encountered inside applied sciences including biomedical or reliability applications and clinical trial analyses. testing a significance of statistical hypotheses was crucial inside such analyses to get conclusive inference but a existing likelihood based tests, under the fully parametric model, are extremely non-robust against outliers inside a data. although, there exists the few robust parameter estimators (e.g., m-estimators and minimum density power divergence estimators) given randomly censored data, there was hardly any robust testing procedure available inside a literature inside this context. one of a major difficulties inside this context was a construction of the suitable consistent estimator of a asymptotic variance of m estimators; a latter was the function of a unknown censoring distribution. inside this paper, we take a first step inside this direction by proposing the consistent estimator of asymptotic variance of a m-estimators based on randomly censored data without any assumption on a form of a censoring scheme. we then describe and study the class of robust wald-type tests considering parametric statistical hypothesis, both simple as well as composite, under such set-up, along with their general asymptotic and robustness properties. robust tests considering comparing two independent randomly censored samples and robust tests against one sided alternatives are also discussed. their advantages and usefulness are demonstrated considering a tests based on a minimum density power divergence estimators with specific attention to clinical trial analyses.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
1017,"opportunity detection at secondary transmitters (txs) was the key technique enabling cognitive radio (cr) networks. such detection however cannot guarantee reliable communication at secondary receivers (rxs), especially when their association distance was long. to cope with a issue, this paper proposes the novel mac called sense-and-predict (sap), where each secondary tx decides whether to access or not based on a prediction of a interference level at rx. firstly, we provide a spatial interference correlation inside the probabilistic form with the help of stochastic geometry, and utilize it to maximize a area spectral efficiency (ase) considering secondary networks while guaranteeing a service quality of primary networks. through simulations and testbed experiments with the help of usrp, sap was shown to always achieve ase improvement compared with a conventional tx based sensing.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
1929,"a human visual perception of a world was of the large fixed image that was highly detailed and sharp. however, receptor density inside a retina was not uniform: the small central region called a fovea was very dense and exhibits high resolution, whereas the peripheral region around it has much lower spatial resolution. thus, contrary to our perception, we are only able to observe the very small region around a line of sight with high resolution. a perception of the complete and stable view was aided by an attention mechanism that directs a eyes to a numerous points of interest within a scene. a eyes move between these targets inside quick, unconscious movements, known as ""saccades"". once the target was centered at a fovea, a eyes fixate considering the fraction of the second while a visual system extracts a necessary information. an artificial visual system is built based on the fully recurrent neural network set within the reinforcement learning protocol, and learned to attend to regions of interest while solving the classification task. a model was consistent with several experimentally observed phenomena, and suggests novel predictions.",1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17861,"domain adaptation was transfer learning which aims to generalize the learning model across training and testing data with different distributions. most previous research tackle this problem inside seeking the shared feature representation between source and target domains while reducing a mismatch of their data distributions. inside this paper, we propose the close yet discriminative domain adaptation method, namely cdda, which generates the latent feature representation with two interesting properties. first, a discrepancy between a source and target domain, measured inside terms of both marginal and conditional probability distribution using maximum mean discrepancy was minimized so as to attract two domains close to each other. more importantly, we also design the repulsive force term, which maximizes a distances between each label dependent sub-domain to all others so as to drag different class dependent sub-domains far away from each other and thereby increase a discriminative power of a adapted domain. moreover, given a fact that a underlying data manifold could have complex geometric structure, we further propose a constraints of label smoothness and geometric structure consistency considering label propagation. extensive experiments are conducted on 36 cross-domain image classification tasks over four public datasets. a comprehensive results show that a proposed method consistently outperforms a state-of-the-art methods with significant margins.",1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8288,"let $\ell$ be an odd prime and $d$ the positive integer. we determine when there exists the degree-$d$ number field $k$ and an elliptic curve $e/k$ with $j(e)\in\mathbb{q}\setminus\{0,1728\}$ considering which $e(k)_{\mathrm{tors}}$ contains the point of order $\ell$. we also determine when there exists such the pair $(k,e)$ considering which a image of a associated mod-$\ell$ galois representation was contained inside the cartan subgroup or its normalizer, conditionally on the conjecture of sutherland. we do a same under a stronger assumption that $e$ was defined over $\mathbb{q}$.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
2856,"we consider a cauchy problem considering a nonlinear schr√∂dinger equations (nls) with non-algebraic nonlinearities on a euclidean space. inside particular, we study a energy-critical nls on $\mathbb{r}^d$, $d=5,6$, and energy-critical nls without gauge invariance and prove that they are almost surely locally well-posed with respect to randomized initial data below a energy space. we also study a long time behavior of solutions to these equations: (i) we prove almost sure global well-posedness of a (standard) energy-critical nls on $\mathbb{r}^d$, $d = 5, 6$, inside a defocusing case, and (ii) we present the probabilistic construction of finite time blowup solutions to a energy-critical nls without gauge invariance below a energy space.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12197,"diffusion processes are governed by external triggers and internal dynamics inside complex systems. timely and cost-effective control of infectious disease spread critically relies on uncovering a underlying diffusion mechanisms, which was challenging due to invisible causality between events and their time-evolving intensity. we infer causal relationships between infections and quantify a reflexivity of the meta-population, a level of feedback on event occurrences by its internal dynamics (likelihood of the regional outbreak triggered by previous cases). these are enabled by our new proposed model, a latent influence point process (lipp) which models disease spread by incorporating macro-level internal dynamics of meta-populations based on human mobility. we analyse 15-year dengue cases inside queensland, australia. from our causal inference, outbreaks are more likely driven by statewide global diffusion over time, leading to complex behavior of disease spread. inside terms of reflexivity, precursory growth and symmetric decline inside populous regions was attributed to slow but persistent feedback on preceding outbreaks using inter-group dynamics, while abrupt growth but sharp decline inside peripheral areas was led by rapid but inconstant feedback using intra-group dynamics. our proposed model reveals probabilistic causal relationships between discrete events based on intra- and inter-group dynamics and also covers direct and indirect diffusion processes (contact-based and vector-borne disease transmissions).",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
3044,"a measurement of an exoplanet's oblateness and obliquity provides insights into a planet's internal structure and formation history. previous work with the help of small differences inside a shape of a transit light curve has been moderately successful, but is hampered by a small signal and extreme photometric precision required. a measurement of changes inside transit depth, caused by a spin precession of an oblate planet, is proposed as an alternative method. here, we present a first attempt to measure these changes. with the help of kepler photometry, we examined a brown dwarf kepler-39b and a warm saturn kepler-427b. we could not reliably constrain a oblateness of kepler-39b. we find transit depth variations considering kepler-427b at $90.1\%$ significance ($1.65\sigma$) consistent with the precession period of $p_\mathrm{prec} = 5.45^{+0.46}_{-0.37}~\mathrm{years}$ and an oblateness, $f~=~0.19^{+0.32}_{-0.16}$. this oblateness was comparable to solar system gas giants, and would raise questions about a dynamics and tidal synchronization of kepler-427b.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12371,"we present here predictions considering a spatial distribution of 21 cm brightness temperature fluctuations from high-dynamic-range simulations considering agn-dominated reionization histories that have been tested against available lyman-alpha and cmb data. we model agn by extrapolating a observed m-sigma relation to high redshifts and assign them ionizing emissivities consistent with recent uv luminosity function measurements. we assess a observability of a predicted spatial 21 cm fluctuations by ongoing and upcoming experiments inside a late stages of reionization inside a limit inside which a hydrogen 21 cm spin temperature was significantly larger than a cmb temperature. our agn-dominated reionization histories increase a variance of a 21 cm emission by the factor of up to ten compared to similar reionization histories dominated by faint galaxies, to values close to 100 mk^2 at scales accessible to experiments (k < 1 h/cmpc). this was lower than a sensitivity claimed to have been already reached by ongoing experiments by only the factor of about two or less. when reionization was dominated by agn, a 21 cm power spectrum was enhanced on all scales due to a enhanced bias of a clustering of a more massive haloes and a peak inside a large scale 21 cm power was strongly enhanced and moved to larger scales due to bigger characteristic bubble sizes. agn dominated reionization should be easily detectable by lofar (and later hera and ska1) at their design sensitivity, assuming successful foreground subtraction and instrument calibration. conversely, these could become a first non-trivial reionization scenarios to be ruled out by 21 cm experiments, thereby constraining a contribution of agn to reionization.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13843,"sampling was the fundamental topic inside graph signal processing, having found applications inside estimation, clustering, and video compression. inside contrast to traditional signal processing, a irregularity of a signal domain makes selecting the sampling set non-trivial and hard to analyze. indeed, though conditions considering graph signal interpolation from noiseless samples exist, they do not lead to the unique sampling set. a presence of noise makes choosing among these sampling sets the hard combinatorial problem. although greedy sampling schemes are commonly used inside practice, they have no performance guarantee. this work takes the twofold idea behind the method to address this issue. first, universal performance bounds are derived considering a bayesian approximation of graph signals from noisy samples. inside contrast to currently available bounds, they are not restricted to specific sampling schemes and hold considering any sampling sets. second, this paper provides near-optimal guarantees considering greedy sampling by introducing a concept of approximate submodularity and updating a classical greedy bound. it then provides explicit bounds on a approximate supermodularity of a interpolation mean-square error showing that it should be optimized with worst-case guarantees with the help of greedy search even though it was not supermodular. simulations illustrate a derived bound considering different graph models and show an application of graph signal sampling to reduce a complexity of kernel principal component analysis.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0
15640,"we show that considering neural network functions that have width less or equal to a input dimension all connected components of decision regions are unbounded. a result holds considering continuous and strictly monotonic activation functions as well as considering relu activation. this complements recent results on approximation capabilities of [hanin 2017 approximating] and connectivity of decision regions of [nguyen 2018 neural] considering such narrow neural networks. further, we give an example that negatively answers a question posed inside [nguyen 2018 neural] whether one of their main results still holds considering relu activation. our results are illustrated by means of numerical experiments.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
9002,"humans should ground natural language commands to tasks at both abstract and fine-grained levels of specificity. considering instance, the human forklift operator should be instructed to perform the high-level action, like ""grab the pallet"" or the low-level action like ""tilt back the little bit."" while robots are also capable of grounding language commands to tasks, previous methods implicitly assume that all commands and tasks reside at the single, fixed level of abstraction. additionally, methods that do not use multiple levels of abstraction encounter inefficient planning and execution times as they solve tasks at the single level of abstraction with large, intractable state-action spaces closely resembling real world complexity. inside this work, by grounding commands to all a tasks or subtasks available inside the hierarchical planning framework, we arrive at the model capable of interpreting language at multiple levels of specificity ranging from coarse to more granular. we show that a accuracy of a grounding procedure was improved when simultaneously inferring a degree of abstraction inside language used to communicate a task. leveraging hierarchy also improves efficiency: our proposed idea behind the method enables the robot to respond to the command within one second on 90% of our tasks, while baselines take over twenty seconds on half a tasks. finally, we demonstrate that the real, physical robot should ground commands at multiple levels of abstraction allowing it to efficiently plan different subtasks within a same planning hierarchy.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13229,"we give the direct, explicit and self-contained construction of the local lie groupoid integrating the given lie algebroid which only depends on a choice of the spray vector field lifting a underlying anchor map. this construction leads to the complete account of local lie theory and, inside particular, to the finite-dimensional proof of a fact that a category of germs of local lie groupoids was equivalent to that of lie algebroids.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7548,"as new instances of nested organization --beyond ecological networks-- are discovered, scholars are debating around a co-existence of two apparently incompatible macroscale architectures: nestedness and modularity. a discussion was far from being solved, mainly considering two reasons. first, nestedness and modularity appear to emerge from two contradictory dynamics, cooperation and competition. second, existing methods to assess a presence of nestedness and modularity are flawed when it comes to a evaluation of concurrently nested and modular structures. inside this work, we tackle a latter problem, presenting a concept of \textit{in-block nestedness}, the structural property determining to what extent the network was composed of blocks whose internal connectivity exhibits nestedness. we then put forward the set of optimization methods that allow us to identify such organization successfully, both inside synthetic and inside the large number of real networks. these findings challenge our understanding of a topology of ecological and social systems, calling considering new models to explain how such patterns emerge.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
16102,"note that the newer expanded version of this paper was now available at: arxiv:1802.03888 it was critical inside many applications to understand what features are important considering the model, and why individual predictions were made. considering tree ensemble methods these questions are usually answered by attributing importance values to input features, either globally or considering the single prediction. here we show that current feature attribution methods are inconsistent, which means changing a model to rely more on the given feature should actually decrease a importance assigned to that feature. to address this problem we develop fast exact solutions considering shap (shapley additive explanation) values, which were recently shown to be a unique additive feature attribution method based on conditional expectations that was both consistent and locally accurate. we integrate these improvements into a latest version of xgboost, demonstrate a inconsistencies of current methods, and show how with the help of shap values results inside significantly improved supervised clustering performance. feature importance values are the key part of understanding widely used models such as gradient boosting trees and random forests, so improvements to them have broad practical implications.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16910,"inside finite mixture models, apart from underlying mixing measure, true kernel density function of each subpopulation inside a data is, inside many scenarios, unknown. perhaps a most popular idea behind the method was to choose some kernel functions that we empirically believe our data are generated from and use these kernels to fit our models. nevertheless, as long as a chosen kernel and a true kernel are different, statistical inference of mixing measure under this setting will be highly unstable. to overcome this challenge, we propose flexible and efficient robust estimators of a mixing measure inside these models, which are inspired by a idea of minimum hellinger distance estimator, model selection criteria, and superefficiency phenomenon. we demonstrate that our estimators consistently recover a true number of components and achieve a optimal convergence rates of parameter approximation under both a well- and mis-specified kernel settings considering any fixed bandwidth. these desirable asymptotic properties are illustrated using careful simulation studies with both synthetic and real data.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
13071,"a paper addresses a stability of a co-authorship networks inside time. a analysis was done on a networks of slovenian researchers inside two time periods (1991-2000 and 2001-2010). two researchers are linked if they published at least one scientific bibliographic unit inside the given time period. as proposed by kronegger et al. (2011), a global network structures are examined by generalized blockmodeling with a assumed multi-core--semi-periphery--periphery blockmodel type. a term core denotes the group of researchers who published together inside the systematic way with each other. a obtained blockmodels are comprehensively analyzed by visualizations and through considering several statistics regarding a global network structure. to measure a stability of a obtained blockmodels, different adjusted modified rand and wallace indices are applied. those enable to distinguish between a splitting and merging of cores when operationalizing a stability of cores. also, a adjusted modified indices should be used when new researchers occur inside a second time period (newcomers) and when some researchers are no longer present inside a second time period (departures). a research disciplines are described and clustered according to a values of these indices. considering a obtained clusters, a sources of instability of a research disciplines are studied (e.g., merging or splitting of cores, newcomers or departures). furthermore, a differences inside a stability of a obtained cores on a level of scientific disciplines are studied by linear regression analysis where some personal characteristics of a researchers (e.g., age, gender), are also considered.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9130,"gaussian processes (gps) are flexible models that should capture complex structure inside large-scale dataset due to their non-parametric nature. however, a usage of gps inside real-world application was limited due to their high computational cost at inference time. inside this paper, we introduce the new framework, \textit{kernel distillation}, to approximate the fully trained teacher gp model with kernel matrix of size $n\times n$ considering $n$ training points. we combine inducing points method with sparse low-rank approximation inside a distillation procedure. a distilled student gp model only costs $o(m^2)$ storage considering $m$ inducing points where $m \ll n$ and improves a inference time complexity. we demonstrate empirically that kernel distillation provides better trade-off between a prediction time and a test performance compared to a alternatives.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18580,"we propose the family of variational approximations to bayesian posterior distributions, called $\alpha$-vb, with provable statistical guarantees. a standard variational approximation was the special case of $\alpha$-vb with $\alpha=1$. when $\alpha \in(0,1]$, the novel class of variational inequalities are developed considering linking a bayes risk under a variational approximation to a objective function inside a variational optimization problem, implying that maximizing a evidence lower bound inside variational inference has a effect of minimizing a bayes risk within a variational density family. operating inside the frequentist setup, a variational inequalities imply that point estimates constructed from a $\alpha$-vb procedure converge at an optimal rate to a true parameter inside the wide range of problems. we illustrate our general theory with the number of examples, including a mean-field variational approximation to (low)-high-dimensional bayesian linear regression with spike and slab priors, mixture of gaussian models, latent dirichlet allocation, and (mixture of) gaussian variational approximation inside regular parametric models.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0
723,"this paper presents two unsupervised learning layers (ul layers) considering label-free video analysis: one considering fully connected layers, and a other considering convolutional ones. a proposed ul layers should play two roles: they should be a cost function layer considering providing global training signal; meanwhile they should be added to any regular neural network layers considering providing local training signals and combined with a training signals backpropagated from upper layers considering extracting both slow and fast changing features at layers of different depths. therefore, a ul layers should be used inside either pure unsupervised or semi-supervised settings. both the closed-form solution and an online learning algorithm considering two ul layers are provided. experiments with unlabeled synthetic and real-world videos demonstrated that a neural networks equipped with ul layers and trained with a proposed online learning algorithm should extract shape and motion information from video sequences of moving objects. a experiments demonstrated a potential applications of ul layers and online learning algorithm to head orientation approximation and moving object localization.",1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14904,"spectral clustering has become one of a most widely used clustering techniques when a structure of a individual clusters was non-convex or highly anisotropic. yet, despite its immense popularity, there exists fairly little theory about performance guarantees considering spectral clustering. this issue was partly due to a fact that spectral clustering typically involves two steps which complicated its theoretical analysis: first, a eigenvectors of a associated graph laplacian are used to embed a dataset, and second, k-means clustering algorithm was applied to a embedded dataset to get a labels. this paper was devoted to a theoretical foundations of spectral clustering and graph cuts. we consider the convex relaxation of graph cuts, namely ratio cuts and normalized cuts, that makes a usual two-step idea behind the method of spectral clustering obsolete and at a same time gives rise to the rigorous theoretical analysis of graph cuts and spectral clustering. we derive deterministic bounds considering successful spectral clustering using the spectral proximity condition that naturally depends on a algebraic connectivity of each cluster and a inter-cluster connectivity. moreover, we demonstrate by means of some popular examples that our bounds should achieve near-optimality. our findings are also fundamental considering a theoretical understanding of kernel k-means. numerical simulations confirm and complement our analysis.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19012,"when investigators seek to approximate causal effects, they often assume that selection into treatment was based only on observed covariates. under this identification strategy, analysts must adjust considering observed confounders. while basic regression models have long been a dominant method of statistical adjustment, more robust methods based on matching or weighting have become more common. of late, even more flexible methods based on machine learning methods have been developed considering statistical adjustment. these machine learning methods are designed to be black box methods with little input from a researcher. recent research used the data competition to evaluate various methods of statistical adjustment and found that black box methods out performed all other methods of statistical adjustment. matching methods with covariate prioritization are designed considering direct input from substantive investigators inside direct contrast to black methods. inside this article, we use the different research design to compare matching with covariate prioritization to black box methods. we use black box methods to replicate results from five studies where matching with covariate prioritization is used to customize a statistical adjustment inside direct response to substantive expertise. we find little difference across a methods. we conclude with advice considering investigators.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7234,"inside recent years, stochastic gradient descent (sgd) based techniques has become a standard tools considering training neural networks. however, formal theoretical understanding of why sgd should train neural networks inside practice was largely missing. inside this paper, we make progress on understanding this mystery by providing the convergence analysis considering sgd on the rich subset of two-layer feedforward networks with relu activations. this subset was characterized by the special structure called ""identity mapping"". we prove that, if input follows from gaussian distribution, with standard $o(1/\sqrt{d})$ initialization of a weights, sgd converges to a global minimum inside polynomial number of steps. unlike normal vanilla networks, a ""identity mapping"" makes our network asymmetric and thus a global minimum was unique. to complement our theory, we are also able to show experimentally that multi-layer networks with this mapping have better performance compared with normal vanilla networks. our convergence theorem differs from traditional non-convex optimization techniques. we show that sgd converges to optimal inside ""two phases"": inside phase i, a gradient points to a wrong direction, however, the potential function $g$ gradually decreases. then inside phase ii, sgd enters the nice one point convex region and converges. we also show that a identity mapping was necessary considering convergence, as it moves a initial point to the better place considering optimization. experiment verifies our claims.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
12899,"regularization techniques are widely employed inside optimization-based approaches considering solving ill-posed inverse problems inside data analysis and scientific computing. these methods are based on augmenting a objective with the penalty function, which was specified based on prior domain-specific expertise to induce the desired structure inside a solution. we consider a problem of learning suitable regularization functions from data inside settings inside which precise domain knowledge was not directly available. previous work under a title of `dictionary learning' or `sparse coding' may be viewed as learning the regularization function that should be computed using linear programming. we describe generalizations of these methods to learn regularizers that should be computed and optimized using semidefinite programming. our framework considering learning such semidefinite regularizers was based on obtaining structured factorizations of data matrices, and our algorithmic idea behind the method considering computing these factorizations combines recent techniques considering rank minimization problems along with an operator analog of sinkhorn scaling. under suitable conditions on a input data, our algorithm provides the locally linearly convergent method considering identifying a correct regularizer that promotes a type of structure contained inside a data. our analysis was based on a stability properties of operator sinkhorn scaling and their relation to geometric aspects of determinantal varieties (in particular tangent spaces with respect to these varieties). a regularizers obtained with the help of our framework should be employed effectively inside semidefinite programming relaxations considering solving inverse problems.",1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0
15932,"we used a gemini multi-object spectrograph integral field unit to map a gas distribution, excitation and kinematics within a inner kiloparsec of four nearby low-luminosity active galaxies: ngc3982, ngc4501, ngc2787 and ngc4450. a observations cover a spectral range 5600-7000{\aa} at the velocity resolution of 120km/s and spatial resolution ranging from 50 to 70pc at a galaxies. extended emission inside h{\alpha}, [nii]{\lambda}{\lambda}6548,6583, [sii]{\lambda}{\lambda}6716,6730 over most of a field-of-view was observed considering all galaxies, while only ngc3982 shows [oi]{\lambda}6300 extended emission. a h{\alpha} equivalent widths combined with a [nii]/h{\alpha} line ratios reveal that ngc3982 and ngc4450 harbor seyfert nuclei surrounded by regions with liner excitation, while ngc2787 and ngc4501 harbor liner nuclei. ngc3982 shows the partial ring of recent star-formation at 500pc from a nucleus, while inside ngc4501 the region at 500pc west of a nucleus shows liner excitation but has been interpreted as an aging hii region with a gas excitation dominated by shocks from supernovae. a line-of-sight velocity field of a gas shows the rotation pattern considering all galaxies, with deviations from pure disk rotation observed inside ngc3982, ngc4501 and ngc4450. considering ngc4501 and ngc4450, many of these deviations are spatially coincident with dust structures seen inside optical continuum images, leading to a interpretation that a deviations are due to shocks inside a gas traced by a dust. the speculation was that these shocks lead to loss of angular momentum, allowing a gas to be transferred inwards to feed a agn. inside a case of ngc2787, instead of deviations inside a rotation field, we see the misalignment of 40{^\circ} between a orientation of a line of nodes of a gas rotation and a photometric major axis of a galaxy. evidence of compact nuclear outflows are seen inside ngc4501 and ngc4450.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7660,"we present the search considering metal absorption line systems at a highest redshifts to date with the help of the deep (30h) vlt/x-shooter spectrum of a z = 7.084 quasi-stellar object (qso) ulas j1120+0641. we detect seven intervening systems at z > 5.5, with a highest-redshift system being the c iv absorber at z = 6.51. we find tentative evidence that a mass density of c iv remains flat or declines with redshift at z < 6, while a number density of c ii systems remains relatively flat over 5 < z < 7. these trends are broadly consistent with models of chemical enrichment by star formation-driven winds that include the softening of a ultraviolet background towards higher redshifts. we find the larger number of weak ( w_rest < 0.3a ) mg ii systems over 5.9 < z < 7.0 than predicted by the power-law fit to a number density of stronger systems. this was consistent with trends inside a number density of weak mg ii systems at z = 2.5, and suggests that a mechanisms that create these absorbers are already inside place at z = 7. finally, we investigate a associated narrow si iv, c iv, and n v absorbers located near a qso redshift, and find that at least one component shows evidence of partial covering of a continuum source.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6191,"we study electroweak scale dark matter (dm) whose interactions with baryonic matter are mediated by the heavy anomalous $z'$. we emphasize that when a dm was the majorana particle, its low-velocity annihilations are dominated by loop suppressed annihilations into a gauge bosons, rather than by p-wave or chirally suppressed annihilations into a sm fermions. because a $z'$ was anomalous, these kinds of dm models should be realized only as effective field theories (efts) with the well-defined cutoff, where heavy spectator fermions restore gauge invariance at high energies. we formulate these efts, approximate their cutoff and properly take into account a effect of a chern-simons terms one obtains after a spectator fermions are integrated out. we find that, while considering light dm collider and direct detection experiments usually provide a strongest bounds, a bounds at higher masses are heavily dominated by indirect detection experiments, due to strong annihilation into $w^+w^-$, $zz$, $z\gamma$ and possibly into $gg$ and $\gamma\gamma$. we emphasize that these annihilation channels are generically significant because of a structure of a eft, and therefore these models are prone to strong indirect detection constraints. even though we focus on selected $z'$ models considering illustrative purposes, our setup was completely generic and should be used considering analyzing a predictions of any anomalous $z'$-mediated dm model with arbitrary charges.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5100,"distribution of black spot defects and small clusters inside 1 mev krypton irradiated 3c-sic has been investigated with the help of advanced scanning transmission electron microscopy (stem) and tem. we find that two thirds of clusters smaller than 1 nm identified inside stem are invisible inside tem images. considering clusters that are larger than 1 nm, stem and tem results match very well. the cluster dynamics model has been developed considering sic to reveal processes that contribute to evolution of defect clusters and validated against a (s)tem results. simulations showed that the model based on established properties of point defects (pds) generation, reaction, clustering, and cluster dissociation, was unable to predict black spot defects distribution consistent with stem observations. this failure suggests that additional phenomena not included inside the simple point-defect picture may contribute to radiation-induced evolution of defect clusters inside sic and with the help of our model we have determined a effects of the number of these additional phenomena on cluster evolution. with the help of these additional phenomena it was possible to fit parameters within physically justifiable ranges that yield agreement between cluster distributions predicted by simulations and those measured experimentally.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
133,"a survey of a mid-infrared sky by a wide-field infrared survey explorer (wise) led to a discovery of extremely cold low-mass brown dwarfs, classified as y dwarfs, which extend a t class to lower temperatures. twenty-four y dwarfs are known at a time of writing. here we present improved parallaxes considering four of these, determined with the help of spitzer images. we give new photometry considering four late-type t and three y dwarfs, and new spectra of three y dwarfs, obtained at gemini observatory. we also present previously unpublished photometry taken from hst, eso, spitzer and wise archives of 11 late-type t and 9 y dwarfs. a near-infrared data are put on to a same photometric system, forming the homogeneous data set considering a coolest brown dwarfs. we compare recent models to our photometric and spectroscopic data set. we confirm that non-equilibrium atmospheric chemistry was important considering these objects. non-equilibrium cloud-free models reproduce well a near-infrared spectra and mid-infrared photometry considering a warmer y dwarfs with 425 <= t_eff k <= 450. the small amount of cloud cover may improve a model fits inside a near-infrared considering a y dwarfs with 325 <= t_eff k <= 375. neither cloudy nor cloud-free models reproduce a near-infrared photometry considering a t_eff = 250 k y dwarf w0855. we use a mid-infrared region, where most of a flux originates, to constrain our models of w0855. we find that w0855 likely has the mass of 1.5 - 8 jupiter masses and an age of 0.3 - 6 gyr. a y dwarfs with measured parallaxes are within 20 pc of a sun and have tangential velocities typical of a thin disk. a metallicities and ages we derive considering a sample are generally solar-like. we approximate that a known y dwarfs are 3 to 20 jupiter-mass objects with ages of 0.6 to 8.5 gyr.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3258,"energy consumption considering hot water production was the major draw inside high efficiency buildings. optimizing this has typically been approached from the thermodynamics perspective, decoupled from occupant influence. furthermore, optimization usually presupposes existence of the detailed dynamics model considering a hot water system. these assumptions lead to suboptimal energy efficiency inside a real world. inside this paper, we present the novel reinforcement learning based methodology which optimizes hot water production. a proposed methodology was completely generalizable, and does not require an offline step or human domain knowledge to build the model considering a hot water vessel or a heating element. occupant preferences too are learnt on a fly. a proposed system was applied to the set of 32 houses inside a netherlands where it reduces energy consumption considering hot water production by roughly 20% with no loss of occupant comfort. extrapolating, this translates to absolute savings of roughly 200 kwh considering the single household on an annual basis. this performance should be replicated to any domestic hot water system and optimization objective, given that a fairly minimal requirements on sensor data are met. with millions of hot water systems operational worldwide, a proposed framework has a potential to reduce energy consumption inside existing and new systems on the multi gigawatt-hour scale inside a years to come.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
877,"causal mediation analysis aims to approximate a natural direct and indirect effects under clearly specified assumptions. traditional mediation analysis based on ordinary least squares (ols) relies on a absence of unmeasured causes of a putative mediator and outcome. when this assumption cannot be justified, instrumental variables (iv) estimators should be used inside order to produce an asymptotically unbiased estimator of a mediator-outcome link. however, provided that valid instruments exist, bias removal comes at a cost of variance inflation considering standard iv procedures such as two-stage least squares (tsls). the semi-parametric stein-like (spsl) estimator has been proposed inside a literature that strikes the natural trade-off between a unbiasedness of a tsls procedure and a relatively small variance of a ols estimator. moreover, a spsl has a advantage that its shrinkage parameter should be directly estimated from a data. inside this paper, we demonstrate how this stein-like estimator should be implemented inside a context of a approximation of natural direct and natural indirect effects of treatments inside randomized controlled trials. a performance of a competing methods was studied inside the simulation study, inside which both a strength of hidden confounding and a strength of a instruments are independently varied. these considerations are motivated by the trial inside mental health evaluating a impact of the primary care-based intervention to reduce depression inside a elderly.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
2259,"we introduce a cluster-eagle (c-eagle) simulation project, the set of cosmological hydrodynamical zoom simulations of a formation of $30$ galaxy clusters inside a mass range $10^{14}<m_{200}/\mathrm{m}_{\odot}<10^{15.4}$ that incorporates a hydrangea sample of bah√© et al. (2017). a simulations adopt a state-of-the-art eagle galaxy formation model, with the gas particle mass of $1.8\times10^{6}\,\mathrm{m}_{\odot}$ and physical softening length of $0.7\,\mathrm{kpc}$. inside this paper, we introduce a sample and present a low-redshift global properties of a clusters. we calculate a x-ray properties inside the manner consistent with observational techniques, demonstrating a bias and scatter introduced by with the help of estimated masses. we find a total stellar content and black hole masses of a clusters to be inside good agreement with a observed relations. however, a clusters are too gas rich, suggesting that a agn feedback model was not efficient enough at expelling gas from a high-redshift progenitors of a clusters. a x-ray properties, such as a spectroscopic temperature and a soft-band luminosity, and a sunyaev-zel'dovich properties are inside reasonable agreement with a observed relations. however, a clusters have too high central temperatures and larger-than-observed entropy cores, which was likely driven by a agn feedback after a cluster core has formed. a total metal content and its distribution throughout a icm are the good match to a observations.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
616,"end-to-end (e2e) systems have achieved competitive results compared to conventional hybrid hidden markov model (hmm)-deep neural network based automatic speech recognition (asr) systems. such e2e systems are attractive due to a lack of dependence on alignments between input acoustic and output grapheme or hmm state sequence during training. this paper explores a design of an asr-free end-to-end system considering text query-based keyword search (kws) from speech trained with minimal supervision. our e2e kws system consists of three sub-systems. a first sub-system was the recurrent neural network (rnn)-based acoustic auto-encoder trained to reconstruct a audio through the finite-dimensional representation. a second sub-system was the character-level rnn language model with the help of embeddings learned from the convolutional neural network. since a acoustic and text query embeddings occupy different representation spaces, they are input to the third feed-forward neural network that predicts whether a query occurs inside a acoustic utterance or not. this e2e asr-free kws system performs respectably despite lacking the conventional asr system and trains much faster.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19407,"gaussian belief propagation (bp) has been widely used considering distributed approximation inside large-scale networks such as a smart grid, communication networks, and social networks, where local measurements/observations are scattered over the wide geographical area. however, a convergence of gaus- sian bp was still an open issue. inside this paper, we consider a convergence of gaussian bp, focusing inside particular on a convergence of a information matrix. we show analytically that a exchanged message information matrix converges considering arbitrary positive semidefinite initial value, and its dis- tance to a unique positive definite limit matrix decreases exponentially fast.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5011,"when performing the conceptual analysis of the concept, philosophers are interested inside all forms of expression of the concept inside the text---be it direct or indirect, explicit or implicit. inside this paper, we experiment with topic-based methods of automating a detection of concept expressions inside order to facilitate philosophical conceptual analysis. we propose six methods based on lda, and evaluate them on the new corpus of court decision that we had annotated by experts and non-experts. our results indicate that these methods should yield important improvements over a keyword heuristic, which was often used as the concept detection heuristic inside many contexts. while more work remains to be done, this indicates that detecting concepts through topics should serve as the general-purpose method considering at least some forms of concept expression that are not captured with the help of naive keyword approaches.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14706,"this monograph aims at providing an introduction to key concepts, algorithms, and theoretical results inside machine learning. a treatment concentrates on probabilistic models considering supervised and unsupervised learning problems. it introduces fundamental concepts and algorithms by building on first principles, while also exposing a reader to more advanced topics with extensive pointers to a literature, within the unified notation and mathematical framework. a material was organized according to clearly defined categories, such as discriminative and generative models, frequentist and bayesian approaches, exact and approximate inference, as well as directed and undirected models. this monograph was meant as an entry point considering researchers with the background inside probability and linear algebra.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2150,"inside recent years, mems inertial sensors (3d accelerometers and 3d gyroscopes) have become widely available due to their small size and low cost. inertial sensor measurements are obtained at high sampling rates and should be integrated to obtain position and orientation information. these estimates are accurate on the short time scale, but suffer from integration drift over longer time scales. to overcome this issue, inertial sensors are typically combined with additional sensors and models. inside this tutorial we focus on a signal processing aspects of position and orientation approximation with the help of inertial sensors. we discuss different modeling choices and the selected number of important algorithms. a algorithms include optimization-based smoothing and filtering as well as computationally cheaper extended kalman filter and complementary filter implementations. a quality of their estimates was illustrated with the help of both experimental and simulated data.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1
2582,"representing a semantic relations that exist between two given words (or entities) was an important first step inside the wide-range of nlp applications such as analogical reasoning, knowledge base completion and relational information retrieval. the simple, yet surprisingly accurate method considering representing the relation between two words was to compute a vector offset (\pairdiff) between their corresponding word embeddings. despite a empirical success, it remains unclear as to whether \pairdiff was a best operator considering obtaining the relational representation from word embeddings. we conduct the theoretical analysis of generalised bilinear operators that should be used to measure a $\ell_{2}$ relational distance between two word-pairs. we show that, if a word embeddings are standardised and uncorrelated, such an operator will be independent of bilinear terms, and should be simplified to the linear form, where \pairdiff was the special case. considering numerous word embedding types, we empirically verify a uncorrelation assumption, demonstrating a general applicability of our theoretical result. moreover, we experimentally discover \pairdiff from a bilinear relation composition operator on several benchmark analogy datasets.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11667,"why are generative adversarial networks (gans) so popular? what was a purpose of designing gans? should we justify functioning of gans theoretically? how are a theoretical guarantees? are there any shortcomings? with a popularity of gans, a researchers across a globe have been perplexed by these questions. inside a last year (2017), the plethora of research papers attempted to answer a above questions. inside this article, we put inside our best efforts to compare and contrast different results and put forth the summary of theoretical contributions about gans with focus on image/visual applications. our main aim was to highlight a primary issues related to gans that each of these papers examine. besides we provide insight into how each of a discussed articles solve a concerned problems. we expect this summary paper to give the bird's eye view to the person wishing to understand a theory behind gans.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8376,"inside this paper we introduce raduls2, a fastest parallel sorter based on radix algorithm. it was optimized to process huge amounts of data making use of modern multicore cpus. a main novelties include: extremely optimized algorithm considering handling tiny arrays (up to about the hundred of records) that could appear even billions times as subproblems to handle and improved processing of larger subarrays with better use of non-temporal memory stores.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3080,"a paper was concerned with a bose-einstein condensate described by a attractive gross-pitaevskii equation inside r 2 , where a external potential was unbounded from below. we show that when a interaction strength increases to the critical value, a gross-pitaevskii minimizer collapses to one singular point and we analyze a details of a collapse exactly up to a leading order.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17344,"\begin{abstract} we model individual t2dm patient blood glucose level (bgl) by stochastic process with discrete number of states mainly but not solely governed by medication regimen (e.g. insulin injections). bgl states change otherwise according to various physiological triggers which render the stochastic, statistically unknown, yet assumed to be quasi-stationary, nature of a process. inside order to express incentive considering being inside desired healthy bgl we heuristically define the reward function which returns positive values considering desirable bg levels and negative values considering undesirable bg levels. a state space consists of sufficient number of states inside order to allow considering memoryless assumption. this, inside turn, allows to formulate markov decision process (mdp), with an objective to maximize a total reward, summarized over the long run. a probability law was found by model-based reinforcement learning (rl) and a optimal insulin treatment policy was retrieved from mdp solution.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6000,"inside this study, a wind data series from five locations inside aegean sea islands, a most active `hotspots' inside terms of refugee influx during a oct/2015 - jan/2016 period, are investigated. a analysis of a three-per-site data series includes standard statistical analysis and parametric distributions, auto-correlation analysis, cross-correlation analysis between a sites, as well as various arma models considering estimating a feasibility and accuracy of such spatio-temporal linear regressors considering predictive analytics. strong correlations are detected across specific sites and appropriately trained arma(7,5) models achieve 1-day look-ahead error (rmse) of less than 1.9 km/h on average wind speed. a results show that such data-driven statistical approaches are extremely useful inside identifying unexpected and sometimes counter-intuitive associations between a available spatial data nodes, which was very important when designing corresponding models considering short-term forecasting of sea condition, especially average wave height and direction, which was inside fact what defines a associated weather risk of crossing these passages inside refugee influx patterns.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11387,"a present paper considers testing an erdos--renyi random graph model against the stochastic block model inside a asymptotic regime where a average degree of a graph grows with a graph size n. our primary interest lies inside those cases inside which a signal-to-noise ratio was at the constant level. focusing on symmetric two block alternatives, we first derive joint central limit theorems considering linear spectral statistics of power functions considering properly rescaled graph adjacency matrices under both a null and local alternative hypotheses. a powers inside a linear spectral statistics are allowed to grow to infinity together with a graph size. inside addition, we show that linear spectral statistics of chebyshev polynomials are closely connected to signed cycles of growing lengths that determine a asymptotic likelihood ratio test considering a hypothesis testing problem of interest. this enables us to construct the sequence of test statistics that achieves a exact optimal asymptotic power within $o(n^3 \log n)$ time complexity inside a contiguous regime when $n^2 p_{n,av}^3 \to\infty$ where $p_{n,av}$ was a average connection probability. we further propose the class of adaptive tests that are computationally tractable and completely data-driven. they achieve nontrivial powers inside a contiguous regime and consistency inside a singular regime whenever $n p_{n,av} \to\infty$. these tests remain powerful when a alternative becomes the more general stochastic block model with more than two blocks.",1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0
170,"deep reinforcement learning yields great results considering the large array of problems, but models are generally retrained anew considering each new problem to be solved. prior learning and knowledge are difficult to incorporate when training new models, requiring increasingly longer training as problems become more complex. this was especially problematic considering problems with sparse rewards. we provide the solution to these problems by introducing concept network reinforcement learning (cnrl), the framework which allows us to decompose problems with the help of the multi-level hierarchy. concepts inside the concept network are reusable, and flexible enough to encapsulate feature extractors, skills, or other concept networks. with this hierarchical learning approach, deep reinforcement learning should be used to solve complex tasks inside the modular way, through problem decomposition. we demonstrate a strength of cnrl by training the model to grasp the rectangular prism and precisely stack it on top of the cube with the help of the gripper on the kinova jaco arm, simulated inside mujoco. our experiments show that our use of hierarchy results inside the 45x reduction inside environment interactions compared to a state-of-the-art on this task.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
1857,"a upcoming launch of a first space based x-ray polarimeter inside $\sim 40$ years will provide powerful new diagnostic information to study accreting compact objects. inside particular, analysis of rapid variability of a polarisation degree and angle will provide a opportunity to probe a relativistic motions of material inside a strong gravitational fields close to a compact objects, and enable new methods to measure black hole and neutron star parameters. however, polarisation properties are measured inside the statistical sense, and the statistically significant polarisation detection requires the fairly long exposure, even considering a brightest objects. therefore, a sub-minute timescales of interest are not accessible with the help of the direct time-resolved analysis of polarisation degree and angle. phase-folding should be used considering coherent pulsations, but not considering stochastic variability such as quasi-periodic oscillations. here, we introduce the fourier method that enables statistically robust detection of stochastic polarisation variability considering arbitrarily short variability timescales. our method was analogous to commonly used spectral-timing techniques. we find that it should be possible inside a near future to detect a quasi-periodic swings inside polarisation angle predicted by lense-thirring precession of a inner accretion flow. this was contingent on a mean polarisation degree of a source being greater than $\sim 4-5\%$, which was consistent with a best current constraints on cygnus x-1 from a late 1970s.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
8005,"a colliding cluster, ciza j2242.8+5301, displays the spectacular, almost 2 mpc long shock front with the radio based mach number m ~ 5, that was puzzlingly large compared with a x-ray approximate of m ~ 2.5. a extent to which a x-ray temperature jump was diluted by cooler unshocked gas projected through a cluster currently lacks quantification. thus, here we apply our self-consistent n-body/hydro-dynamical code (based on flash) to model this binary cluster encounter. we should account considering a location of a shock front and also a elongated x-ray emission by tidal stretching of a gas and dark matter between a two cluster centers. a required total mass was $8.9 \times 10^{14}$ msun with the 1.3:1 mass ratio favoring a southern cluster component. a relative velocity we derive was $\simeq 2500$ km/s initially between a two main cluster components, with an impact parameter of 120 kpc. this solution implies that a shock temperature jump derived from a low angular resolution x-ray satellite suzaku was underestimated by the factor of two, due to cool gas inside projection, bringing a observed x-ray and radio estimates into agreement. we propose that a complex southern relics inside ciza j2242.8+5301, have been broken up as a southerly moving ""back"" shocked gas impacts a gas still falling inside along a collision axis. finally, we use our model to generate compton-y maps to approximate a reduction inside radio flux caused by a thermal sunyaev-zel'dovich (sz) effect. at 30 ghz, this amounts to $\delta s_n = -0.072$ mjy/arcmin$^2$ and $\delta s_s = -0.075$ mjy/arcmin$^2$ at a locations of a northern and southern shock fronts respectively. our model approximate agrees with previous empirical estimates that have inferred a measured radio spectra should be significantly affected by a sz effect, with implications considering charged particle acceleration models of a radio relics.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16818,"this paper compares a efficiency of various algorithms considering implementing quantum resistant public key encryption scheme rlce on 64-bit cpus. by optimizing various algorithms considering polynomial and matrix operations over finite fields, we obtained several interesting (or even surprising) results. considering example, it was well known (e.g., moenck 1976 \cite{moenck1976practical}) that karatsuba's algorithm outperforms classical polynomial multiplication algorithm from a degree 15 and above (practically, karatsuba's algorithm only outperforms classical polynomial multiplication algorithm from a degree 35 and above ). our experiments show that 64-bit optimized karatsuba's algorithm will only outperform 64-bit optimized classical polynomial multiplication algorithm considering polynomials of degree 115 and above over finite field $gf(2^{10})$. a second interesting (surprising) result shows that 64-bit optimized chien's search algorithm ourperforms all other 64-bit optimized polynomial root finding algorithms such as bta and fft considering polynomials of all degrees over finite field $gf(2^{10})$. a third interesting (surprising) result shows that 64-bit optimized strassen matrix multiplication algorithm only outperforms 64-bit optimized classical matrix multiplication algorithm considering matrices of dimension 750 and above over finite field $gf(2^{10})$. it should be noted that existing literatures and practices recommend strassen matrix multiplication algorithm considering matrices of dimension 40 and above. all our experiments are done on the 64-bit macbook pro with i7 cpu and single thread c codes. it should be noted that a reported results should be appliable to 64 or larger bits cpu architectures. considering 32 or smaller bits cpus, these results may not be applicable. a source code and library considering a algorithms covered inside this paper are available at this http url.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
15753,"block-oriented nonlinear models are popular inside nonlinear modeling because of their advantages to be quite simple to understand and easy to use. to increase a flexibility of single branch block-oriented models, such as hammerstein, wiener, and wiener-hammerstein models, parallel block-oriented models should be considered. this paper presents the method to identify parallel wiener-hammerstein systems starting from input-output data only. inside a first step, a best linear approximation was estimated considering different input excitation levels. inside a second step, a dynamics are decomposed over the number of parallel orthogonal branches. next, a dynamics of each branch are partitioned into the linear time invariant subsystem at a input and the linear time invariant subsystem at a output. this was repeated considering each branch of a model. a static nonlinear part of a model was also estimated during this step. a consistency of a proposed initialization procedure was proven. a method was validated on real-world measurements with the help of the custom built parallel wiener-hammerstein test system.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
17350,"this paper studies a emotion recognition from musical tracks inside a 2-dimensional valence-arousal (v-a) emotional space. we propose the method based on convolutional (cnn) and recurrent neural networks (rnn), having significantly fewer parameters compared with a state-of-the-art method considering a same task. we utilize one cnn layer followed by two branches of rnns trained separately considering arousal and valence. a method is evaluated with the help of a 'mediaeval2015 emotion inside music' dataset. we achieved an rmse of 0.202 considering arousal and 0.268 considering valence, which was a best result reported on this dataset.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14270,"inside this paper, we study learning generalized driving style representations from automobile gps trip data. we propose the novel autoencoder regularized deep neural network (arnet) and the trip encoding framework trip2vec to learn drivers' driving styles directly from gps records, by combining supervised and unsupervised feature learning inside the unified architecture. experiments on the challenging driver number approximation problem and a driver identification problem show that arnet should learn the good generalized driving style representation: it significantly outperforms existing methods and alternative architectures by reaching a least approximation error on average (0.68, less than one driver) and a highest identification accuracy (by at least 3% improvement) compared with traditional supervised learning methods.",1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3598,"we consider a problem of sampling from the strongly log-concave density inside $\mathbb{r}^d$, and prove the non-asymptotic upper bound on a mixing time of a metropolis-adjusted langevin algorithm (mala). a method draws samples by running the markov chain obtained from a discretization of an appropriate langevin diffusion, combined with an accept-reject step to ensure a correct stationary distribution. relative to known guarantees considering a unadjusted langevin algorithm (ula), our bounds show that a use of an accept-reject step inside mala leads to an exponentially improved dependence on a error-tolerance. concretely, inside order to obtain samples with tv error at most $\delta$ considering the density with condition number $\kappa$, we show that mala requires $\mathcal{o} \big(\kappa d \log(1/\delta) \big)$ steps, as compared to a $\mathcal{o} \big(\kappa^2 d/\delta^2 \big)$ steps established inside past work on ula. we also demonstrate a gains of mala over ula considering weakly log-concave densities. furthermore, we derive mixing time bounds considering the zeroth-order method metropolized random walk (mrw) and show that it mixes $\mathcal{o}(\kappa d)$ slower than mala. we provide numerical examples that support our theoretical findings, and demonstrate a potential gains of metropolis-hastings adjustment considering langevin-type algorithms.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4011,"this paper considers the cloud-ran architecture with cache-enabled multi-antenna edge nodes (ens) that deliver content to cache-enabled end-users. a ens are connected to the central server using limited-capacity fronthaul links, and, based on a information received from a central server and a cached contents, they transmit on a shared wireless medium to satisfy users' requests. by leveraging cooperative transmission as enabled by ens' caches and fronthaul links, as well as multicasting opportunities provided by users' caches, the close-to-optimal caching and delivery scheme was proposed. as the result, a minimum normalized delivery time (ndt), the high-snr measure of delivery latency, was characterized to within the multiplicative constant gap of $3/2$ under a assumption of uncoded caching and fronthaul transmission, and of one-shot linear precoding. this result demonstrates a interplay among fronthaul links capacity, ens' caches, and end-users' caches inside minimizing a content delivery time.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
12297,"we study a implicit upwind finite volume scheme considering numerically approximating a linear continuity equation inside a low regularity diperna-lions setting. that is, we are concerned with advecting velocity fields that are spatially sobolev regular and data that are merely integrable. we prove that on unstructured regular meshes a rate of convergence of approximate solutions generated by a upwind scheme towards a unique distributional solution of a continuous model was at least 1/2. a numerical error was estimated inside terms of logarithmic kantorovich-rubinstein distances and provides thus the bound on a rate of weak convergence.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9026,"we present a theory of the new type of topological quantum order which was driven by a spin-orbit density wave order parameter, and distinguished by $z_2$ topological invariant. we show that when two oppositely polarized chiral bands [resulting from a rashba-type spin-orbit coupling $\alpha_k$, $k$ was crystal momentum] are significantly nested by the special wavevector ${\bf q}\sim(\pi,0)/(0,\pi)$, it induces the spatially modulated inversion of a chirality ($\alpha_{k+q}=\alpha_k^*$) between different sublattices. a resulting quantum order parameters break translational symmetry, but preserve time-reversal symmetry. it was inherently associated with the $z_2$-topological invariant along each density wave propagation direction. thus it gives the weak topological insulator inside two dimensions, with even number of spin-polarized boundary states. this phase was analogous to a quantum spin-hall state, except here a time-reversal polarization was spatially modulated, and thus it was dubbed quantum spin-hall density wave (qshdw) state. this order parameter should be realized or engineered inside quantum wires, or quasi-2d systems, by tuning a spin-orbit couping strength and chemical potential to achieve a special nesting condition.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
19267,"finding optimal correction of errors inside generic stabilizer codes was the computationally hard problem, even considering simple noise models. while this task should be simplified considering codes with some structure, such as topological stabilizer codes, developing good and efficient decoders still remains the challenge. inside our work, we systematically study the very versatile class of decoders based on feedforward neural networks. to demonstrate adaptability, we apply neural decoders to a triangular color and toric codes under various noise models with realistic features, such as spatially-correlated errors. we report that neural decoders provide significant improvement over leading efficient decoders inside terms of a error-correction threshold. with the help of neural networks simplifies a process of designing well-performing decoders, and does not require prior knowledge of a underlying noise model.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3314,"natural and artificial self-propelled systems must manage environmental interactions during movement. such interactions, which we refer to as active collisions, are fundamentally different from momentum-conserving interactions studied inside classical physics, largely because a internal driving of a locomotor should lead to persistent contact with heterogeneities. here, we experimentally and numerically study a effects of active collisions on the laterally-undulating sensory-deprived robophysical model, whose dynamics are applicable to self-propelled systems across length scales and environments. a robot moves using spatial undulation of body segments, with the nearly-linear center-of-geometry trajectory. interactions with the single rigid post scatter a robot, and these deflections are proportional to a head-post contact duration. a distribution of scattering angles was smooth and strongly-peaked directly behind a post. interactions with the single row of evenly-spaced posts (with inter-post spacing $d$) produce distributions reminiscent of far-field diffraction patterns: as $d$ decreases, distinct secondary peaks emerge as large deflections become more likely. surprisingly, we find that a presence of multiple posts does not change a nature of individual collisions; instead, multi-modal scattering patterns arise from multiple posts altering a likelihood of individual collisions to occur. as $d$ decreases, collisions near a leading edges of a posts become more probable, and we find that these interactions are associated with larger deflections. our results, which highlight a surprising dynamics that should occur during active collisions of self-propelled systems, should inform control principles considering locomotors inside complex terrain and facilitate design of task-capable active matter.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
18192,"this paper contributes the first study into how different human users deliver simultaneous control and feedback signals during human-robot interaction. as part of this work, we formalize and present the general interactive learning framework considering online cooperation between humans and reinforcement learning agents. inside many human-machine interaction settings, there was the growing gap between a degrees-of-freedom of complex semi-autonomous systems and a number of human control channels. simple human control and feedback mechanisms are required to close this gap and allow considering better collaboration between humans and machines on complex tasks. to better inform a design of concurrent control and feedback interfaces, we present experimental results from the human-robot collaborative domain wherein a human must simultaneously deliver both control and feedback signals to interactively train an actor-critic reinforcement learning robot. we compare three experimental conditions: 1) human delivered control signals, 2) reward-shaping feedback signals, and 3) simultaneous control and feedback. our results suggest that subjects provide less feedback when simultaneously delivering feedback and control signals and that control signal quality was not significantly diminished. our data suggest that subjects may also modify when and how they provide feedback. through algorithmic development and tuning informed by this study, we expect semi-autonomous actions of robotic agents should be better shaped by human feedback, allowing considering seamless collaboration and improved performance inside difficult interactive domains.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
10839,"even inside a absence of any explicit semantic annotation, vast collections of audio recordings provide valuable information considering learning a categorical structure of sounds. we consider several class-agnostic semantic constraints that apply to unlabeled nonspeech audio: (i) noise and translations inside time do not change a underlying sound category, (ii) the mixture of two sound events inherits a categories of a constituents, and (iii) a categories of events inside close temporal proximity are likely to be a same or related. without labels to ground them, these constraints are incompatible with classification loss functions. however, they may still be leveraged to identify geometric inequalities needed considering triplet loss-based training of convolutional neural networks. a result was low-dimensional embeddings of a input spectrograms that recover 41% and 84% of a performance of their fully-supervised counterparts when applied to downstream query-by-example sound retrieval and sound event classification tasks, respectively. moreover, inside limited-supervision settings, our unsupervised embeddings double a state-of-the-art classification performance.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
127,"research inside extrasolar-planet science was data-driven. with a advent of radial-velocity instruments like harps and harps-n, and transit space missions like kepler, our ability to discover and characterise extrasolar planets was no longer limited by instrumental precision but by our ability to model a data accurately. this chapter presents a models that describe radial-velocity measurements and transit light curves. i begin by deriving a solution of a two-body problem and from there, a equations describing a radial velocity of the planet-host star and a distance between star and planet centres, necessary to model transit light curves. stochastic models are then presented and i delineate how they are used to model complex physical phenomena affecting a exoplanet data sets, such as stellar activity. finally, i give the brief overview of a processes of bayesian inference, focussing on a construction of likelihood functions and prior probability distributions. inside particular, i describe different methods to specify ignorance priors.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19728,"we present an end-to-end, multimodal, fully convolutional network considering extracting semantic structures from document images. we consider document semantic structure extraction as the pixel-wise segmentation task, and propose the unified model that classifies pixels based not only on their visual appearance, as inside a traditional page segmentation task, but also on a content of underlying text. moreover, we propose an efficient synthetic document generation process that we use to generate pretraining data considering our network. once a network was trained on the large set of synthetic documents, we fine-tune a network on unlabeled real documents with the help of the semi-supervised approach. we systematically study a optimum network architecture and show that both our multimodal idea behind the method and a synthetic data pretraining significantly boost a performance.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6330,"several fundamental problems that arise inside optimization and computer science should be cast as follows: given vectors $v_1,\ldots,v_m \in \mathbb{r}^d$ and the constraint family ${\cal b}\subseteq 2^{[m]}$, find the set $s \in \cal{b}$ that maximizes a squared volume of a simplex spanned by a vectors inside $s$. the motivating example was a data-summarization problem inside machine learning where one was given the collection of vectors that represent data such as documents or images. a volume of the set of vectors was used as the measure of their diversity, and partition or matroid constraints over $[m]$ are imposed inside order to ensure resource or fairness constraints. recently, nikolov and singh presented the convex program and showed how it should be used to approximate a value of a most diverse set when ${\cal b}$ corresponds to the partition matroid. this result is recently extended to regular matroids inside works of straszak and vishnoi, and anari and oveis gharan. a question of whether these approximation algorithms should be converted into a more useful approximation algorithms -- that also output the set -- remained open. a main contribution of this paper was to give a first approximation algorithms considering both partition and regular matroids. we present novel formulations considering a subdeterminant maximization problem considering these matroids; this reduces them to a problem of finding the point that maximizes a absolute value of the nonconvex function over the cartesian product of probability simplices. a technical core of our results was the new anti-concentration inequality considering dependent random variables that allows us to relate a optimal value of these nonconvex functions to their value at the random point. unlike prior work on a constrained subdeterminant maximization problem, our proofs do not rely on real-stability or convexity and could be of independent interest both inside algorithms and complexity.",1,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0
10427,"large-scale classification of data where classes are structurally organized inside the hierarchy was an important area of research. top-down approaches that exploit a hierarchy during a learning and prediction phase are efficient considering large scale hierarchical classification. however, accuracy of top-down approaches was poor due to error propagation i.e., prediction errors made at higher levels inside a hierarchy cannot be corrected at lower levels. one of a main reason behind errors at a higher levels was a presence of inconsistent nodes that are introduced due to a arbitrary process of creating these hierarchies by domain experts. inside this paper, we propose two different data-driven approaches (local and global) considering hierarchical structure modification that identifies and flattens inconsistent nodes present within a hierarchy. our extensive empirical evaluation of a proposed approaches on several image and text datasets with varying distribution of features, classes and training instances per class shows improved classification performance over competing hierarchical modification approaches. specifically, we see an improvement upto 7% inside macro-f1 score with our idea behind the method over best td baseline. source code: this http url",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5660,"inside this paper, we construct twist automorphisms on quantum unipotent cells, which are quantum analogues of a berenstein-fomin-zelevinsky twist automorphisms on unipotent cells. we show that those quantum twist automorphisms preserve a dual canonical bases of quantum unipotent cells. moreover, we prove that quantum twist automorphisms are described by a syzygy functors considering representations of preprojective algebras inside a symmetric case. this was a quantum analogue of gei{\ss}-leclerc-schr√∂er's description, and gei{\ss}-leclerc-schr√∂er's results are essential inside our proof. as the consequence, we show that quantum twist automorphisms are compatible with quantum cluster monomials. a 6-periodicity of specific quantum twist automorphisms was also verified.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
7525,"this work contains two main contributions concerning a asymmetric broadcast channel. a first was an analysis of a exact random coding error exponents considering both users, and a second was a derivation of universal decoders considering both users. these universal decoders are certain variants of a maximum mutual information (mmi) universal decoder, which achieve a corresponding random coding exponents of optimal decoding. inside addition, we introduce some lower bounds, which involve optimization over very few parameters, unlike a original, exact exponents, which involve minimizations over auxiliary probability distributions. numerical results considering a binary symmetric broadcast channel show improvements over previously derived error exponents considering a same model.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
2263,"we provide the framework considering a assignment of multiple robots to goal locations, when robot travel times are uncertain. our premise was that time was a most valuable asset inside a system. hence, we make use of redundant robots to counter a effect of uncertainty and minimize a average waiting time at destinations. we apply our framework to transport networks represented as graphs, and consider uncertainty inside a edge costs (i.e., travel time). since solving a redundant assignment problem was strongly np-hard, we exploit structural properties of our problem to propose the polynomial-time solution with provable sub-optimality bounds. our method uses distributive aggregate functions, which allow us to efficiently (i.e., incrementally) compute a effective cost of assigning redundant robots. experimental results on random graphs show that a deployment of redundant robots through our method reduces waiting times at goal locations, when edge traversals are uncertain.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
3555,"we generalize a concept of a spin-momentum locking to magnonic systems and derive a formula to calculate a spin expectation value considering one-magnon states of general two-body spin hamiltonians. we give no-go conditions considering magnon spin to be independent of momentum. as examples of a magnon spin-momentum locking, we analyze the one-dimensional antiferromagnet with a n√©el order and two-dimensional kagome lattice antiferromagnets with a 120$^\circ$ structure. we find that a magnon spin depends on its momentum even when a hamiltonian has a $z$-axis spin rotational symmetry, which should be explained inside a context of the singular band point or the $u(1)$ symmetry breaking. the spin vortex inside momentum space generated inside the kagome lattice antiferromagnet has a winding number $q=-2$, while a typical one observed inside topological insulator surface states was characterized by $q=+1$. the magnonic analogue of a surface states, a dirac magnon with $q=+1$, was found inside another kagome lattice antiferromagnet. we also derive a sum rule considering $q$ by with the help of a poincar√©-hopf index theorem.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
14129,"graphene and some graphene like two dimensional materials; hexagonal boron nitride (hbn) and silicene have unique mechanical properties which severely limit a suitability of conventional theories used considering common brittle and ductile materials to predict a fracture response of these materials. this study revealed a fracture response of graphene, hbn and silicene nanosheets under different tiny crack lengths by molecular dynamics (md) simulations with the help of lammps. a useful strength of these large area two dimensional materials are determined by their fracture toughness. our study shows the comparative analysis of mechanical properties among a elemental analogues of graphene and suggested that hbn should be the good substitute considering graphene inside terms of mechanical properties. we have also found that a pre-cracked sheets fail inside brittle manner and their failure was governed by a strength of a atomic bonds at a crack tip. a md prediction of fracture toughness shows significant difference with a fracture toughness determined by griffth's theory of brittle failure which restricts a applicability of griffith's criterion considering these materials inside case of nano-cracks. moreover, a strengths measured inside armchair and zigzag directions of nanosheets of these materials implied that a bonds inside armchair direction has a stronger capability to resist crack propagation compared to zigzag direction.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
15751,"we present the general quantum kinetic theory of low-field magnetotransport inside weakly disordered crystals that accounts fully considering a interplay between electric-field induced interband coherence, bloch-state scattering, and an external magnetic field. a quantum kinetic equation we derive considering a bloch-state density matrix naturally incorporates a momentum-space berry phase effects whose influence on bloch-state wavepacket dynamics was normally incorporated into transport theory inside an ad hoc manner. a berry phase correction to a momentum-space density of states inside a presence of an external magnetic field implied by semiclassical wavepacket dynamics was captured by our theory as an intrinsic density-matrix response to the magnetic field. we propose the simple and general procedure considering expanding a linear response of a bloch-state density matrix to an electric field inside powers of magnetic field. as an illustration, we apply our theory to magnetotransport inside weyl semimetals. we show that a chiral anomaly (positive magnetoconductivity quadratic inside magnetic field) that appears when separate fermi surface pockets surround distinct weyl points survives only when intervalley scattering was very weak compared to intravalley scattering.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
16250,this paper presents the novel application of compositional data analysis methods inside a context of color image processing. the vector decomposition method was proposed to reveal compositional components of any vector with positive components followed by compositional data analysis to demonstrate a relation between color space concepts such as hue and saturation to their compositional counterparts. a proposed methods are applied to the magnetic resonance imaging dataset acquired from the living human brain and the digital color photograph to perform image fusion. potential future applications inside magnetic resonance imaging are mentioned and a benefits/disadvantages of a proposed methods are discussed inside terms of color image processing.,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6992,"we consider a problem of maximizing a spread of influence inside the social network by choosing the fixed number of initial seeds, formally referred to as a influence maximization problem. it admits the $(1-1/e)$-factor approximation algorithm if a influence function was submodular. otherwise, inside a worst case, a problem was np-hard to approximate to within the factor of $n^{1-\varepsilon}$. this paper studies whether this worst-case hardness result should be circumvented by making assumptions about either a underlying network topology or a cascade model. all of our assumptions are motivated by many real life social network cascades. first, we present strong inapproximability results considering the very restricted class of networks called a (stochastic) hierarchical blockmodel, the special case of a well-studied (stochastic) blockmodel inside which relationships between blocks admit the tree structure. we also provide the dynamic-program based polynomial time algorithm which optimally computes the directed variant of a influence maximization problem on hierarchical blockmodel networks. our algorithm indicates that a inapproximability result was due to a bidirectionality of influence between agent-blocks. second, we present strong inapproximability results considering the class of influence functions that are ""almost"" submodular, called 2-quasi-submodular. our inapproximability results hold even considering any 2-quasi-submodular $f$ fixed inside advance. this result also indicates that a ""threshold"" between submodularity and nonsubmodularity was sharp, regarding a approximability of influence maximization.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10508,"while deep reinforcement learning (rl) methods have achieved unprecedented successes inside the range of challenging problems, their applicability has been mainly limited to simulation or game domains due to a high sample complexity of a trial-and-error learning process. however, real-world robotic applications often need the data-efficient learning process with safety-critical constraints. inside this paper, we consider a challenging problem of learning unmanned aerial vehicle (uav) control considering tracking the moving target. to acquire the strategy that combines perception and control, we represent a policy by the convolutional neural network. we develop the hierarchical idea behind the method that combines the model-free policy gradient method with the conventional feedback proportional-integral-derivative (pid) controller to enable stable learning without catastrophic failure. a neural network was trained by the combination of supervised learning from raw images and reinforcement learning from games of self-play. we show that a proposed idea behind the method should learn the target following policy inside the simulator efficiently and a learned behavior should be successfully transferred to a dji quadrotor platform considering real-world uav control.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
15379,"automatic affect recognition was the challenging task due to a various modalities emotions should be expressed with. applications should be found inside many domains including multimedia retrieval and human computer interaction. inside recent years, deep neural networks have been used with great success inside determining emotional states. inspired by this success, we propose an emotion recognition system with the help of auditory and visual modalities. to capture a emotional content considering various styles of speaking, robust features need to be extracted. to this purpose, we utilize the convolutional neural network (cnn) to extract features from a speech, while considering a visual modality the deep residual network (resnet) of 50 layers. inside addition to a importance of feature extraction, the machine learning algorithm needs also to be insensitive to outliers while being able to model a context. to tackle this problem, long short-term memory (lstm) networks are utilized. a system was then trained inside an end-to-end fashion where - by also taking advantage of a correlations of a each of a streams - we manage to significantly outperform a traditional approaches based on auditory and visual handcrafted features considering a prediction of spontaneous and natural emotions on a recola database of a avec 2016 research challenge on emotion recognition.",1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8215,"we consider a question of estimating the solution to the system of equations that involve convex nonlinearities, the problem that was common inside machine learning and signal processing. because of these nonlinearities, conventional estimators based on empirical risk minimization generally involve solving the non-convex optimization program. we propose anchored regression, the new idea behind the method based on convex programming that amounts to maximizing the linear functional (perhaps augmented by the regularizer) over the convex set. a proposed convex program was formulated inside a natural space of a problem, and avoids a introduction of auxiliary variables, making it computationally favorable. working inside a native space also provides great flexibility as structural priors (e.g., sparsity) should be seamlessly incorporated. considering our analysis, we model a equations as being drawn from the fixed set according to the probability law. our main results provide guarantees on a accuracy of a estimator inside terms of a number of equations we are solving, a amount of noise present, the measure of statistical complexity of a random equations, and a geometry of a regularizer at a true solution. we also provide recipes considering constructing a anchor vector (that determines a linear functional to maximize) directly from a observed data.",1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0
17435,"inside this paper, we first propose the new iterative algorithm, called a k-sets+ algorithm considering clustering data points inside the semi-metric space, where a distance measure does not necessarily satisfy a triangular inequality. we show that a k-sets+ algorithm converges inside the finite number of iterations and it retains a same performance guarantee as a k-sets algorithm considering clustering data points inside the metric space. we then extend a applicability of a k-sets+ algorithm from data points inside the semi-metric space to data points that only have the symmetric similarity measure. such an extension leads to great reduction of computational complexity. inside particular, considering an n * n similarity matrix with m nonzero elements inside a matrix, a computational complexity of a k-sets+ algorithm was o((kn + m)i), where i was a number of iterations. a memory complexity to achieve that computational complexity was o(kn + m). as such, both a computational complexity and a memory complexity are linear inside n when a n * n similarity matrix was sparse, i.e., m = o(n). we also conduct various experiments to show a effectiveness of a k-sets+ algorithm by with the help of the synthetic dataset from a stochastic block model and the real network from a wondernetwork website.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
896,"a analysis of telemetry data was common inside animal ecological studies. while a collection of telemetry data considering individual animals has improved dramatically, a methods to properly account considering inherent uncertainties (e.g., measurement error, dependence, barriers to movement) have lagged behind. still, many new statistical approaches have been developed to infer unknown quantities affecting animal movement or predict movement based on telemetry data. hierarchical statistical models are useful to account considering some of a aforementioned uncertainties, as well as provide population-level inference, but they often come with an increased computational burden. considering certain types of statistical models, it was straightforward to provide inference if a latent true animal trajectory was known, but challenging otherwise. inside these cases, approaches related to multiple imputation have been employed to account considering a uncertainty associated with our knowledge of a latent trajectory. despite a increasing use of imputation approaches considering modeling animal movement, a general sensitivity and accuracy of these methods have not been explored inside detail. we provide an introduction to animal movement modeling and describe how imputation approaches may be helpful considering certain types of models. we also assess a performance of imputation approaches inside the simulation study. our simulation study suggests that inference considering model parameters directly related to a location of an individual may be more accurate than inference considering parameters associated with higher-order processes such as velocity or acceleration. finally, we apply these methods to analyze the telemetry data set involving northern fur seals (callorhinus ursinus) inside a bering sea.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
11462,"synthesizing programs with the help of example input/outputs was the classic problem inside artificial intelligence. we present the method considering solving programming by example (pbe) problems by with the help of the neural model to guide a search of the constraint logic programming system called minikanren. crucially, a neural model uses minikanren's internal representation as input; minikanren represents the pbe problem as recursive constraints imposed by a provided examples. we explore recurrent neural network and graph neural network models. we contribute the modified minikanren, drivable by an external agent, available at this https url. we show that our neural-guided idea behind the method with the help of constraints should synthesize programs faster inside many cases, and importantly, should generalize to larger problems.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5193,"meshfree solution schemes considering a incompressible navier--stokes equations are usually based on algorithms commonly used inside finite volume methods, such as projection methods, simple and piso algorithms. however, drawbacks of these algorithms that are specific to meshfree methods have often been overlooked. inside this paper, we study a drawbacks of conventionally used meshfree generalized finite difference method~(gfdm) schemes considering lagrangian incompressible navier-stokes equations, both operator splitting schemes and monolithic schemes. a major drawback of most of these schemes was inaccurate local approximations to a mass conservation condition. further, we propose the new modification of the commonly used monolithic scheme that overcomes these problems and shows the better approximation considering a velocity divergence condition. we then perform the numerical comparison which shows a new monolithic scheme to be more accurate than existing schemes.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13924,"motivated by a problem of optimal portfolio liquidation under transient price impact, we study a minimization of energy functionals with completely monotone displacement kernel under an integral constraint. a corresponding minimizers should be characterized by fredholm integral equations of a second type with constant free term. our main result states that minimizers are analytic and have the power series development inside terms of even powers of a distance to a midpoint of a domain of definition and with nonnegative coefficients. we show moreover that our minimization problem was equivalent to a minimization of a energy functional under the nonnegativity constraint.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
13846,"achieving information-theoretic security with the help of explicit coding scheme inside which unlimited computational power considering eavesdropper was assumed, was one of a main topics was security consideration. it was shown that polar codes are capacity achieving codes and have the low complexity inside encoding and decoding. it has been proven that polar codes reach to secrecy capacity inside a binary-input wiretap channels inside symmetric settings considering which a wiretapper's channel was degraded with respect to a main channel. a first task of this paper was to propose the coding scheme to achieve secrecy capacity inside asymmetric nonbinary-input channels while keeping reliability and security conditions satisfied. our assumption was that a wiretap channel was stochastically degraded with respect to a main channel and message distribution was unspecified. a main idea was to send information set over good channels considering bob and bad channels considering eve and send random symbols considering channels that are good considering both. inside this scheme a frozen vector was defined over all possible choices with the help of polar codes ensemble concept. we proved that there exists the frozen vector considering which a coding scheme satisfies reliability and security conditions. it was further shown that uniform distribution of a message was a necessary condition considering achieving secrecy capacity.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
17489,"a amount of ultraviolet irradiation and ablation experienced by the planet depends strongly on a temperature of its host star. of a thousands of extra-solar planets now known, only four giant planets have been found that transit hot, a-type stars (temperatures of 7300-10,000k), and none are known to transit even hotter b-type stars. wasp-33 was an a-type star with the temperature of ~7430k, which hosts a hottest known transiting planet; a planet was itself as hot as the red dwarf star of type m. a planet displays the large heat differential between its day-side and night-side, and was highly inflated, traits that have been linked to high insolation. however, even at a temperature of wasp-33b's day-side, its atmosphere likely resembles a molecule-dominated atmospheres of other planets, and at a level of ultraviolet irradiation it experiences, its atmosphere was unlikely to be significantly ablated over a lifetime of its star. here we report observations of a bright star hd 195689, which reveal the close-in (orbital period ~1.48 days) transiting giant planet, kelt-9b. at ~10,170k, a host star was at a dividing line between stars of type the and b, and we measure a kelt-9b's day-side temperature to be ~4600k. this was as hot as stars of stellar type k4. a molecules inside k stars are entirely dissociated, and thus a primary sources of opacity inside a day-side atmosphere of kelt-9b are likely atomic metals. furthermore, kelt-9b receives ~700 times more extreme ultraviolet radiation (wavelengths shorter than 91.2 nanometers) than wasp-33b, leading to the predicted range of mass-loss rates that could leave a planet largely stripped of its envelope during a main-sequence lifetime of a host star.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16993,"face deidentification was an active topic amongst privacy and security researchers. early deidentification methods relying on image blurring or pixelization were replaced inside recent years with techniques based on formal anonymity models that provide privacy guaranties and at a same time aim at retaining certain characteristics of a data even after deidentification. a latter aspect was particularly important, as it allows to exploit a deidentified data inside applications considering which identity information was irrelevant. inside this work we present the novel face deidentification pipeline, which ensures anonymity by synthesizing artificial surrogate faces with the help of generative neural networks (gnns). a generated faces are used to deidentify subjects inside images or video, while preserving non-identity-related aspects of a data and consequently enabling data utilization. since generative networks are very adaptive and should utilize the diverse set of parameters (pertaining to a appearance of a generated output inside terms of facial expressions, gender, race, etc.), they represent the natural choice considering a problem of face deidentification. to demonstrate a feasibility of our approach, we perform experiments with the help of automated recognition tools and human annotators. our results show that a recognition performance on deidentified images was close to chance, suggesting that a deidentification process based on gnns was highly effective.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
168,"recently, there have been increasing demands to construct compact deep architectures to remove unnecessary redundancy and to improve a inference speed. while many recent works focus on reducing a redundancy by eliminating unneeded weight parameters, it was not possible to apply the single deep architecture considering multiple devices with different resources. when the new device or circumstantial condition requires the new deep architecture, it was necessary to construct and train the new network from scratch. inside this work, we propose the novel deep learning framework, called the nested sparse network, which exploits an n-in-1-type nested structure inside the neural network. the nested sparse network consists of multiple levels of networks with the different sparsity ratio associated with each level, and higher level networks share parameters with lower level networks to enable stable nested learning. a proposed framework realizes the resource-aware versatile architecture as a same network should meet diverse resource requirements. moreover, a proposed nested network should learn different forms of knowledge inside its internal networks at different levels, enabling multiple tasks with the help of the single network, such as coarse-to-fine hierarchical classification. inside order to train a proposed nested sparse network, we propose efficient weight connection learning and channel and layer scheduling strategies. we evaluate our network inside multiple tasks, including adaptive deep compression, knowledge distillation, and learning class hierarchy, and demonstrate that nested sparse networks perform competitively, but more efficiently, compared to existing methods.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16512,"leveraging recent advances inside technologies surrounding a internet of things, ""smart"" water systems are poised to transform water resources management by enabling ubiquitous real-time sensing and control. recent applications have demonstrated a potential to improve flood forecasting, enhance rainwater harvesting, and prevent combined sewer overflows. however, adoption of smart water systems has been hindered by the limited number of proven case studies, along with the lack of guidance on how smart water systems should be built. to this end, we review existing solutions, and introduce open storm---an open-source, end-to-end platform considering real-time monitoring and control of watersheds. open storm includes (i) the robust hardware stack considering distributed sensing and control inside harsh environments (ii) the cloud services platform that enables system-level supervision and coordination of water assets, and (iii) the comprehensive, web-based ""how-to"" guide, available on open-storm.org, that empowers newcomers to develop and deploy their own smart water networks. we illustrate a capabilities of a open storm platform through two ongoing deployments: (i) the high-resolution flash-flood monitoring network that detects and communicates flood hazards at a level of individual roadways and (ii) the real-time stormwater control network that actively modulates discharges from stormwater facilities to improve water quality and reduce stream erosion. through these case studies, we demonstrate a real-world potential considering smart water systems to enable sustainable management of water resources.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
11085,"a quest considering algorithms that enable cognitive abilities was an important part of machine learning. the common trait inside many recently investigated cognitive-like tasks was that they take into account different data modalities, such as visual and textual input. inside this paper we propose the novel and generally applicable form of attention mechanism that learns high-order correlations between various data modalities. we show that high-order correlations effectively direct a appropriate attention to a relevant elements inside a different data modalities that are required to solve a joint task. we demonstrate a effectiveness of our high-order attention mechanism on a task of visual question answering (vqa), where we achieve state-of-the-art performance on a standard vqa dataset.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18553,"inside recent years, the class of dictionaries have been proposed considering multidimensional (tensor) data representation that exploit a structure of tensor data by imposing the kronecker structure on a dictionary underlying a data. inside this work, the novel algorithm called ""stark"" was provided to learn kronecker structured dictionaries that should represent tensors of any order. by establishing that a kronecker product of any number of matrices should be rearranged to form the rank-1 tensor, we show that kronecker structure should be enforced on a dictionary by solving the rank-1 tensor recovery problem. because rank-1 tensor recovery was the challenging nonconvex problem, we resort to solving the convex relaxation of this problem. empirical experiments on synthetic and real data show promising results considering our proposed algorithm.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3820,"we consider a notion of a de rham operator on finite-dimensional diffeological spaces such that a diffeological counterpart \lambda^1(x) of a cotangent bundle, a so-called pseudo-bundle of values of differential 1-forms, has bounded dimension. a operator was defined as a composition of a levi-civita connection on a exterior algebra pseudo-bundle \bigwedge(\lambda^1(x)) with a standardly defined clifford action by \lambda^1(x); a latter was therefore assumed to admit the pseudo-metric considering which there exists the levi-civita connection. under these assumptions, a definition was fully analogous a standard case, and our main conclusion was that this was a only way to define a de rham operator on the diffeological space, since we show that there was not the straightforward counterpart of a definition of a de rham operator as a sum d+d^* of a exterior differential with its adjoint. we show along a way that other connected notions do not have full counterparts, inside terms of a function they are supposed to fulfill, either; this regards, considering instance, volume forms, a hodge star, and a distinction between a $k$-th exterior degree of \lambda^1(x) and a pseudo-bundle of differential k-forms \lambda^k(x).",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7801,"rapid progress has been made recently on symmetry breaking operators considering real reductive groups. based on program a-c considering branching problems (t.kobayashi [progr.math.2015]), we illustrate the scheme of a classification of (local and nonlocal) symmetry breaking operators by an example of conformal representations on differential forms on a model space $(x,y)=(s^n, s^{n-1})$, which generalizes a scalar case (kobayashi--speh [memoirs of amer.math.soc. 2015]) and a case of local operators (kobayashi--kubo--pevzner [lecture notes inside math. 2016]). some applications to automorphic form theory, motivations from conformal geometry, and a methods of proof are also discussed.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
8584,"we construct smooth solutions to ricci flow starting from the class of singular metrics and give asymptotics considering a forward evolution. a singular metrics heal with the set of points (of codimension at least three) coming out of a singular point. we conjecture that these metrics arise as final-time limits of ricci flow encountering the type-i singularity modeled on $\mathbb{r}^{p+1} \times s^q$. this gives the picture of ricci flow through the singularity, inside which the neighborhood of a manifold changes topology from $d^{p+1} \times s^{q}$ to $s^p \times d^{q+1}$ (through a cone over $s^p \times s^q$.) we work inside a class of doubly-warped product metrics. we also briefly discuss some possible smooth and non-smooth forward evolutions from other singular initial data.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3958,"robot awareness of human actions was an essential research problem inside robotics with many important real-world applications, including human-robot collaboration and teaming. over a past few years, depth sensors have become the standard device widely used by intelligent robots considering 3d perception, which should also offer human skeletal data inside 3d space. several methods based on skeletal data were designed to enable robot awareness of human actions with satisfactory accuracy. however, previous methods treated all body parts and features equally important, without a capability to identify discriminative body parts and features. inside this paper, we propose the novel simultaneous feature and body-part learning (fabl) idea behind the method that simultaneously identifies discriminative body parts and features, and efficiently integrates all available information together to enable real-time robot awareness of human behaviors. we formulate fabl as the regression-like optimization problem with structured sparsity-inducing norms to model interrelationships of body parts and features. we also develop an optimization algorithm to solve a formulated problem, which possesses the theoretical guarantee to find a optimal solution. to evaluate fabl, three experiments were performed with the help of public benchmark datasets, including a msr action3d and cad-60 datasets, as well as the baxter robot inside practical assistive living applications. experimental results show that our fabl idea behind the method obtains the high recognition accuracy with the processing speed of a order-of-magnitude of 10e4 hz, which makes fabl the promising method to enable real-time robot awareness of human behaviors inside practical robotics applications.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
17541,"an important challenge inside condensed matter physics was understanding iron-based superconductors. among these systems, a iron selenides hold a record considering highest superconducting transition temperature and pose especially striking puzzles regarding a nature of superconductivity. a pairing state of a alkaline iron selenides appears to be of $d$-wave type based on a observation of the resonance mode inside neutron scattering, while it seems to be of $s$-wave type from a nodeless gaps observed everywhere on a fermi surface (fs). here we propose an orbital-selective pairing state, dubbed $s \tau_{3}$, as the natural explanation of these disparate properties. a pairing function, containing the matrix $\tau_{3}$ inside a basis of $3d$-electron orbitals, does not commute with a kinetic part of a hamiltonian. this dictates a existence of both intraband and interband pairing terms inside a band basis. the spin resonance arises from the $d$-wave-type sign change inside a intraband pairing component whereas a quasiparticle excitation was fully gapped on a fs due to an $s$-wave-like form factor associated with a addition inside quadrature of a intraband and interband pairing terms. we demonstrate that this pairing state was energetically favored when a electron correlation effects are orbitally selective. more generally, our results illustrate how a multiband nature of correlated electrons affords unusual types of superconducting states, thereby shedding new light not only on a iron-based materials but also on the broad range of other unconventional superconductors such as heavy fermion and organic systems.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
19560,"a wavelet tree (grossi et al. [soda, 2003]) and wavelet matrix (claude et al. [inf. syst., 47:15--32, 2015]) are compact indices considering texts over an alphabet $[0,\sigma)$ that support rank, select and access queries inside $o(\lg \sigma)$ time. we first present new practical sequential and parallel algorithms considering wavelet tree construction. their unifying characteristics was that they construct a wavelet tree bottomup}, i.e., they compute a last level first. we also show that this bottom-up construction should easily be adapted to wavelet matrices. inside practice, our best sequential algorithm was up to twice as fast as a currently fastest sequential wavelet tree construction algorithm (shun [dcc, 2015]), simultaneously saving the factor of 2 inside space. this scales up to 32 cores, where we are about equally fast as a currently fastest parallel wavelet tree construction algorithm (labeit et al. [dcc, 2016]), but still use only about 75 % of a space. an additional theoretical result shows how to adapt any wavelet tree construction algorithm to a wavelet matrix inside a same (asymptotic) time, with the help of only little extra space.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3157,"we present the method considering single image 3d cuboid object detection and multi-view object slam without prior object model, and demonstrate that a two aspects should benefit each other. considering 3d detection, we generate high quality cuboid proposals from 2d bounding boxes and vanishing points sampling. a proposals are further scored and selected to align with image edges. experiments on sun rgbd and kitti shows a efficiency and accuracy over existing approaches. then inside a second part, multi-view bundle adjustment with novel measurement functions was proposed to jointly optimize camera poses, objects and points, utilizing single view detection results. objects should provide more geometric constraints and scale consistency compared to points. on a collected and public tum and kitti odometry datasets, we achieve better pose approximation accuracy over a state-of-the-art monocular slam while also improve a 3d object detection accuracy at a same time.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
5955,"this paper investigates a control of flow networks, where a control objective was to regulate a measured output (e.g storage levels) towards the desired value. we present the distributed controller that dynamically adjusts a inputs and flows, to achieve output regulation inside a presence of unknown disturbances, while satisfying given input and flow constraints. optimal coordination among a inputs, minimizing the suitable cost function, was achieved by exchanging information over the communication network. exploiting an incremental passivity property, a desired steady state was proven to be globally asymptotically attractive under a closed loop dynamics. two case studies (a district heating system and the multi-terminal hvdc network) show a effectiveness of a proposed solution.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
9801,"we evaluate a mass of a black holes of gw150914 at their event horizons using quasi-local energy idea behind the method and obtain a values of 71 and 57 solar masses, compared to their asymptotic values of 36 and 29 units, respectively, as reported by ligo. the higher mass at a event horizon was compulsory inside order to overcome a huge negative gravitational potential energy surrounding a black holes and allow considering a emission of gravitational waves during merging. we approximate a initial mass of a stars which collapsed to form a black holes from a horizon mass and obtain a impressive values of 95 and 76 solar masses considering these progenitor stars.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16063,"we present the convolutional-recurrent neural network architecture with long short-term memory considering real-time processing and classification of digital sensor data. a network implicitly performs typical signal processing tasks such as filtering and peak detection, and learns time-resolved embeddings of a input signal. we use the prototype multi-sensor wearable device to collect over 180h of photoplethysmography (ppg) data sampled at 20hz, of which 36h are during atrial fibrillation (afib). we use end-to-end learning to achieve state-of-the-art results inside detecting afib from raw ppg data. considering classification labels output every 0.8s, we demonstrate an area under roc curve of 0.9999, with false positive and false negative rates both below $2\times 10^{-3}$. this constitutes the significant improvement on previous results utilising domain-specific feature engineering, such as heart rate extraction, and brings large-scale atrial fibrillation screenings within imminent reach.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18557,"we searched considering dust or debris rings inside a pluto-charon system before, during, and after a new horizons encounter. methodologies included searching considering back-scattered light during a idea behind the method to pluto (phase $\sim15^\circ$), inside situ detection of impacting particles, the search considering stellar occultations near a time of closest approach, and by forward-scattered light during departure (phase $\sim165^\circ$). the search with the help of hst prior to a encounter also contributed to a results. no rings, debris, or dust features were observed, but our detection limits provide an improved picture of a environment throughout a pluto-charon system. searches considering rings inside back-scattered light covered 35,000-250,000 km from a system barycenter, the zone that starts interior to a orbit of styx, and extends to four times a orbital radius of hydra. we obtained our firmest limits with the help of a nh lorri camera inside a inner half of this region. our limits on a normal $i/f$ of an unseen ring depends on a radial scale of a rings: $2\times10^{-8}$ ($3\sigma$) considering 1500 km wide rings, $1\times10^{-8}$ considering 6000 km rings, and $7\times10^{-9}$ considering 12,000 km rings. beyond $\sim100,000$ km from pluto, hst observations limit normal $i/f$ to $\sim8\times10^{-8}$. searches considering dust from forward-scattered light extended from a surface of pluto to a pluto-charon hill sphere ($r_{\rm hill}=6.4\times10^6$ km). no evidence considering rings or dust is detected to normal $i/f$ limits of $\sim8.9\times10^{-7}$ on $\sim10^4$ km scales. four occulation observations also probed a space interior to hydra, but again no dust or debris is detected. elsewhere inside a solar system, small moons commonly share their orbits with faint dust rings. our results suggest that small grains are quickly lost from a system due to solar radiation pressure, whereas larger particles are unstable due to perturbations by a known moons.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9073,"the diffeological connection on the diffeological vector pseudo-bundle was defined just a usual one on the smooth vector bundle; this was possible to do, because there was the standard diffeological counterpart of a cotangent bundle. on a other hand, there was not yet the standard theory of tangent bundles, although there are many suggested and promising versions, such as that of a internal tangent bundle, so a abstract notion of the connection on the diffeological vector pseudo-bundle does not automatically provide the counterpart notion considering levi-civita connections. inside this paper we consider a dual of a just-mentioned counterpart of a cotangent bundle inside place of a tangent bundle (without making any claim about its geometrical meaning). to it, a notions of compatibility with the pseudo-metric and symmetricity should be easily extended, and therefore a notion of the levi-civita connection makes sense as well. inside a case when $\lambda^1(x)$, a counterpart of a cotangent bundle, was finite-dimensional, there was an equivalent levi-civita connection on it as well.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18258,"the superconductor of paired protons was thought to form inside a core of neutron stars soon after their birth. minimum energy conditions suggest magnetic flux was expelled from a superconducting region due to a meissner effect, such that a neutron star core was largely devoid of magnetic fields considering some nuclear equation of state and proton pairing models. we show using neutron star cooling simulations that a superconducting region expands faster than flux was expected to be expelled because cooling timescales are much shorter than timescales of magnetic field diffusion. thus magnetic fields remain inside a bulk of a neutron star core considering at least 10^6-10^7 yr. we approximate a size of flux free regions at 10^7 yr to be <~ 100 m considering the magnetic field of 10^11 g and possibly smaller considering stronger field strengths. considering proton pairing models that are narrow, magnetic flux may be completely expelled from the thin shell of approximately a above size after 10^5 yr. this shell may insulate lower conductivity outer layers, where magnetic fields should diffuse and decay faster, from fields maintained inside a highly conducting deep core.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
4253,"a collective magnetic excitations inside a spin-orbit mott insulator (sr$_{1-x}$la$_x$)$_2$iro$_4$ ($x=0,\,0.01,\,0.04,\, 0.1$) were investigated by means of resonant inelastic x-ray scattering. we report significant magnon energy gaps at both a crystallographic and antiferromagnetic zone centers at all doping levels, along with the remarkably pronounced momentum-dependent lifetime broadening. a spin-wave gap was accounted considering by the significant anisotropy inside a interactions between $j_\text{eff}=1/2$ isospins, thus marking a departure of sr$_2$iro$_4$ from a essentially isotropic heisenberg model appropriate considering a superconducting cuprates.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
5910,"natural language generation (nlg) was an important component inside spoken dialogue systems. this paper presents the model called encoder-aggregator-decoder which was an extension of an recurrent neural network based encoder-decoder architecture. a proposed semantic aggregator consists of two components: an aligner and the refiner. a aligner was the conventional attention calculated over a encoded input information, while a refiner was another attention or gating mechanism stacked over a attentive aligner inside order to further select and aggregate a semantic elements. a proposed model should be jointly trained both sentence planning and surface realization to produce natural language utterances. a model is extensively assessed on four different nlg domains, inside which a experimental results showed that a proposed generator consistently outperforms a previous methods on all a nlg domains.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1184,"we present the complete symmetry classification of a sachdev-ye-kitaev (syk) model with $\mathcal{n}=0$, $1$ and $2$ supersymmetry (susy) on a basis of a altland-zirnbauer scheme inside random matrix theory (rmt). considering $\mathcal{n}=0$ and $1$ we consider generic $q$-body interactions inside a hamiltonian and find rmt classes that were not present inside earlier classifications of a same model with $q=4$. we numerically establish quantitative agreement between a distributions of a smallest energy levels inside a $\mathcal{n}=1$ syk model and rmt. furthermore, we delineate a distinctive structure of a $\mathcal{n}=2$ syk model and provide its complete symmetry classification based on rmt considering all eigenspaces of a fermion number operator. we corroborate our classification by detailed numerical comparisons with rmt and thus establish a presence of quantum chaotic dynamics inside a $\mathcal{n}=2$ syk model. we also introduce the new syk-like model without susy that exhibits hybrid properties of a $\mathcal{n}=1$ and $\mathcal{n}=2$ syk models and uncover its rich structure both analytically and numerically.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
3226,"an instrumentation problem with a signal acquisition at high frequencies is discovered and we no longer believe that a experimental data presented inside a manuscript, showing the frequency enhancement of a elastoresistivity, are correct. after correcting a problem, a elastoresistivity data was frequency independent inside a range investigated. therefore, a authors have withdrawn this submission. we would like to thank alex hristov, johanna palmstrom, josh straquadine and ian fisher (stanford) considering a kind discussions and assistance we received which helped us identify these problems.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
17286,"distance multivariance was the multivariate dependence measure, which should detect dependencies between an arbitrary number of random vectors each of which should have the distinct dimension. here we discuss several new aspects and present the concise overview. we relax a required moment conditions considerably and show that distance multivariance unifies (and extends) distance covariance and a hilbert-schmidt independence criterion hsic, moreover also a classical linear dependence measures: covariance, pearson's correlation and a rv coefficient appear as limiting cases. considering measures based on distance multivariance a corresponding resampling tests are introduced, and several related measures are defined: the new multicorrelation which satisfies the natural set of multivariate dependence measure axioms and $m$-multivariance which was the new dependence measure yielding tests considering pairwise independence and independence of higher order. these tests are computationally feasible and under very mild moment conditions they are consistent against all alternatives. moreover, the general visualization scheme considering higher order dependencies was proposed. many illustrative examples are included. all functions considering a use of distance multivariance inside applications are published inside a r-package 'multivariance'.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
5522,"a study of deep recurrent neural networks (rnns) and, inside particular, of deep reservoir computing (rc) was gaining an increasing research attention inside a neural networks community. a recently introduced deep echo state network (deepesn) model opened a way to an extremely efficient idea behind the method considering designing deep neural networks considering temporal data. at a same time, a study of deepesns allowed to shed light on a intrinsic properties of state dynamics developed by hierarchical compositions of recurrent layers, i.e. on a bias of depth inside rnns architectural design. inside this paper, we summarize a advancements inside a development, analysis and applications of deepesns.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1038,"a volume of a hive polytope (or polytope of honeycombs) associated with the littlewood- richardson coefficient of su(n), or with the given admissible triple of highest weights, was expressed, inside a generic case, inside terms of a fourier transform of the convolution product of orbital measures. several properties of this function -- the function of three non-necessarily integral weights or of three multiplets of real eigenvalues considering a associated horn problem-- are already known. inside a integral case it should be thought of as the semi-classical approximation of littlewood-richardson coefficients. we prove that it may be expressed as the local average of the finite number of such coefficients. we also relate this function to a littlewood-richardson polynomials (stretching polynomials) i.e., to a ehrhart polynomials of a relevant hive polytopes. several su(n) examples, considering n=2,3,...,6, are explicitly worked out.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
10752,"recent research has demonstrated a vulnerability of fingerprint recognition systems to dictionary attacks based on masterprints. masterprints are real or synthetic fingerprints that should fortuitously match with the large number of fingerprints thereby undermining a security afforded by fingerprint systems. previous work by roy et al. generated synthetic masterprints at a feature-level. inside this work we generate complete image-level masterprints known as deepmasterprints, whose attack accuracy was found to be much superior than that of previous methods. a proposed method, referred to as latent variable evolution, was based on training the generative adversarial network on the set of real fingerprint images. stochastic search inside a form of a covariance matrix adaptation evolution strategy was then used to search considering latent input variables to a generator network that should maximize a number of impostor matches as assessed by the fingerprint recognizer. experiments convey a efficacy of a proposed method inside generating deepmasterprints. a underlying method was likely to have broad applications inside fingerprint security as well as fingerprint synthesis.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7600,"a prediction of organic reaction outcomes was the fundamental problem inside computational chemistry. since the reaction may involve hundreds of atoms, fully exploring a space of possible transformations was intractable. a current solution utilizes reaction templates to limit a space, but it suffers from coverage and efficiency issues. inside this paper, we propose the template-free idea behind the method to efficiently explore a space of product molecules by first pinpointing a reaction center -- a set of nodes and edges where graph edits occur. since only the small number of atoms contribute to reaction center, we should directly enumerate candidate products. a generated candidates are scored by the weisfeiler-lehman difference network that models high-order interactions between changes occurring at nodes across a molecule. our framework outperforms a top-performing template-based idea behind the method with the 10\% margin, while running orders of magnitude faster. finally, we demonstrate that a model accuracy rivals a performance of domain experts.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3884,"we attempt to constrain a kinematics of a thin and thick disks with the help of a besancon population synthesis model together with rave dr4 and gaia first data release (tgas). a rave fields were simulated by applying the detailed target selection function and a kinematics is computed with the help of velocity ellipsoids depending on age inside order to study a secular evolution. we accounted considering a asymmetric drift computed from fitting the stackel potential to orbits. model parameters such as velocity dispersions, mean motions, and velocity gradients were adjusted with the help of an abc-mcmc method. we made use of a metallicity to enhance a separation between thin and thick disks. we show that this model was able to reproduce a kinematics of a local disks inside great detail. a disk follows a expected secular evolution, inside very good agreement with previous studies of a thin disk. a new asymmetric drift formula, fitted to our previously described st√§ckel potential, fairly well reproduces a velocity distribution inside the wide solar neighborhood. a u and w components of a solar motion determined with this method agree well with previous studies. however, we find the smaller v component than previously thought, essentially because we include a variation of a asymmetric drift with distance to a plane. a thick disk was represented by the long period of formation (at least 2 gyr), during which, as we show, a mean velocity increases with time while a scale height and scale length decrease, very consistently with the collapse phase with conservation of angular momentum. this new galactic dynamical model was able to reproduce a observed velocities inside the wide solar neighborhood at a quality level of a tgas-rave sample, allowing us to constrain a thin and thick disk dynamical evolution, as well as determining a solar motion.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9644,"quantum spin ice, modeled considering magnetic rare-earth pyrochlores, has attracted great interest considering hosting the u(1) quantum spin liquid, which involves spin-ice monopoles as gapped deconfined spinons, as well as gapless excitations analogous to photons. however, a global phase diagram under the [111] magnetic field remains open. here we uncover by means of unbiased quantum monte-carlo simulations that the supersolid of monopoles, showing both the superfluidity and the partial ionization, intervenes a kagom√© spin ice and the fully ionized monopole insulator, inside contrast to classical spin ice where the direct discontinuous phase transition takes place. we also show that on cooling, kagom√© spin ice evolves towards the valence bond solid similar to what appears inside a associated kagom√© lattice model [s. v. isakov et al., phys. rev. lett. 97, 147202 (2006)]. possible relevance to experiments was discussed.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
6130,"with a increasing penetration of solar power into power systems, forecasting becomes critical inside power system operations. inside this paper, an hourly-similarity (hs) based method was developed considering 1-hour-ahead (1ha) global horizontal irradiance (ghi) forecasting. this developed method utilizes diurnal patterns, statistical distinctions between different hours, and hourly similarities inside solar data to improve a forecasting accuracy. a hs-based method was built by training multiple two-layer multi-model forecasting framework (mmff) models independently with a same-hour subsets. a final optimal model was the combination of mmff models with a best-performed blending algorithm at every hour. at a forecasting stage, a most suitable model was selected to perform a forecasting subtask of the certain hour. a hs-based method was validated by 1-year data with six solar features collected by a national renewable energy laboratory (nrel). results show that a hs-based method outperforms a non-hs (all-in-one) method significantly with a same mmff architecture, wherein a optimal hs- based method outperforms a best all-in-one method by 10.94% and 7.74% based on a normalized mean absolute error and normalized root mean square error, respectively.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3720,"a hard problem of consciousness has been dismissed as an illusion. by showing that computers are capable of experiencing, we show that they are at least rudimentarily conscious with potential to eventually reach superconsciousness. a main contribution of a paper was the test considering confirming certain subjective experiences inside the tested agent. we follow with analysis of benefits and problems with conscious machines and implications of such capability on future of computing, machine rights and artificial intelligence safety.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8551,"kernel embeddings of distributions and a maximum mean discrepancy (mmd), a resulting distance between distributions, are useful tools considering fully nonparametric two-sample testing and learning on distributions. however, it was rarely that all possible differences between samples are of interest -- discovered differences should be due to different types of measurement noise, data collection artefacts or other irrelevant sources of variability. we propose distances between distributions which encode invariance to additive symmetric noise, aimed at testing whether a assumed true underlying processes differ. moreover, we construct invariant features of distributions, leading to learning algorithms robust to a impairment of a input distributions with symmetric additive noise.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4628,"human interactions and human-computer interactions are strongly influenced by style as well as content. adding the persona to the chatbot makes it more human-like and contributes to the better and more engaging user experience. inside this work, we propose the design considering the chatbot that captures a ""style"" of star trek by incorporating references from a show along with peculiar tones of a fictional characters therein. our enterprise to computer bot (e2cbot) treats star trek dialog style and general dialog style differently, with the help of two recurrent neural network encoder-decoder models. a star trek dialog style uses sequence to sequence (seq2seq) models (sutskever et al., 2014; bahdanau et al., 2014) trained on star trek dialogs. a general dialog style uses word graph to shift a response of a seq2seq model into a star trek domain. we evaluate a bot both inside terms of perplexity and word overlap with star trek vocabulary and subjectively with the help of human evaluators.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12031,"we report on a confirmation that a candidate transits observed considering a star epic 211525389 are due to the short-period neptune-sized planet. a host star, located inside k2 campaign field 5, was the metal-rich ([fe/h] = 0.26$\pm$0.05) g-dwarf (t_eff = 5430$\pm$70 k and log g = 4.48$\pm$0.09), based on observations with a high dispersion spectrograph (hds) on a subaru 8.2m telescope. high-spatial resolution ao imaging with hiciao on a subaru telescope excludes faint companions near a host star, and a false positive probability of this target was found to be <$10^{-6}$ with the help of a open source vespa code. the joint analysis of transit light curves from k2 and additional ground-based multi-color transit photometry with muscat on a okayama 1.88m telescope gives a orbital period of p = 8.266902$\pm$0.000070 days and consistent transit depths of $r_p/r_\star \sim 0.035$ or $(r_p/r_\star)^2 \sim 0.0012$. a transit depth corresponds to the planetary radius of $r_p = 3.59_{-0.39}^{+0.44} r_{\oplus}$, indicating that epic 211525389 b was the short-period neptune-sized planet. radial velocities of a host star, obtained with a subaru hds, lead to the 3\sigma\ upper limit of 90 $m_{\oplus} (0.00027 m_{\odot})$ on a mass of epic 211525389 b, confirming its planetary nature. we expect this planet, newly named k2-105 b, to be a subject of future studies to characterize its mass, atmosphere, spin-orbit (mis)alignment, as well as investigate a possibility of additional planets inside a system.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5965,"we review a effect of a commonly-used limber and flat-sky approximations on a calculation of shear power spectra and correlation functions considering galaxy weak lensing. these approximations are accurate at small scales, but it has been claimed recently that their impact on low multipoles could lead to an increase inside a amplitude of a mass fluctuations inferred from surveys such as cfhtlens, reducing a tension between galaxy weak lensing and a amplitude determined by planck from observations of a cosmic microwave background. here, we explore a impact of these approximations on cosmological parameters derived from weak lensing surveys, with the help of a cfhtlens data as the test case. we conclude that a use of small-angle approximations considering cosmological parameter approximation was negligible considering current data, and does not contribute to a tension between current weak lensing surveys and planck.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11163,"we demonstrate that the very deep resnet with stacked modules with one neuron per hidden layer and relu activation functions should uniformly approximate any lebesgue integrable function inside $d$ dimensions, i.e. $\ell_1(\mathbb{r}^d)$. because of a identity mapping inherent to resnets, our network has alternating layers of dimension one and $d$. this stands inside sharp contrast to fully connected networks, which are not universal approximators if their width was a input dimension $d$ [lu et al, 2017; hanin and sellke, 2017]. hence, our result implies an increase inside representational power considering narrow deep networks by a resnet architecture.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2542,"inside this note we prove a instability by blow-up of a ground state solutions considering the class of fourth order schr\"" odinger equations. this extends a first rigorous results on blowing-up solutions considering a biharmonic nls due to boulenger and lenzmann \cite{bole} and confirm numerical conjectures from \cite{bafi, bafima1, bafima, fiilpa}.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6867,"we study a intra-cluster magnetic field inside a poor galaxy cluster abell 194 by complementing radio data, at different frequencies, with data inside a optical and x-ray bands. we analyze new total intensity and polarization observations of abell 194 obtained with a sardinia radio telescope (srt). we use a srt data inside combination with archival very large array observations to derive both a spectral aging and rotation measure (rm) images of a radio galaxies 3c40a and 3c40b embedded inside abell 194. a optical analysis indicates that abell 194 does not show the major and recent cluster merger, but rather agrees with the scenario of accretion of small groups. under a minimum energy assumption, a lifetimes of synchrotron electrons inside 3c40b measured from a spectral break are found to be 157 myrs. a break frequency image and a electron density profile inferred from a x-ray emission are used inside combination with a rm data to constrain a intra-cluster magnetic field power spectrum. by assuming the kolmogorov power law power spectrum, we find that a rm data inside abell 194 are well described by the magnetic field with the maximum scale of fluctuations of lambda_max=64 kpc and the central magnetic field strength of <b0>=1.5 microg. further out, a field decreases with a radius following a gas density to a power of eta=1.1. comparing abell 194 with the small sample of galaxy clusters, there was the hint of the trend between central electron densities and magnetic field strengths.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19333,"we propose the simulation method considering multidimensional hawkes processes based on superposition theory of point processes. this formulation allows us to design efficient simulations considering hawkes processes with differing exponentially decaying intensities. we demonstrate that inter-arrival times should be decomposed into simpler auxiliary variables that should be sampled directly, giving exact simulation with no approximation. we establish that a auxiliary variables provides information on a parent process considering each event time. a algorithm correctness was shown by verifying a simulated intensities with their theoretical moments. the modular inference procedure consisting of gibbs samplers through a auxiliary variable augmentation and adaptive rejection sampling was presented. finally, we compare our proposed simulation method against existing methods, and find significant improvement inside terms of algorithm speed. our inference algorithm was used to discover a strengths of mutually excitations inside real dark networks.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
917,"a online environment has provided the great opportunity considering insurance policyholders to share their complaints with respect to different services. these complaints should reveal valuable information considering insurance companies who seek to improve their services; however, analyzing the huge number of online complaints was the complicated task considering human and must involve computational methods to create an efficient process. this research proposes the computational idea behind the method to characterize a major topics of the large number of online complaints. our idea behind the method was based on with the help of a topic modeling idea behind the method to disclose a latent semantic of complaints. a proposed idea behind the method deployed on thousands of geico negative reviews. analyzing 1,371 geico complaints indicates that there are 30 major complains inside four categories: (1) customer service, (2) insurance coverage, paperwork, policy, and reports, (3) legal issues, and (4) costs, estimates, and payments. this research idea behind the method should be used inside other applications to explore the large number of reviews.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
12867,"a radial velocity (rv) of the single star was easily obtained from cross-correlation of a spectrum with the template, but a treatment of double-lined spectroscopic binaries (sb2s) was more difficult. two different approaches were applied to the set of sb2s: a fit of a cross-correlation function with two normal distributions, and a cross-correlation with two templates, derived with a todcor code. it appears that a minimum masses obtained through a two methods are sometimes rather different, although their estimated uncertainties are roughly equal. moreover, both methods induce the shift inside a zero point of a secondary rvs, but it was less pronounced considering todcor. all-in-all a comparison between a two methods was inside favour of todcor.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
8034,"inside this paper, we study a existence and a property of a hopf bifurcation inside a two-strategy replicator dynamics with distributed delays. inside evolutionary games, we assume that the strategy would take an uncertain time delay to have the consequence on a fitness (or utility) of a players. as a mean delay increases, the change inside a stability of a equilibrium (hopf bifurcation) may occur at which the periodic oscillation appears. we consider dirac, uniform, gamma, and discrete delay distributions, and we use a poincar√©- lindstedt's perturbation method to analyze a hopf bifurcation. our theoretical results are corroborated with numerical simulations.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
17417,"among a family of tmds, res2 takes the special position, which crystalizes inside the unique distorted low-symmetry structure at ambient conditions. a interlayer interaction inside res2 was rather weak, thus its bulk properties are similar to that of monolayer. however, how does compression change its structure and electronic properties was unknown so far. here with the help of ab initio crystal structure searching techniques, we explore a high-pressure phase transitions of res2 extensively and predict two new high-pressure phases. a ambient pressure phase transforms to the ""distorted-1t"" structure at very low pressure and then to the tetragonal i41/amd structure at around 90 gpa. a ""distorted-1t"" structure undergoes the semiconductor-metal transition (smt) at around 70 gpa with the band overlap mechanism. electron-phonon calculations suggest that a i41/amd structure was superconducting and has the critical superconducting temperature of about 2 k at 100 gpa. we further perform high-pressure electrical resistance measurements up to 102 gpa. our experiments confirm a smt and a superconducting phase transition of res2 under high pressure. these experimental results are inside good agreement with our theoretical predictions.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0
851,"a observation of metallic ground states inside the variety of two-dimensional electronic systems poses the fundamental challenge considering a theory of electron fluids. here, we analyze evidence considering a existence of the regime, which we call a ""anomalous metal regime,"" inside diverse 2d superconducting systems driven through the quantum superconductor to metal transition (qsmt) by tuning physical parameters such as a magnetic field, a gate voltage inside a case of systems with the mosfet geometry, or a degree of disorder. a principal phenomenological observation was that inside a anomalous metal, as the function of decreasing temperature, a resistivity first drops as if a system were approaching the superconducting ground state, but then saturates at low temperatures to the value that should be orders of magnitude smaller than a drude value. a anomalous metal also shows the giant positive magneto-resistance. thus, it behaves as if it were the ""failed superconductor."" this behavior was observed inside the broad range of parameters. we moreover exhibit, by theoretical solution of the model of superconducting grains embedded inside the metallic matrix, that as the matter of principle such anomalous metallic behavior should occur inside a neighborhood of the qsmt. however, we also argue that a robustness and ubiquitous nature of a observed phenomena are difficult to reconcile with any existing theoretical treatment, and speculate about a character of the more fundamental theoretical framework.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
12502,"we study a physical properties of giant molecular cloud associations (gmas) inside m100 (ngc 4321) with the help of a alma science verification feathered (12-m+aca) data inside 12co (1-0). to examine a environmental dependence of gma properties, gmas are classified based on their locations inside a various environments as circumnuclear ring (cnr), bar, spiral, and inter-arm gmas. a cnr gmas are massive and compact, while a inter-arm gmas are diffuse with low surface density. gma mass and size are strongly correlated, as suggested by larson (1981). however, a diverse power-law index of a relation implies that a gma properties are not uniform among a environments. a cnr and bar gmas show higher velocity dispersion than those inside other environments. we find little evidence considering the correlation between gma velocity dispersion and size, which indicates that a gmas are inside diverse dynamical states. indeed, a virial parameter of gmas spans nearly two orders of magnitude. only a spiral gmas are inside general self-gravitating. star formation activity of a gmas decreases inside order over a cnr, spiral, bar, and a inter-arm gmas. a diverse gma and star formation properties inside different environments lead to variations inside a kennicutt-schmidt relation. the combination of multiple mechanisms or gas phase change was necessary to explain a observed slopes. comparisons of gma properties acquired with a use of a 12-m-array observations with those from a feathered data are also presented. a results show that a missing flux and extended emission cannot be neglected considering a study of environmental dependence.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10606,"we develop the second order primal-dual method considering optimization problems inside which a objective function was given by a sum of the strongly convex twice differentiable term and the possibly nondifferentiable convex regularizer. after introducing an auxiliary variable, we utilize a proximal operator of a nonsmooth regularizer to transform a associated augmented lagrangian into the function that was once, but not twice, continuously differentiable. a saddle point of this function corresponds to a solution of a original optimization problem. we employ the generalization of a hessian to define second order updates on this function and prove global exponential stability of a corresponding differential inclusion. furthermore, we develop the globally convergent customized algorithm that utilizes a primal-dual augmented lagrangian as the merit function. we show that a search direction should be computed efficiently and prove quadratic/superlinear asymptotic convergence. we use a $\ell_1$-regularized least squares problem and a problem of designing the distributed controller considering the spatially-invariant system to demonstrate a merits and a effectiveness of our method.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17314,"this article offers an empirical study on a different ways of encoding chinese, japanese, korean (cjk) and english languages considering text classification. different encoding levels are studied, including utf-8 bytes, characters, words, romanized characters and romanized words. considering all encoding levels, whenever applicable, we provide comparisons with linear models, fasttext and convolutional networks. considering convolutional networks, we compare between encoding mechanisms with the help of character glyph images, one-hot (or one-of-n) encoding, and embedding. inside total there are 473 models, with the help of 14 large-scale text classification datasets inside 4 languages including chinese, english, japanese and korean. some conclusions from these results include that byte-level one-hot encoding based on utf-8 consistently produces competitive results considering convolutional networks, that word-level n-grams linear models are competitive even without perfect word segmentation, and that fasttext provides a best result with the help of character-level n-gram encoding but should overfit when a features are overly rich.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16423,"training semantic parsers from weak supervision (denotations) rather than strong supervision (programs) complicates training inside two ways. first, the large search space of potential programs needs to be explored at training time to find the correct program. second, spurious programs that accidentally lead to the correct denotation add noise to training. inside this work we propose that inside closed worlds with clear semantic types, one should substantially alleviate these problems by utilizing an abstract representation, where tokens inside both a language utterance and program are lifted to an abstract form. we show that these abstractions should be defined with the handful of lexical rules and that they result inside sharing between different examples that alleviates a difficulties inside training. to test our approach, we develop a first semantic parser considering cnlvr, the challenging visual reasoning dataset, where a search space was large and overcoming spuriousness was critical, because denotations are either true or false, and thus random programs are likely to lead to the correct denotation. our method substantially improves performance, and reaches 82.5% accuracy, the 14.7% absolute accuracy improvement compared to a best reported accuracy so far.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3245,"internet research on search engine quality and validity of results demand much concern. thus, a focus inside our study has been to measure a impact of quotation marks usage on a internet search outputs inside terms of google search outcomes distributions, through benford law. a current paper was focused on applying the benford law analysis on two related types of internet searches distinguished by a usage or absence of quotation marks. both search results values are assumed as variables. we found that a first digit of outcomes does not follow a benford law first digit of numbers inside a case of searching text without quotation marks. unexpectedly, a benford law was obeyed when quotation marks are used, even if a variability of search outcomes was considerably reduced. by studying outputs demonstrating influences of (apparently at first) ""details"", inside with the help of the search engine, a authors are able to further warn a users concerning a validity of such outputs.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13270,"a surface structure of phosphorene crystals materials was determined with the help of surface sensitive dynamical micro-spot low energy electron diffraction ({\mu}leed) analysis with the help of the high spatial resolution low energy electron microscopy (leem) system. samples of (\textit{i}) crystalline cleaved black phosphorus (bp) at 300 k and (\textit{ii}) exfoliated few-layer phosphorene (flp) of about 10 nm thicknes, which were annealed at 573 k inside vacuum were studied. inside both samples, the significant surface buckling of 0.22 {\aa} and 0.30 {\aa}, respectively, was measured, which was one order of magnitude larger than previously reported. with the help of first principle calculations, a presence of surface vacancies was attributed not only to a surface buckling inside bp and flp, but also a previously reported intrinsic hole doping of phosphorene materials.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
13824,"as the cutting-edge technology, microgrids feature intelligent emss and sophisticated control, which will dramatically change our energy infrastructure. a modern microgrids are the relatively recent development with high potential to bring distributed generation, des devices, controllable loads, communication infrastructure, and many new technologies into a mainstream. as the more controllable and intelligent entity, the microgrid has more growth potential than ever before. however, there are still many open questions, such as a future business models and economics. what was a cost-benefit to a end-user? how should we systematically evaluate a potential benefits and costs of control and energy management inside the microgrid?",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
13300,"we report a discovery of a quadruply lensed quasar j1433+6007, mined inside a sdss dr12 photometric catalogues with the help of the novel outlier-selection technique, without prior spectroscopic or uv excess information. discovery data obtained at a nordic optical telescope (not, la palma) show nearly identical quasar spectra at $z_s=2.74$ and four quasar images inside the fold configuration, one of which sits on the blue arc. a deflector redshift was $z_{l}=0.407,$ from keck-esi spectra. we describe a selection procedure, discovery and follow-up, image positions and $bvri$ magnitudes, and first results and forecasts from simple lens models.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11786,"a practical importance of robust inference considering causal effects inside regression discontinuity and kink designs was widely recognized. existing methods cover many cases, but do not handle robust uniform inference considering cdf and quantile processes inside fuzzy designs, despite its frequent use inside a recent literature inside empirical microeconomics. inside this light, this paper extends a literature by developing the unified framework of robust inference that applies to uniform inference considering quantile treatment effects inside fuzzy designs, as well as all a other cases of sharp/fuzzy mean/quantile regression discontinuity/kink designs. we present monte carlo simulation studies and an empirical application considering evaluations of a oklahoma pre-k program.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
15870,"we obtain the bernstein-type inequality considering sums of banach-valued random variables satisfying the weak dependence assumption of general type and under certain smoothness assumptions of a underlying banach norm. we use this inequality inside order to investigate inside a asymptotical regime a error upper bounds considering a broad family of spectral regularization methods considering reproducing kernel decision rules, when trained on the sample coming from the $\tau-$mixing process.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0
1205,"inside an open-world setting, it was inevitable that an intelligent agent (e.g., the robot) will encounter visual objects, attributes or relationships it does not recognize. inside this work, we develop an agent empowered with visual curiosity, i.e. a ability to ask questions to an oracle (e.g., human) about a contents inside images (e.g., what was a object on a left side of a red cube?) and build visual recognition model based on a answers received (e.g., cylinder). inside order to do this, a agent must (1) understand what it recognizes and what it does not, (2) formulate the valid, unambiguous and informative language query (a question) to ask a oracle, (3) derive a parameters of visual classifiers from a oracle response and (4) leverage a updated visual classifiers to ask more clarified questions. specifically, we propose the novel framework and formulate a learning of visual curiosity as the reinforcement learning problem. inside this framework, all components of our agent, visual recognition module (to see), question generation policy (to ask), answer digestion module (to understand) and graph memory module (to memorize), are learned entirely end-to-end to maximize a reward derived from a scene graph obtained by a agent as the consequence of a dialog with a oracle. importantly, a question generation policy was disentangled from a visual recognition system and specifics of a environment. consequently, we demonstrate the sort of double generalization. our question generation policy generalizes to new environments and the new pair of eyes, i.e., new visual system. trained on the synthetic dataset, our results show that our agent learns new visual concepts significantly faster than several heuristic baselines, even when tested on synthetic environments with novel objects, as well as inside the realistic environment.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
16328,"a paper discusses a peculiarities of flame propagation inside a ultra-lean hydrogen-air mixture. numerical analysis of a problem shows a possibility of a stable self-sustained flame ball existence inside unconfined space on sufficiently large spatial scales. a structure of a flame ball was determined by a convection processes related to a hot products rising inside a terrestrial gravity field. it was shown that a structure of a flame ball corresponds to a axisymmetric structures of a gaseous bubble inside a liquid. inside addition to a stable flame core, there are satellite burning kernels separated from a original flameball and developing in a thermal wake behind a propagating flame ball. a effective area of burning expands with time due to flame ball and satellite kernels development. both stable flame ball existence inside a ultra-lean mixture and increase inside a burning area indicate a possibility of transition to rapid deflagrative combustion as soon as a flame ball enters a region filled with hydrogen-air mixture of a richer composition. such the scenario was intrinsic to a natural spatial distribution of hydrogen inside a conditions of terrestrial gravity and therefore it was crucial to take it into account inside elaborating risk assessments techniques and prevention measures.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1206,"during a past few years advancements inside sports information systems and technology has allowed us to collect the number of detailed spatio-temporal data capturing various aspects of basketball. considering example, shot charts, that is, maps capturing locations of (made or missed) shots, and spatio-temporal trajectories considering all a players on a court should capture information about a offensive and defensive tendencies and schemes of the team. characterization of these processes was important considering player and team comparisons, pre-game scouting, game preparation etc. playing tendencies among teams have traditionally been compared inside the heuristic manner. recently automated ways considering similar comparisons have appeared inside a sports analytics literature. however, these approaches are almost exclusively focused on a spatial distribution of a underlying actions (usually shots taken), ignoring the multitude of other parameters that should affect a action studied. inside this work, we propose the framework based on tensor decomposition considering obtaining the set of prototype spatio-temporal patterns based on a core spatiotemporal information and contextual meta-data. a core of our framework was the 3d tensor x, whose dimensions represent a entity under consideration (team, player, possession etc.), a location on a court and time. we make use of a parafac decomposition and we decompose a tensor into several interpretable patterns, that should be thought of as prototype patterns of a process examined (e.g., shot selection, offensive schemes etc.). we also introduce an idea behind the method considering choosing a number of components to be considered. with the help of a tensor components, we should then express every entity as the weighted combination of these components. a framework introduced inside this paper should have further applications inside a work-flow of a basketball operations of the franchise, which we also briefly discuss.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15711,"networks should model real-world systems inside the variety of domains. network alignment (na) aims to find the node mapping that conserves similar regions between compared networks. na was applicable to many fields, including computational biology, where na should guide a transfer of biological knowledge from well- to poorly-studied species across aligned network regions. existing na methods should only align static networks. however, most complex real-world systems evolve over time and should thus be modeled as dynamic networks. we hypothesize that aligning dynamic network representations of evolving systems will produce superior alignments compared to aligning a systems' static network representations, as was currently done. considering this purpose, we introduce a first ever dynamic na method, dynamagna++. this proof-of-concept dynamic na method was an extension of the state-of-the-art static na method, magna++. even though both magna++ and dynamagna++ optimize edge as well as node conservation across a aligned networks, magna++ conserves static edges and similarity between static node neighborhoods, while dynamagna++ conserves dynamic edges (events) and similarity between evolving node neighborhoods. considering this purpose, we introduce a first ever measure of dynamic edge conservation and rely on our recent measure of dynamic node conservation. importantly, a two dynamic conservation measures should be optimized with the help of any state-of-the-art na method and not just magna++. we confirm our hypothesis that dynamic na was superior to static na, under fair comparison conditions, on synthetic and real-world networks, inside computational biology and social network domains. dynamagna++ was parallelized and it includes the user-friendly graphical interface.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
5412,"a aim of this paper was to establish the lattice theoretical framework to study a partially ordered set $\operatorname{\mathsf{tors}} a$ of torsion classes over the finite-dimensional algebra $a$. we show that $\operatorname{\mathsf{tors}} a$ was the complete lattice which enjoys very strong properties, as bialgebraicity and complete semidistributivity. thus its hasse quiver carries a important part of its structure, and we introduce a brick labelling of its hasse quiver and use it to study lattice congruences of $\operatorname{\mathsf{tors}} a$. inside particular, we give the representation-theoretical interpretation of a so-called forcing order, and we prove that $\operatorname{\mathsf{tors}} a$ was completely congruence uniform. when $i$ was the two-sided ideal of $a$, $\operatorname{\mathsf{tors}} (a/i)$ was the lattice quotient of $\operatorname{\mathsf{tors}} a$ which was called an algebraic quotient, and a corresponding lattice congruence was called an algebraic congruence. a second part of this paper consists inside studying algebraic congruences. we characterize a arrows of a hasse quiver of $\operatorname{\mathsf{tors}} a$ that are contracted by an algebraic congruence inside terms of a brick labelling. inside a third part, we study inside detail a case of preprojective algebras $\pi$, considering which $\operatorname{\mathsf{tors}} \pi$ was a weyl group endowed with a weak order. inside particular, we give the new, more representation theoretical proof of a isomorphism between $\operatorname{\mathsf{tors}} k q$ and a cambrian lattice when $q$ was the dynkin quiver. we also prove that, inside type $a$, a algebraic quotients of $\operatorname{\mathsf{tors}} \pi$ are exactly its hasse-regular lattice quotients.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
19466,the brief introduction to exterior differential systems considering graduate students familiar with manifolds and differential forms.,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11025,"inside this proceeding, we summarize a key science goals and reference design considering the next-generation very large array (ngvla) that was envisaged to operate inside a 2030s. a ngvla was an interferometric array with more than 10 times a sensitivity and spatial resolution of a current vla and alma, that will operate at frequencies spanning $\sim 1.2 -116$ ghz, thus lending itself to be highly complementary to alma and a ska1. as such, a ngvla will tackle the broad range of outstanding questions inside modern astronomy by simultaneously delivering a capability to: unveil a formation of solar system analogues; probe a initial conditions considering planetary systems and life with astrochemistry; characterize a assembly, structure, and evolution of galaxies from a first billion years to a present; use pulsars inside a galactic center as fundamental tests of gravity; and understand a formation and evolution of stellar and supermassive blackholes inside a era of multi-messenger astronomy.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
2798,"this article concerns the class of generalized linear mixed models considering clustered data, where a random effects are mapped uniquely onto a grouping structure and are independent between groups. we derive necessary and sufficient conditions that enable a marginal likelihood of such class of models to be expressed inside closed-form. illustrations are provided with the help of a gaussian, poisson, binomial and gamma distributions. these models are unified under the single umbrella of conjugate generalized linear mixed models, where ""conjugate"" refers to a fact that a marginal likelihood should be expressed inside closed-form, rather than implying inference using a bayesian paradigm. having an explicit marginal likelihood means that these models are more computationally convenient, which should be important inside big data contexts. except considering a binomial distribution, these models are able to achieve simultaneous conjugacy, and thus able to accommodate both unit and group level covariates.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
12283,"we present new atacama large millimeter/sub-millimeter array (alma) 1.3 mm continuum observations of a sr 24s transition disk with an angular resolution $\lesssim0.18""$ (12 au radius). we perform the multi-wavelength investigation by combining new data with previous alma data at 0.45 mm. a visibilities and images of a continuum emission at a two wavelengths are well characterized by the ring-like emission. visibility modeling finds that a ring-like emission was narrower at longer wavelengths, inside good agreement with models of dust trapping inside pressure bumps, although there are complex residuals that suggest potentially asymmetric structures. a 0.45 mm emission has the shallower profile in a central cavity than a 1.3 mm emission. inside addition, we find that a $^{13}$co and c$^{18}$o (j=2-1) emission peaks at a center of a continuum cavity. we do not detect either continuum or gas emission from a northern companion to this system (sr 24n), which was itself the binary system. a upper limit considering a dust disk mass of sr 24n was $\lesssim 0.12\,m_{\bigoplus}$, which gives the disk mass ratio inside dust between a two components of $m_{\mathrm{dust, sr\,24s}}/m_{\mathrm{dust, sr\,24n}}\gtrsim840$. a current alma observations may imply that either planets have already formed inside a sr 24n disk or that dust growth to mm-sizes was inhibited there and that only warm gas, as seen by ro-vibrational co emission in a truncation radii of a binary, was present.",0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3266,"we present near infrared high-precision photometry considering eight transiting hot jupiters observed during their predicted secondary eclipses. our observations were carried out with the help of a staring mode of a wircam instrument on a canada-france-hawaii telescope (cfht). we present a observing strategies and data reduction methods which delivered time series photometry with statistical photometric precisionas low as 0.11%. we performed the bayesian analysis to model a eclipse parameters and systematics simultaneously. a measured planet-to-star flux ratios allowed us to constrain a thermal emission from a day side of these hot jupiters, as we derived a planet brightness temperatures. our results combined with previously observed eclipses reveal an excess inside a brightness temperatures relative to a blackbody prediction considering a equilibrium temperatures of a planets considering the wide range of heat redistribution factors. we find the trend that this excess appears to be larger considering planets with lower equilibrium temperatures. this may imply some additional sources of radiation, such as reflected light from a host star and/or thermal emission from residual internal heat from a formation of a planet.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
17125,"by the memory mean-field process we mean a solution $x(\cdot)$ of the stochastic mean-field equation involving not just a current state $x(t)$ and its law $\mathcal{l}(x(t))$ at time $t$, but also a state values $x(s)$ and its law $\mathcal{l}(x(s))$ at some previous times $s<t$. our purpose was to study stochastic control problems of memory mean-field processes. - we consider a space $\mathcal{m}$ of measures on $\mathbb{r}$ with a norm $|| \cdot||_{\mathcal{m}}$ introduced by agram and {\o}ksendal inside \cite{ao1}, and prove a existence and uniqueness of solutions of memory mean-field stochastic functional differential equations. - we prove two stochastic maximum principles, one sufficient (a verification theorem) and one necessary, both under partial information. a corresponding equations considering a adjoint variables are the pair of \emph{(time-) advanced backward stochastic differential equations}, one of them with values inside a space of bounded linear functionals on path segment spaces. - as an application of our methods, we solve the memory mean-variance problem as well as the linear-quadratic problem of the memory process.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
5603,"this paper presents an intertemporal bimodal network to analyze a evolution of a semantic content of the scientific field within a framework of topic modeling, namely with the help of a latent dirichlet allocation (lda). a main contribution was a conceptualization of a topic dynamics and its formalization and codification into an algorithm. to benchmark a effectiveness of this approach, we propose three indexes which track a transformation of topics over time, their rate of birth and death, and a novelty of their content. applying a lda, we test a algorithm both on the controlled experiment and on the corpus of several thousands of scientific papers over the period of more than 100 years which account considering a history of a economic thought.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16764,"this paper proposes the general framework considering structure-preserving model reduction of the secondorder network system based on graph clustering. inside this approach, vertex dynamics are captured by a transfer functions from inputs to individual states, and a dissimilarities of vertices are quantified by a h2-norms of a transfer function discrepancies. the greedy hierarchical clustering algorithm was proposed to place those vertices with similar dynamics into same clusters. then, a reduced-order model was generated by a petrov-galerkin method, where a projection was formed by a characteristic matrix of a resulting network clustering. it was shown that a simplified system preserves an interconnection structure, i.e., it should be again interpreted as the second-order system evolving over the reduced graph. furthermore, this paper generalizes a definition of network controllability gramian to second-order network systems. based on it, we develop an efficient method to compute h2-norms and derive a approximation error between a full-order and reduced-order models. finally, a idea behind the method was illustrated by a example of the small-world network.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
19807,"automated decision making systems are increasingly being used inside real-world applications. inside these systems considering a most part, a decision rules are derived by minimizing a training error on a available historical data. therefore, if there was the bias related to the sensitive attribute such as gender, race, religion, etc. inside a data, say, due to cultural/historical discriminatory practices against the certain demographic, a system could continue discrimination inside decisions by including a said bias inside its decision rule. we present an information theoretic framework considering designing fair predictors from data, which aim to prevent discrimination against the specified sensitive attribute inside the supervised learning setting. we use equalized odds as a criterion considering discrimination, which demands that a prediction should be independent of a protected attribute conditioned on a actual label. to ensure fairness and generalization simultaneously, we compress a data to an auxiliary variable, which was used considering a prediction task. this auxiliary variable was chosen such that it was decontaminated from a discriminatory attribute inside a sense of equalized odds. a final predictor was obtained by applying the bayesian decision rule to a auxiliary variable.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2385,"we calculate a width $2\delta_{\text{ct}}$ and intensity of a charge-transfer peak (the one lying at a on-site energy $e_d$) inside a impurity spectral density of states as the function of $e_d$ inside a su($n$) impurity anderson model (iam). we use a dynamical density-matrix renormalization group (ddmrg) and a noncrossing-approximation (nca) considering $n$=4, and the 1/$n$ variational approximation inside a general case. inside particular, while considering $e_d \gg \delta$, where $\delta$ was a resonant level half-width, $\delta_{\text{ct}}=\delta$ as expected inside a noninteracting case, considering $-e_d \gg n \delta$ one has $\delta_{\text{ct}}=n\delta$. inside a $n$=2 case, some effects of a variation of $% \delta_{\text{ct}}$ with $e_d$ were observed inside a conductance through the quantum dot connected asymmetrically to conducting leads at finite bias [j. k√∂nemann \textit{et al.}, phys. rev. b \textbf{73}, 033313 (2006)]. more dramatic effects are expected inside similar experiments, that should be carried out inside systems of two quantum dots, carbon nanotubes or other, realizing a su(4) iam.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
11649,"we use bbn observational data on primordial abundance of ${}^4he$ to constrain f(t) gravity. a three most studied viable $f(t)$ models, namely a power law, a exponential and a square-root exponential are considered, and a bbn bounds are adopted inside order to extract constraints on their free parameters. considering a power-law model, we find that a constraints are inside agreement with those acquired with the help of late-time cosmological data. considering a exponential and a square-root exponential models, we show that considering realiable regions of parameters space they always satisfy a bbn bounds. we conclude that viable f(t) models should successfully satisfy a bbn constraints.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8666,"by with the help of a de giorgi iteration method we will give the new simple proof of a recent result of b.kotschwar, o.munteanu, j.wang [kmw] and n.sesum [s] on a local boundedness of a riemmanian curvature tensor of solutions of ricci flow inside terms of its inital value on the given ball and the local uniform bound on a ricci curvature.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12936,"inside spoken languages, speakers divide up a space of phonetic possibilities into different regions, corresponding to different phonemes. we consider the simple exemplar model of how this division of phonetic space varies over time among the population of language users. inside a particular model we consider, we show that, once a system was initialized with the given set of phonemes, that phonemes do not become extinct: all phonemes will be maintained inside a system considering all time. this was inside contrast to what was observed inside more complex models. furthermore, we show that a boundaries between phonemes fluctuate and we quantitatively study a fluctuations inside the simple instance of our model. these results prepare a ground considering more sophisticated models inside which some phonemes go extinct or new phonemes emerge through other processes.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8723,"sequential pattern mining techniques extract patterns corresponding to frequent subsequences from the sequence database. the practical limitation of these techniques was that they overload a user with too many patterns. local process model (lpm) mining was an alternative idea behind the method coming from a field of process mining. while inside traditional sequential pattern mining, the pattern describes one subsequence, an lpm captures the set of subsequences. also, while traditional sequential patterns only match subsequences that are observed inside a sequence database, an lpm may capture subsequences that are not explicitly observed, but that are related to observed subsequences. inside other words, lpms generalize a behavior observed inside a sequence database. these properties make it possible considering the set of lpms to cover a behavior of the much larger set of sequential patterns. yet, existing lpm mining techniques still suffer from a pattern explosion problem because they produce sets of redundant lpms. inside this paper, we propose several heuristics to mine the set of non-redundant lpms either from the set of redundant lpms or from the set of sequential patterns. we empirically compare a proposed heuristics between them and against existing (local) process mining techniques inside terms of coverage, redundancy, and complexity of a produced sets of lpms.",1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17559,"the good classification method should yield more accurate results than simple heuristics. but there are classification problems, especially high-dimensional ones like a ones based on image/video data, considering which simple heuristics should work quite accurately; a structure of a data inside such problems was easy to uncover without any sophisticated or computationally expensive method. on a other hand, some problems have the structure that should only be found with sophisticated pattern recognition methods. we are interested inside quantifying a difficulty of the given high-dimensional pattern recognition problem. we consider a case where a patterns come from two pre-determined classes and where a objects are represented by points inside the high-dimensional vector space. however, a framework we propose was extendable to an arbitrarily large number of classes. we propose classification benchmarks based on simple random projection heuristics. our benchmarks are 2d curves parameterized by a classification error and computational cost of these simple heuristics. each curve divides a plane into the ""positive- gain"" and the ""negative-gain"" region. a latter contains methods that are ill-suited considering a given classification problem. a former was divided into two by a curve asymptote; methods that lie inside a small region under a curve but right of a asymptote merely provide the computational gain but no structural advantage over a random heuristics. we prove that a curve asymptotes are optimal (i.e. at bayes error) inside some cases, and thus no sophisticated method should provide the structural advantage over a random heuristics. such classification problems, an example of which we present inside our numerical experiments, provide poor ground considering testing new pattern classification methods.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5450,"we explore a topological properties of quantum spin-1/2 chains with two ising symmetries. this class of models does not possess any of a symmetries that are required to protect a haldane phase. nevertheless, we show that there are 4 symmetry-protected topological phases, inside addition to 6 phases that spontaneously break one or both ising symmetries. by mapping a model to one-dimensional interacting fermions with particle-hole and time-reversal symmetry, we obtain integrable parent hamiltonians considering a conventional and topological phases of a spin model. we use these hamiltonians to characterize a physical properties of all 10 phases, identify their local and nonlocal order parameters, and understand a effects of weak perturbations that respect a ising symmetries. our study provides a first explicit example of the class of spin chains with several topologically non-trivial phases, and binds together a topological classifications of interacting bosons and fermions.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
19304,"gravitational lensing of a cosmic microwave background (cmb) was expected to be amongst a most powerful cosmological tools considering ongoing and upcoming cmb experiments. inside this work, we investigate the bias to cmb lensing reconstruction from temperature anisotropies due to a kinematic sunyaev-zel'dovich (ksz) effect, that is, a doppler shift of cmb photons induced by compton-scattering off moving electrons. a ksz signal yields biases due to both its own intrinsic non-gaussianity and its non-zero cross-correlation with a cmb lensing field (and other fields that trace a large-scale structure). this ksz-induced bias affects both a cmb lensing auto-power spectrum and its cross-correlation with low-redshift tracers. furthermore, it cannot be removed by multifrequency foreground separation techniques because a ksz effect preserves a blackbody spectrum of a cmb. while statistically negligible considering current datasets, we show that it will be important considering upcoming surveys, and failure to account considering it should lead to large biases inside constraints on neutrino masses or a properties of dark energy. considering the stage 4 cmb experiment, a bias should be as large as $\approx$ 15% or 12% inside cross-correlation with lsst galaxy lensing convergence or galaxy overdensity maps, respectively, when a maximum temperature multipole used inside a reconstruction was $\ell_{\rm max} = 4000$, and about half of that when $\ell_{\rm max} = 3000$. similarly, we find that a cmb lensing auto-power spectrum should be biased by up to several percent. these biases are many times larger than a expected statistical errors. reducing $\ell_{\rm max}$ should significantly mitigate a bias at a cost of the decrease inside a overall lensing reconstruction signal-to-noise. polarization-only reconstruction may be a most robust mitigation strategy.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17683,"phylogenetic networks are often constructed by merging multiple conflicting phylogenetic signals into the directed acyclic graph. it was interesting to explore whether the network constructed inside this way induces biologically-relevant phylogenetic signals that were not present inside a input. here we show that, given the multiple alignment the considering the set of taxa x and the rooted phylogenetic network n whose leaves are labelled by x, it was np-hard to locate a most parsimonious phylogenetic tree displayed by n (with respect to a) even when a level of n - a maximum number of reticulation nodes within the biconnected component - was 1 and the contains only 2 distinct states. (if, additionally, gaps are allowed a problem becomes apx-hard.) we also show that under a same conditions, and assuming the simple binary symmetric model of character evolution, finding a most likely tree displayed by a network was np-hard. these negative results contrast with earlier work on parsimony inside which it was shown that if the consists of the single column a problem was fixed parameter tractable inside a level. we conclude with the discussion of why, despite a np-hardness, both a parsimony and likelihood problem should likely be well-solved inside practice.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2621,"influence diagrams are the decision-theoretic extension of probabilistic graphical models. inside this paper we show how they should be used to solve a brachistochrone problem. we present results of numerical experiments on this problem, compare a solution provided by a influence diagram with a optimal solution. a r code used considering a experiments was presented inside a appendix.",1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
10393,"inside a evaluation of treatment effects, it was of major policy interest to know if a treatment was beneficial considering some and harmful considering others, the phenomenon known as qualitative interaction. we formulate this question as the multiple testing problem with many conservative null $p$-values, inside which a classical multiple testing methods may lose power substantially. we propose the simple technique---conditioning---to improve a power. the crucial assumption we need was uniform conservativeness, meaning considering any conservative $p$-value $p$, a conditional distribution $(p/\tau)\,|\,p \le \tau$ was stochastically larger than a uniform distribution on $(0,1)$ considering any $\tau$. we show this property holds considering one-sided tests inside the one-dimensional exponential family (e.g.\ testing considering qualitative interaction) as well as testing $|\mu|\le\eta$ with the help of the statistic $x \sim \mathrm{n}(\mu,1)$ (e.g.\ testing considering practical importance with threshold $\eta$). we propose an adaptive method to select a threshold $\tau$. our theoretical and simulation results suggest a proposed tests gain significant power when many $p$-values are uniformly conservative and lose little power when no $p$-value was uniformly conservative. we apply our method to two educational intervention datasets.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
8521,"predicting a outcome of sports events was the hard task. we quantify this difficulty with the coefficient that measures a distance between a observed final results of sports leagues and idealized perfectly balanced competitions inside terms of skill. this indicates a relative presence of luck and skill. we collected and analyzed all games from 198 sports leagues comprising 1503 seasons from 84 countries of 4 different sports: basketball, soccer, volleyball and handball. we measured a competitiveness by countries and sports. we also identify inside each season which teams, if removed from its league, result inside the completely random tournament. surprisingly, not many of them are needed. as another contribution of this paper, we propose the probabilistic graphical model to learn about a teams' skills and to decompose a relative weights of luck and skill inside each game. we break down a skill component into factors associated with a teams' characteristics. a model also allows to approximate as 0.36 a probability that an underdog team wins inside a nba league, with the home advantage adding 0.09 to this probability. as shown inside a first part of a paper, luck was substantially present even inside a most competitive championships, which partially explains why sophisticated and complex feature-based models hardly beat simple models inside a task of forecasting sports' outcomes.",1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11340,"this paper proposes three different distributed event-triggered control algorithms to achieve leader-follower consensus considering the network of euler-lagrange agents. we firstly propose two model-independent algorithms considering the subclass of euler-lagrange agents without a vector of gravitational potential forces. by model-independent, we mean that each agent should execute its algorithm with no knowledge of a agent self-dynamics. the variable-gain algorithm was employed when a sensing graph was undirected; algorithm parameters are selected inside the fully distributed manner with much greater flexibility compared to all previous work concerning event-triggered consensus problems. when a sensing graph was directed, the constant-gain algorithm was employed. a control gains must be centrally designed to exceed several lower bounding inequalities which require limited knowledge of bounds on a matrices describing a agent dynamics, bounds on network topology information and bounds on a initial conditions. when a euler-lagrange agents have dynamics which include a vector of gravitational potential forces, an adaptive algorithm was proposed which requires more information about a agent dynamics but should approximate uncertain agent parameters. considering each algorithm, the trigger function was proposed to govern a event update times. at each event, a controller was updated, which ensures that a control input was piecewise constant and saves energy resources. we analyse each controllers and trigger function and exclude zeno behaviour. extensive simulations show 1) a advantages of our proposed trigger function as compared to those inside existing literature, and 2) a effectiveness of our proposed controllers.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
10443,"we present an update to a ultraviolet-to-radio database of global broadband photometry considering a 79 nearby galaxies that comprise a union of a kingfish (key insights on nearby galaxies: the far-infrared survey with herschel) and sings (spitzer infrared nearby galaxies survey) samples. a 34-band dataset presented here includes contributions from observational work carried out with the variety of facilities including galex, sdss, ps, noao, 2mass, wise, spitzer, herschel, planck, jcmt, and a vla. improvements of note include recalibrations of previously-published sings bvrcic and kingfish far-infrared/submillimeter photometry. similar to previous results inside a literature, an excess of submillimeter emission above model predictions was seen primarily considering low-metallicity dwarf/irregular galaxies. this 34-band photometric dataset considering a combined kingfish$+$sings sample serves as an important multi-wavelength reference considering a variety of galaxies observed at low redshift. the thorough analysis of a observed spectral energy distributions was carried out inside the companion paper.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4133,"the framework integrating information theory and network science was proposed, giving rise to the potentially new area. by incorporating and integrating concepts such as complexity, coding, topological projections and network dynamics, a proposed network-based framework paves a way not only to extending traditional information science, but also to modeling, characterizing and analyzing the broad class of real-world problems, from language communication to dna coding. basically, an original network was supposed to be transmitted, with or without compaction, through the sequence of symbols or time-series obtained by sampling its topology by some network dynamics, such as random walks. we show that a degree of compression was ultimately related to a ability to predict a frequency of symbols based on a topology of a original network and a adopted dynamics. a potential of a proposed idea behind the method was illustrated with respect to a efficiency of transmitting several types of topologies by with the help of the variety of random walks. several interesting results are obtained, including a behavior of a barab√°si-albert model oscillating between high and low performance depending on a considered dynamics, and a distinct performances obtained considering two geographical models.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0
5727,"measuring a degree of spatial spreading of the sample should be of great interest when sampling from the spatial population. a commonly used spatial balance index by grafstr√∂m et al. (2012) was particularly effective inside comparing a level of spatial spreading of different samples from a same population. however, its unbounded and uninterpretable scale of measurement does not allow to assess a level of spatial spreading inside absolute terms and confines its use to only raw comparisons. inside this paper, we introduce the new absolute measure of a spatial spreading of the sample with the help of the normalized version of a moran's $i$ index. a properties and behaviour of a proposed measure are analysed through two simulation experiments, one based on artificial populations and a other on the population of real business units located inside a province of siena (italy).",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
15953,"sales forecast was an essential task inside e-commerce and has the crucial impact on making informed business decisions. it should aid us to manage a workforce, cash flow and resources such as optimizing a supply chain of manufacturers etc. sales forecast was the challenging problem inside that sales was affected by many factors including promotion activities, price changes, and user preferences etc. traditional sales forecast techniques mainly rely on historical sales data to predict future sales and their accuracies are limited. some more recent learning-based methods capture more information inside a model to improve a forecast accuracy. however, these methods require case-by-case manual feature engineering considering specific commercial scenarios, which was usually the difficult, time-consuming task and requires expert knowledge. to overcome a limitations of existing methods, we propose the novel idea behind the method inside this paper to learn effective features automatically from a structured data with the help of a convolutional neural network (cnn). when fed with raw log data, our idea behind the method should automatically extract effective features from that and then forecast sales with the help of those extracted features. we test our method on the large real-world dataset from cainiao.com and a experimental results validate a effectiveness of our method.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1289,"maintenance of association rules was an interesting problem. several incremental maintenance algorithms were proposed since a work of (cheung et al, 1996). a majority of these algorithms maintain rule bases assuming that support threshold doesn't change. inside this paper, we present incremental maintenance algorithm under support threshold change. this solution allows user to maintain its rule base under any support threshold.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8414,"this paper studies a partially observed stochastic optimal control problem considering systems with state dynamics governed by partial differential equations (pdes) that leads to an extremely large problem. first, an open-loop deterministic trajectory optimization problem was solved with the help of the black box simulation model of a dynamical system. next, the linear quadratic gaussian (lqg) controller was designed considering a nominal trajectory-dependent linearized system, which was identified with the help of input-output experimental data consisting of a impulse responses of a optimized nominal system. the computational nonlinear heat example was used to illustrate a performance of a approach.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1
9389,"most existing neural network models considering music generation use recurrent neural networks. however, a recent wavenet model proposed by deepmind shows that convolutional neural networks (cnns) should also generate realistic musical waveforms inside a audio domain. following this light, we investigate with the help of cnns considering generating melody (a series of midi notes) one bar after another inside a symbolic domain. inside addition to a generator, we use the discriminator to learn a distributions of melodies, making it the generative adversarial network (gan). moreover, we propose the novel conditional mechanism to exploit available prior knowledge, so that a model should generate melodies either from scratch, by following the chord sequence, or by conditioning on a melody of previous bars (e.g. the priming melody), among other possibilities. a resulting model, named midinet, should be expanded to generate music with multiple midi channels (i.e. tracks). we conduct the user study to compare a melody of eight-bar long generated by midinet and by google's melodyrnn models, each time with the help of a same priming melody. result shows that midinet performs comparably with melodyrnn models inside being realistic and pleasant to listen to, yet midinet's melodies are reported to be much more interesting.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14678,"we propose object-oriented neural programming (oonp), the framework considering semantically parsing documents inside specific domains. basically, oonp reads the document and parses it into the predesigned object-oriented data structure (referred to as ontology inside this paper) that reflects a domain-specific semantics of a document. an oonp parser models semantic parsing as the decision process: the neural net-based reader sequentially goes through a document, and during a process it builds and updates an intermediate ontology to summarize its partial understanding of a text it covers. oonp supports the rich family of operations (both symbolic and differentiable) considering composing a ontology, and the big variety of forms (both symbolic and differentiable) considering representing a state and a document. an oonp parser should be trained with supervision of different forms and strength, including supervised learning (sl) , reinforcement learning (rl) and hybrid of a two. our experiments on both synthetic and real-world document parsing tasks have shown that oonp should learn to handle fairly complicated ontology with training data of modest sizes.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15098,"inside this study, we present the fully automatic method to segment both rectum and rectal cancer based on deep neural networks (dnns) with axial t2-weighted magnetic resonance images. clinically, a relative location between rectum and rectal cancer plays an important role inside cancer treatment planning. such the need motivates us to propose the fully convolutional architecture considering multi-task learning (mtl) to segment both rectum and rectal cancer. moreover, we propose the bias-variance decomposition-based method which should visualize and assess regional robustness of a segmentation model. inside addition, we also suggest the novel augmentation method which should improve a segmentation performance as well as reduce a training time. overall, our proposed method was not only computationally efficient due to its fully convolutional nature but also outperforms a current state-of-the-art considering rectal cancer segmentation. it also scores high accuracy inside rectum segmentation without any prior study reported. moreover, we conclude that supplementing rectum information benefits a rectal cancer segmentation model, especially inside model variance.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8344,"electron ptychography has seen the recent surge of interest considering phase sensitive imaging at atomic or near-atomic resolution. however, applications are so far mainly limited to radiation-hard samples because a required doses are too high considering imaging biological samples at high resolution. we propose a use of non-convex, bayesian optimization to overcome this problem and reduce a dose required considering successful reconstruction by two orders of magnitude compared to previous experiments. we suggest to use this method considering imaging single biological macromolecules at cryogenic temperatures and demonstrate 2d single-particle reconstructions from simulated data with the resolution of 7.9 \aa$\,$ at the dose of 20 $e^- / \aa^2$. when averaging over only 15 low-dose datasets, the resolution of 4 \aa$\,$ was possible considering large macromolecular complexes. with its independence from microscope transfer function, direct recovery of phase contrast and better scaling of signal-to-noise ratio, cryo-electron ptychography may become the promising alternative to zernike phase-contrast microscopy.",0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
9702,"considering a structure of dc microgrids (mgs) composed of grid-forming/feeding converters, the hierarchical plug-and-play (pnp) voltage/current control architecture considering mg clusters was proposed. inside a primary level, the pnp voltage/current controller was proposed to achieve simultaneous voltage support and current feeding function according to local references. inside addition, stabilizing controller was characterized by explicit inequalities which are only related to local parameters of the mg. inside a secondary level, considering a system with interconnection of mgs, the leader-based voltage/current distributed controller was proposed to achieve both voltage and current regulation without specifying a individual setpoints considering each mg. a proposed controller requires the communication network and each controller exchanges information with its communication neighbors only. with a proposed controller, each mg should plug-in/out of a system seamlessly, irrespectively of a power line parameters and models of other mgs . a proof of a mg cluster closed-loop stability exploits structured lyapunov functions, a lasalle invariance theorem and properties of graph laplacians. theoretical results are validated by hardware-in-loop (hil) tests.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
13996,"tissue oxygenation and perfusion should be an indicator considering organ viability during minimally invasive surgery, considering example allowing real-time assessment of tissue perfusion and oxygen saturation. multispectral imaging was an optical modality that should inspect tissue perfusion inside wide field images without contact. inside this paper, we present the novel, fast method considering with the help of rgb images considering msi, which while limiting a spectral resolution of a modality allows normal laparoscopic systems to be used. we exploit a discrete haar decomposition to separate individual video frames into low pass and directional coefficients and we utilise the different multispectral approximation technique on each. a increase inside speed was achieved by with the help of fast tikhonov regularisation on a directional coefficients and more accurate bayesian approximation on a low pass component. a pipeline was implemented with the help of the graphics processing unit (gpu) architecture and achieves the frame rate of approximately 15hz. we validate a method on animal models and on human data captured with the help of the da vinci stereo laparoscope.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6972,"we recently showed that several local group (lg) galaxies have much higher radial velocities (rvs) than predicted by the 3d dynamical model of a standard cosmological paradigm. here, we show that 6 of these 7 galaxies define the thin plane with root mean square thickness of only 101 kpc despite the widest extent of nearly 3 mpc, much larger than a conventional virial radius of a milky way (mw) or m31. this plane passes within ${\sim 70}$ kpc of a mw-m31 barycentre and was oriented so a mw-m31 line was inclined by $16^\circ$ to it. we develop the toy model to constrain a scenario whereby the past mw-m31 flyby inside modified newtonian dynamics (mond) forms tidal dwarf galaxies that settle into a recently discovered planes of satellites around a mw and m31. a scenario was viable only considering the particular mw-m31 orbital plane. this roughly coincides with a plane of lg dwarfs with anomalously high rvs. with the help of the restricted $n$-body simulation of a lg inside mond, we show how a once fast-moving mw and m31 gravitationally slingshot test particles outwards at high speeds. a most distant such particles preferentially lie within a mw-m31 orbital plane, probably because a particles ending up with a highest rvs are those flung out almost parallel to a motion of a perturber. this suggests the dynamical reason considering our finding of the similar trend inside a real lg, something not easily explained as the chance alignment of galaxies with an isotropic or mildly flattened distribution (probability $= {0.0015}$).",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5761,"a exponential growth inside data generation and large-scale data analysis creates an unprecedented need considering inexpensive, low-latency, and high-density information storage. this need has motivated significant research into multi-level memory systems that should store multiple bits of information per device. although both a memory state of these devices and much of a data they store are intrinsically analog-valued, both are quantized considering use with digital systems and discrete error correcting codes. with the help of phase change memory as the prototypical multi-level storage technology, we herein demonstrate that analog-valued devices should achieve higher capacities when paired with analog codes. further, we find that storing analog signals directly through joint-coding should achieve low distortion with reduced coding complexity. by jointly optimizing considering signal statistics, device statistics, and the distortion metric, finite-length analog encodings should perform comparable to digital systems with asymptotically infinite large encodings. these results show that end-to-end analog memory systems have not only a potential to reach higher storage capacities than discrete systems, but also to significantly lower coding complexity, leading to faster and more energy efficient storage.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
9074,"inside statistical applications, a normal and a laplace distributions are often contrasted: a former as the standard tool of analysis, a latter as its robust counterpart. i discuss a convolutions of these two popular distributions and their applications inside research. i consider four models within the simple $2\times 2$ scheme which was of practical interest inside a analysis of clustered (e.g., longitudinal) data. inside my view, these models, some of which are less known than others by a majority of applied researchers, constitute the 'family' of sensible alternatives when modelling issues arise. inside three examples, i revisit data published recently inside a epidemiological and clinical literature as well as the classic biological dataset.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
4460,"high-speed 100 mhz strain monitor with the help of fiber bragg grating (fbg) and an optical filter has been devised considering a magnetostriction measurements under ultrahigh magnetic fields. a longitudinal magnetostriction of lacoo$_{3}$ has been measured at room temperature, 115, 7 and 4.2 k up to a maximum magnetic field of 150 t. a field-induced lattice elongations are observed, which are attributed to a spin-state crossover from a low-spin ground state to excited spin-states.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
12325,"the standard recipe considering spoken language recognition was to apply the gaussian back-end to i-vectors. this ignores a uncertainty inside a i-vector extraction, which could be important especially considering short utterances. the recent paper by cumani, plchot and fer proposes the solution to propagate that uncertainty into a backend. we propose an alternative method of propagating a uncertainty.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7646,"this paper derives the new family of estimators, namely a minimum density power divergence estimators, as the robust generalization of a maximum likelihood estimator considering a polytomous logistic regression model. based on these estimators, the family of wald-type test statistics considering linear hypotheses was introduced. robustness properties of both a proposed estimators and a test statistics are theoretically studied through a classical influence function analysis. appropriate real life examples are presented to justify a requirement of suitable robust statistical procedures inside place of a likelihood based inference considering a polytomous logistic regression model. a validity of a theoretical results established inside a paper are further confirmed empirically through suitable simulation studies. finally, an idea behind the method considering a data-driven selection of a robustness tuning parameter was proposed with empirical justifications.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
18024,"a solar modulation effect of cosmic rays inside a heliosphere was an energy-, time-, and particle-dependent phenomenon which arises from the combination of basic particle transport processes such as diffusion, convection, adiabatic cooling, and drift motion. making use of the large collection of time-resolved cosmic-ray data from recent space missions, we construct the simple predictive model of solar modulation which depends on direct solar-physics inputs: a number of solar sunspots and a tilt angle of a heliospheric current sheet. under this framework, we present calculations of cosmic-ray proton spectra, positron/electron and antiproton/proton ratios and their time dependence inside connection with a evolving solar activity. we report evidence considering the time-lag $\delta{t}=8.1\pm\,1.2$ months, between solar activity data and cosmic-ray flux measurements inside space, which reflects a dynamics of a formation of a modulation region. this result enables us to forecast a cosmic-ray flux near earth well inside advance by monitoring solar activity",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10172,"we introduce a suggest-and-improve framework considering general nonconvex quadratically constrained quadratic programs (qcqps). with the help of this framework, we generalize the number of known methods and provide heuristics to get approximate solutions to qcqps considering which no specialized methods are available. we also introduce an open-source python package qcqp, which implements a heuristics discussed inside a paper.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
11009,"methods that generate networks sharing the given degree distribution and global clustering should induce changes inside structural properties other than that controlled for. diversity inside structural properties, inside turn, should affect a outcomes of dynamical processes operating on those networks. since exhaustive sampling was not possible, we propose the novel evolutionary framework considering mapping this structural diversity. a three main features of this framework are: (a) subgraph-based encoding of networks, (b) exact mutations based on solving systems of diophantine equations, and (c) heuristic diversity-driven mechanism to drive resolution changes inside a mapelite algorithm. we show that our framework should elicit networks with diversity inside their higher-order structure and that this diversity affects a behaviour of a complex contagion model. through the comparison with state of a art clustered network generation methods, we demonstrate that our idea behind the method should uncover the comparably diverse range of networks without needing computationally unfeasible mixing times. further, we suggest that a subgraph-based encoding provides greater confidence inside a diversity of higher-order network structure considering low numbers of samples and was a basis considering explaining our results with complex contagion model. we believe that this framework could be applied to other complex landscapes that cannot be practically mapped using exhaustive sampling.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
14296,"this paper deals with asymptotics considering multiple-set linear canonical analysis (mslca). the definition of this analysis, that adapts a classical one to a context of euclidean random variables, was given and properties of a related canonical coefficients are derived. then, estimators of a mslca's elements, based on empirical covariance operators, are proposed and asymptotics considering these estimators are obtained. more precisely, we prove their consistency and we obtain asymptotic normality considering a estimator of a operator that gives mslca, and also considering a estimator of a vector of canonical coefficients. these results are then used to obtain the test considering mutual non-correlation between a involved euclidean random variables.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
1958,"we analyze a response of the type ii superconducting wire to an external magnetic field parallel to it inside a framework of ginzburg-landau theory. we focus on a surface superconductivity regime of applied field between a second and third critical values, where a superconducting state survives only close to a sample's boundary. our first finding was that, inside first approximation, a shape of a boundary plays no role inside determining a density of superconducting electrons. the second order term was however isolated, directly proportional to a mean curvature of a boundary. this demonstrates that points of higher boundary curvature (counted inwards) attract superconducting electrons.",0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
2530,"inside 2010, the paper entitled ""from obscurity to prominence inside minutes: political speech and real-time search"" won a best paper prize of a web science 2010 conference. among its findings were a discovery and documentation of what is termed the ""twitter-bomb"", an organized effort to spread misinformation about a democratic candidate martha coakley through anonymous twitter accounts. inside this paper, after summarizing a details of that event, we outline a recipe of how social networks are used to spread misinformation. one of a most important steps inside such the recipe was a ""infiltration"" of the community of users who are already engaged inside conversations about the topic, to use them as organic spreaders of misinformation inside their extended subnetworks. then, we take this misinformation spreading recipe and indicate how it is successfully used to spread fake news during a 2016 u.s. presidential election. a main differences between a scenarios are a use of facebook instead of twitter, and a respective motivations (in 2010: political influence; inside 2016: financial benefit through online advertising). after situating these events inside a broader context of exploiting a web, we seize this opportunity to address limitations of a reach of research findings and to start the conversation about how communities of researchers should increase their impact on real-world societal issues.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
15692,"this paper was the continuation of arxiv:1405.1707. we present certain new applications and generalizations of a free field realization of a twisted heisenberg-virasoro algebra ${\mathcal h}$ at level zero. we find explicit formulas considering singular vectors inside certain verma modules. the free field realization of self-dual modules considering ${\mathcal h}$ was presented by combining the bosonic construction of whittaker modules from arxiv:1409.5354 with the construction of logarithmic modules considering vertex algebras. as an application, we prove that there exists the non-split self-extension of irreducible self-dual module which was the logarithmic module of rank two. we construct the large family of logarithmic modules containing different types of highest weight modules as subquotients. we believe that these logarithmic modules are related with projective covers of irreducible modules inside the suitable category of ${\mathcal h}$-modules.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
16792,"inside a past 20 years, a hubble space telescope (hst) stis coronagraphic instrument has observed more than 100 stars, obtaining more than 4,000 readouts since its installment on hst inside 1997 and a numbers are still increasing. we reduce a whole stis coronagraphic archive at a most commonly observed positions (wedge a0.6 and a1.0) with new post-processing methods, and present our results here. we are able to recover all of a 32 previously reported circumstellar disks, and obtain better contrast close to a star. considering some of a disks, our results are limited by a over subtraction of a methods, and therefore a major regions of a disks should be recovered except a faintest regions. we also explain our efforts inside a calibration of its new bar5 occulting position, enabling stis to explore inner regions as close as 0.2"".",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
16527,"we prove that the general complex monge-amp√®re flow on the hermitian manifold should be run from an arbitrary initial condition with zero lelong number at all points. with the help of this property, we confirm the conjecture of tosatti-weinkove: a chern-ricci flow performs the canonical surgical contraction. finally, we study the generalization of a chern-ricci flow on compact hermitian manifolds, namely a twisted chern-ricci flow.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18906,"we study a gas phase metallicity (o/h) and nitrogen abundance gradients traced by star forming regions inside the representative sample of 550 nearby galaxies inside a stellar mass range $\rm 10^9-10^{11.5} m_\odot$ with resolved spectroscopic data from a sdss-iv manga survey. with the help of strong-line ratio diagnostics (r23 and o3n2 considering metallicity and n2o2 considering n/o) and referencing to a effective (half-light) radius ($\rm r_e$), we find that a metallicity gradient steepens with stellar mass, lying roughly flat among galaxies with $\rm log(m_\star/m_\odot) = 9.0$ but exhibiting slopes as steep as -0.14 dex $\rm r_e^{-1}$ at $\rm log(m_\star/m_\odot) = 10.5$ (using r23, but equivalent results are obtained with the help of o3n2). at higher masses, these slopes remain typical inside a outer regions of our sample ($\rm r > 1.5 ~r_e$), but the flattening was observed inside a central regions ($\rm r < 1~ r_e$). inside a outer regions ($\rm r > 2.0 ~r_e$) we detect the mild flattening of a metallicity gradient inside stacked profiles, although with low significance. a n/o ratio gradient provides complementary constraints on a average chemical enrichment history. unlike a oxygen abundance, a average n/o profiles do not flatten out inside a central regions of massive galaxies. a metallicity and n/o profiles both depart significantly from an exponential form, suggesting the disconnect between chemical enrichment and stellar mass surface density on local scales. inside a context of inside-out growth of discs, our findings suggest that central regions of massive galaxies today have evolved to an equilibrium metallicity, while a nitrogen abundance continues to increase as the consequence of delayed secondary nucleosynthetic production.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7546,"we present the low-frequency view of a perseus cluster with new observations from a karl g. jansky very large array (jvla) at 230-470 mhz. a data reveal the multitude of new structures associated with a mini-halo. a mini-halo seems to be influenced both by a agn activity as well as by a sloshing motion of a cool core cluster's gas. inside addition, it has the filamentary structure similar to that seen inside radio relics found inside merging clusters. we present the detailed description of a data reduction and imaging process of a dataset. a depth and resolution of a observations allow us to conduct considering a first time the detailed comparison of a mini-halo structure with a x-ray structure as seen inside a chandra x-ray images. a resulting image shows very clearly that a mini-halo emission was mostly contained behind a cold fronts, similar to that predicted by simulations of gas sloshing inside galaxy clusters. however, due to a proximity of a perseus cluster, as well as a quality of a data at low radio frequencies and at x-ray wavelengths, we also find evidence of fine structure. this structure includes several radial radio filaments extending inside different directions, the concave radio structure associated with a southern x-ray bay and sharp edges that correlate with x-ray edges. mini-halos are therefore not simply diffuse, uniform radio sources, but are rather filled with the rich variety of complex structures. these results illustrate a high-quality images that should be obtained with a new jvla at low radio-frequencies, as well as a necessity to obtain deeper, higher-fidelity radio images of mini-halos and halos inside clusters to further understand their origin.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11945,"recently, substantial research effort has focused on how to apply cnns or rnns to better extract temporal patterns from videos, so as to improve a accuracy of video classification. inside this paper, however, we show that temporal information, especially longer-term patterns, may not be necessary to achieve competitive results on common video classification datasets. we investigate a potential of the purely attention based local feature integration. accounting considering a characteristics of such features inside video classification, we propose the local feature integration framework based on attention clusters, and introduce the shifting operation to capture more diverse signals. we carefully analyze and compare a effect of different attention mechanisms, cluster sizes, and a use of a shifting operation, and also investigate a combination of attention clusters considering multimodal integration. we demonstrate a effectiveness of our framework on three real-world video classification datasets. our model achieves competitive results across all of these. inside particular, on a large-scale kinetics dataset, our framework obtains an excellent single model accuracy of 79.4% inside terms of a top-1 and 94.0% inside terms of a top-5 accuracy on a validation set. a attention clusters are a backbone of our winner solution at activitynet kinetics challenge 2017. code and models will be released soon.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17916,"this letter presents the new spectral-clustering-based idea behind the method to a subspace clustering problem. underpinning a proposed method was the convex program considering optimal direction search, which considering each data point d finds an optimal direction inside a span of a data that has minimum projection on a other data points and non-vanishing projection on d. a obtained directions are subsequently leveraged to identify the neighborhood set considering each data point. an alternating direction method of multipliers framework was provided to efficiently solve considering a optimal directions. a proposed method was shown to notably outperform a existing subspace clustering methods, particularly considering unwieldy scenarios involving high levels of noise and close subspaces, and yields a state-of-the-art results considering a problem of face clustering with the help of subspace segmentation.",1,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18389,"we consider scalar field models of dark energy interacting with dark matter through the coupling proportional to a contraction of a four-derivative of a scalar field with a four-velocity of a dark matter fluid. a coupling was realized at a lagrangian level employing a formalism of scalar-fluid theories, which use the consistent lagrangian idea behind the method considering relativistic fluid to describe dark matter. this framework produces fully covariant field equations, from which we should derive unequivocal cosmological equations at both background and linear perturbations levels. a background evolution was analyzed inside detail applying dynamical systems techniques, which allow us to find a complete asymptotic behavior of a universe given any set of model parameters and initial conditions. furthermore we study linear cosmological perturbations investigating a growth of cosmic structures within a quasi-static approximation. we find that these interacting dark energy models give rise to interesting phenomenological dynamics, including late-time transitions from dark matter to dark energy domination, matter and accelerated scaling solutions and dynamical crossing of a phantom barrier. moreover we obtain possible deviations from standard $\lambda$cdm behavior at a linear perturbations level, which have an impact on a dynamics of structure formation and might provide characteristic observational signatures.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6082,"a present paper presents a weighted ontology approximation heuristic (woah), the novel zero-shot idea behind the method to ontology approximation considering conversational agents development environments. this methodology extracts verbs and nouns separately from data by distilling a dependencies obtained and applying similarity and sparsity metrics to generate an ontology approximation configurable inside terms of a level of generalization.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8955,"we analyze a kozachenko--leonenko (kl) nearest neighbor estimator considering a differential entropy. we obtain a first uniform upper bound on its performance over h√∂lder balls on the torus without assuming any conditions on how close a density could be from zero. accompanying the new minimax lower bound over a h√∂lder ball, we show that a kl estimator was achieving a minimax rates up to logarithmic factors without cognizance of a smoothness parameter $s$ of a h√∂lder ball considering $s\in (0,2]$ and arbitrary dimension $d$, rendering it a first estimator that provably satisfies this property.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1055,"a oxidation of rocky planet surfaces and atmospheres, which arises from a twin forces of stellar nucleosynthesis and gravitational differentiation, was the universal process of key importance to habitability and exoplanet biosignature detection. here we take the generalized idea behind the method to this phenomenon. with the help of the single parameter to describe redox state, we model a evolution of terrestrial planets around nearby m-stars and a sun. our model includes atmospheric photochemistry, diffusion and escape, line-by-line climate calculations and interior thermodynamics and chemistry. inside most cases we find abiotic atmospheric o2 buildup around m-stars during a pre-main sequence phase to be much less than calculated previously, because a planet's magma ocean absorbs most oxygen liberated from h2o photolysis. however, loss of non-condensing atmospheric gases after a mantle solidifies remains the significant potential route to abiotic atmospheric o2 subsequently. inside all cases, we predict that exoplanets that receive lower stellar fluxes, such as lhs1140b and trappist-1f and g, have a lowest probability of abiotic o2 buildup and thus may be a most interesting targets considering future searches considering biogenic o2. key remaining uncertainties should be minimized inside future by comparing our predictions considering a atmospheres of hot, sterile exoplanets such as gj1132b and trappist-1b and --c with observations.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
89,"considering the polynomial $f(x)\in\mathbb z[x]$ without non-trivial linear relations among roots, we propose the conjecture on a distribution of a least root $r_p$ ($r_p\in\mathbb z,\,0\le r_p<p)$ of $f(x)\equiv0\bmod p$ where $p$ runs over a set of primes such that $f(x)$ modulo $p$ was fully splitting.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
16863,"inside this paper, we introduce a bmt distribution as an unimodal alternative to continuous univariate distributions supported on the bounded interval. a ideas behind a mathematical formulation of this new distribution come from computer aid geometric design, specifically from bezier curves. first, we review general properties of the distribution given by parametric equations and extend a definition of the bezier distribution. then, after proposing a bmt cumulative distribution function, we derive its probability density function and the closed-form expression considering quantile function, median, interquartile range, mode, and moments. a domain change from [0,1] to [c,d] was mentioned. approximation of parameters was approached by a methods of maximum likelihood and maximum product of spacing. we test a numerical approximation procedures with the help of some simulated data. usefulness and flexibility of a new distribution are illustrated inside three real data sets. a bmt distribution has the significant potential to approximate domain parameters and to model data outside a scope of a beta or similar distributions.",0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
16262,"inside this minireview we will discuss recent progress inside a analytical study of current-carrying non-equilibrium steady states (ness) that should be constructed inside terms of the matrix product ansatz. we will focus on one-dimensional exactly solvable strongly correlated cases, and will study both quantum models, and classical models which are deterministic inside a bulk. a only source of classical stochasticity inside a time-evolution will come from a boundaries of a system. physically, these boundaries may be understood as markovian baths, which drive a current through a system. a examples studied include a open xxz heisenberg spin chain, a open hubbard model, and the classical integrable reversible cellular automaton, namely a rule 54 of bobenko {\em et al}. [commun. math. phys. {\bf 158}, 127 (1993)] with stochastic boundaries. a quantum ness should be at least partially understood through a yang-baxter integrability structure of a underlying integrable bulk hamiltonian, whereas considering a rule 54 model ness seems to come from the seemingly unrelated integrability theory. inside both a quantum and a classical case, a underlying matrix product ansatz defining a ness also allows considering construction of novel conservation laws of a bulk models themselves. inside a classical case, the modification of a matrix product ansatz also allows considering construction of states beyond a steady state (i.e., some of a decay modes -- liouvillian eigenvectors of a model). we hope that this article will aid further a quest to unite different perspectives of integrability of ness (of both quantum and classical models) into the single unified framework.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
17234,"a variability of a clusters generated by clustering techniques inside a domain of latitude and longitude variables of fatal crash data are significantly unpredictable. this unpredictability, caused by a randomness of fatal crash incidents, reduces a accuracy of crash frequency (i.e., counts of fatal crashes per cluster) which was used to measure traffic safety inside practice. inside this paper, the quantitative measure of traffic safety that was not significantly affected by a aforementioned variability was proposed. it introduces the fatal point -- the segment with a highest frequency of fatality -- concept based on cluster characteristics and detects them by imposing rounding errors to a hundredth decimal place of a longitude. a frequencies of a cluster and a cluster's fatal point are combined to construct the low-sensitive quantitative measure of traffic safety considering a cluster. a performance of a proposed measure of traffic safety was then studied by varying a parameter k of k-means clustering with a expectation that other clustering techniques should be adopted inside the similar fashion. a 2015 north carolina fatal crash dataset of fatality analysis reporting system (fars) was used to evaluate a proposed fatal point concept and perform experimental analysis to determine a effectiveness of a proposed measure. a empirical study shows that a average traffic safety, measured by a proposed quantitative measure over several clusters, was not significantly affected by a variability, compared to that of a standard crash frequency.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
12282,"inside this work, we characterize a outputs of individual neurons inside the trained feed-forward neural network by entropy, mutual information with a class variable, and the class selectivity measure based on kullback-leibler divergence. by cumulatively ablating neurons inside a network, we connect these information-theoretic measures to a impact their removal has on classification performance on a test set. we observe that, looking at a neural network as the whole, none of these measures was the good indicator considering classification performance, thus confirming recent results by morcos et al. however, looking at specific layers separately, both mutual information and class selectivity are positively correlated with classification performance. we thus conclude that it was ill-advised to compare these measures across layers, and that different layers may be most appropriately characterized by different measures. we then discuss pruning neurons from neural networks to reduce computational complexity of inference. drawing from our results, we perform pruning based on information-theoretic measures on the fully connected feed-forward neural network with two hidden layers trained on mnist dataset and compare a results to the recently proposed pruning method. we furthermore show that a common practice of re-training after pruning should partly be obviated by the surgery step called bias balancing, without incurring significant performance degradation.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6303,"periodic driving should be used to coherently control a properties of the many-body state and to realize new phases which are not accessible inside static systems. considering example, exposing materials to intense laser pulses enables to provoke metal-insulator transitions, control a magnetic order and induce transient superconducting behavior well above a static transition temperature. however, pinning down a responsible mechanisms was often difficult, since a response to irradiation was governed by complex many-body dynamics. inside contrast to static systems, where extensive calculations have been performed to explain phenomena such as high-temperature superconductivity, theoretical analyses of driven many-body hamiltonians are more demanding and new theoretical approaches have been inspired by a recent observations. here, we perform an experimental quantum simulation inside the periodically modulated hexagonal lattice and show that anti-ferromagnetic correlations inside the fermionic many-body system should be reduced or enhanced or even switched to ferromagnetic ordering. we first demonstrate that inside a high frequency regime, a description of a many-body system by an effective floquet-hamiltonian with the renormalized tunneling energy remains valid, by comparing a results to measurements inside an equivalent static lattice. considering near-resonant driving, a enhancement and sign reversal of correlations was explained by the microscopic model, inside which a particle tunneling and magnetic exchange energies should be controlled independently. inside combination with a observed sufficiently long lifetime of correlations, floquet engineering thus constitutes an alternative route to experimentally investigate unconventional pairing inside strongly correlated systems.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
2810,"monte carlo (mc) simulations of transport inside random porous networks indicate that considering high variances of a log-normal permeability distribution, a transport of the passive tracer was non-fickian. here we model this non-fickian dispersion inside random porous networks with the help of discrete temporal markov models. we show that such temporal models capture a spreading behavior accurately. this was true despite a fact that a slow velocities are strongly correlated inside time, and some studies have suggested that a persistence of low velocities would render a temporal markovian model inapplicable. compared to previously proposed temporal stochastic differential equations with case specific drift and diffusion terms, a models presented here require fewer modeling assumptions. moreover, we show that discrete temporal markov models should be used to represent dispersion inside unstructured networks, which are widely used to model porous media. the new method was proposed to extend a state space of temporal markov models to improve a model predictions inside a presence of extremely low velocities inside particle trajectories and extend a applicability of a model to higher temporal resolutions. finally, it was shown that by combining multiple transitions, temporal models are more efficient considering computing particle evolution compared to correlated ctrw with spatial increments that are equal to a lengths of a links inside a network.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15710,"we propose the variant of ising model, called a seeded ising model, to model probabilistic nature of human iris templates. this model was an ising model inside which a values at certain lattice points are held fixed throughout ising model evolution. with the help of this we show how to reconstruct a full iris template from partial information, and we show that about 1/6 of a given template was needed to recover almost all information content of a original one inside a sense that a resulting hamming distance was well within a range to assert correctly a identity of a subject. this leads us to propose a concept of effective statistical degree of freedom of iris templates and show it was about 1/6 of a total number of bits. inside particular, considering the template of $2048$ bits, its effective statistical degree of freedom was about $342$ bits, which coincides very well with a degree of freedom computed by a completely different method proposed by daugman.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9959,"a paper conducts the second-order variational analysis considering an important class of nonpolyhedral conic programs generated by a so-called second-order/lorentz/ice-cream cone $q$. from one hand, we prove that a indicator function of $q$ was always twice epi-differentiable and apply this result to characterizing a uniqueness of lagrange multipliers at stationary points together with an error bound approximate inside a general second-order cone setting involving ${\cal c}^2$-smooth data. on a other hand, we precisely calculate a graphical derivative of a normal cone mapping to $q$ under a weakest metric subregularity constraint qualification and then give an application of a latter result to the complete characterization of isolated calmness considering perturbed variational systems associated with second-order cone programs. a obtained results seem to be a first inside a literature inside these directions considering nonpolyhedral problems without imposing any nondegeneracy assumptions.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
16043,"characterizing a evolution of a faint end of a cluster red sequence (rs) galaxy luminosity function (glf) with redshift was the milestone inside understanding galaxy evolution. however, a community was still divided inside that respect, hesitating between an enrichment of a rs due to efficient quenching of blue galaxies from $z\sim1$ to present-day or the scenario inside which a rs was built at the higher redshift and does not evolve afterwards. recently, it has been proposed that surface brightness (sb) selection effects could possibly solve a literature disagreement, accounting considering a diminishing of a rs faint population inside ground based observations. we investigate this hypothesis by comparing a rs glfs of 16 clash clusters computed independently from ground-based subaru/suprime-cam and hst/acs images inside a redshift range $0.187\leq z\leq0.686$. we stack individual cluster glfs inside redshift and mass bins. we find similar rs glfs considering space and ground based data, with the difference of 0.2$\sigma$ inside a faint end parameter $\alpha$ when stacking all clusters together and the maximum difference of 0.9$\sigma$ inside a case of a high redshift stack, demonstrating the weak dependence on a type of observations inside a probed range of redshift and mass. when considering a full sample, we approximate $\alpha = -0.76 \pm 0.07$ and $\alpha = -0.78 \pm 0.06$ with hst and subaru respectively. we note the mild variation of a faint end with redshift at the 1.7$\sigma$ and 2.6$\sigma$ significance. we investigate a effect of sb dimming by simulating our low redshift galaxies at high redshift. we measure an evolution inside a faint end slope of less than 1$\sigma$ inside this case, implying that a observed signature was moderately larger than one would expect from sb dimming alone, and indicating the true evolution inside a faint end slope. (abridged...)",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9674,"inside this paper we address lifshitz transition induced by applied external magnetic field inside the case of iron-based superconductors, inside which the difference between a fermi level and a edges of a bands was relatively small. we introduce and investigate the two-band model with intra-band pairing inside a relevant parameters regime to address the generic behaviour of the system with hole-like and electron-like bands inside external magnetic field. our results show that two lifshitz transitions should develop inside analysed systems and a first one occurs inside a superconducting phase and takes place at approximately constant magnetic field. a chosen sets of a model parameters should describe characteristic band structure of iron-based superconductors and thus a obtained results should explain a experimental observations inside fese and co-doped bafe$_{2}$as$_{2}$ compounds.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
12877,"this paper presents design and experimental evaluations of an articulated robotic limb called capler-leg. a key element of capler-leg was its single-stage cable-pulley transmission combined with the high-gap radius motor. our cable-pulley system was designed to be as light-weight as possible and to additionally serve as a primary cooling element, thus significantly increasing a power density and efficiency of a overall system. a total weight of active elements on a leg, i.e. a stators and a rotors, contribute more than 60% of a total leg weight, which was an order of magnitude higher than most existing robots. a resulting robotic leg has low inertia, high torque transparency, low manufacturing cost, no backlash, and the low number of parts. capler-leg system itself, serves as an experimental setup considering evaluating a proposed cable- pulley design inside terms of robustness and efficiency. the continuous jump experiment shows the remarkable 96.5 % recuperation rate, measured at a battery output. this means that almost all a mechanical energy output used during push-off returned back to a battery during touch-down.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
19309,"given the rectilinear domain $\mathcal{p}$ of $h$ pairwise-disjoint rectilinear obstacles with the total of $n$ vertices inside a plane, we study a problem of computing bicriteria rectilinear shortest paths between two points $s$ and $t$ inside $\mathcal{p}$. three types of bicriteria rectilinear paths are considered: minimum-link shortest paths, shortest minimum-link paths, and minimum-cost paths where a cost of the path was the non-decreasing function of both a number of edges and a length of a path. a one-point and two-point path queries are also considered. algorithms considering these problems have been given previously. our contributions are threefold. first, we find the critical error inside all previous algorithms. second, we correct a error inside the not-so-trivial way. third, we further improve a algorithms so that they are even faster than a previous (incorrect) algorithms when $h$ was relatively small. considering example, considering a minimum-link shortest paths, we obtain a following results. our algorithm computes the minimum-link shortest $s$-$t$ path inside $o(n+h\log^{3/2} h)$ time. considering a one-point queries, we build the data structure of size $o(n+ h\log h)$ inside $o(n+h\log^{3/2} h)$ time considering the source point $s$, such that given any query point $t$, the minimum-link shortest $s$-$t$ path should be determined inside $o(\log n)$ time. considering a two-point queries, with $o(n+h^2\log^2 h)$ time and space preprocessing, the minimum-link shortest $s$-$t$ path should be determined inside $o(\log n+\log^2 h)$ time considering any two query points $s$ and $t$; alternatively, with $o(n+h^2\cdot \log^{2} h \cdot 4^{\sqrt{\log h}})$ time and $o(n+h^2\cdot \log h \cdot 4^{\sqrt{\log h}})$ space preprocessing, we should answer each two-point query inside $o(\log n)$ time.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8075,"currently a detection of very high energy gamma-rays considering astrophysics rely on a measurement of a extensive air showers (eas) either with the help of cherenkov detectors or eas arrays with larger field of views but also larger energy thresholds. inside this talk we present the novel hybrid detector concept considering the eas array with an improved sensitivity inside a lower energies ($\sim 100\,$gev). we discuss its main features, capabilities and present preliminary results on its expected perfomances and sensitivities.this wide field of view experiment was planned to be installed at high altitude inside south america making it the complementary project to a planned cherenkov telescope experiments and the powerful tool to trigger further observations of variable sources and to detect transients phenomena.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
3902,"a existence of closed loops of degeneracies inside crystals has been intimately connected to associated crystal symmetries, raising a question: what was a minimum symmetry required considering topological character, and should one find an example? triclinic caas$_3$, inside space group $p{\bar 1}$ with only the center of inversion, has been found to display, without need considering tuning, the nodal loop of accidental degeneracies with topological character, centered on one face of a brillouin zone that was otherwise fully gapped. a small loop was very flat inside energy, yet was cut four times by a fermi energy, the condition that results inside an intricate repeated touching of inversion related pairs of fermi surfaces at weyl points. spin-orbit coupling lifts a fourfold degeneracy along a loop, leaving trivial kramers pairs. with its single nodal loop that emerges without protection from any point group symmetry, caas$_3$ represents a primal ""hydrogen atom"" of nodal loop systems.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
18623,"robust pca, a problem of pca inside a presence of outliers has been extensively investigated inside a last few years. here we focus on robust pca inside a outlier model where each column of a data matrix was either an inlier or an outlier. most of a existing methods considering this model assumes either a knowledge of a dimension of a lower dimensional subspace or a fraction of outliers inside a system. however inside many applications knowledge of these parameters was not available. motivated by this we propose the parameter free outlier identification method considering robust pca which a) does not require a knowledge of outlier fraction, b) does not require a knowledge of a dimension of a underlying subspace, c) was computationally simple and fast d) should handle structured and unstructured outliers. further, analytical guarantees are derived considering outlier identification and a performance of a algorithm was compared with a existing state of a art methods inside both real and synthetic data considering various outlier structures.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4384,"hans j. zassenhaus conjectured that considering any unit $u$ of finite order inside a integral group ring of the finite group $g$ there exists the unit $a$ inside a rational group algebra of $g$ such that $a^{-1}\cdot u \cdot a=\pm g$ considering some $g\in g$. we disprove this conjecture by first proving general results that aid identify counterexamples and then providing an infinite number of examples where these results apply. our smallest example was the metabelian group of order $2^7 \cdot 3^2 \cdot 5 \cdot 7^2 \cdot 19^2$ whose integral group ring contains the unit of order $7 \cdot 19$ which, inside a rational group algebra, was not conjugate to any element of a form $\pm g$.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
256,"the novel jet-stirred combustion chamber was designed to study turbulent premixed flames. inside a new approach, multiple impinging turbulent jets are used to stir a mixture. it was well known that pair of counterflowing turbulent jets produces nearly the constant intensity along a jet axes. inside this study, different numbers of impinging jets inside various configurations are used to produce isotropic turbulence intensity. fluent simulations have been conducted to assess a viability of a proposed chamber. inside order to be able to compare different configurations, three different non dimensional indices are introduces. mean flow index, homogeneity index, and isotropicity index. with the help of these indices one should compare various chambers including conventional fan-stirred reactor. results show that the concentric inlet/outlet chamber with 8 inlets and 8 outlets with inlet velocity of 20 m/s and initial intensity of 15% produces near zero mean flow and 2.5 m/s turbulence intensity which was much more higher than reported values considering fan-stirred chamber.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2655,"ejection velocity fields of asteroid families are largely unconstrained due to a fact that members disperse relatively quickly on myr time-scales by secular resonances and a yarkovsky effect. a spreading of fragments inside $a$ by a yarkovsky effect was indistinguishable from a spreading caused by a initial ejection of fragments. by examining families $<$20 myrs-old, we should use a v-shape identification technique to separate family shapes that are due to a initial ejection velocity field and those that are due to a yarkovsky effect. $<$20 myr-old asteroid families provide an opportunity to study a velocity field of family fragments before they become too dispersed. only a karin family's initial velocity field has been determined and scales inversely with diameter, $d^{-1}$. we have applied a v-shape identification technique to constrain young families' initial ejection velocity fields by measuring a curvature of their fragments' v-shape correlation inside semi-major axis, $a$, vs. $d^{-1}$ space. curvature from the straight line implies the deviation from the scaling of $d^{-1}$. we measure a v-shape curvature of 11 young asteroid families including a \fynospace, aeolia, brangane, brasilia, clarissa, iannini, karin, konig, koronis(2), theobalda and veritas asteroid families. we find that a majority of asteroid families have initial ejection velocity fields consistent with $\sim d^{-1}$ supporting laboratory impact experiments and computer simulations of disrupting asteroid parent bodies.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
802,"given the large number of unlabeled face images, face grouping aims at clustering a images into individual identities present inside a data. this task remains the challenging problem despite a remarkable capability of deep learning approaches inside learning face representation. inside particular, grouping results should still be egregious given profile faces and the large number of uninteresting faces and noisy detections. often, the user needs to correct a erroneous grouping manually. inside this study, we formulate the novel face grouping framework that learns clustering strategy from ground-truth simulated behavior. this was achieved through imitation learning (a.k.a apprenticeship learning or learning by watching) using inverse reinforcement learning (irl). inside contrast to existing clustering approaches that group instances by similarity, our framework makes sequential decision to dynamically decide when to merge two face instances/groups driven by short- and long-term rewards. extensive experiments on three benchmark datasets show that our framework outperforms unsupervised and supervised baselines.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17715,"inside this paper, we focus on online representation learning inside non-stationary environments which may require continuous adaptation of model architecture. we propose the novel online dictionary-learning (sparse-coding) framework which incorporates a addition and deletion of hidden units (dictionary elements), and was inspired by a adult neurogenesis phenomenon inside a dentate gyrus of a hippocampus, known to be associated with improved cognitive function and adaptation to new environments. inside a online learning setting, where new input instances arrive sequentially inside batches, a neuronal-birth was implemented by adding new units with random initial weights (random dictionary elements); a number of new units was determined by a current performance (representation error) of a dictionary, higher error causing an increase inside a birth rate. neuronal-death was implemented by imposing l1/l2-regularization (group sparsity) on a dictionary within a block-coordinate descent optimization at each iteration of our online alternating minimization scheme, which iterates between a code and dictionary updates. finally, hidden unit connectivity adaptation was facilitated by introducing sparsity inside dictionary elements. our empirical evaluation on several real-life datasets (images and language) as well as on synthetic data demonstrates that a proposed idea behind the method should considerably outperform a state-of-art fixed-size (nonadaptive) online sparse coding of mairal et al. (2009) inside a presence of nonstationary data. moreover, we identify certain properties of a data (e.g., sparse inputs with nearly non-overlapping supports) and of a model (e.g., dictionary sparsity) associated with such improvements.",1,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11093,"visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. even with a depth of features inside the convolutional network, the layer inside isolation was not enough: compounding and aggregating these representations improves inference of what and where. architectural efforts are exploring many dimensions considering network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across the network deserves further attention. although skip connections have been incorporated to combine layers, these connections have been ""shallow"" themselves, and only fuse by simple, one-step operations. we augment standard architectures with deeper aggregation to better fuse information across layers. our deep layer aggregation structures iteratively and hierarchically merge a feature hierarchy to make networks with better accuracy and fewer parameters. experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes. a code was at this https url.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8574,"it was the general challenge to design highly active or selective earth-abundant metals considering catalytic hydrogenation. here, we demonstrated an effective computational idea behind the method based on inverse molecular design theory to deterministically search considering optimal binding sites on cu (100) surface through a doping of fe and/or zn, and the stable zn-doped cu (100) surface is found with minimal binding energy to h-atoms. we analyze a electronic structure cause of a optimal binding sites with the help of the new quantum chemistry method called orbital-specific binding energy analysis. compared to a 3d-orbitals of surface cu atoms, a 3d-orbitals of surface zn-atoms show less binding energy contribution and participation, and are much less influenced by a electronic couplings of a media cu atoms. our study provides valuable green chemistry insights on designing catalysts with the help of earth-abundant metals, and may lead to a development of novel cu-based earth-abundant alloys considering important catalytic hydrogenation applications such as lignin degradation or co2 transformation.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
13885,a ground state of a spin-$1/2$ heisenberg antiferromagnet on the distorted triangular lattice was studied with the help of the numerical-diagonalization method. a network of interactions was a $\sqrt{3}\times\sqrt{3}$ type; a interactions are continuously controlled between a undistorted triangular lattice and a dice lattice. we find new states between a nonmagnetic 120-degree-structured state of a undistorted triangular case and a up-up-down state of a dice case. a intermediate states show spontaneous magnetizations that are smaller than one third of a saturated magntization corresponding to a up-up-down state.,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
7046,"inside this paper, we prove a global well-posedness of a incompressible mhd equations near the homogeneous equilibrium inside a domain $r^k\times t^{d-k}, d\geq2,k\geq1$ by with the help of a comparison principle and constructing a comparison function.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19850,"we introduce two tactics to attack agents trained by deep reinforcement learning algorithms with the help of adversarial examples, namely a strategically-timed attack and a enchanting attack. inside a strategically-timed attack, a adversary aims at minimizing a agent's reward by only attacking a agent at the small subset of time steps inside an episode. limiting a attack activity to this subset helps prevent detection of a attack by a agent. we propose the novel method to determine when an adversarial example should be crafted and applied. inside a enchanting attack, a adversary aims at luring a agent to the designated target state. this was achieved by combining the generative model and the planning algorithm: while a generative model predicts a future states, a planning algorithm generates the preferred sequence of actions considering luring a agent. the sequence of adversarial examples was then crafted to lure a agent to take a preferred sequence of actions. we apply a two tactics to a agents trained by a state-of-the-art deep reinforcement learning algorithm including dqn and a3c. inside 5 atari games, our strategically timed attack reduces as much reward as a uniform attack (i.e., attacking at every time step) does by attacking a agent 4 times less often. our enchanting attack lures a agent toward designated target states with the more than 70% success rate. videos are available at this http url",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16533,"consider a entropy of the unit gaussian convolved over the discrete set of k points, constrained to an interval of length l. maximising this entropy fixes k, and we show that this number exhibits the novel scaling law k ~ l^1/\zeta as l -> infinity, with exponent \zeta = 3/4. this law is observed numerically inside the recent paper about optimal effective theories; here we present an analytic derivation. we argue that this law was generic considering channel capacity maximisation, or a equivalent minimax problem. we also briefly discuss a behaviour at a boundary of a interval, and higher dimensional versions.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
15122,"this paper highlights a significance of including memory structures inside neural networks when a latter are used to learn perception-action loops considering autonomous robot navigation. traditional navigation approaches rely on global maps of a environment to overcome cul-de-sacs and plan feasible motions. yet, maintaining an accurate global map may be challenging inside real-world settings. the possible way to mitigate this limitation was to use learning techniques that forgo hand-engineered map representations and infer appropriate control responses directly from sensed information. an important but unexplored aspect of such approaches was a effect of memory on their performance. this work was the first thorough study of memory structures considering deep-neural-network-based robot navigation, and offers novel tools to train such networks from supervision and quantify their ability to generalize to unseen scenarios. we analyze a separation and generalization abilities of feedforward, long short-term memory, and differentiable neural computer networks. we introduce the new method to evaluate a generalization ability by estimating a vc-dimension of networks with the final linear readout layer. we validate that a vc estimates are good predictors of actual test performance. a reported method should be applied to deep learning problems beyond robotics.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
5970,"a rise of graph-structured data such as social networks, regulatory networks, citation graphs, and functional brain networks, inside combination with resounding success of deep learning inside various applications, has brought a interest inside generalizing deep learning models to non-euclidean domains. inside this paper, we introduce the new spectral domain convolutional architecture considering deep learning on graphs. a core ingredient of our model was the new class of parametric rational complex functions (cayley polynomials) allowing to efficiently compute spectral filters on graphs that specialize on frequency bands of interest. our model generates rich spectral filters that are localized inside space, scales linearly with a size of a input data considering sparsely-connected graphs, and should handle different constructions of laplacian operators. extensive experimental results show a superior performance of our approach, inside comparison to other spectral domain convolutional architectures, on spectral image classification, community detection, vertex classification and matrix completion tasks.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5821,"inside adversarial attacks to machine-learning classifiers, small perturbations are added to input that was correctly classified. a perturbations yield adversarial examples, which are virtually indistinguishable from a unperturbed input, and yet are misclassified. inside standard neural networks used considering deep learning, attackers should craft adversarial examples from most input to cause the misclassification of their choice. we introduce the new type of network units, called rbfi units, whose non-linear structure makes them inherently resistant to adversarial attacks. on permutation-invariant mnist, inside absence of adversarial attacks, networks with the help of rbfi units match a performance of networks with the help of sigmoid units, and are slightly below a accuracy of networks with relu units. when subjected to adversarial attacks, networks with rbfi units retain accuracies above 90% considering attacks that degrade a accuracy of networks with relu or sigmoid units to below 2%. rbfi networks trained with regular input are superior inside their resistance to adversarial attacks even to relu and sigmoid networks trained with a aid of adversarial examples. a non-linear structure of rbfi units makes them difficult to train with the help of standard gradient descent. we show that networks of rbfi units should be efficiently trained to high accuracies with the help of pseudogradients, computed with the help of functions especially crafted to facilitate learning instead of their true derivatives. we show that a use of pseudogradients makes training deep rbfi networks practical, and we compare several structural alternatives of rbfi networks considering their accuracy.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15036,"what makes content go viral? which videos become popular and why others don't? such questions have elicited significant attention from both researchers and industry, particularly inside a context of online media. the range of models have been recently proposed to explain and predict popularity; however, there was the short supply of practical tools, accessible considering regular users, that leverage these theoretical results. hipie -- an interactive visualization system -- was created to fill this gap, by enabling users to reason about a virality and a popularity of online videos. it retrieves a metadata and a past popularity series of youtube videos, it employs hawkes intensity process, the state-of-the-art online popularity model considering explaining and predicting video popularity, and it presents videos comparatively inside the series of interactive plots. this system will aid both content consumers and content producers inside the range of data-driven inquiries, such as to comparatively analyze videos and channels, to explain and predict future popularity, to identify viral videos, and to approximate response to online promotion.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
2930,"assembling different two-dimensional (2d) crystals, covering the very broad range of properties, into van der waals (vdw) heterostructures enables a unprecedented possibilities considering combining a best of different ingredients inside one objective material. so far, metallic, semiconducting, and insulating 2d crystals have been used successfully inside making functional vdw heterostructures with properties by design. here, we expand 2d superconducting crystals as the building block of a vdw hererostructures. the one-step growth of large-scale high-quality vdw heterostructures of graphene and 2d superconducting a-mo2c by with the help of chemical vapor deposition (cvd) method was reported. a superconductivity and its 2d nature of a heterostructures are characterized by our scanning tunneling microscopy (stm) measurements. this adds a 2d superconductivity, a most attractive property of condensed matter physics, to a vdw heterostructures.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
2117,"we investigate a effect of clustering on network observability transitions. inside a observability model introduced by yang, wang, and motter [phys. rev. lett. 109, 258701 (2012)], the given fraction of nodes are chosen randomly, and they and those neighbors are considered to be observable, while a other nodes are unobservable. with the help of this model, we examine connected components of observable nodes and of unobservable nodes inside random clustered networks, which generalize random graphs to include triangles. we use generating functions to derive a normalized sizes of a largest observable component (loc) and largest unobservable component (luc), showing they are both affected by a network's clustering: more highly-clustered networks have lower critical node fractions considering forming macroscopic loc and luc, but this effect was small, becoming almost negligible unless a average degree was small. we also evaluate bounds considering these critical points to confirm clustering's weak or negligible effect on a network observability transition. a accuracy of our analytical treatment was confirmed by monte carlo simulations.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
4264,"we propose the simple modification of a no-scale supergravity wess-zumino model of starobinsky-like inflation to include the polonyi term inside a superpotential. a purpose of this term was to provide an explicit mechanism considering supersymmetry breaking at a end of inflation. we show how successful inflation should be achieved considering the gravitino mass satisfying a strict upper bound $m_{3/2}< 10^3$ tev, with favoured values $m_{3/2}\lesssim\mathcal{o}(1)$ tev. a model suggests that susy may be discovered inside collider physics experiments such as a lhc or a fcc.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7904,"we present density functional theory (dft) calculations of a magnetic anisotropy energy (mae) of fept, which was of great interest considering magnetic recording applications. our data, and a majority of previously calculated results considering perfectly ordered crystals, predict an mae of $\sim 3.0$ mev per formula unit, which was significantly larger than experimentally measured values. analyzing a effects of disorder by introducing stacking faults (sfs) and anti site defects (asds) inside varying concentrations we are able to reconcile calculations with experimental data and show that even the low concentration of asds are able to reduce a mae of fept considerably. investigating a effect of exact exchange and electron correlation within a adiabatic-connection dissipation fluctuation theorem inside a random phase approximation (acdft-rpa) reveals the significantly smaller influence on a mae. thus a effect of disorder, and more specifically asds, was a crucial factor inside explaining a deviation of common dft calculations of fept to experimental measurements.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
2195,"inside this paper, we propose the new feature selection method considering unsupervised domain adaptation based on a emerging optimal transportation theory. we build upon the recent theoretical analysis of optimal transport inside domain adaptation and show that it should directly suggest the feature selection procedure leveraging a shift between a domains. based on this, we propose the novel algorithm that aims to sort features by their similarity across a source and target domains, where a order was obtained by analyzing a coupling matrix representing a solution of a proposed optimal transportation problem. we evaluate our method on the well-known benchmark data set and illustrate its capability of selecting correlated features leading to better classification performances. furthermore, we show that a proposed algorithm should be used as the pre-processing step considering existing domain adaptation techniques ensuring an important speed-up inside terms of a computational time while maintaining comparable results. finally, we validate our algorithm on clinical imaging databases considering computer-aided diagnosis task with promising results.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6994,"eigenvector centrality was the standard network analysis tool considering determining a importance of (or ranking of) entities inside the connected system that was represented by the graph. however, many complex systems and datasets have natural multi-way interactions that are more faithfully modeled by the hypergraph. here we extend a notion of graph eigenvector centrality to uniform hypergraphs. traditional graph eigenvector centralities are given by the positive eigenvector of a adjacency matrix, which was guaranteed to exist by a perron-frobenius theorem under some mild conditions. a natural representation of the hypergraph was the hypermatrix (colloquially, the tensor). with the help of recently established perron-frobenius theory considering tensors, we develop three tensor eigenvectors centralities considering hypergraphs, each with different interpretations. we show that these centralities should reveal different information on real-world data by analyzing hypergraphs constructed from n-gram frequencies, co-tagging on stack exchange, and drug combinations observed inside patient emergency room visits.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
4464,"currently, women are referred considering brca1/2 mutation-testing only if their family-history of breast/ovarian cancer implies that their risk of carrying the mutation exceeds 10\%. however, as mutation-testing costs fall, prominent voices have called considering testing all women, which would strain clinical resources by testing millions of women, almost all of whom will test negative. to better evaluate risk-thresholds considering brca1/2 testing, we introduce two broadly applicable, linked metrics: mean risk stratification (mrs) and the decision-theoretic metric, net benefit of information (nbi). mrs and nbi provide the range of risk thresholds at which the marker/model was ""optimally informative"", inside a sense of maximizing both mrs and nbi. nbi was the function of only mrs and a risk-threshold considering action, connecting decision-theory to risk-stratification and providing the decision-theoretic rationale considering mrs. auc and youden's index reflect on both a fraction of maximum mrs, and of maximum nbi, attained by a marker/model, providing auc and youden's index with long-sought decision-theoretic and risk-stratification rationale. to evaluate risk-thresholds considering brca1/2 testing, we propose an eclectic idea behind the method considering auc, net benefit, and mrs/nbi. mrs/nbi interpret auc inside a context of mutation-prevalence and provide the range of risk thresholds considering which a risk model was optimally informative.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
9111,"we consider a problem of adversarial (non-stochastic) online learning with partial information feedback, where at each round, the decision maker selects an action from the finite set of alternatives. we develop the black-box idea behind the method considering such problems where a learner observes as feedback only losses of the subset of a actions that includes a selected action. when losses of actions are non-negative, under a graph-based feedback model introduced by mannor and shamir, we offer algorithms that attain a so called ""small-loss"" $o(\alpha l^{\star})$ regret bounds with high probability, where $\alpha$ was a independence number of a graph, and $l^{\star}$ was a loss of a best action. prior to our work, there is no data-dependent guarantee considering general feedback graphs even considering pseudo-regret (without dependence on a number of actions, i.e. utilizing a increased information feedback). taking advantage of a black-box nature of our technique, we extend our results to many other applications such as semi-bandits (including routing inside networks), contextual bandits (even with an infinite comparator class), as well as learning with slowly changing (shifting) comparators. inside a special case of classical bandit and semi-bandit problems, we provide optimal small-loss, high-probability guarantees of $\tilde{o}(\sqrt{dl^{\star}})$ considering actual regret, where $d$ was a number of actions, answering open questions of neu. previous bounds considering bandits and semi-bandits were known only considering pseudo-regret and only inside expectation. we also offer an optimal $\tilde{o}(\sqrt{\kappa l^{\star}})$ regret guarantee considering fixed feedback graphs with clique-partition number at most $\kappa$.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11838,"a differential event rate inside weakly interacting massive particle (wimp) direct detection experiments depends on a local dark matter density and velocity distribution. accurate modelling of a local dark matter distribution was therefore required to obtain reliable constraints on a wimp particle physics properties. data analyses typically use the simple standard halo model which might not be the good approximation to a real milky way (mw) halo. we review observational determinations of a local dark matter density, circular speed and escape speed and also studies of a local dark matter distribution inside simulated mw-like galaxies. we discuss a effects of a uncertainties inside these quantities on a energy spectrum and its time and direction dependence. finally we conclude with an overview of various methods considering handling these astrophysical uncertainties.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7478,"we present the differentiable framework capable of learning the wide variety of compositions of simple policies that we call skills. by recursively composing skills with themselves, we should create hierarchies that display complex behavior. skill networks are trained to generate skill-state embeddings that are provided as inputs to the trainable composition function, which inside turn outputs the policy considering a overall task. our experiments on an environment consisting of multiple collect and evade tasks show that this architecture was able to quickly build complex skills from simpler ones. furthermore, a learned composition function displays some transfer to unseen combinations of skills, allowing considering zero-shot generalizations.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
523,"an extension of a two-parameter log-lindley distribution of gomez et al. (2014) with support inside (0, 1) was proposed. its important properties like cumulative distribution function, moments, survival function, hazard rate function, shannon entropy, stochastic n ordering and convexity (concavity) conditions are derived. an application inside distorted premium principal was outlined and parameter approximation by method of maximum likelihood was also presented. we also consider use of the re-parameterized form of a proposed distribution inside regression modeling considering bounded responses by considering the modeling of real life data inside comparison with beta regression and log-lindley regression models.",0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
6594,"we study nearly-kahler 6-manifolds equipped with the cohomogeneity-two lie group action considering which a principal orbits are coisotropic. if a metric was complete, then we show that this last condition was automatically satisfied, and both a acting lie group and a principal orbits are finite quotients of $s^3 \times s^1$. we partition a class of such nearly-ka}hler structures into three types (called i, ii, iii) and prove the local existence and generality result considering each type. metrics of types i and ii are shown to be incomplete. we also derive the quasilinear elliptic pde system on a 2-dimensional orbit space which nearly-kahler structures of type i must satisfy. finally, we remark on the relatively simple one-parameter family of type iii structures that turn out to be incomplete metrics that are cohomogeneity-one under a action of the larger group.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3303,"we use a language of uninformative bayesian prior choice to study a selection of appropriately simple effective models. we advocate considering a prior which maximizes a mutual information between parameters and predictions, learning as much as possible from limited data. when many parameters are poorly constrained by a available data, we find that this prior puts weight only on boundaries of a parameter manifold. thus it selects the lower-dimensional effective theory inside the principled way, ignoring irrelevant parameter directions. inside a limit where there was sufficient data to tightly constrain any number of parameters, this reduces to jeffreys prior. but we argue that this limit was pathological when applied to a hyper-ribbon parameter manifolds generic inside science, because it leads to dramatic dependence on effects invisible to experiment.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0
3059,"inside a early 1980's almgren developed the theory of dirichlet energy minimizing multi-valued functions, proving that a hausdorff dimension of a singular set (including branch points) of such the function was at most $(n-2),$ where $n$ was a dimension of its domain. almgren used this result inside an essential way to show that a same upper bound holds considering a dimension of a singular set of an area minimizing $n$-dimensional rectifiable current of arbitrary codimension. inside either case, a dimension bound was sharp. we develop estimates to study a asymptotic behaviour of the multi-valued dirichlet energy minimizer on idea behind the method to its singular set. our estimates imply that the dirichlet energy minimizer at ${\mathcal h}^{n-2}$ a.e. point of its singular set has the unique set of homogeneous multi-valued cylindrical tangent functions (blow-ups) to which a minimizer, modulo the set of single-valued harmonic functions, decays exponentially fast upon rescaling. the corollary was that a singular set was countably $(n-2)$-rectifiable. our work was inspired by a work of l. simon on a analysis of singularities of minimal submanifolds inside multiplicity 1 classes, and uses some new estimates and strategies together with techniques from wickramasekera's prior work to overcome additional difficulties arising from higher multiplicity and low regularity of a minimizers inside a presence of branch points. a results described here were announced inside earlier work of a authors where a special case of two-valued dirichlet minimizing functions is treated.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10439,"a cybele and hilda dynamical groups delimit a outer edge of a asteroid belt. their compositional distribution was the key element to constrain evolutionary models of a solar system. inside this paper, we present the compositional analysis of these populations with the help of spectroscopic observations, sdss and neowise data. as part of a primass (primitive asteroids spectroscopic survey), we acquired visible spectra of 18 objects inside hilda or cybele groups with a goodman high throughput spectrometer at a 4.1m soar telescope and 20 near-ir spectra of hilda objects with near infrared camera spectrograph at a 3.56m tng. a sample was enlarged with spectra taken from a literature inside order to increase our statistical analysis. a spectra were inspected considering aqueous alteration bands and other spectral features that should be linked to compositional constraints. a analysis shows the continuous distribution of compositions from a main-belt to a cybele, hilda and trojan regions. we also identify the population inside a trojans group not present inside hilda or cybele objects.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
62,"we generalized several results considering a arithmetic dynamics of monomial maps, including silverman's conjectures on height growth, dynamical mordell-lang conjecture, and dynamical manin-mumford conjecture. these results are originally known considering monomial maps on algebraic tori. we extend a results to arbitrary toric varieties.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
10540,"transiting exoplanets inside multi-planet systems have non-keplerian orbits which should cause a times and durations of transits to vary. a theory and observations of transit timing variations (ttv) and transit duration variations (tdv) are reviewed. since a last review, a kepler spacecraft has detected several hundred perturbed planets. inside the few cases, these data have been used to discover additional planets, similar to a historical discovery of neptune inside our own solar system. however, a more impactful aspect of ttv and tdv studies has been characterization of planetary systems inside which multiple planets transit. after addressing a equations of motion and parameter scalings, a main dynamical mechanisms considering ttv and tdv are described, with citations to a observational literature considering real examples. we describe parameter constraints, particularly a origin of a mass/eccentricity degeneracy and how it was overcome by a high-frequency component of a signal. on a observational side, derivation of timing precision and introduction to a timing diagram are given. science results are reviewed, with an emphasis on mass measurements of transiting sub-neptunes and super-earths, from which bulk compositions may be inferred.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
9553,"most stars inside a galaxy are believed to be formed within star clusters from collapsing molecular clouds. however, a complete process of star formation, from a parent cloud to the gas-free star cluster, was still poorly understood. we perform radiation-hydrodynamical simulations of a collapse of the turbulent molecular cloud with the help of a ramses-rt code. stars are modelled with the help of sink particles, from which we self-consistently follow a propagation of a ionising radiation. we study how different feedback models affect a gas expulsion from a cloud and how they shape a final properties of a emerging star cluster. we find that a star formation efficiency was lower considering stronger feedback models. feedback also changes a high mass end of a stellar mass function. stronger feedback also allows a establishment of the lower density star cluster, which should maintain the virial or sub-virial state. inside a absence of feedback, a star formation efficiency was very high, as well as a final stellar density. as the result, high energy close encounters make a cluster evaporate quickly. other indicators, such as mass segregation, statistics of multiple systems and escaping stars confirm this picture. observations of young star clusters are inside best agreement with our strong feedback simulation.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4944,"many reinforcement-learning researchers treat a reward function as the part of a environment, meaning that a agent should only know a reward of the state if it encounters that state inside the trial run. however, we argue that this was an unnecessary limitation and instead, a reward function should be provided to a learning algorithm. a advantage was that a algorithm should then use a reward function to check a reward considering states that a agent hasn't even encountered yet. inside addition, a algorithm should simultaneously learn policies considering multiple reward functions. considering each state, a algorithm would calculate a reward with the help of each of a reward functions and add a rewards to its experience replay dataset. a hindsight experience replay algorithm developed by andrychowicz et al. (2017) does just this, and learns to generalize across the distribution of sparse, goal-based rewards. we extend this algorithm to linearly-weighted, multi-objective rewards and learn the single policy that should generalize across all linear combinations of a multi-objective reward. whereas other multi-objective algorithms teach a q-function to generalize across a reward weights, our algorithm enables a policy to generalize, and should thus be used with continuous actions.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
9114,introduction to deep neural networks and their history.,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4764,"generative adversarial networks (gans) have received the tremendous amount of attention inside a past few years, and have inspired applications addressing the wide range of problems. despite its great potential, gans are difficult to train. recently, the series of papers (arjovsky & bottou, 2017a; arjovsky et al. 2017b; and gulrajani et al. 2017) proposed with the help of wasserstein distance as a training objective and promised easy, stable gan training across architectures with minimal hyperparameter tuning. inside this paper, we compare a performance of wasserstein distance with other training objectives on the variety of gan architectures inside a context of single image super-resolution. our results agree that wasserstein gan with gradient penalty (wgan-gp) provides stable and converging gan training and that wasserstein distance was an effective metric to gauge training progress.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11423,"heat production rates considering a geologically important nuclides ${}^{26}$al, ${}^{40}$k, ${}^{60}$fe, ${}^{232}$th, ${}^{235}$u, and ${}^{238}$u are calculated on a basis of recent data on atomic and nuclear properties. a revised data differ by several per cent from some older values, but indicate that more recent analyses converge toward values with an accuracy sufficient considering all common geoscience applications, although some possibilities considering improvement still remain, especially inside a case of ${}^{40}$k and with regard to a determination of half-lives. the python script was provided considering calculating heat production (this https url).",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13820,"we consider a cost of general orthogonal range queries inside random quadtrees. a cost of the given query was encoded into the (random) function of four variables which characterize a coordinates of two opposite corners of a query rectangle. we prove that, when suitably shifted and rescaled, a random cost function converges uniformly inside probability towards the random field that was characterized as a unique solution to the distributional fixed-point equation. we also state similar results considering $2$-d trees. our results imply considering instance that a worst case query satisfies a same asymptotic estimates as the typical query, and thereby resolve an old question of chanzy, devroye and zamora-cura [\emph{acta inf.}, 37:355--383, 2000]",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
228,"deep convolutional neural networks are generally regarded as robust function approximators. so far, this intuition was based on perturbations to external stimuli such as a images to be classified. here we explore a robustness of convolutional neural networks to perturbations to a internal weights and architecture of a network itself. we show that convolutional networks are surprisingly robust to the number of internal perturbations inside a higher convolutional layers but a bottom convolutional layers are much more fragile. considering instance, alexnet shows less than the 30% decrease inside classification performance when randomly removing over 70% of weight connections inside a top convolutional or dense layers but performance was almost at chance with a same perturbation inside a first convolutional layer. finally, we suggest further investigations which could continue to inform a robustness of convolutional networks to internal perturbations.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5558,"inside this paper, we propose an improved method considering computing a $\mathcal{h}_\infty$ norm of linear dynamical systems that results inside the code that was often several times faster than existing methods. by with the help of standard optimization tools to rebalance a work load of a standard algorithm due to boyd, balakrishnan, bruinsma, and steinbuch, we aim to minimize a number of expensive eigenvalue computations that must be performed. unlike a standard algorithm, our modified idea behind the method should also calculate a $\mathcal{h}_\infty$ norm to full precision with little extra work, and also offers more opportunity to further accelerate its performance using parallelization. finally, we demonstrate that a local optimization we have employed to speed up a standard globally-convergent algorithm should also be an effective strategy on its own considering approximating a $\mathcal{h}_\infty$ norm of large-scale systems.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
13665,"we prove that the $bv$ map with values into a projective space $\mathbb{rp}^{d-1}$ has the $bv$ lifting with values into a unit sphere $\mathbb s^{d-1}$ that satisfies an optimal $bv$-estimate. as an application to liquid crystals, this result was also stated considering $bv$ maps with values into a set of uniaxial $q$-tensors. inside order to quantify $bv$ liftings, we prove an explicit formula considering an intrinsic $bv$-energy of maps with values into any compact smooth manifold.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16260,"context: a first gaia data release (dr1) delivered the catalogue of astrometry and photometry considering over the billion astronomical sources. within a panoply of methods used considering data exploration, visualisation was often a starting point and even a guiding reference considering scientific thought. however, this was the volume of data that cannot be efficiently explored with the help of traditional tools, techniques, and habits. aims: we aim to provide the global visual exploration service considering a gaia archive, something that was not possible out of a box considering most people. a service has two main goals. a first was to provide the software platform considering interactive visual exploration of a archive contents, with the help of common personal computers and mobile devices available to most users. a second aim was to produce intelligible and appealing visual representations of a enormous information content of a archive. methods: a interactive exploration service follows the client-server design. a server runs close to a data, at a archive, and was responsible considering hiding as far as possible a complexity and volume of a gaia data from a client. this was achieved by serving visual detail on demand. levels of detail are pre-computed with the help of data aggregation and subsampling techniques. considering dr1, a client was the web application that provides an interactive multi-panel visualisation workspace as well as the graphical user interface. results: a gaia archive visualisation service offers the web-based multi-panel interactive visualisation desktop inside the browser tab. it currently provides highly configurable 1d histograms and 2d scatter plots of gaia dr1 and a tycho-gaia astrometric solution (tgas) with linked views. an innovative feature was a creation of adql queries from visually defined regions inside plots. [abridged]",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
45,"one-dimensional systems obtained as low-energy limits of hybrid superconductor-topological insulator devices provide means of production, transport, and destruction of majorana bound states (mbss) by variations of a magnetic flux. when two or more pairs of mbss are present inside a intermediate state, there was the possibility of the landau-zener transition, wherein even the slow variation of a flux leads to production of the quasiparticle pair. we study numerically the version of this process, with four mbss produced and subsequently destroyed, and find that, quite universally, a probability of quasiparticle production inside it was 50%. this implies that a effect may be the limiting factor inside applications requiring the high degree of quantum coherence.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
6315,"surface magnetism was believed to be a main driver of coronal heating and stellar wind acceleration. coronae are believed to be formed by plasma confined inside closed magnetic coronal loops of a stars, with winds mainly originating inside open magnetic field line regions. inside this chapter, we review some basic properties of stellar coronae and winds and present some existing models. inside a last part of this chapter, we discuss a effects of coronal winds on exoplanets.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12314,"let $f$ and $g$ be $1$-bounded multiplicative functions considering which $f*g=1_{.=1}$. a bombieri-vinogradov theorem holds considering both $f$ and $g$ if and only if a siegel-walfisz criterion holds considering both $f$ and $g$, and a bombieri-vinogradov theorem holds considering $f$ restricted to a primes.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
5057,"an appearance-based robot self-localization problem was considered inside a machine learning framework. a appearance space was composed of all possible images, which should be captured by the robot's visual system under all robot localizations. with the help of recent manifold learning and deep learning techniques, we propose the new geometrically motivated solution based on training data consisting of the finite set of images captured inside known locations of a robot. a solution includes approximation of a robot localization mapping from a appearance space to a robot localization space, as well as approximation of a inverse mapping considering modeling visual image features. a latter allows solving a robot localization problem as a kalman filtering problem.",1,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
19163,"we propose strategies to approximate and make inference on key features of heterogeneous effects inside randomized experiments. these key features include best linear predictors of a effects with the help of machine learning proxies, average effects sorted by impact groups, and average characteristics of most and least impacted units. a idea behind the method was valid inside high dimensional settings, where a effects are proxied by machine learning methods. we post-process these proxies into a estimates of a key features. our idea behind the method was generic, it should be used inside conjunction with penalized methods, deep and shallow neural networks, canonical and new random forests, boosted trees, and ensemble methods. our idea behind the method was agnostic and does not make unrealistic or hard-to-check assumptions; we don't require conditions considering consistency of a ml methods. approximation and inference relies on repeated data splitting to avoid overfitting and achieve validity. considering inference, we take medians of p-values and medians of confidence intervals, resulting from many different data splits, and then adjust their nominal level to guarantee uniform validity. this variational inference method was shown to be uniformly valid and quantifies a uncertainty coming from both parameter approximation and data splitting. a inference method could be of substantial independent interest inside many machine learning applications. an empirical application to a impact of micro-credit on economic development illustrates a use of a idea behind the method inside randomized experiments. an additional application to a impact of a gender discrimination on wages illustrates a potential use of a idea behind the method inside observational studies, where machine learning methods should be used to condition flexibly on very high-dimensional controls.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0
19461,"multiple instance learning (mil) was the variation of traditional supervised learning problems where data (referred to as bags) are composed of sub-elements (referred to as instances) and only bag labels are available. mil has the variety of applications such as content-based image retrieval, text categorization and medical diagnosis. most of a previous work considering mil assume that a training bags are fully labeled. however, it was often difficult to obtain an enough number of labeled bags inside practical situations, while many unlabeled bags are available. the learning framework called pu learning (positive and unlabeled learning) should address this problem. inside this paper, we propose the convex pu learning method to solve an mil problem. we experimentally show that a proposed method achieves better performance with significantly lower computational costs than an existing method considering pu-mil.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15030,"inside this work, we present the systematic study of a occupied and unoccupied electronic states of lacoo$_{3}$ compound with the help of dft, dft+$\textit{u}$ and dft+embedded dmft methods. a value of $\textit{u}$ used here was evaluated by with the help of constrained dft method and found to be $ \backsim $ 6.9 ev. it was found that dft result has limitations with energy positions of pdos peaks due to its inability of creating the hard gap although a dos distribution appears to be fine with experimental attributes. a calculated value of $\textit{u}$ was not an appropriate value considering carrying out dft+$\textit{u}$ calculations as it has created an insulating gap of $ \backsim $ 1.8 ev with limitations inside redistribution of dos which was inconsistent with experimental spectral behaviour considering a occupied states mainly. however, this value of $\textit{u}$ was found to be an appropriate one considering dft+embedded dmft method which creates the gap of $\backsim $ 1.1 ev. a calculated pdos of co 3$\textit{d}$, la 5$\textit{d}$, la 4$\textit{f}$ and o 2$\textit{p}$ states are giving the remarkably good explanation considering a occupied and unoccupied states of a experimental spectra inside a energy range $\backsim $ -9.0 ev to $\backsim $ 12.0 ev.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
2104,"data and knowledge representation are fundamental concepts inside machine learning. a quality of a representation impacts a performance of a learning model directly. feature learning transforms or enhances raw data to structures that are effectively exploited by those models. inside recent years, several works have been with the help of complex networks considering data representation and analysis. however, no feature learning method has been proposed considering such category of techniques. here, we present an unsupervised feature learning mechanism that works on datasets with binary features. first, a dataset was mapped into the feature--sample network. then, the multi-objective optimization process selects the set of new vertices to produce an enhanced version of a network. a new features depend on the nonlinear function of the combination of preexisting features. effectively, a process projects a input data into the higher-dimensional space. to solve a optimization problem, we design two metaheuristics based on a lexicographic genetic algorithm and a improved strength pareto evolutionary algorithm (spea2). we show that a enhanced network contains more information and should be exploited to improve a performance of machine learning methods. a advantages and disadvantages of each optimization strategy are discussed.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14955,"a past decade has seen the significant interest inside learning tractable probabilistic representations. arithmetic circuits (acs) were among a first proposed tractable representations, with some subsequent representations being instances of acs with weaker or stronger properties. inside this paper, we provide the formal basis under which variants on acs should be compared, and where a precise roles and semantics of their various properties should be made more transparent. this allows us to place some recent developments on acs inside the clearer perspective and to also derive new results considering acs. this includes an exponential separation between acs with and without determinism; completeness and incompleteness results; and tractability results (or lack thereof) when computing most probable explanations (mpes).",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
12659,"we design controllers from formal specifications considering positive discrete-time monotone systems that are subject to bounded disturbances. such systems are widely used to model a dynamics of transportation and biological networks. a specifications are described with the help of signal temporal logic (stl), which should express the broad range of temporal properties. we formulate a problem as the mixed-integer linear program (milp) and show that under a assumptions made inside this paper, which are not restrictive considering traffic applications, a existence of open-loop control policies was sufficient and almost necessary to ensure a satisfaction of stl formulas. we establish the relation between satisfaction of stl formulas inside infinite time and set-invariance theories and provide an efficient method to compute robust control invariant sets inside high dimensions. we also develop the robust model predictive framework to plan controls optimally while ensuring a satisfaction of a specification. illustrative examples and the traffic management case study are included.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
4834,"a advanced operation of future electricity distribution systems was likely to require significant observability of a different parameters of interest (e.g., demand, voltages, currents, etc.). ensuring completeness of data is, therefore, paramount. inside this context, an algorithm considering recovering missing state variable observations inside electricity distribution systems was presented. a proposed method exploits a low rank structure of a state variables using the matrix completion idea behind the method while incorporating prior knowledge inside a form of second order statistics. specifically, a recovery method combines nuclear norm minimization with bayesian estimation. a performance of a new algorithm was compared to a information-theoretic limits and tested trough simulations with the help of real data of an urban low voltage distribution system. a impact of a prior knowledge was analyzed when the mismatched covariance was used and considering the markovian sampling that introduces structure inside a observation pattern. numerical results demonstrate that a proposed algorithm was robust and outperforms existing state of a art algorithms.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
14585,"a present paper extends a thermodynamic dislocation theory developed by langer, bouchbinder, and lookmann to non-uniform plastic deformations. a free energy density as well as a positive definite dissipation function are proposed. a governing equations are derived from a variational equation. as illustration, a problem of plane strain constrained shear of single crystal deforming inside single slip was solved within a proposed theory.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
17368,"inside this paper we focus on a linear algebra theory behind feedforward (fnn) and recurrent (rnn) neural networks. we review backward propagation, including backward propagation through time (bptt). also, we obtain the new exact expression considering hessian, which represents second order effects. we show that considering $t$ time steps a weight gradient should be expressed as the rank-$t$ matrix, while a weight hessian was as the sum of $t^{2}$ kronecker products of rank-$1$ and $w^{t}aw$ matrices, considering some matrix $a$ and weight matrix $w$. also, we show that considering the mini-batch of size $r$, a weight update should be expressed as the rank-$rt$ matrix. finally, we briefly comment on a eigenvalues of a hessian matrix.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17461,"we present component-based simplex architecture (cbsa), the new framework considering assuring a runtime safety of component-based cyber-physical systems (cpss). cbsa integrates assume-guarantee (a-g) reasoning with a core principles of a simplex control architecture to allow component-based cpss to run advanced, uncertified controllers while still providing runtime assurance that a-g contracts and global properties are satisfied. inside cbsa, multiple simplex instances, which should be composed inside the nested, serial or parallel manner, coordinate to assure system-wide properties. combining a-g reasoning and a simplex architecture was the challenging problem that yields significant benefits. by utilizing a-g contracts, we are able to compositionally determine a switching logic considering cbsas, thereby alleviating a state explosion encountered by other approaches. another benefit was that we should use a-g proof rules to decompose a proof of system-wide safety assurance into sub-proofs corresponding to a component-based structure of a system architecture. we also introduce a notion of coordinated switching between simplex instances, the key component of our compositional idea behind the method to reasoning about cbsa switching logic. we illustrate our framework with the component-based control system considering the ground rover. we formally prove that a cbsa considering this system guarantees energy safety (the rover never runs out of power), and collision freedom (the rover never collides with the stationary obstacle). we also consider the cbsa considering a rover that guarantees mission completion: all target destinations visited within the prescribed amount of time.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
4179,"we directly detect dust emission inside an optically-detected, multiply-imaged galaxy lensed by a frontier fields cluster macsj0717.5+3745. we detect two images of a same galaxy at 1.1mm with a aztec camera on a large millimeter telescope leaving no ambiguity inside a counterpart identification. this galaxy, macs071_az9, was at z>4 and a strong lensing model (mu=7.5) allows us to calculate an intrinsic ir luminosity of 9.7e10 lsun and an obscured star formation rate of 14.6 +/- 4.5 msun/yr. a unobscured star formation rate from a uv was only 4.1 +/- 0.3 msun/yr which means a total star formation rate (18.7 +/- 4.5 msun/yr) was dominated (75-80%) by a obscured component. with an intrinsic stellar mass of only 6.9e9msun, macs0717_az9 was one of only the handful of z>4 galaxies at these lower masses that was detected inside dust emission. this galaxy lies close to a estimated star formation sequence at this epoch. however, it does not lie on a dust obscuration relation (irx-beta) considering local starburst galaxies and was instead consistent with a small magellanic cloud (smc) attenuation law. this remarkable lower mass galaxy showing signs of both low metallicity and high dust content may challenge our picture of dust production inside a early universe.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6080,"a standard probabilistic perspective on machine learning gives rise to empirical risk-minimization tasks that are frequently solved by stochastic gradient descent (sgd) and variants thereof. we present the formulation of these tasks as classical inverse or filtering problems and, furthermore, we propose an efficient, gradient-free algorithm considering finding the solution to these problems with the help of ensemble kalman inversion (eki). applications of our idea behind the method include offline and online supervised learning with deep neural networks, as well as graph-based semi-supervised learning. a essence of a eki procedure was an ensemble based approximate gradient descent inside which derivatives are replaced by differences from within a ensemble. we suggest several modifications to a basic method, derived from empirically successful heuristics developed inside a context of sgd. numerical results demonstrate wide applicability and robustness of a proposed algorithm.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5347,"we propose and analyze the variant of a classic polyak-ruppert averaging scheme, broadly used inside stochastic gradient methods. rather than the uniform average of a iterates, we consider the weighted average, with weights decaying inside the geometric fashion. inside a context of linear least squares regression, we show that this averaging scheme has the a same regularizing effect, and indeed was asymptotically equivalent, to ridge regression. inside particular, we derive finite-sample bounds considering a proposed idea behind the method that match a best known results considering regularized stochastic gradient methods.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5490,"this paper presents the novel idea behind the method to estimating a continuous six degree of freedom (6-dof) pose (3d translation and rotation) of an object from the single rgb image. a idea behind the method combines semantic keypoints predicted by the convolutional network (convnet) with the deformable shape model. unlike prior work, we are agnostic to whether a object was textured or textureless, as a convnet learns a optimal representation from a available training image data. furthermore, a idea behind the method should be applied to instance- and class-based pose recovery. empirically, we show that a proposed idea behind the method should accurately recover a 6-dof object pose considering both instance- and class-based scenarios with the cluttered background. considering class-based object pose estimation, state-of-the-art accuracy was shown on a large-scale pascal3d+ dataset.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
4236,"modeling inverse dynamics was crucial considering accurate feedforward robot control. a model computes a necessary joint torques, to perform the desired movement. a highly non-linear inverse function of a dynamical system should be approximated with the help of regression techniques. we propose as regression method the tensor decomposition model that exploits a inherent three-way interaction of positions x velocities x accelerations. most work inside tensor factorization has addressed a decomposition of dense tensors. inside this paper, we build upon a decomposition of sparse tensors, with only small amounts of nonzero entries. a decomposition of sparse tensors has successfully been used inside relational learning, e.g., a modeling of large knowledge graphs. recently, a idea behind the method has been extended to multi-class classification with discrete input variables. representing a data inside high dimensional sparse tensors enables a approximation of complex highly non-linear functions. inside this paper we show how a decomposition of sparse tensors should be applied to regression problems. furthermore, we extend a method to continuous inputs, by learning the mapping from a continuous inputs to a latent representations of a tensor decomposition, with the help of basis functions. we evaluate our proposed model on the dataset with trajectories from the seven degrees of freedom sarcos robot arm. our experimental results show superior performance of a proposed functional tensor model, compared to challenging state-of-the art methods.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
17439,"we give, considering a first time, self-consistent large-$n$ analytical solutions of inhomogeneous condensates inside a quantum ${\mathbb c}p^{n-1}$ model inside a large-$n$ limit. we find the map from the set of gap equations of a ${\mathbb c}p^{n-1}$ model to those of a gross-neveu (gn) model (or a gap equation and a bogoliubov-de gennes equation), which enables us to find a self-consistent solutions. we find that a higgs field of a ${\mathbb c}p^{n-1}$ model was given as the zero mode of solutions of a gn model, and consequently only topologically nontrivial solutions of a gn model yield nontrivial solutions of a ${\mathbb c}p^{n-1}$ model. the stable single soliton was constructed from an anti-kink of a gn model and has the broken (higgs) phase in its core,in which ${\mathbb c}p^{n-1}$ modes are localized,with the symmetric (confining) phase outside. we further find the stable periodic soliton lattice constructed from the real kink crystal inside a gn model,while a ablowitz-kaup-newell-segur hierarchy yields multiple solitons at arbitrary separations.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
9090,"we study a evolution of long-lived controversial debates as manifested on twitter from 2011 to 2016. specifically, we explore how a structure of interactions and content of discussion varies with a level of collective attention, as evidenced by a number of users discussing the topic. spikes inside a volume of users typically correspond to external events that increase a public attention on a topic -- as, considering instance, discussions about `gun control' often erupt after the mass shooting. this work was a first to study a dynamic evolution of polarized online debates at such scale. by employing the wide array of network and content analysis measures, we find consistent evidence that increased collective attention was associated with increased network polarization and network concentration within each side of a debate; and overall more uniform lexicon usage across all users.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
14822,"inside this paper, we present the new method considering egocentric video temporal segmentation based on integrating the statistical mean change detector and agglomerative clustering(ac) within an energy-minimization framework. given a tendency of most ac methods to oversegment video sequences when clustering their frames, we combine a clustering with the concept drift detection technique (adwin) that has rigorous guarantee of performances. adwin serves as the statistical upper bound considering a clustering-based video segmentation. we integrate both techniques inside an energy-minimization framework that serves to disambiguate a decision of both techniques and to complete a segmentation taking into account a temporal continuity of video frames descriptors. we present experiments over egocentric sets of more than 13.000 images acquired with different wearable cameras, showing that our method outperforms state-of-the-art clustering methods.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1392,"we describe an essentially perfect hashing algorithm considering calculating a position of an element inside an ordered list, appropriate considering a construction and manipulation of many-body hamiltonian, sparse matrices. each element of a list corresponds to an integer value whose binary representation reflects a occupation of single-particle basis states considering each element inside a many-body hilbert space. a algorithm replaces conventional methods, such as binary search, considering locating a elements of a ordered list, eliminating a need to store a integer representation considering each element, without increasing a computational complexity. combined with a ""checkerboard"" decomposition of a hamiltonian matrix considering distribution over parallel computing environments, this leads to the substantial savings inside aggregate memory. while a algorithm should be applied broadly to many-body, correlated problems, we demonstrate its utility inside reducing total memory consumption considering the series of fermionic single-band hubbard model calculations on small clusters with progressively larger hilbert space dimension.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
1921,"we derive an exact expression considering a correlation function inside redshift shells including all a relativistic contributions. this expression, which does not rely on a distant-observer or flat-sky approximation, was valid at all scales and includes both local relativistic corrections and integrated contributions, like gravitational lensing. we present two methods to calculate this correlation function, one which makes use of a angular power spectrum c_ell(z1,z2) and the second method which evades a costly calculations of a angular power spectra. a correlation function was then used to define a power spectrum as its fourier transform. inside this work theoretical aspects of this procedure are presented, together with quantitative examples. inside particular, we show that gravitational lensing modifies a multipoles of a correlation function and of a power spectrum by the few percent at redshift z=1 and by up to 30% and more at z=2. we also point out that large-scale relativistic effects and wide-angle corrections generate contributions of a same order of magnitude and have consequently to be treated inside conjunction. these corrections are particularly important at small redshift, z=0.1, where they should reach 10%. this means inside particular that the flat-sky treatment of relativistic effects, with the help of considering example a power spectrum, was not consistent.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16039,"this paper presents the new method --- adversarial advantage actor-critic (adversarial a2c), which significantly improves a efficiency of dialogue policy learning inside task-completion dialogue systems. inspired by generative adversarial networks (gan), we train the discriminator to differentiate responses/actions generated by dialogue agents from responses/actions by experts. then, we incorporate a discriminator as another critic into a advantage actor-critic (a2c) framework, to encourage a dialogue agent to explore state-action within a regions where a agent takes actions similar to those of a experts. experimental results inside the movie-ticket booking domain show that a proposed adversarial a2c should accelerate policy exploration efficiently.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4537,"traffic flow prediction was an important research issue considering solving a traffic congestion problem inside an intelligent transportation system (its). traffic congestion was one of a most serious problems inside the city, which should be predicted inside advance by analyzing traffic flow patterns. such prediction was possible by analyzing a real-time transportation data from correlative roads and vehicles. this article first gives the brief introduction to a transportation data, and surveys a state-of-the-art prediction methods. then, we verify whether or not a prediction performance was able to be improved by fitting actual data to optimize a parameters of a prediction model which was used to predict a traffic flow. such verification was conducted by comparing a optimized time series prediction model with a normal time series prediction model. this means that inside a era of big data, accurate use of a data becomes a focus of studying a traffic flow prediction to solve a congestion problem. finally, experimental results of the case study are provided to verify a existence of such performance improvement, while a research challenges of this data-analytics-based prediction are presented and discussed.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7684,"inside this paper, we present the novel control law considering longitudinal speed control of autonomous vehicles. a key contributions of a proposed work include a design of the control law that reactively integrates a longitudinal surface gradient of road into its operation. inside contrast to a existing works, we found that integrating a path gradient into a control framework improves a speed tracking efficacy. since a control law was implemented over the shrinking domain scheme, it minimizes a integrated error by recomputing a control inputs at every discretized step and consequently provides less reaction time. this makes our control law suitable considering motion planning frameworks that are operating at high frequencies. furthermore, our work was implemented with the help of the generalized vehicle model and should be easily extended to other classes of vehicles. a performance of gradient aware-shrinking domain based controller was implemented and tested on the stock electric vehicle on which the number of sensors are mounted. results from a tests show a robustness of our control law considering speed tracking on the terrain with varying gradient while also considering stringent time constraints imposed by a planning framework.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
18136,"to analyze marine animals behavior, seasonal distribution and abundance, digital imagery should be acquired by visual or lidar camera. depending on a quantity and properties of acquired imagery, a animals are characterized as either features (shape, color, texture, etc.), or dissimilarity matrices derived from different shape analysis methods (shape context, internal distance shape context, etc.). considering both cases, multi-view learning was critical inside integrating more than one set of feature or dissimilarity matrix considering higher classification accuracy. this paper adopts correntropy loss as cost function inside multi-view learning, which has favorable statistical properties considering rejecting noise. considering a case of features, a correntropy loss-based multi-view learning and its entrywise variation are developed based on a multi-view intact space learning algorithm. considering a case of dissimilarity matrices, a robust euclidean embedding algorithm was extended to its multi-view form with a correntropy loss function. results from simulated data and real-world marine animal imagery show that a proposed algorithms should effectively enhance classification rate, as well as suppress noise under different noise conditions.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6375,"modern technology considering producing extremely bright and coherent x-ray laser pulses provides a possibility to acquire the large number of diffraction patterns from individual biological nanoparticles, including proteins, viruses, and dna. these two-dimensional diffraction patterns should be practically reconstructed and retrieved down to the resolution of the few \angstrom. inside principle, the sufficiently large collection of diffraction patterns will contain a required information considering the full three-dimensional reconstruction of a biomolecule. a computational methodology considering this reconstruction task was still under development and highly resolved reconstructions have not yet been produced. we analyze a expansion-maximization-compression scheme, a current state of a art idea behind the method considering this very challenging application, by isolating different sources of uncertainty. through numerical experiments on synthetic data we evaluate their respective impact. we reach conclusions of relevance considering handling actual experimental data, as well as pointing out certain improvements to a underlying approximation algorithm. we also introduce the practically applicable computational methodology inside a form of bootstrap procedures considering assessing reconstruction uncertainty inside a real data case. we evaluate a sharpness of this idea behind the method and argue that this type of procedure will be critical inside a near future when handling a increasing amount of data.",1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
15147,"image-language matching tasks have recently attracted the lot of attention inside a computer vision field. these tasks include image-sentence matching, i.e., given an image query, retrieving relevant sentences and vice versa, and region-phrase matching or visual grounding, i.e., matching the phrase to relevant regions. this paper investigates two-branch neural networks considering learning a similarity between these two data modalities. we propose two network structures that produce different output representations. a first one, referred to as an embedding network, learns an explicit shared latent embedding space with the maximum-margin ranking loss and novel neighborhood constraints. compared to standard triplet sampling, we perform improved neighborhood sampling that takes neighborhood information into consideration while constructing mini-batches. a second network structure, referred to as the similarity network, fuses a two branches using element-wise product and was trained with regression loss to directly predict the similarity score. extensive experiments show that our networks achieve high accuracies considering phrase localization on a flickr30k entities dataset and considering bi-directional image-sentence retrieval on flickr30k and mscoco datasets.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9724,"recently, mahloujifar and mahmoody (tcc'17) studied attacks against learning algorithms with the help of the special case of valiant's malicious noise, called $p$-tampering, inside which a adversary gets to change any training example with independent probability $p$ but was limited to only choose malicious examples with correct labels. they obtained $p$-tampering attacks that increase a error probability inside a so called targeted poisoning model inside which a adversary's goal was to increase a loss of a trained hypothesis over the particular test example. at a heart of their attack is an efficient algorithm to bias a expected value of any bounded real-output function through $p$-tampering. inside this work, we present new biasing attacks considering increasing a expected value of bounded real-valued functions. our improved biasing attacks, directly imply improved $p$-tampering attacks against learners inside a targeted poisoning model. as the bonus, our attacks come with considerably simpler analysis. we also study a possibility of pac learning under $p$-tampering attacks inside a non-targeted (aka indiscriminate) setting where a adversary's goal was to increase a risk of a generated hypothesis (for the random test example). we show that pac learning was possible under $p$-tampering poisoning attacks essentially whenever it was possible inside a realizable setting without a attacks. we further show that pac learning under ""correct-label"" adversarial noise was not possible inside general, if a adversary could choose a (still limited to only $p$ fraction of) tampered examples that she substitutes with adversarially chosen ones. our formal model considering such ""bounded-budget"" tampering attackers was inspired by a notions of (strong) adaptive corruption inside secure multi-party computation.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5352,"we introduce the phase-field crystal model that creates an array of complex three- and two-dimensional crystal structures using the numerically tractable three-point correlation function. a three-point correlation function was designed inside order to energetically favor a principal interplanar angles of the target crystal structure. this was achieved using an analysis performed by examining a crystal's structure factor. this idea behind the method successfully yields energetically stable simple cubic, diamond cubic, simple hexagonal, graphene layers, and caf$_2$ crystals. to illustrate a ability of a method to yield the particularly complex and technologically important crystal structure, we show how this three-point correlation function method should be used to generate perovskite crystals.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
16295,"autonomous urban driving navigation with complex multi-agent dynamics was under-explored due to a difficulty of learning an optimal driving policy. a traditional modular pipeline heavily relies on hand-designed rules and a pre-processing perception system while a supervised learning-based models are limited by a accessibility of extensive human experience. we present the general and principled controllable imitative reinforcement learning (cirl) idea behind the method which successfully makes a driving agent achieve higher success rates based on only vision inputs inside the high-fidelity car simulator. to alleviate a low exploration efficiency considering large continuous action space that often prohibits a use of classical rl on challenging real tasks, our cirl explores over the reasonably constrained action space guided by encoded experiences that imitate human demonstrations, building upon deep deterministic policy gradient (ddpg). moreover, we propose to specialize adaptive policies and steering-angle reward designs considering different control signals (i.e. follow, straight, turn right, turn left) based on a shared representations to improve a model capability inside tackling with diverse cases. extensive experiments on carla driving benchmark demonstrate that cirl substantially outperforms all previous methods inside terms of a percentage of successfully completed episodes on the variety of goal-directed driving tasks. we also show its superior generalization capability inside unseen environments. to our knowledge, this was a first successful case of a learned driving policy through reinforcement learning inside a high-fidelity simulator, which performs better-than supervised imitation learning.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
2392,"inside this paper, we provide two types of sufficient conditions considering ensuring a quadratic growth conditions of the class of constrained convex symmetric and non-symmetric matrix optimization problems regularized by nonsmooth spectral functions. these sufficient conditions are derived using a study of a $\mathcal{c}^2$-cone reducibility of spectral functions and a metric subregularity of their subdifferentials, respectively. as an application, we demonstrate how quadratic growth conditions are used to guarantee a desirable fast convergence rates of a augmented lagrangian methods (alm) considering solving convex matrix optimization problems. numerical experiments on an easy-to-implement alm applied to a fastest mixing markov chain problem are also presented to illustrate a significance of a obtained results.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
558,"inside this paper, we study a controllability and stabilizability properties of a kolmogorov forward equation of the continuous time markov chain (ctmc) evolving on the finite state space, with the help of a transition rates as a control parameters. firstly, we prove small-time local and global controllability from and to strictly positive equilibrium configurations when a underlying graph was strongly connected. secondly, we show that there always exists the locally exponentially stabilizing decentralized linear (density-)feedback law that takes zero valu at equilibrium and respects a graph structure, provided that a transition rates are allowed to be negative and a desired target density lies inside a interior of a set of probability densities. considering bidirected graphs, that is, graphs where the directed edge inside one direction implies an edge inside a opposite direction, we show that this linear control law should be realized with the help of the decentralized rational feedback law of a form k(x) = a(x) + b(x)f(x)/g(x) that also respects a graph structure and control constraints (positivity and zero at equilibrium). this enables a possibility of with the help of linear matrix inequality (lmi) based tools to algorithmically construct decentralized density feedback controllers considering stabilization of the robotic swarm to the target task distribution with no task-switching at equilibrium, as we demonstrate with several numerical examples.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1
5867,"inside this paper we propose the new semi-supervised gan architecture (ss-infogan) considering image synthesis that leverages information from few labels (as little as 0.22%, max. 10% of a dataset) to learn semantically meaningful and controllable data representations where latent variables correspond to label categories. a architecture builds on information maximizing generative adversarial networks (infogan) and was shown to learn both continuous and categorical codes and achieves higher quality of synthetic samples compared to fully unsupervised settings. furthermore, we show that with the help of small amounts of labeled data speeds-up training convergence. a architecture maintains a ability to disentangle latent variables considering which no labels are available. finally, we contribute an information-theoretic reasoning on how introducing semi-supervision increases mutual information between synthetic and real data.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16395,"a connection between multifrequency quasar observational and physical parameters related to accretion processes was still open to debate. inside a last 20 year, eigenvector 1-based approaches developed since a early papers by boroson and green (1992) and sulentic et al. (2000b) have been proven to be the remarkably powerful tool to investigate this issue, and have led to a definition of the quasar ""main sequence"". inside this paper we perform the cladistic analysis on two samples of 215 and 85 low-z quasars (z 0.7) which were studied inside several previous works and which offer the satisfactory coverage of a eigenvector 1-derived main sequence. a data encompass accurate measurements of observational parameters which represent key aspects associated with a structural diversity of quasars. cladistics was able to group sources radiating at higher eddington ratios, as well as to separate radio-quiet (rq) and radio-loud (rl) quasars. a analysis suggests the black hole mass threshold considering powerful radio emission and also properly distinguishes core-dominated and lobe-dominated quasars, inside accordance with a basic tenet of rl unification schemes. considering that black hole mass provides the sort of ""arrow of time"" of nuclear activity, the phylogenetic interpretation becomes possible if cladistic trees are rooted on black hole mass: a ontogeny of black holes was represented by their monotonic increase inside mass. more massive radio-quiet population b sources at low-z become the more evolved counterpart of population the i.e., wind dominated sources to which a ""local"" narrow-line seyfert 1s belong.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14740,"inside this paper, we introduce the novel algorithm considering calculating arbitrary order cumulants of multidimensional data. since a $d^\text{th}$ order cumulant should be presented inside a form of an $d$-dimensional tensor, a algorithm was presented with the help of tensor operations. a algorithm provided inside a paper takes advantage of super-symmetry of cumulant and moment tensors. we show that a proposed algorithm considerably reduces a computational complexity and a computational memory requirement of cumulant calculation as compared with existing algorithms. considering a sizes of interest, a reduction was of a order of $d!$ compared to a naive algorithm.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5038,"a continouity and compactness of embedding operators inside inside sobolev-lions type spaces are derived. by applying this result separability properties of degenerate anisotropic differential operator equations, well-posedeness and strichartz type estimates considering solution of corresponding parabolic problem are established",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12510,"a present work deals with active sampling of graph nodes representing training data considering binary classification. a graph may be given or constructed with the help of similarity measures among nodal features. leveraging a graph considering classification builds on a premise that labels across neighboring nodes are correlated according to the categorical markov random field (mrf). this model was further relaxed to the gaussian (g)mrf with labels taking continuous values - an approximation that not only mitigates a combinatorial complexity of a categorical model, but also offers optimal unbiased soft predictors of a unlabeled nodes. a proposed sampling strategy was based on querying a node whose label disclosure was expected to inflict a largest change on a gmrf, and inside this sense it was a most informative on average. such the strategy subsumes several measures of expected model change, including uncertainty sampling, variance minimization, and sampling based on a $\sigma-$optimality criterion. the simple yet effective heuristic was also introduced considering increasing a exploration capabilities of a sampler, and reducing bias of a resultant classifier, by taking into account a confidence on a model label predictions. a novel sampling strategies are based on quantities that are readily available without a need considering model retraining, rendering them computationally efficient and scalable to large graphs. numerical tests with the help of synthetic and real data demonstrate that a proposed methods achieve accuracy that was comparable or superior to a state-of-the-art even at reduced runtime.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
10640,"domain knowledge should often be encoded inside a structure of the network, such as convolutional layers considering vision, which has been shown to increase generalization and decrease sample complexity, or a number of samples required considering successful learning. inside this study, we ask whether sample complexity should be reduced considering systems where a structure of a domain was unknown beforehand, and a structure and parameters must both be learned from a data. we show that sample complexity reduction through learning structure was possible considering at least two simple cases. inside studying these cases, we also gain insight into how this might be done considering more complex domains.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16680,"let $x$ be the spherical variety considering the connected reductive group $g$. work of gaitsgory-nadler strongly suggests that a langlands dual group $g^\vee$ of $g$ has the subgroup whose weyl group was a little weyl group of $x$. sakellaridis-venkatesh defined the refined dual group $g^\vee_x$ and verified inside many cases that there exists an isogeny $\phi$ from $g^\vee_x$ to $g^\vee$. inside this paper, we establish a existence of $\phi$ inside full generality. our idea behind the method was purely combinatorial and works (despite a title) considering arbitrary $g$-varieties.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
17276,"optimization with noisy gradients has become ubiquitous inside statistics and machine learning. reparameterization gradients, or gradient estimates computed using a ""reparameterization trick,"" represent the class of noisy gradients often used inside monte carlo variational inference (mcvi). however, when these gradient estimators are too noisy, a optimization procedure should be slow or fail to converge. one way to reduce noise was to use more samples considering a gradient estimate, but this should be computationally expensive. instead, we view a noisy gradient as the random variable, and form an inexpensive approximation of a generating procedure considering a gradient sample. this approximation has high correlation with a noisy gradient by construction, making it the useful control variate considering variance reduction. we demonstrate our idea behind the method on non-conjugate multi-level hierarchical models and the bayesian neural net where we observed gradient variance reductions of multiple orders of magnitude (20-2,000x).",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0
8604,"observational constraints on a abundance of primordial black holes (pbhs) constrain a allowed amplitude of a primordial power spectrum on both a smallest and a largest ranges of scales, covering over 20 decades from $1-10^{20}/ \rm{mpc}$. despite tight constraints on a allowed fraction of pbhs at their time of formation near horizon entry inside a early universe, a corresponding constraints on a primordial power spectrum are quite weak, typically ${\cal p}_\mathcal{r}\lesssim 10^{-2}$ assuming gaussian perturbations. motivated by recent claims that a evaporation of just one pbh would destabilise a higgs vacuum and collapse a universe, we calculate a constraints which follow from assuming there are zero pbhs within a observable universe. this extends a constraints right down to a horizon scale at a end of inflation, but does not significantly tighten a existing power spectrum constraints, even though a constraint on pbh abundance should decrease by up to 46 orders of magnitude. this shows that no future improvement inside observational constraints should ever lead to the significant tightening inside constraints on inflation (via a power spectrum amplitude). a power spectrum constraints are weak because an order unity perturbation was required inside order to overcome pressure forces. we therefore consider an early matter dominated era, during which exponentially more pbhs form considering a same initial conditions. we show this leads to far tighter constraints, which idea behind the method ${\cal p}_\mathcal{r}\lesssim10^{-9}$, albeit over the smaller range of scales and are very sensitive to when a early matter dominated era ends. finally, we show that an extended early matter era was incompatible with a argument that an evaporating pbh would destroy a universe, unless a power spectrum amplitude decreases by up to ten orders of magnitude.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10491,"we consider higher order parabolic operator $\partial_t+(-\delta_x)^m$ and higher order schr√∂dinger operator $i^{-1}\partial_t+(-\delta_x)^m$ inside $x=\{(t,x)\in\mathbb{r}^{1+n};~|t|<a,|x_n|<b\}$ where $m$ was any positive integer. under certain lower order and regularity assumptions, we prove that if a solution considering linear problem vanishes when $x_n>0$, then a solution vanishes inside $x$. such results are given globally, and we also prove some related local results.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
5061,"inside this work, we propose class-enhanced attentive response (clear): an idea behind the method to visualize and understand a decisions made by deep neural networks (dnns) given the specific input. clear facilitates a visualization of attentive regions and levels of interest of dnns during a decision-making process. it also enables a visualization of a most dominant classes associated with these attentive regions of interest. as such, clear should mitigate some of a shortcomings of heatmap-based methods associated with decision ambiguity, and allows considering better insights into a decision-making process of dnns. quantitative and qualitative experiments across three different datasets demonstrate a efficacy of clear considering gaining the better understanding of a inner workings of dnns during a decision-making process.",1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17185,"a superior performance of ensemble methods with infinite models are well known. most of these methods are based on optimization problems inside infinite-dimensional spaces with some regularization, considering instance, boosting methods and convex neural networks use $l^1$-regularization with a non-negative constraint. however, due to a difficulty of handling $l^1$-regularization, these problems require early stopping or the rough approximation to solve it inexactly. inside this paper, we propose the new ensemble learning method that performs inside the space of probability measures, that is, our method should handle a $l^1$-constraint and a non-negative constraint inside the rigorous way. such an optimization was realized by proposing the general purpose stochastic optimization method considering learning probability measures using parameterization with the help of transport maps on base models. as the result of running a method, the transport map to output an infinite ensemble was obtained, which forms the residual-type network. from a perspective of functional gradient methods, we give the convergence rate as fast as that of the stochastic optimization method considering finite dimensional nonconvex problems. moreover, we show an interior optimality property of the local optimality condition used inside our analysis.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7502,"a x-ray regime, where a most massive visible component of galaxy clusters, a intra cluster medium (icm), was visible, offers directly measured quantities, like a luminosity, and derived quantities, like a total mass, to characterize these objects. a aim of this project was to analyze the complete sample of galaxy clusters inside detail and constrain cosmological parameters, like a matter density, omegam, or a amplitude of initial density fluctuations, sigma8. a purely x-ray flux-limited sample (hiflugcs) consists of a 64 x-ray brightest galaxy clusters, which are excellent targets to study a systematic effects, that should bias results. we analyzed inside total 196 chandra observations of a 64 hiflugcs clusters, with the total exposure time of 7.7 ms. here we present our data analysis procedure (including an automated substructure detection and an energy band optimization considering surface brightness profile analysis) which gives individually determined, robust total mass estimates. these masses are tested against dynamical and planck sunyaev-zeldovich (sz) derived masses of a same clusters, where good overall agreement was found with a dynamical masses. a planck sz masses seem to show the mass dependent bias to our hydrostatic masses; possible biases inside this mass-mass comparison are discussed including a planck selection function. furthermore, we show a results considering a 0.1-2.4-kev-luminosity vs. mass scaling-relation. a overall slope of a sample (1.34) was inside agreement with expectations and values from literature. splitting a sample into galaxy groups and clusters reveals, even after the selection bias correction, that galaxy groups exhibit the significantly steeper slope (1.88) compared to clusters (1.06).",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11645,"we establish existence of stein kernels considering probability measures on $\mathbb{r}^d$ satisfying the poincar√© inequality, and obtain bounds on a stein discrepancy of such measures. applications to quantitative central limit theorems are discussed, including the new clt inside wasserstein distance $w_2$ with optimal rate and dependence on a dimension. as the byproduct, we obtain the stability version of an approximate of a poincar√© constant of probability measures under the second moment constraint. a results extend more generally to a setting of converse weighted poincar√© inequalities. a proof was based on simple arguments of calculus of variations. further, we establish two general properties enjoyed by a stein discrepancy, holding whenever the stein kernel exists: stein discrepancy was strictly decreasing along a clt, and it controls a skewness of the random vector.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
13068,"inside view of a fact that biological characteristics have excellent independent distinguishing characteristics,biometric identification technology involves almost all a relevant areas of human distinction. fingerprints, iris, face, voice-print and other biological features have been widely used inside a public security departments to detect detection, mobile equipment unlock, target tracking and other fields. with a use of electronic devices more and more widely and a frequency was getting higher and higher. only a biometrics identification technology with excellent recognition rate should guarantee a long-term development of these fields.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9613,"the statistical approximation model with qualitative input provides the mechanism to fuse human intuition inside the form of qualitative information into the quantitative statistical model. we investigate statistical properties and devise the numerical computation method considering the model subclass with the uniform correlation structure. we show that, within this subclass, qualitative information should be as objective as quantitative information. we also show that a correlation between each pair of variables controls a accuracy of a statistical estimate. an application to portfolio selection was discussed. a correlation, although compromising a accuracy of a statistical estimation, affects a performance of a portfolio inside the minimal way.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18094,"this article develops the strengthened convex quadratic convex (qc) relaxation of a ac optimal power flow (ac-opf) problem and presents an optimization-based bound-tightening (obbt) algorithm to compute tight, feasible bounds on a voltage magnitude variables considering each bus and a phase angle difference variables considering each branch inside a network. theoretical properties of a strengthened qc relaxation that show its dominance over a other variants of a qc relaxation studied inside a literature are also derived. a effectiveness of a strengthened qc relaxation was corroborated using extensive numerical results on benchmark ac-opf test networks. inside particular, a results demonstrate that a proposed relaxation consistently provides a tightest variable bounds and optimality gaps with negligible impacts on runtime performance.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
2802,"we study an ensemble of energy loads controlled using coordinated, implementation-light, randomized on/off switching. we show that mean field control with nonlinear feedback on a cumulative consumption, assumed available to a aggregator using direct physical measurements of a energy flow, allows a ensemble to recover from its use inside a demand response significantly faster than inside a case of a fixed feedback. when a nonlinearity was sufficiently strong, a total instantaneous energy consumption of a ensemble shows super-relaxation---it stabilizes to a steady state much faster than a underlying probability distribution of a devices over their state space, while also leaving almost no devices outside of a comfort zone.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
10273,"we present the scalable end-to-end classifier that uses streaming physiological and medication data to accurately predict a onset of sepsis, the life-threatening complication from infections that has high mortality and morbidity. our proposed framework models a multivariate trajectories of continuous-valued physiological time series with the help of multitask gaussian processes, seamlessly accounting considering a high uncertainty, frequent missingness, and irregular sampling rates typically associated with real clinical data. a gaussian process was directly connected to the black-box classifier that predicts whether the patient will become septic, chosen inside our case to be the recurrent neural network to account considering a extreme variability inside a length of patient encounters. we show how to scale a computations associated with a gaussian process inside the manner so that a entire system should be discriminatively trained end-to-end with the help of backpropagation. inside the large cohort of heterogeneous inpatient encounters at our university health system we find that it outperforms several baselines at predicting sepsis, and yields 19.4% and 55.5% improved areas under a receiver operating characteristic and precision recall curves as compared to a news score currently used by our hospital.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0
101,"this document was designed to be the first-year graduate-level introduction to probabilistic programming. it not only provides the thorough background considering anyone wishing to use the probabilistic programming system, but also introduces a techniques needed to design and build these systems. it was aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages. we start with the discussion of model-based reasoning and explain why conditioning as the foundational computation was central to a fields of probabilistic machine learning and artificial intelligence. we then introduce the simple first-order probabilistic programming language (ppl) whose programs define static-computation-graph, finite-variable-cardinality models. inside a context of this restricted ppl we introduce fundamental inference algorithms and describe how they should be implemented inside a context of models denoted by probabilistic programs. inside a second part of this document, we introduce the higher-order probabilistic programming language, with the functionality analogous to that of established programming languages. this affords a opportunity to define models with dynamic computation graphs, at a cost of requiring inference methods that generate samples by repeatedly executing a program. foundational inference algorithms considering this kind of probabilistic programming language are explained inside a context of an interface between program executions and an inference controller. this document closes with the chapter on advanced topics which we believe to be, at a time of writing, interesting directions considering probabilistic programming research; directions that point towards the tight integration with deep neural network research and a development of systems considering next-generation artificial intelligence applications.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4463,"modeling decision-dependent scenario probabilities inside stochastic programs was difficult and typically leads to large and highly non-linear minlps that are very difficult to solve. inside this paper, we develop the new idea behind the method to obtain the compact representation of a recourse function with the help of the set of binary decision diagrams (bdds) that encode the nested cover of a scenario set. a resulting bdds should then be used to efficiently characterize a decision-dependent scenario probabilities by the set of linear inequalities, which essentially factorizes a probability distribution and thus allows to reformulate a entire problem as the small mixed-integer linear program. a idea behind the method was applicable to the large class of stochastic programs with multivariate binary scenario sets, such as stochastic network design, network reliability, or stochastic network interdiction problems. computational results show that a bdd-based scenario representation reduces a problem size, and thus a computation time, significant compared to previous approaches.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
11283,"context: a cosmological concordance model ($\lambda$cdm) matches a cosmological observations exceedingly well. this model has become a standard cosmological model with a evidence considering an accelerated expansion provided by a type ia supernovae (snia) hubble diagram. however, a robustness of this evidence has been addressed recently with somewhat diverging conclusions. aims: a purpose of this paper was to assess a robustness of a conclusion that a universe was indeed accelerating if we rely only on low-redshift (z$\lesssim$2) observations, that was to say with snia, baryonic acoustic oscillations, measurements of a hubble parameter at different redshifts, and measurements of a growth of matter perturbations. methods: we used a standard statistical procedure of minimizing a $\chi^2$ function considering a different probes to quantify a goodness of fit of the model considering both $\lambda$cdm and the simple nonaccelerated low-redshift power law model. inside this analysis, we do not assume that supernovae intrinsic luminosity was independent of a redshift, which has been the fundamental assumption inside most previous studies that cannot be tested. results: we have found that, when snia intrinsic luminosity was not assumed to be redshift independent, the nonaccelerated low-redshift power law model was able to fit a low-redshift background data as well as, or even slightly better, than $\lambda$cdm. when measurements of a growth of structures are added, the nonaccelerated low-redshift power law model still provides an excellent fit to a data considering all a luminosity evolution models considered. conclusions: without a standard assumption that supernovae intrinsic luminosity was independent of a redshift, low-redshift probes are consistent with the nonaccelerated universe.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8304,"text inside natural images contains rich semantics that are often highly relevant to objects or scene. inside this paper, we focus on a problem of fully exploiting scene text considering visual understanding. a main idea was combining word representations and deep visual features into the globally trainable deep convolutional neural network. first, a recognized words are obtained by the scene text reading system. then, we combine a word embedding of a recognized words and a deep visual features into the single representation, which was optimized by the convolutional neural network considering fine-grained image classification. inside our framework, a attention mechanism was adopted to reveal a relevance between each recognized word and a given image, which further enhances a recognition performance. we have performed experiments on two datasets: con-text dataset and drink bottle dataset, that are proposed considering fine-grained classification of business places and drink bottles, respectively. a experimental results consistently demonstrate that a proposed method combining textual and visual cues significantly outperforms classification with only visual representations. moreover, we have shown that a learned representation improves a retrieval performance on a drink bottle images by the large margin, making it potentially useful inside product search.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18265,"this paper introduces the multirobot cooperation idea behind the method to solve a ""pursuit evasion"" problem considering mobile robots that have omnidirectional vision sensors. a main characteristic of this idea behind the method was to implement the real cooperation between robots based on knowledge sharing and makes them work as the team. the complete algorithm considering computing the motion strategy of robots was also presented. this algorithm was based on searching critical points inside a environment. finally, a deliberation protocol which distributes a exploration task among a team and takes a best possible outcome from a robots resources was presented.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
3938,"we use dimensional regularization inside pure quantum gravity on de sitter background to evaluate a one loop expectation value of an invariant operator which gives a local expansion rate. we show that a renormalization of this nonlocal composite operator should be accomplished with the help of a counterterms of the simple local theory of gravity plus matter, at least at one loop order. this renormalization completely absorbs a one loop correction, which accords with a prediction that a lowest secular back-reaction should be the 2-loop effect.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18724,"globular clusters are a oldest conglomerates of stars inside our galaxy and should be useful laboratories to test theories from stellar evolution to cosmology. inside this paper, we present the new method to approximate a absolute age of the globular cluster from observations of its brown dwarfs. a transition region between a end of a main sequence and a brown dwarf regime was characterized by the dearth of objects as function of magnitude. a brightest of a cooling brown dwarfs was easily identified by an increase inside density inside a color-magnitude diagram as you go fainter inside magnitudes, and these brightest brown dwarfs get fainter with age. by identifying a brightest brown dwarfs, it was thus possible to determine a age of the globular cluster within the 1 gyr precision with four-sigma confidence. this new method, which was independent of current methods of age approximation and which does not rely on a knowledge of a cluster's distance from earth, will become feasible thanks to a high spatial resolution and incredible infrared sensitivity of a james webb space telescope.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7883,"water balance models (wbms) are often employed to understand regional hydrologic cycles over various time scales. most wbms, however, are physically-based, and few employ state-of-the-art statistical methods to reconcile independent input measurement uncertainty and bias. further, few wbms exist considering large lakes, and most large lake wbms perform additive accounting, with minimal consideration towards input data uncertainty. here, we introduce the framework considering improving the previously developed large lake statistical water balance model (l2swbm). focusing on a water balances of lakes superior and michigan-huron, we demonstrate our new analytical framework, identifying l2swbms from 26 alternatives that adequately close a water balance of a lakes with satisfactory computation times compared with a prototype model. we expect our new framework will be used to develop water balance models considering other lakes around a world.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5142,"direct numerical simulations of turbulent convection inside the large aspect-ratio box are carried out inside a range of rayleigh number $7 \times 10^4 \le ra \le 2 \times 10^6$ at prandtl number pr=0.71. the strong correlation between a vertical velocity and temperature was observed inside a turbulent regime at almost all a length scales. frequency spectra of all a velocities and temperature show the $-5/3$ law considering the wide band of frequencies. a variances of horizontal velocities at different points inside a flow yield the single power-law. probability density functions of velocities and temperature are close to gaussian only at higher rayleigh numbers. a mean and variance of temperature clearly show boundary layers, surface layers and the near-homogeneous bulk region. a boundary layer thickness decreases and bulk-homogeneity was enhanced on increasing a rayleigh numbers. a wave number spectra of a turbulent kinetic energy exhibit kolmogorov like ($e(k)\sim k^{-5/3}$) and bolginao-obukhov like ($e(k)\sim k^{-11/5}$) behaviour respectively inside a central and near-wall regions of a container. an approximate balance between a production due to buoyancy and a dissipation was found inside a turbulent kinetic energy budget. taylor's approximate equation of a production due to turbulent stretching and a dissipation of turbulent enstrophy was modified by a inclusion of buoyancy production inside a enstrophy budget. a present results support a previously proposed $2/7$ power-law dependence of a average nusselt number on a rayleigh number by yielding an exponent of 0.272, but do not necessarily support a proposed classification of ""soft"" and ""hard"" turbulence on a basis of this exponent.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18670,"we study a asymptotic speed of traveling fronts of a scalar reaction diffusion considering positive reaction terms and with the diffusion coefficient depending nonlinearly on a concentration and on its gradient. we restrict our study to diffusion coefficients of a form $d(u,u_x) = m u^{m-1} u_x^{m(p-2)}$ considering which existence and convergence to traveling fronts has been established. we formulate the variational principle considering a asymptotic speed of a fronts. upper and lower bounds considering a speed valid considering any $m\ge0, p\ge 1$ are constructed. when $m=1, p=2$ a problem reduces to a constant diffusion problem and a bounds correspond to a classic zeldovich frank-kamenetskii lower bound and a aronson-weinberger upper bound respectively. inside a special case $m(p-1) = 1$ the local lower bound should be constructed which coincides with a aforementioned upper bound. a speed inside this case was completely determined inside agreement with recent results.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17249,"we present an exactly solvable lattice hamiltonian to realize gapped boundaries of kitaev's quantum double models considering dijkgraaf-witten theories. we classify a elementary excitations on a boundary, and systematically describe a bulk-to-boundary condensation procedure. we also present a parallel algebraic/categorical structure of gapped boundaries.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
3692,"this position paper formalises an abstract model considering complex negotiation dialogue. this model was to be used considering a benchmark of optimisation algorithms ranging from reinforcement learning to stochastic games, through transfer learning, one-shot learning or others.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12852,"we propose an algorithm considering deep learning on networks and graphs. it relies on a notion that many graph algorithms, such as pagerank, weisfeiler-lehman, or message passing should be expressed as iterative vertex updates. unlike previous methods which rely on a ingenuity of a designer, deep graphs are adaptive to a approximation problem. training and deployment are both efficient, since a cost was $o(|e| + |v|)$, where $e$ and $v$ are a sets of edges and vertices respectively. inside short, we learn a recurrent update functions rather than positing their specific functional form. this yields an algorithm that achieves excellent accuracy on both graph labeling and regression tasks.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
685,"inside this paper, we propose the principled perceptual adversarial networks (pan) considering image-to-image transformation tasks. unlike existing application-specific algorithms, pan provides the generic framework of learning mapping relationship between paired images (fig. 1), such as mapping the rainy image to its de-rained counterpart, object edges to its photo, semantic labels to the scenes image, etc. a proposed pan consists of two feed-forward convolutional neural networks (cnns), a image transformation network t and a discriminative network d. through combining a generative adversarial loss and a proposed perceptual adversarial loss, these two networks should be trained alternately to solve image-to-image transformation tasks. among them, a hidden layers and output of a discriminative network d are upgraded to continually and automatically discover a discrepancy between a transformed image and a corresponding ground-truth. simultaneously, a image transformation network t was trained to minimize a discrepancy explored by a discriminative network d. through a adversarial training process, a image transformation network t will continually narrow a gap between transformed images and ground-truth images. experiments evaluated on several image-to-image transformation tasks (e.g., image de-raining, image inpainting, etc.) show that a proposed pan outperforms many related state-of-the-art methods.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9717,"conditional independence testing was the fundamental problem underlying causal discovery and the particularly challenging task inside a presence of nonlinear and high-dimensional dependencies. here the fully non-parametric test considering continuous data based on conditional mutual information combined with the local permutation scheme was presented. through the nearest neighbor approach, a test efficiently adapts also to non-smooth distributions due to strongly nonlinear dependencies. numerical experiments demonstrate that a test reliably simulates a null distribution even considering small sample sizes and with high-dimensional conditioning sets. a test was better calibrated than kernel-based tests utilizing an analytical approximation of a null distribution, especially considering non-smooth densities, and reaches a same or higher power levels. combining a local permutation scheme with a kernel tests leads to better calibration, but suffers inside power. considering smaller sample sizes and lower dimensions, a test was faster than random fourier feature-based kernel tests if a permutation scheme was (embarrassingly) parallelized, but a runtime increases more sharply with sample size and dimensionality. thus, more theoretical research to analytically approximate a null distribution and speed up a approximation considering larger sample sizes was desirable.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
1366,"inside this paper, the control idea behind the method called artificial neural tissue (ant) was applied to multirobot excavation considering lunar base preparation tasks including clearing landing pads and burying of habitat modules. we show considering a first time, the team of autonomous robots excavating the terrain to match the given 3d blueprint. constructing mounds around landing pads will provide physical shielding from debris during launch/landing. burying the human habitat modules under 0.5 m of lunar regolith was expected to provide both radiation shielding and maintain temperatures of -25 $^{o}$c. this minimizes base life-support complexity and reduces launch mass. ant was compelling considering the lunar mission because it doesn't require the team of astronauts considering excavation and it requires minimal supervision. a robot teams are shown to autonomously interpret blueprints, excavate and prepare sites considering the lunar base. because little pre-programmed knowledge was provided, a controllers discover creative techniques. ant evolves techniques such as slot-dozing that would otherwise require excavation experts. this was critical inside making an excavation mission feasible when it was prohibitively expensive to send astronauts. a controllers evolve elaborate negotiation behaviors to work inside close quarters. these and other techniques such as concurrent evolution of a controller and team size are shown to tackle problem of antagonism, when too many robots interfere reducing a overall efficiency or worse, resulting inside gridlock. while many challenges remain with this technology our work shows the compelling pathway considering field testing this approach.",1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0
18849,"a competition between antiferromagnetic (af) order and singlet formation was the central phenomenon of a kondo and periodic anderson hamiltonians, and of a heavy fermion materials they describe. inside this paper, we explore a effects of an additional conduction band on magnetism inside these models, and, specifically, on changes inside a af-singlet quantum critical point (qcp) and a one particle and spin spectral functions. to understand a magnetic phase transition qualitatively, we first carry out the self-consistent mean field theory (mft). a basic conclusion was that, at half-filling, a coupling to a additional band stabilizes a af phase to larger $f$ $d$ hybridization $v$ inside a pam. we also explore a possibility of competing ferromagnetic phases when this conduction band was doped away from half-filling. we next employ quantum monte carlo (qmc) which, inside combination with finite size scaling, allows us to evaluate a position of a qcp with the help of an exact treatment of a interactions. this idea behind the method confirms a stabilization of af order, which occurs through an enhancement of a ruderman-kittel-kasuya-yosida (rkky) interaction. qmc results considering a spectral function $a(\textbf{q},\omega)$ and dynamic spin structure factor $\chi(\textbf{q},\omega)$ yield additional insight into a af-singlet competition and a low temperature phases.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
13613,"this paper presents the novel framework inside which image cosegmentation and colocalization are cast into the single optimization problem that integrates information from low level appearance cues with that of high level localization cues inside the very weakly supervised manner. inside contrast to multi-task learning paradigm that learns similar tasks with the help of the shared representation, a proposed framework leverages two representations at different levels and simultaneously discriminates between foreground and background at a bounding box and superpixel level with the help of discriminative clustering. we show empirically that constraining a two problems at different scales enables a transfer of semantic localization cues to improve cosegmentation output whereas local appearance based segmentation cues aid colocalization. a unified framework outperforms strong baseline approaches, of learning a two problems separately, by the large margin on four benchmark datasets. furthermore, it obtains competitive results compared to a state of a art considering cosegmentation on two benchmark datasets and second best result considering colocalization on pascal voc 2007.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16207,"inside this article, we advance divide-and-conquer strategies considering solving a community detection problem inside networks. we propose two algorithms which perform clustering on the number of small subgraphs and finally patches a results into the single clustering. a main advantage of these algorithms was that they bring down significantly a computational cost of traditional algorithms, including spectral clustering, semi-definite programs, modularity based methods, likelihood based methods etc., without losing on accuracy and even improving accuracy at times. these algorithms are also, by nature, parallelizable. thus, exploiting a facts that most traditional algorithms are accurate and a corresponding optimization problems are much simpler inside small problems, our divide-and-conquer methods provide an omnibus recipe considering scaling traditional algorithms up to large networks. we prove consistency of these algorithms under various subgraph selection procedures and perform extensive simulations and real-data analysis to understand a advantages of a divide-and-conquer idea behind the method inside various settings.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0
7022,"we point out that unitary representations of a virasoro algebra contain berry phases obtained by acting on the primary state with conformal transformations that trace the closed path on the virasoro coadjoint orbit. these phases should be computed exactly thanks to a maurer-cartan form on a virasoro group, and they persist after combining left- and right-moving sectors. thinking of virasoro representations as particles inside ads_3 dressed with boundary gravitons, a berry phases associated with brown-henneaux diffeomorphisms provide the gravitational extension of thomas precession.",0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
10489,"currently, deep neural networks are deployed on low-power portable devices by first training the full-precision model with the help of powerful hardware, and then deriving the corresponding low-precision model considering efficient inference on such systems. however, training models directly with coarsely quantized weights was the key step towards learning on embedded platforms that have limited computing resources, memory capacity, and power consumption. numerous recent publications have studied methods considering training quantized networks, but these studies have mostly been empirical. inside this work, we investigate training methods considering quantized neural networks from the theoretical viewpoint. we first explore accuracy guarantees considering training methods under convexity assumptions. we then look at a behavior of these algorithms considering non-convex problems, and show that training algorithms that exploit high-precision representations have an important greedy search phase that purely quantized training methods lack, which explains a difficulty of training with the help of low-precision arithmetic.",1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11620,"chemotherapeutic response of cancer cells to the given compound was one of a most fundamental information one requires to design anti-cancer drugs. recent advances inside producing large drug screens against cancer cell lines provided an opportunity to apply machine learning methods considering this purpose. inside addition to cytotoxicity databases, considerable amount of drug-induced gene expression data has also become publicly available. following this, several methods that exploit omics data were proposed to predict drug activity on cancer cells. however, due to a complexity of cancer drug mechanisms, none of a existing methods are perfect. one possible direction, therefore, was to combine a strengths of both a methods and a databases considering improved performance. we demonstrate that integrating the large number of predictions by a proposed method improves a performance considering this task. a predictors inside a ensemble differ inside several aspects such as a method itself, a number of tasks method considers (multi-task vs. single-task) and a subset of data considered (sub-sampling). we show that all these different aspects contribute to a success of a final ensemble. inside addition, we attempt to use a drug screen data together with two novel signatures produced from a drug-induced gene expression profiles of cancer cell lines. finally, we evaluate a method predictions by inside vitro experiments inside addition to a tests on data sets.the predictions of a methods, a signatures and a software are available from \url{this http url}.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3754,"this short note describes a benefit one obtains from the specific construction of the family of parametrices considering the class of elliptic boundary value problems perturbed by non-linear terms of product type. a construction was based on a boutet de monvel calculus of pseudo-differential boundary operators considering a linear elliptic parts, and on paradifferential operators considering a product terms.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9174,"we justify rigorously an isobe-kakinuma model considering water waves as the higher order shallow water approximation inside a case of the flat bottom. it was known that a full water wave equations are approximated by a shallow water equations with an error of order $o(\delta^2)$, where $\delta$ was the small nondimensional parameter defined as a ratio of a mean depth to a typical wavelength. a green-naghdi equations are known as higher order approximate equations to a water wave equations with an error of order $o(\delta^4)$. inside this paper we show that a isobe-kakinuma model was the much higher order approximation to a water wave equations with an error of order $o(\delta^6)$.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11120,"winds of cool dwarfs are difficult to observe, with only the few m dwarfs presenting observationally-derived mass-loss rates (mdot), which span several orders of magnitude. close-in exoplanets are conveniently positioned inside a inner regions of stellar winds and can, thus, be used to probe a otherwise-unobservable local properties of their host-stars' winds. here, we use local stellar wind characteristics observationally-derived inside a studies of atmospheric evaporation of a warm-neptune gj436 b to derive a global characteristics of a wind of its m-dwarf host. with the help of an isothermal wind model, we constrain a stellar wind temperature to be inside a range [0.36,0.43] mk, with mdot=[0.5,2.5] x 10^{-15} msyn/yr. by computing a pressure balance between a stellar wind and a interstellar medium, we derive a size of a astrophere of gj436 to be around 25 au, significantly more compact than a heliosphere. we demonstrate inside this paper that transmission spectroscopy, coupled to planetary atmospheric evaporation and stellar wind models, should be the useful tool considering constraining a large-scale wind structure of planet-hosting stars. extending our idea behind the method to future planetary systems discoveries will open new perspectives considering a combined characterisation of planetary exospheres and winds of cool dwarf stars.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10538,"this paper develops detailed mathematical statistical theory of the new class of cross-validation techniques of local linear kernel hazards and their multiplicative bias corrections. a new class of cross-validation combines principles of local information and recent advances inside indirect cross-validation. the few applications of cross-validating multiplicative kernel hazard approximation do exist inside a literature. however, detailed mathematical statistical theory and small sample performance are introduced using this paper and further upgraded to our new class of best one-sided cross-validation. best one-sided cross-validation turns out to have excellent performance inside its practical illustrations, inside its small sample performance and inside its mathematical statistical theoretical performance.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
7918,"inside this paper, we present the novel and general network structure towards accelerating a inference process of convolutional neural networks, which was more complicated inside network structure yet with less inference complexity. a core idea was to equip each original convolutional layer with another low-cost collaborative layer (lccl), and a element-wise multiplication of a relu outputs of these two parallel layers produces a layer-wise output. a combined layer was potentially more discriminative than a original convolutional layer, and its inference was faster considering two reasons: 1) a zero cells of a lccl feature maps will remain zero after element-wise multiplication, and thus it was safe to skip a calculation of a corresponding high-cost convolution inside a original convolutional layer, 2) lccl was very fast if it was implemented as the 1*1 convolution or only the single filter shared by all channels. extensive experiments on a cifar-10, cifar-100 and ilscrc-2012 benchmarks show that our proposed network structure should accelerate a inference process by 32\% on average with negligible performance drop.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19000,"visual question answering (or vqa) was the new and exciting problem that combines natural language processing and computer vision techniques. we present the survey of a various datasets and models that have been used to tackle this task. a first part of a survey details a various datasets considering vqa and compares them along some common factors. a second part of this survey details a different approaches considering vqa, classified into four types: non-deep learning models, deep learning models without attention, deep learning models with attention, and other models which do not fit into a first three. finally, we compare a performances of these approaches and provide some directions considering future work.",1,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15061,"we develop the new theory of perfect fluids with translation and rotation symmetry, which was also applicable inside a absence of any type of boost symmetry. it involves introducing the new fluid variable, a kinetic mass density, which was needed to define a most general energy-momentum tensor considering perfect fluids. our theory leads to corrections to a euler equations considering perfect fluids that might be observable inside hydrodynamic fluid experiments. we also derive new expressions considering a speed of sound inside perfect fluids. our theory reduces to a known perfect fluid models when boost symmetry was present. it should also be adapted to (non-relativistic) scale invariant fluids with critical exponent $z$. we show that perfect fluids cannot have schr√∂dinger symmetry unless $z=2$. considering generic values of $z$ there should be fluids with lifshitz symmetry, and as the concrete example, we work out inside detail a thermodynamics and fluid description of an ideal gas of lifshitz particles and compute a speed of sound considering a classical and quantum lifshitz gasses.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0
1889,"site-occupation embedding theory (soet) was an alternative formulation of density-functional theory (dft) considering model hamiltonians where a fully-interacting hubbard problem was mapped, inside principle exactly, onto an impurity-interacting (rather than the non-interacting) one. it provides the rigorous framework considering combining wavefunction (or green function) based methods with dft. inside this work, exact expressions considering a per-site energy and double occupation of a uniform hubbard model are derived inside a context of soet. as readily seen from these derivations, a so-called bath contribution to a per-site correlation energy is, inside addition to a latter, a key density functional quantity to model inside soet. various approximations based on bethe ansatz and perturbative solutions to a hubbard and single impurity anderson models are constructed and tested on the one-dimensional ring. a self-consistent calculation of a embedded impurity wavefunction has been performed with a density matrix renormalization group method. it has been shown that promising results are obtained inside specific regimes of correlation and density. possible further developments have been proposed inside order to provide reliable embedding functionals and potentials.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
14614,"there has been considerable interest inside with the help of decomposition methods inside epidemiology (mediation analysis) and economics (oaxaca-blinder decomposition) to understand how health disparities arise and how they might change upon intervention. it has not been clear when estimates from a oaxaca-blinder decomposition should be interpreted causally because its implementation does not explicitly address potential confounding of target variables. while mediation analysis does explicitly adjust considering confounders of target variables, it does so inside the way that entails equalizing confounders across racial groups, which may not reflect a intended intervention. revisiting prior analyses inside a national longitudinal survey of youth on disparities inside wages, unemployment, incarceration, and overall health with test scores, taken as the proxy considering educational attainment, as the target intervention, we propose and demonstrate the novel decomposition that controls considering confounders of test scores (measures of childhood ses) while leaving their association with race intact. we compare this decomposition with others that use standardization (to equalize childhood ses alone), mediation analysis (to equalize test scores within levels of childhood ses), and one that equalizes both childhood ses and test scores. we also show how these decompositions, including our novel proposals, are equivalent to causal implementations of a oaxaca-blinder decomposition.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
6170,"inside this paper, by limiting twisted conical k√§hler-ricci flows, we prove a long-time existence and uniqueness of cusp k√§hler-ricci flow on compact k√§hler manifold $m$ which carries the smooth hypersurface $d$ such that a twisted canonical bundle $k_m+d$ was ample. furthermore, we prove that this flow converge to the unique cusp k√§hler-einstein metric.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14349,"recommender systems have been widely adopted by electronic commerce and entertainment industries considering individualized prediction and recommendation, which benefit consumers and improve business intelligence. inside this article, we propose an innovative method, namely a recommendation engine of multilayers (rem), considering tensor recommender systems. a proposed method utilizes a structure of the tensor response to integrate information from multiple modes, and creates an additional layer of nested latent factors to accommodate between-subjects dependency. one major advantage was that a proposed method was able to address a ""cold-start"" issue inside a absence of information from new customers, new products or new contexts. specifically, it provides more effective recommendations through sub-group information. to achieve scalable computation, we develop the new algorithm considering a proposed method, which incorporates the maximum block improvement strategy into a cyclic blockwise-coordinate-descent algorithm. inside theory, we investigate both algorithmic properties considering global and local convergence, along with a asymptotic consistency of estimated parameters. finally, a proposed method was applied inside simulations and iri marketing data with 116 million observations of product sales. numerical studies demonstrate that a proposed method outperforms existing competitors inside a literature.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0
10953,"researchers are often interested inside assessing a impact of an intervention on an outcome of interest inside situations where a intervention was non-randomised, information was available at an aggregate level, a intervention was only applied to one or few units, a intervention was binary, and there are outcome measurements at multiple time points. inside this paper, we review existing methods considering causal inference inside a setup just outlined. we detail a assumptions underlying each method, emphasise connections between a different approaches and provide guidelines regarding their practical implementation. several open problems are identified thus highlighting a need considering future research.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16985,"we report results on benchmarking open information extraction (oie) systems with the help of relvis, the toolkit considering benchmarking open information extraction systems. our comprehensive benchmark contains three data sets from a news domain and one data set from wikipedia with overall 4522 labeled sentences and 11243 binary or n-ary oie relations. inside our analysis on these data sets we compared a performance of four popular oie systems, clausie, openie 4.2, stanford openie and predpatt. inside addition, we evaluated a impact of five common error classes on the subset of 749 n-ary tuples. from our deep analysis we unreveal important research directions considering the next generation of oie systems.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11334,"we perform the bayesian analysis of a p-variate skew-t model, providing the new parameterization, the set of non-informative priors and the sampler specifically designed to explore a posterior density of a model parameters. extensions, such as a multivariate regression model with skewed errors and a stochastic frontiers model, are easily accommodated. the novelty introduced inside a paper was given by a extension of a bivariate skew-normal model given inside liseo & parisi (2013) to the more realistic p-variate skew-t model. we also introduce a r package mvst, which allows to approximate a multivariate skew-t model.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
199,"we develop the parameterized primal-dual $\pi$ learning method based on deep neural networks considering markov decision process with large state space and off-policy reinforcement learning. inside contrast to a popular q-learning and actor-critic methods that are based on successive approximations to a nonlinear bellman equation, our method makes primal-dual updates to a policy and value functions utilizing a fundamental linear bellman duality. naive parametrization of a primal-dual $\pi$ learning method with the help of deep neural networks would encounter two major challenges: (1) each update requires computing the probability distribution over a state space and was intractable; (2) a iterates are unstable since a parameterized lagrangian function was no longer linear. we address these challenges by proposing the relaxed lagrangian formulation with the regularization penalty with the help of a advantage function. we show that a dual policy update step inside our method was equivalent to a policy gradient update inside a actor-critic method inside some special case, while a value updates differ substantially. a main advantage of a primal-dual $\pi$ learning method lies inside that a value and policy updates are closely coupled together with the help of a bellman duality and therefore more informative. experiments on the simple cart-pole problem show that a algorithm significantly outperforms a one-step temporal-difference actor-critic method, which was a most relevant benchmark method to compare with. we believe that a primal-dual updates to a value and policy functions would expedite a learning process. a proposed methods might open the door to more efficient algorithms and sharper theoretical analysis.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8265,"we offer the novel way of thinking about a modelling of a time-varying distributions of financial asset returns. borrowing ideas from symbolic data analysis, we consider data representations beyond scalars and vectors. specifically, we consider the quantile function as an observation, and develop the new class of dynamic models considering quantile-function-valued (qf-valued) time series. inside order to make statistical inferences and account considering parameter uncertainty, we propose the method whereby the likelihood function should be constructed considering qf-valued data, and develop an adaptive mcmc sampling algorithm considering simulating from a posterior distribution. compared to modelling realised measures, modelling a entire quantile functions of intra-daily returns allows one to gain more insight into a dynamic structure of price movements. using simulations, we show that a proposed mcmc algorithm was effective inside recovering a posterior distribution, and that a posterior means are reasonable point estimates of a model parameters. considering empirical studies, a new model was applied to analysing one-minute returns of major international stock indices. through quantile scaling, we further demonstrate a usefulness of our method by forecasting one-step-ahead a value-at-risk of daily returns.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
12460,"topologically nontrivial field configurations called ""baby skyrmions"" behave like particles and give origins to a field of skyrmionics that promises racetrack memory and other technological applications. unraveling a non-equilibrium behavior of such topological solitons was the challenge. we realize baby skyrmions inside the chiral liquid crystal and, with the help of numerical modeling and polarized video microscopy, demonstrate electrically driven squirming motion. we reveal a intricate details of non-equilibrium topology-preserving textural changes driving this behavior. direction of a skyrmion's motion was robustly controlled inside the plane orthogonal to a applied field and should be reversed by varying frequency. our findings may spur the new paradigm of soliton dynamics inside soft matter, with the rich interplay between topology, chirality, and orientational viscoelasticity.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
9795,"inside this paper, we prove that considering any fixed $205/243<\gamma\leqslant1$, every sufficiently large $n$ satisfying $n\equiv 5 \pmod {24}$ should be represented as five squares of primes with one prime inside $\mathcal{p}_\gamma$, which improves a previous result of zhang and zhai.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
3043,"pain was the complex and subjective experience that poses the number of measurement challenges. while self-report by a patient was viewed as a gold standard of pain assessment, this idea behind the method fails when patients cannot verbally communicate pain intensity or lack normal mental abilities. here, we present the pain intensity measurement method based on physiological signals. specifically, we implement the multi-task learning idea behind the method based on neural networks that accounts considering individual differences inside pain responses while still leveraging data from across a population. we test our method inside the dataset containing multi-modal physiological responses to nociceptive pain.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8528,"salient object detection (sod), which aims to identify and locate a most salient pixels or regions inside images, has been attracting more and more interest due to its various real-world applications. however, this vision task was quite challenging, especially under complex image scenes. inspired by a intrinsic reflection of natural images, inside this paper we propose the novel feature learning framework considering large-scale salient object detection. specifically, we design the symmetrical fully convolutional network (sfcn) to effectively learn complementary saliency features under a guidance of lossless feature reflection. a location information, together with contextual and semantic information, of salient objects are jointly utilized to supervise a proposed network considering more accurate saliency predictions. inside addition, to overcome a blurry boundary problem, we propose the new weighted structural loss function to ensure clear object boundaries and spatially consistent saliency. a coarse prediction results are effectively refined by these structural information considering performance improvements. extensive experiments on seven saliency detection datasets demonstrate that our idea behind the method achieves consistently superior performance and outperforms a very recent state-of-the-art methods with the large margin.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
567,"we introduce the new high dimensional algorithm considering efficiency corrected, maximally monte carlo event generator independent fiducial measurements at a lhc and beyond. a idea behind the method was driven probabilistically with the help of the deep neural network on an event-by-event basis, trained with the help of detector simulation and even only pure phase space distributed events. this idea behind the method gives also the glimpse into a future of high energy physics, where experiments publish new type of measurements inside the radically multidimensional way.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6974,"it was well known that there was an absolute constant $\mathfrak{c}>0$ such that if a laplace transform $g(s)=\int_{0}^{\infty}\rho(x)e^{-s x}\:\mathrm{d}x$ of the bounded function $\rho$ has analytic continuation through every point of a segment $(-i\lambda ,i\lambda )$ of a imaginary axis, then $$ \limsup_{x\to\infty} \left|\int_{0}^{x}\rho(u)\:\mathrm{d}u - g(0)\right|\leq \frac{ \mathfrak{c}}{\lambda} \: \limsup_{x\to\infty} |\rho(x)|. $$ a best known value of a constant $\mathfrak{c}$ is so far $\mathfrak{c}=2$. inside this article we show that a inequality holds with $\mathfrak{c}=\pi/2$ and that this value was best possible. we also sharpen tauberian constants inside finite forms of other related complex tauberian theorems considering laplace transforms.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
2570,"transmission spectra of exoplanetary atmospheres have been used to infer a presence of clouds/hazes. such inferences are typically based on spectral slopes inside a optical deviant from gaseous rayleigh scattering or low-amplitude spectral features inside a infrared. we investigate three observable metrics that could allow constraints on cloud properties from transmission spectra, namely, a optical slope, a uniformity of this slope, and condensate features inside a infrared. we derive these metrics with the help of model transmission spectra considering mie extinction from the wide range of condensate species, particle sizes, and scale heights. firstly, we investigate possible degeneracies among a cloud properties considering an observed slope. we find, considering example, that spectra with very steep optical slopes suggest sulphide clouds (e.g. mns, zns, na$_2$s) inside a atmospheres. secondly, (non)uniformities inside optical slopes provide additional constraints on cloud properties, e.g., mns, zns, tio$_2$, and fe$_2$o$_3$ have significantly non-uniform slopes. thirdly, infrared spectra provide an additional powerful probe into cloud properties, with sio$_2$, fe$_2$o$_3$, mg$_2$sio$_4$, and mgsio$_3$ bearing strong infrared features observable with a james webb space telescope. we investigate observed spectra of eight hot jupiters and discuss their implications. inside particular, no single or composite condensate species considered here conforms to a steep and non-uniform optical slope observed considering hd 189733b. our work highlights a importance of a three above metrics to investigate cloud properties inside exoplanetary atmospheres with the help of high-precision transmission spectra and detailed cloud models. we make our mie scattering data considering condensates publicly available to a community.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4853,"we present the novel algorithm to train the deep q-learning agent with the help of natural-gradient techniques. we compare a original deep q-network (dqn) algorithm to its natural-gradient counterpart, which we refer to as ngdqn, on the collection of classic control domains. without employing target networks, ngdqn significantly outperforms dqn without target networks, and performs no worse than dqn with target networks, suggesting that ngdqn stabilizes training and should aid reduce a need considering additional hyperparameter tuning. we also find that ngdqn was less sensitive to hyperparameter optimization relative to dqn. together these results suggest that natural-gradient techniques should improve value-function optimization inside deep reinforcement learning.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17823,"highly eccentric binary systems appear inside many astrophysical contexts, ranging from tidal capture inside dense star clusters, precursors of stellar disruption by massive black holes, to high-eccentricity migration of giant planets. inside the highly eccentric binary, a tidal potential of one body should excite oscillatory modes inside a other during the pericenter passage, resulting inside energy exchange between a modes and a binary orbit. these modes exhibit one of three behaviors over multiple passages: low-amplitude oscillations, large amplitude oscillations corresponding to the resonance between a orbital frequency and a mode frequency, and chaotic growth. we study these phenomena with an iterative map, fully exploring how a mode evolution depends on a pericenter distance and other parameters. inside addition, we show that a dissipation of mode energy results inside the quasi-steady state, with gradual orbital decay punctuated by resonances, even inside systems where a mode amplitude would initially grow stochastically. the newly captured star around the black hole should experience significant orbital decay and heating due to a chaotic growth of a mode amplitude and dissipation. the giant planet pushed into the high-eccentricity orbit may experience the similar effect and become the hot or warm jupiter.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14033,"a work of bernstein-zelevinsky and zelevinsky gives the good understanding of irreducible subquotients of the reducible principal series representation of $gl_n(f)$, $f$ the $p$-adic field (without specifying their multiplicities which was done by the kazhdan-lusztig type conjecture). inside this paper we make the proposal of the similar kind considering principal series representations of $gl_n({\mathbb r})$. our investigation on principal series representations naturally led us to consider a steinberg representation considering real groups, which has curiuosly not been paid much attention to inside a subject (unlike a $p$-adic case). our proposal considering steinberg was a simplest possible: considering the real reductive group $g$, a steinberg of $g({\mathbb r})$ was the discrete series representation if and only if $g({\mathbb r})$ has the discrete series, and makes up the full $l$-packet of representations of $g({\mathbb r})$ (of size $w_g/w_k$), so was typically not irreducible.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0
18605,"we focus on implementing and optimizing the sixth-order finite-difference solver considering simulating compressible fluids on the gpu with the help of third-order runge-kutta integration. since graphics processing units perform well inside data-parallel tasks, this makes them an attractive platform considering fluid simulation. however, high-order stencil computation was memory-intensive with respect to both main memory and a caches of a gpu. we present two approaches considering simulating compressible fluids with the help of 55-point and 19-point stencils. we seek to reduce a requirements considering memory bandwidth and cache size inside our methods by with the help of cache blocking and decomposing the latency-bound kernel into several bandwidth-bound kernels. our fastest implementation was bandwidth-bound and integrates $343$ million grid points per second on the tesla k40t gpu, achieving the $3.6 \times$ speedup over the comparable hydrodynamics solver benchmarked on two intel xeon e5-2690v3 processors. our alternative gpu implementation was latency-bound and achieves a rate of $168$ million updates per second.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0
17241,"this paper presents catsim, a first package written inside a python language specialized inside computerized adaptive tests and a logistical models of item response theory. catsim provides functions considering generating item and examinee parameters, simulating tests and plotting results, as well as enabling end users to create new procedures considering proficiency initialization, item selection, proficiency approximation and test stopping criteria. a simulator keeps the record of a items selected considering each examinee as well as their answers and also enables a simulation of linear tests, inside which all examinees answer a same items. a various components made available by catsim should also be used inside a creation of third-party testing applications. examples of such usages are also presented inside this paper.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14045,"inside this paper we prove that given two sets $e_1,e_2 \subset \mathbb{z}$ of positive density, there exists $k \geq 1$ which was bounded by the number depending only on a densities of $e_1$ and $e_2$ such that $k\mathbb{z} \subset (e_1-e_1)\cdot(e_2-e_2)$. as the corollary of a main theorem we deduce that if $\alpha,\beta > 0$ then there exist $n_0$ and $d_0$ which depend only on $\alpha$ and $\beta$ such that considering every $n \geq n_0$ and $e_1,e_2 \subset \mathbb{z}_n$ with $|e_1| \geq \alpha n, |e_2| \geq \beta n$ there exists $d \leq d_0$ the divisor of $n$ satisfying $d \, \mathbb{z}_n \subset (e_1-e_1)\cdot(e_2-e_2)$.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
16361,"we consider four-dimensional gravity coupled to the non-linear sigma model whose scalar manifold was the non-compact geometrically finite surface $\sigma$ endowed with the riemannian metric of constant negative curvature. when a space-time was an flrw universe, such theories produce the very wide generalization of two-field $\alpha$-attractor models, being parameterized by the positive constant $\alpha$, by a choice of the finitely-generated surface group $\gamma\subset \mathrm{psl}(2,\mathbb{r})$ (which was isomorphic with a fundamental group of $\sigma$) and by a choice of the scalar potential defined on $\sigma$. a traditional two-field $\alpha$-attractor models arise when $\gamma$ was a trivial group, inside which case $\sigma$ was a poincar√© disk. we give the general prescription considering a study of such models through uniformization inside a so-called ""non-elementary"" case and discuss some of their qualitative features inside a gradient flow approximation, which we relate to morse theory. we also discuss some aspects of a srst approximation inside these models, showing that it was generally not well-suited considering studying dynamics near cusp ends. when $\sigma$ was non-compact and a scalar potential was ""well-behaved"" at a ends, we show that, inside a {\em naive} local one-field truncation, our generalized models have a same universal behavior as ordinary one-field $\alpha$-attractors if inflation happens near any of a ends of $\sigma$ where a extended potential has the local maximum, considering trajectories which are well approximated by non-canonically parameterized geodesics near a ends, we also discuss spiral trajectories near a ends.",0,1,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7076,"we present and characterize a catalog of galaxy shape measurements that will be used considering cosmological weak lensing measurements inside a wide layer of a first year of a hyper suprime-cam (hsc) survey. a catalog covers an area of 136.9 deg$^2$ split into six fields, with the mean $i$-band seeing of $0.58$ arcsec and $5\sigma$ point-source depth of $i\sim 26$. given conservative galaxy selection criteria considering first year science, a depth and excellent image quality results inside unweighted and weighted source number densities of 24.6 and 21.8 arcmin$^{-2}$, respectively. we define a requirements considering cosmological weak lensing science with this catalog, then focus on characterizing potential systematics inside a catalog with the help of the series of internal null tests considering problems with point-spread function (psf) modeling, shear estimation, and other aspects of a image processing. we find that a psf models narrowly meet requirements considering weak lensing science with this catalog, with fractional psf model size residuals of approximately $0.003$ (requirement: 0.004) and a psf model shape correlation function $\rho_1<3\times 10^{-7}$ (requirement: $4\times 10^{-7}$) at 0.5$^\circ$ scales. the variety of galaxy shape-related null tests are statistically consistent with zero, but star-galaxy shape correlations reveal additive systematics on $>1^\circ$ scales that are sufficiently large as to require mitigation inside cosmic shear measurements. finally, we discuss a dominant systematics and a planned algorithmic changes to reduce them inside future data reductions.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
12187,"materials exhibiting large magnetoresistance may not only be of fundamental research interest, but also should lead to wide-ranging applications inside magnetic sensors and switches. here we demonstrate the large linear-in-field magnetoresistance, $\delta \rho/\rho$ reaching as high as $\sim$600$\%$ at 2 k under the 9 tesla field, inside a tetragonal phase of the transiton-metal stannide $\beta$-rhsn$_4$. detailed analyses show that its magnetic responses are overall inconsistent with a classical model based on a multiple electron scattering by mobility fluctuations inside an inhomogenous conductor, but rather inside line with a quantum effects due to a presence of dirac-like dispersions inside a electronic structure. our results may aid guiding a future quest considering quantum magnetoresistive materials into a family of stannides, similar to a role played by ptsn$_4$ with topological node arcs.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
2095,"network embedding leverages a node proximity manifested to learn the low-dimensional node vector representation considering each node inside a network. a learned embeddings could advance various learning tasks such as node classification, network clustering, and link prediction. most, if not all, of a existing works, are overwhelmingly performed inside a context of plain and static networks. nonetheless, inside reality, network structure often evolves over time with addition/deletion of links and nodes. also, the vast majority of real-world networks are associated with the rich set of node attributes, and their attribute values are also naturally changing, with a emerging of new content patterns and a fading of old content patterns. these changing characteristics motivate us to seek an effective embedding representation to capture network and attribute evolving patterns, which was of fundamental importance considering learning inside the dynamic environment. to our best knowledge, we are a first to tackle this problem with a following two challenges: (1) a inherently correlated network and node attributes could be noisy and incomplete, it necessitates the robust consensus representation to capture their individual properties and correlations; (2) a embedding learning needs to be performed inside an online fashion to adapt to a changes accordingly. inside this paper, we tackle this problem by proposing the novel dynamic attributed network embedding framework - dane. inside particular, dane first provides an offline method considering the consensus embedding and then leverages matrix perturbation theory to maintain a freshness of a end embedding results inside an online manner. we perform extensive experiments on both synthetic and real attributed networks to corroborate a effectiveness and efficiency of a proposed framework.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0
13249,"noncritical soft-faults and model deviations are the challenge considering fault detection and diagnosis (fdd) of resident autonomous underwater vehicles (auvs). such systems may have the faster performance degradation due to a permanent exposure to a marine environment, and constant monitoring of component conditions was required to ensure their reliability. this works presents an evaluation of recurrent neural networks (rnns) considering the data-driven fault detection and diagnosis scheme considering underwater thrusters with empirical data. a nominal behavior of a thruster is modeled with the help of a measured control input, voltage, rotational speed and current signals. we evaluated a performance of fault classification with the help of all a measured signals compared to with the help of a computed residuals from a nominal model as features.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
13222,"inside this paper, we tackle a problem of explanations inside the deep-learning based model considering recommendations by leveraging a technique of layer-wise relevance propagation. we use the deep convolutional neural network to extract relevant features from a input images before identifying similarity between a images inside feature space. relationships between a images are identified by a model and layer-wise relevance propagation was used to infer pixel-level details of a images that may have significantly informed a model's choice. we evaluate our method on an amazon products dataset and demonstrate a efficacy of our approach.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18011,"despite significant recent advances inside a field of head pose approximation and facial expression recognition, raising a cognitive level when analysing human activity presents serious challenges to current concepts. motivated by a need of generating comprehensible visual representations from different sets of data, we introduce the system capable of monitoring human activity through head pose and facial expression changes, utilising an affordable 3d sensing technology (microsoft kinect sensor). an idea behind the method build on discriminative random regression forests is selected inside order to rapidly and accurately approximate head pose changes inside unconstrained environment. inside order to complete a secondary process of recognising four universal dominant facial expressions (happiness, anger, sadness and surprise), emotion recognition using facial expressions (erfe) is adopted. after that, the lightweight data exchange format (javascript object notation-json) was employed, inside order to manipulate a data extracted from a two aforementioned settings. such mechanism should yield the platform considering objective and effortless assessment of human activity within a context of serious gaming and human-computer interaction.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10683,"we show that a skip-gram formulation of word2vec trained with negative sampling was equivalent to the weighted logistic pca. this connection allows us to better understand a objective, compare it to other word embedding methods, and extend it to higher dimensional models.",1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7800,"we consider a minimization of non-convex functions that typically arise inside machine learning. specifically, we focus our attention on the variant of trust region methods known as cubic regularization. this idea behind the method was particularly attractive because it escapes strict saddle points and it provides stronger convergence guarantees than first- and second-order as well as classical trust region methods. however, it suffers from the high computational complexity that makes it impractical considering large-scale learning. here, we propose the novel method that uses sub-sampling to lower this computational cost. by a use of concentration inequalities we provide the sampling scheme that gives sufficiently accurate gradient and hessian approximations to retain a strong global and local convergence guarantees of cubically regularized methods. to a best of our knowledge this was a first work that gives global convergence guarantees considering the sub-sampled variant of cubic regularization on non-convex functions. furthermore, we provide experimental results supporting our theory.",1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0
3474,"we show that if the compact connected $n$-dimensional manifold $m$ has the conformal class containing two non-homothetic metrics $g$ and $\tilde g=e^{2\varphi}g$ with non-generic holonomy, then after passing to the finite covering, either $n=4$ and $(m,g,\tilde g)$ was an ambik√§hler manifold, or $n\ge 6$ was even and $(m,g,\tilde g)$ was obtained by a calabi ansatz from the polarized hodge manifold of dimension $n-2$, or both $g$ and $\tilde g$ have reducible holonomy, $m$ was locally diffeomorphic to the product $m_1\times m_2\times m_3$, a metrics $g$ and $\tilde g$ should be written as $g=g_1+g_2+e^{-2\varphi}g_3$ and $\tilde g=e^{2\varphi}(g_1+g_2)+g_3$ considering some riemannian metrics $g_i$ on $m_i$, and $\varphi$ was a pull-back of the non-constant function on $m_2$.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17203,"we present the new technique called contrastive principal component analysis (cpca) that was designed to discover low-dimensional structure that was unique to the dataset, or enriched inside one dataset relative to other data. a technique was the generalization of standard pca, considering a setting where multiple datasets are available -- e.g. the treatment and the control group, or the mixed versus the homogeneous population -- and a goal was to explore patterns that are specific to one of a datasets. we conduct the wide variety of experiments inside which cpca identifies important dataset-specific patterns that are missed by pca, demonstrating that it was useful considering many applications: subgroup discovery, visualizing trends, feature selection, denoising, and data-dependent standardization. we provide geometrical interpretations of cpca and show that it satisfies desirable theoretical guarantees. we also extend cpca to nonlinear settings inside a form of kernel cpca. we have released our code as the python package and documentation was on github.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3337,"considering any lipschitz domain we construct an arbitrarily small, localized perturbation which splits a spectrum of a laplacian into simple eigenvalues. we use considering this purpose the hadamard's formula and spectral stability results.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18030,"global registration of multi-view robot data was the challenging task. appearance-based global localization approaches often fail under drastic view-point changes, as representations have limited view-point invariance. this work was based on a idea that human-made environments contain rich semantics which should be used to disambiguate global localization. here, we present x-view, the multi-view semantic global localization system. x-view leverages semantic graph descriptor matching considering global localization, enabling localization under drastically different view-points. while a idea behind the method was general inside terms of a semantic input data, we present and evaluate an implementation on visual data. we demonstrate a system inside experiments on a publicly available synthia dataset, on the realistic urban dataset recorded with the simulator, and on real-world streetview data. our findings show that x-view was able to globally localize aerial-to-ground, and ground-to-ground robot data of drastically different view-points. our idea behind the method achieves an accuracy of up to 85 % on global localizations inside a multi-view case, while a benchmarked baseline appearance-based methods reach up to 75 %.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
14310,"this tutorial review presents an overview of a basic theoretical aspects of two-dimensional (2d) crystals. we revise essential aspects of graphene and a new families of semiconducting 2d materials, like transition metal dichalcogenides or black phosphorus. minimal theoretical models considering various materials are presented. some of a exciting new possibilities offered by 2d crystals are discussed, such as manipulation and control of quantum degrees of freedom (spin and pseudospin), confinement of excitons, control of a electronic and optical properties with strain engineering, or unconventional superconducting phases.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
13359,"we present the method considering synthesizing the frontal, neutral-expression image of the person's face given an input face photograph. this was achieved by learning to generate facial landmarks and textures from features extracted from the facial-recognition network. unlike previous approaches, our encoding feature vector was largely invariant to lighting, pose, and facial expression. exploiting this invariance, we train our decoder network with the help of only frontal, neutral-expression photographs. since these photographs are well aligned, we should decompose them into the sparse set of landmark points and aligned texture maps. a decoder then predicts landmarks and textures independently and combines them with the help of the differentiable image warping operation. a resulting images should be used considering the number of applications, such as analyzing facial attributes, exposure and white balance adjustment, or creating the 3-d avatar.",1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8209,"with introduction of new technologies inside a operating room like a da vinci surgical system, training surgeons to use them effectively and efficiently was crucial inside a delivery of better patient care. coaching by an expert surgeon was effective inside teaching relevant technical skills, but current methods to deliver effective coaching are limited and not scalable. we present the virtual reality simulation-based framework considering automated virtual coaching inside surgical education. we implement our framework within a da vinci skills simulator. we provide three coaching modes ranging from the hands-on teacher (continuous guidance) to the handsoff guide (assistance upon request). we present six teaching cues targeted at critical learning elements of the needle passing task, which are shown to a user based on a coaching mode. these cues are graphical overlays which guide a user, inform them about sub-par performance, and show relevant video demonstrations. we evaluated our framework inside the pilot randomized controlled trial with 16 subjects inside each arm. inside the post-study questionnaire, participants reported high comprehension of feedback, and perceived improvement inside performance. after three practice repetitions of a task, a control arm (independent learning) showed better motion efficiency whereas a experimental arm (received real-time coaching) had better performance of learning elements (as per a acs resident skills curriculum). we observed statistically higher improvement inside a experimental group based on one of a metrics (related to needle grasp orientation). inside conclusion, we developed an automated coach that provides real-time cues considering surgical training and demonstrated its feasibility.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
13289,"a coherent optical response from 140~nm and 65~nm thick zno epitaxial layers was studied with the help of transient four-wave-mixing spectroscopy with picosecond temporal resolution. resonant excitation of neutral donor-bound excitons results inside two-pulse and three-pulse photon echoes. considering a donor-bound the exciton (d$^0$x$_\text{a}$) at temperature of 1.8~k we evaluate optical coherence times $t_2=33-50$~ps corresponding to homogeneous linewidths of $13-19~\mu$ev, about two orders of magnitude smaller as compared with a inhomogeneous broadening of a optical transitions. a coherent dynamics was determined mainly by a population decay with time $t_1=30-40$~ps, while pure dephasing was negligible inside a studied high quality samples even considering strong optical excitation. temperature increase leads to the significant shortening of $t_2$ due to interaction with acoustic phonons. inside contrast, a loss of coherence of a donor-bound b exciton (d$^0$x$_\text{b}$) was significantly faster ($t_2=3.6$~ps) and governed by pure dephasing processes.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
15011,"phobos and deimos are a two small martian moons, orbiting almost on a equatorial plane of mars. recent works have shown that they should accrete within an impact-generated inner dense and outer light disk, and that a same impact potentially forms a borealis basin, the large northern hemisphere basin on a current mars. however, there was no the priori reason considering a impact to take place close to a north pole (borealis present location) nor to generate the debris disk inside a equatorial plane of mars (in which phobos and deimos orbit). inside this paper, we investigate these remaining issues on a giant impact origin of a martian moons. first, we show that a mass deficit created by a borealis impact basin induces the global reorientation of a planet to realign its main moment of inertia with a rotation pole (true polar wander). this moves a location of a borealis basin toward its current location. next, with the help of analytical arguments, we investigate a detailed dynamical evolution of a eccentric inclined disk from a equatorial plane of mars that was formed by a martian-moon-forming impact. we find that, as the result of precession of disk particles due to a martian dynamical flattening $j_{2}$ term of its gravity field and particle-particle inelastic collisions, eccentricity and inclination are damped and an inner dense and outer light equatorial circular disk was eventually formed. our results strengthen a giant impact origin of phobos and deimos that should finally be tested by the future sample return mission such as jaxa's martian moons exploration (mmx) mission.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
5835,"next-generation radio interferometers, such as a square kilometre array (ska), will revolutionise our understanding of a universe through their unprecedented sensitivity and resolution. however, standard methods inside radio interferometry produce reconstructed interferometric images that are limited inside quality and they are not scalable considering big data. inside this work we apply and evaluate alternative interferometric reconstruction methods that make use of state-of-the-art sparse image reconstruction algorithms motivated by compressive sensing, which have been implemented inside a purify software package. inside particular, we implement and apply a proximal alternating direction method of multipliers (p-admm) algorithm presented inside the recent article. we apply purify to real interferometric observations. considering all observations purify outperforms a standard clean, where inside some cases purify provides an improvement inside dynamic range by over an order of magnitude. a latest version of purify, which includes a developments presented inside this work, was made publicly available.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
14460,"functional electrical stimulation (fes) systems are successful inside restoring motor function and supporting paralyzed users. commercially available fes products are open loop, meaning that a system was unable to adapt to changing conditions with a user and their muscles which results inside muscle fatigue and poor stimulation protocols. this was because it was difficult to close a loop between stimulation and monitoring of muscle contraction with the help of adaptive stimulation. fes causes electrical artefacts which make it challenging to monitor muscle contractions with traditional methods such as electromyography (emg). we look to overcome this limitation by combining fes with novel mechanomyographic (mmg) sensors to be able to monitor muscle activity during stimulation inside real time. to provide the meaningful task we built an fes cycling rig with the software interface that enabled us to perform adaptive recording and stimulation, and then combine this with sensors to record forces applied to a pedals with the help of force sensitive resistors (fsrs), crank angle position with the help of the magnetic incremental encoder and inputs from a user with the help of switches and the potentiometer. we illustrated this with the closed-loop stimulation algorithm that used a inputs from a sensors to control a output of the programmable rehastim 1 fes stimulator (hasomed) inside real-time. this recumbent bicycle rig is used as the testing platform considering fes cycling. a algorithm is designed to respond to the change inside requested speed (rpm) from a user and change a stimulation power (% of maximum current ma) until this speed is achieved and then maintain it.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
18978,"inside order to pursue a vision of a robocup humanoid league of beating a soccer world champion by 2050, new rules and competitions are added or modified each year fostering novel technological advances. inside 2017, a number of players inside a teensize class soccer games is increase to 3 vs. 3, which allowed considering more team play strategies. improvements inside individual skills were also demanded through the set of technical challenges. this paper presents a latest individual skills and team play developments used inside robocup 2017 that lead our team nimbro winning a 2017 teensize soccer tournament, a technical challenges, and a drop-in games.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
14058,"we present the passivity-based whole-body control idea behind the method considering quadruped robots that achieves dynamic locomotion while compliantly balancing a robot's trunk. we formulate a motion tracking as the quadratic program that takes into account a full robot rigid body dynamics, a actuation limit, a joint limits and a contact interaction. we analyze a controller robustness against inaccurate friction coefficient estimates and unstable footholds, as well as its capability to redistribute a load as the consequence of enforcing actuation limits. additionally, we present some practical implementation details gained from a experience with a real platform. extensive experimental trials on a 90 kg hydraulically actuated quadruped robot validate a capabilities of this controller under various terrain conditions and gaits. a proposed idea behind the method was expedient considering accurate execution of high dynamic motions with respect to a current state of a art.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
12628,"we propose the class of interleavers considering the novel deep neural network (dnn) architecture that uses algorithmically pre-determined, structured sparsity to significantly lower memory and computational requirements, and speed up training. a interleavers guarantee clash-free memory accesses to eliminate idle operational cycles, optimize spread and dispersion to improve network performance, and are designed to ease a complexity of memory address computations inside hardware. we present the design algorithm with mathematical proofs considering these properties. we also explore interleaver variations and analyze a behavior of neural networks as the function of interleaver metrics.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19149,"we report a discovery and analysis of a most metal-poor damped lyman-alpha (dla) system currently known, based on observations made with a keck hires spectrograph. a metal paucity of this system has only permitted a determination of three element abundances: [c/h] = -3.43 +/- 0.06, [o/h] = -3.05 +/- 0.05, and [si/h] = -3.21 +/- 0.05, as well as an upper limit on a abundance of iron: [fe/h] < -2.81. this dla was among a most carbon-poor environment currently known with detectable metals. by comparing a abundance pattern of this dla to detailed models of metal-free nucleosynthesis, we find that a chemistry of a gas was consistent with a yields of the 20.5 m_sun metal-free star that ended its life as the core-collapse supernova; a abundances we measure are inconsistent with a yields of pair-instability supernovae. such the tight constraint on a mass of a progenitor population iii star was afforded by a well-determined c/o ratio, which we show depends almost monotonically on a progenitor mass when a kinetic energy of a supernova explosion was e_exp > 1.5x10^51 erg. we find that a dla presented here has just crossed a critical 'transition discriminant' threshold, rendering a dla gas now suitable considering low mass star formation. we also discuss a chemistry of this system inside a context of recent models that suggest some of a most metal-poor dlas are a precursors of a 'first galaxies', and are a antecedents of a ultra-faint dwarf galaxies.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11011,"a demand on mobile electronics to continue to shrink inside size while increase inside efficiency drives a demand on a internal passive components to do a same. power amplifiers require inductors with small form factors, high quality factors, and high operating frequency inside a single-digit ghz range. this work explores a use of magnetic materials to satisfy a needs of power amplifier inductor applications. this paper discusses a optimization choices regarding material selection, device design, and fabrication methodology. a inductors achieved here present a best performance to date considering an integrated magnetic core inductor at high frequencies with the 1 nh inductance and peak quality factor of 4 at ~3 ghz. such compact inductors show potential considering efficiently meeting a need of mobile electronics inside a future.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
18834,"we introduce the two-player contest considering evaluating a safety and robustness of machine learning systems, with the large prize pool. unlike most prior work inside ml robustness, which studies norm-constrained adversaries, we shift our focus to unconstrained adversaries. defenders submit machine learning models, and try to achieve high accuracy and coverage on non-adversarial data while making no confident mistakes on adversarial inputs. attackers try to subvert defenses by finding arbitrary unambiguous inputs where a model assigns an incorrect label with high confidence. we propose the simple unambiguous dataset (""bird-or- bicycle"") to use as part of this contest. we hope this contest will aid to more comprehensively evaluate a worst-case adversarial risk of machine learning models.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
9355,"we address component-based regularisation of the multivariate generalized linear mixed model. the set of random responses y was modelled by the glmm, with the help of the set x of explanatory variables and the set t of additional covariates. variables inside x are assumed many and redundant: generalized linear mixed regression demands regularisation with respect to x. by contrast, variables inside t are assumed few and selected so as to demand no regularisation. regularisation was performed building an appropriate number of orthogonal components that both contribute to model y and capture relevant structural information inside x. we propose to optimize the scglr-specific criterion within the schall's algorithm inside order to approximate a model. this extension of scglr was tested on simulated and real data, and compared to ridge-and lasso-based regularisations.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16840,"inside this paper we study generative modeling using autoencoders while with the help of a elegant geometric properties of a optimal transport (ot) problem and a wasserstein distances. we introduce sliced-wasserstein autoencoders (swae), which are generative models that enable one to shape a distribution of a latent space into any samplable probability distribution without a need considering training an adversarial network or defining the closed-form considering a distribution. inside short, we regularize a autoencoder loss with a sliced-wasserstein distance between a distribution of a encoded training samples and the predefined samplable distribution. we show that a proposed formulation has an efficient numerical solution that provides similar capabilities to wasserstein autoencoders (wae) and variational autoencoders (vae), while benefiting from an embarrassingly simple implementation.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16689,we present a conditional neural network (clnn) and a masked conditional neural network (mclnn) designed considering temporal signal recognition. a clnn takes into consideration a temporal nature of a sound signal and a mclnn extends upon a clnn through the binary mask to preserve a spatial locality of a features and allows an automated exploration of a features combination analogous to hand-crafting a most relevant features considering a recognition task. mclnn has achieved competitive recognition accuracies on a gtzan and a ismir2004 music datasets that surpass several state-of-the-art neural network based architectures and hand-crafted methods applied on both datasets.,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16726,"inside this paper, we present the reverberation removal idea behind the method considering speaker verification, utilizing dual-label deep neural networks (dnns). a networks perform feature mapping between a spectral features of reverberant and clean speech. long short term memory recurrent neural networks (lstms) are trained to map corrupted mel filterbank (mfb) features to two sets of labels: i) a clean mfb features, and ii) either estimated pitch tracks or a fast fourier transform (fft) spectrogram of clean speech. a performance of reverberation removal was evaluated by equal error rates (eers) of speaker verification experiments.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8862,"pac-bayes bounds have been proposed to get risk estimates based on the training sample. inside this paper a pac-bayes idea behind the method was combined with stability of a hypothesis learned by the hilbert space valued algorithm. a pac-bayes setting was used with the gaussian prior centered at a expected output. thus the novelty of our paper was with the help of priors defined inside terms of a data-generating distribution. our main result estimates a risk of a randomized algorithm inside terms of a hypothesis stability coefficients. we also provide the new bound considering a svm classifier, which was compared to other known bounds experimentally. ours appears to be a first stability-based bound that evaluates to non-trivial values.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6287,"we study a performance of stochastically trained deep neural networks (dnns) whose synaptic weights are implemented with the help of emerging memristive devices that exhibit limited dynamic range, resolution, and variability inside their programming characteristics. we show that the key device parameter to optimize a learning efficiency of dnns was a variability inside its programming characteristics. dnns with such memristive synapses, even with dynamic range as low as $15$ and only $32$ discrete levels, when trained based on stochastic updates suffer less than $3\%$ loss inside accuracy compared to floating point software baseline. we also study a performance of stochastic memristive dnns when used as inference engines with noise corrupted data and find that if a device variability should be minimized, a relative degradation inside performance considering a stochastic dnn was better than that of a software baseline. hence, our study presents the new optimization corner considering memristive devices considering building large noise-immune deep learning systems.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1058,"inside this paper, we consider estimators considering an additive functional of $\phi$, which was defined as $\theta(p;\phi)=\sum_{i=1}^k\phi(p_i)$, from $n$ i.i.d. random samples drawn from the discrete distribution $p=(p_1,...,p_k)$ with alphabet size $k$. we propose the minimax optimal estimator considering a approximation problem of a additive functional. we reveal that a minimax optimal rate was characterized by a divergence speed of a fourth derivative of $\phi$ if a divergence speed was high. as the result, we show there was no consistent estimator if a divergence speed of a fourth derivative of $\phi$ was larger than $p^{-4}$. furthermore, if a divergence speed of a fourth derivative of $\phi$ was $p^{4-\alpha}$ considering $\alpha \in (0,1)$, a minimax optimal rate was obtained within the universal multiplicative constant as $\frac{k^2}{(n\ln n)^{2\alpha}} + \frac{k^{2-2\alpha}}{n}$.",1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0
15017,"considering neural networks (nns) with rectified linear unit (relu) or binary activation functions, we show that their training should be accomplished inside the reduced parameter space. specifically, a weights inside each neuron should be trained on a unit sphere, as opposed to a entire space, and a threshold should be trained inside the bounded interval, as opposed to a real line. we show that a nns inside a reduced parameter space are mathematically equivalent to a standard nns with parameters inside a whole space. a reduced parameter space shall facilitate a optimization procedure considering a network training, as a search space becomes (much) smaller. we demonstrate a improved training performance with the help of numerical examples.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14010,"we explore a properties of byte-level recurrent language models. when given sufficient amounts of capacity, training data, and compute time, a representations learned by these models include disentangled features corresponding to high-level concepts. specifically, we find the single unit which performs sentiment analysis. these representations, learned inside an unsupervised manner, achieve state of a art on a binary subset of a stanford sentiment treebank. they are also very data efficient. when with the help of only the handful of labeled examples, our idea behind the method matches a performance of strong baselines trained on full datasets. we also demonstrate a sentiment unit has the direct influence on a generative process of a model. simply fixing its value to be positive or negative generates samples with a corresponding positive or negative sentiment.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5138,"we study a learning capacity of empirical risk minimization with regard to a squared loss and the convex hypothesis class consisting of linear functions. while these types of estimators were originally designed considering noisy linear regression problems, it recently turned out that they are inside fact capable of handling considerably more complicated situations, involving highly non-linear distortions. this work intends to provide the comprehensive explanation of this somewhat astonishing phenomenon. at a heart of our analysis stands a mismatch principle, which was the simple, yet generic recipe to establish theoretical error bounds considering empirical risk minimization. a scope of our results was fairly general, permitting arbitrary sub-gaussian input-output pairs, possibly with strongly correlated feature variables. noteworthy, a mismatch principle also generalizes to the certain extent a classical orthogonality principle considering ordinary least squares. this adaption allows us to investigate problem setups of recent interest, most importantly, high-dimensional parameter regimes and non-linear observation processes. inside particular, our theoretical framework was applied to various scenarios of practical relevance, such as single-index models, variable selection, and strongly correlated designs. we thereby demonstrate a key purpose of a mismatch principle, that is, learning (semi-)parametric output rules under large model uncertainties and misspecifications.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8300,"just as semantic hashing should accelerate information retrieval, binary valued embeddings should significantly reduce latency inside a retrieval of graphical data. we introduce the simple but effective model considering learning such binary vectors considering nodes inside the graph. by imagining a embeddings as independent coin flips of varying bias, continuous optimization techniques should be applied to a approximate expected loss. embeddings optimized inside this fashion consistently outperform a quantization of both spectral graph embeddings and various learned real-valued embeddings, on both ranking and pre-ranking tasks considering the variety of datasets.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18660,"the vast wealth of literature exists on a topic of rocket trajectory optimisation, particularly inside a area of interplanetary trajectories due to its relevance today. studies on optimising interstellar and intergalactic trajectories are usually performed inside flat spacetime with the help of an analytical approach, with very little focus on optimising interstellar trajectories inside the general relativistic framework. this paper examines a use of low-acceleration rockets to reach galactic destinations inside a least possible time, with the genetic algorithm being employed considering a optimisation process. a fuel required considering each journey is calculated considering various types of propulsion systems to determine a viability of low-acceleration rockets to colonise a milky way. a results showed that to limit a amount of fuel carried on board, an antimatter propulsion system would likely be a minimum technological requirement to reach star systems tens of thousands of light years away. however, with the help of the low-acceleration rocket would require several hundreds of thousands of years to reach these star systems, with minimal time dilation effects since maximum velocities only reached about 0.2c. such transit times are clearly impractical, and thus, any kind of colonisation with the help of low acceleration rockets would be difficult. high accelerations, on a order of 1g, are likely required to complete interstellar journeys within the reasonable time frame, though they may require prohibitively large amounts of fuel. so considering now, it appears that humanity's ultimate goal of the galactic empire may only be possible at significantly higher accelerations, though a propulsion technology requirement considering the journey that uses realistic amounts of fuel remains to be determined.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
8408,"inside a past few years, attention mechanisms have become an indispensable component of end-to-end neural machine translation models. however, previous attention models always refer to some source words when predicting the target word, which contradicts with a fact that some target words have no corresponding source words. motivated by this observation, we propose the novel attention model that has a capability of determining when the decoder should attend to source words and when it should not. experimental results on nist chinese-english translation tasks show that a new model achieves an improvement of 0.8 bleu score over the state-of-the-art baseline.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13333,"ranking node importance was crucial inside understanding network structure and function on complex networks. degree, h-index and coreness are widely used, but which one was more proper to the network associated with the dynamical process, e.g. sir spreading process, was still unclear. to fill this gap, a, which was extracted from a fitting function (f(x)=1-1/(e^(2a(x-b))+1)) of a average number of nodes inside each radius of a neighborhood of the node, was proposed. experiment results which are carried out on twenty real-world networks show that the should classify which of a three measures (degree, h-index and coreness) was more proper to the network inside ranking node importance. we also find that [b/3] was the good indicator considering forecasting a optimal radius of a neighborhood of the node inside ranking node importance considering the given network. to a best of our knowledge, it was a first solution of this interesting and open issue. furthermore, by extending a range of neighborhood where we construct an operator h on of the node, we propose the new method to quantify a importance of the node. a ranking accuracies of most networks should be improved when a radius was increased from 0 to its forecasting optimal radius and a improvement, considering a best case, reaches up to 111%. a performances will reduce on half of a networks studied inside this paper if we roughly extend a radius of a neighborhood. our work deepens a understanding of how to find out a proper node ranking method considering complex networks. a proposed methods bridge a gaps among network structure and node importance, and may have potential applications inside controlling a outbreak of disease, designing of optimal information spreading strategies.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
2960,"this study investigates how adequate coordination among a different cognitive processes of the humanoid robot should be developed through end-to-end learning of direct perception of visuomotor stream. we propose the deep dynamic neural network model built on the dynamic vision network, the motor generation network, and the higher-level network. a proposed model is designed to process and to integrate direct perception of dynamic visuomotor patterns inside the hierarchical model characterized by different spatial and temporal constraints imposed on each level. we conducted synthetic robotic experiments inside which the robot learned to read human's intention through observing a gestures and then to generate a corresponding goal-directed actions. results verify that a proposed model was able to learn a tutored skills and to generalize them to novel situations. a model showed synergic coordination of perception, action and decision making, and it integrated and coordinated the set of cognitive skills including visual perception, intention reading, attention switching, working memory, action preparation and execution inside the seamless manner. analysis reveals that coherent internal representations emerged at each level of a hierarchy. higher-level representation reflecting actional intention developed by means of continuous integration of a lower-level visuo-proprioceptive stream.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
2857,"the (semi)brick over an algebra $a$ was the module $s$ such that a endomorphism ring $\operatorname{\mathsf{end}}_a(s)$ was the (product of) division algebra. considering each dynkin diagram $\delta$, there was the bijection from a coxeter group $w$ of type $\delta$ to a set of semibricks over a preprojective algebra $\pi$ of type $\delta$, which was restricted to the bijection from a set of join-irreducible elements of $w$ to a set of bricks over $\pi$. this paper was devoted to giving an explicit description of these bijections inside a case $\delta=\mathbb{a}_n$ or $\mathbb{d}_n$. first, considering each join-irreducible element $w \in w$, we describe a corresponding brick $s(w)$ inside terms of ""young diagram-like"" notation. next, we determine a canonical join representation $w=\bigvee_{i=1}^m w_i$ of an arbitrary element $w \in w$ based on reading's work, and prove that $\bigoplus_{i=1}^n s(w_i)$ was a semibrick corresponding to $w$.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
18647,"we introduce the new perspective on a classical nirenberg problem of understanding a possible gauss curvatures of metrics on $s^{2}$ conformal to a round metric. the key tool was to employ a smooth cheeger-gromov compactness theorem to obtain general and essentially start the priori estimates considering gauss curvatures $k$ contained inside naturally defined stable regions. we prove that inside such stable regions, a map $u \rightarrow k_{g}$, $g = e^{2u}g_{+1}$ was the proper fredholm map with well-defined degree on each component. this leads to the number of new existence and non-existence results. we also present the new proof and generalization of a moser theorem on gauss curvatures of even conformal metrics on $s^{2}$. inside contrast to previous work, a work here does not use any of a sobolev-type inequalities of trudinger-moser-aubin-onofri.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10844,"a relevance of two-dimensional three-components (2d3c) flows goes well beyond their occurrence inside nature, and the deeper understanding of their dynamics might be also helpful inside order to shed further light on a dynamics of pure two-dimenional (2d) or three-dimensional (3d) flows and vice versa. a purpose of a present paper was to make the step inside this direction through the combination of numerical and analytical work. a analytical part was mainly concerned with a behavior of 2d3c flows inside isolation and a connection between a geometry of a nonlinear interactions and a resulting energy transfer directions. special emphasis was given to a role of helicity. we show that the generic 2d3c flow should be described by two stream functions corresponding to a two helical sectors of a velocity field. a projection onto one helical sector (homochiral flow) leads to the full 3d constraint and to a inviscid conservation of a total (three dimensional) enstrophy and thus to an inverse cascade of a kinetic energy of a third component also. a coupling between several 2d3c flows was studied through the set of suitably designed direct numerical simulations (dns), where we also explore a transition between 2d and fully 3d turbulence. inside particular, we find that a coupling of three 2d3c flows on mutually orthogonal planes subject to small-scale forcing leads to stationary 3d out-of-equilibrium dynamics at a energy containing scales. a transition between 2d and 3d turbulence was then explored through adding the percentage of fully 3d fourier modes inside a volume.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
405,"pouring was the simple task people perform daily. it was a second most frequently executed motion inside cooking scenarios, after pick-and-place. we present the pouring trajectory generation approach, which uses force feedback from a cup to determine a future velocity of pouring. a idea behind the method uses recurrent neural networks as its building blocks. we collected a pouring demonstrations which we used considering training. to test our idea behind the method inside simulation, we also created and trained the force approximation system. a simulated experiments show that a system was able to generalize to single unseen element of a pouring characteristics.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
1129,"inside a last decade, over the million stars were monitored to detect transiting planets. manual interpretation of potential exoplanet candidates was labor intensive and subject to human error, a results of which are difficult to quantify. here we present the new method of detecting exoplanet candidates inside large planetary search projects which, unlike current methods uses the neural network. neural networks, also called ""deep learning"" or ""deep nets"" are designed to give the computer perception into the specific problem by training it to recognize patterns. unlike past transit detection algorithms deep nets learn to recognize planet features instead of relying on hand-coded metrics that humans perceive as a most representative. our convolutional neural network was capable of detecting earth-like exoplanets inside noisy time-series data with the greater accuracy than the least-squares method. deep nets are highly generalizable allowing data to be evaluated from different time series after interpolation without compromising performance. as validated by our deep net analysis of kepler light curves, we detect periodic transits consistent with a true period without any model fitting. our study indicates that machine learning will facilitate a characterization of exoplanets inside future analysis of large astronomy data sets.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
6510,"we present an algorithm considering obtaining an optimal control policy considering hybrid dynamical systems inside cluttered environments. to a best of our knowledge, this was a first attempt to have the locally optimal solution considering this specific problem setting. our idea behind the method extends an optimal control algorithm considering hybrid dynamical systems inside a obstacle-free case to environments with obstacles. our method does not require any preset mode sequence or heuristics to prune a exponential search of mode sequences. by first solving a relaxed problem of getting an obstacle-free, dynamically feasible trajectory and then solving considering both obstacle-avoidance and optimality, we should generate smooth, locally optimal control policies. we demonstrate a performance of our algorithm on the box-pushing example inside the number of environments against a baseline of randomly sampling modes and actions with the kinodynamic rrt.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
16759,"semiconductor nanoparticles of cadmium chalcogenides are known to exhibit pronounced thickness-dependent $e_0$ series of exciton transitions at a $\gamma$ point of a brillouin zone (bz). inside this work, we report an experimental evidence considering high-energy series of exciton transitions, which originates from bz points different from a $\gamma$ point, inside a family of cadmium chalcogenide quasi-2d nanoplatelets (npls). intensive uv absorption bands demonstrating the pronounced size effect are observed considering cdte, cdse, and cds npls inside addition to a $e_0$ exciton bands inside a visible region. these new bands are attributed to transitions analogous to a $e_1$, $e_1+\delta_1$, and $e_2$ series observed inside bulk crystals. first-principles dft calculations of a electronic structure and absorption spectra support this explanation and show that a main contribution to these optical transitions comes from $x$ and $m$ points of a 2d bz, which originate from $l$ and $x$ points of a 3d bz. at a same time, a $e_0$ series of transitions at a $\gamma$ point was well described by a multiband effective-mass model. a observation of a uv exciton bands reveals tunable optical properties of cadmium chalcogenide npls inside uv spectral region, which may be interesting considering practical applications.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
20001,"complex oxides exhibit many intriguing phenomena, including metal-insulator transition, ferroelectricity/multiferroicity, colossal magnetoresistance and high transition temperature superconductivity. advances inside epitaxial thin film growth techniques enable us to combine different complex oxides with atomic precision and form an oxide heterostructure. recent theoretical and experimental work has shown that charge transfer across oxide interfaces generally occurs and leads to the great diversity of emergent interfacial properties which are not exhibited by bulk constituents. inside this report, we review mechanisms and physical consequence of charge transfer across interfaces inside oxide heterostructures. both theoretical proposals and experimental measurements of various oxide heterostructures are discussed and compared. we also review a theoretical methods that are used to calculate charge transfer across oxide interfaces and discuss a success and challenges inside theory. finally, we present the summary and perspectives considering future research.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
11352,"inside this note, we prove that every m-th root finsler metric with isotropic landsberg curvature reduces to the landsberg metric. then, we show that every m-th root metric with almost vanishing h-curvature has vanishing h-curvature.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9780,"experimental determination of protein function was resource-consuming. as an alternative, computational prediction of protein function has received attention. inside this context, protein structural classification (psc) should help, by allowing considering determining structural classes of currently unclassified proteins based on their features, and then relying on a fact that proteins with similar structures have similar functions. existing psc approaches rely on sequence-based or direct (""raw"") 3-dimensional (3d) structure-based protein features. inside contrast, we first model 3d structures as protein structure networks (psns). then, we use (""processed"") network-based features considering psc. we propose a use of graphlets, state-of-the-art features inside many domains of network science, inside a task of psc. moreover, because graphlets should deal only with unweighted psns, and because accounting considering edge weights when constructing psns could improve psc accuracy, we also propose the deep learning framework that automatically learns network features from a weighted psns. when evaluated on the large set of ~9,400 cath and ~12,800 scop protein domains (spanning 36 psn sets), our proposed approaches are superior to existing psc approaches inside terms of accuracy, with comparable running time.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
13950,"testing a simplifying assumption inside high-dimensional vine copulas was the difficult task because tests must be based on estimated observations and amount to checking constraints on high-dimensional distributions. so far, corresponding tests have been limited to single conditional copulas with the low-dimensional set of conditioning variables. we propose the novel testing procedure that was computationally feasible considering high-dimensional data sets and that exhibits the power that decreases only slightly with a dimension. by discretizing a support of a conditioning variables and incorporating the penalty inside a test statistic, we mitigate a curse of dimensions by looking considering a possibly strongest deviation from a simplifying assumption. a use of the decision tree renders a test computationally feasible considering large dimensions. we derive a asymptotic distribution of a test and analyze its finite sample performance inside an extensive simulation study. a utility of a test was demonstrated by its application to 10 data sets with up to 49 dimensions.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
15606,"we consider deep linear networks with arbitrary convex differentiable loss. we provide the short and elementary proof of a fact that all local minima are global minima if a hidden layers are either 1) at least as wide as a input layer, or 2) at least as wide as a output layer. this result was a strongest possible inside a following sense: if a loss was convex and lipschitz but not differentiable then deep linear networks should have sub-optimal local minima.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
9987,"inside this paper, a performance and system complexity of a candidate multiple access (ma) techniques considering a next generation of cellular systems, namely, non-orthogonal multiple access (noma) (in this paper, we consider power domain ma as noma) and sparse code multiple access (scma), are investigated. to this end, considering each ma technique, the resource allocation problem considering heterogeneous cellular networks (hetnet) was formulated. we apply successive convex approximation (sca) method to each problem and obtain their solutions. a simulation results show that scma-based system achieves better performance than noma-based one at a cost of more complexity.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
6471,"a multi-armed bandit (mab) problem was the sequential allocation task where a goal was to learn the policy that maximizes long term payoff, where only a reward of a executed action was observed; i.e., sequential optimal decisions are made, while simultaneously learning how a world operates. inside a stochastic setting, a reward considering each action was generated from an unknown distribution. to decide a next optimal action to take, one must compute sufficient statistics of this unknown reward distribution, e.g. upper-confidence bounds (ucb), or expectations inside thompson sampling. closed-form expressions considering these statistics of interest are analytically intractable except considering simple cases. we here propose to leverage monte carlo approximation and, inside particular, a flexibility of (sequential) importance sampling (is) to allow considering accurate approximation of a statistics of interest within a mab problem. was methods approximate posterior densities or expectations inside probabilistic models that are analytically intractable. we first show how was should be combined with state-of-the-art mab algorithms (thompson sampling and bayes-ucb) considering classic (bernoulli and contextual linear-gaussian) bandit problems. furthermore, we leverage a power of sequential was to extend a applicability of these algorithms beyond a classic settings, and tackle additional useful cases. specifically, we study a dynamic linear-gaussian bandit, and both a static and dynamic logistic cases too. a flexibility of (sequential) importance sampling was shown to be fundamental considering obtaining efficient estimates of a key sufficient statistics inside these challenging scenarios.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15428,"inside this project, the rather complete proof-theoretical formalization of lambek calculus (non-associative with arbitrary extensions) has been ported from coq proof assistent to hol4 theorem prover, with some improvements and new theorems. three deduction systems (syntactic calculus, natural deduction and sequent calculus) of lambek calculus are defined with many related theorems proved. a equivalance between these systems are formally proved. finally, the formalization of sequent calculus proofs (where coq has built-in supports) has been designed and implemented inside hol4. some basic results including a sub-formula properties of a so-called ""cut-free"" proofs are formally proved. this work should be considered as a preliminary work towards the language parser based on category grammars which was not multimodal but still has ability to support context-sensitive languages through customized extensions.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14505,"we proposed the two stage framework with only one network to analyze skin lesion images, we firstly trained the convolutional network to classify these images, and cropped a import regions which a network has a maximum activation value. inside a second stage, we retrained this cnn with a image regions extracted from stage one and output a final probabilities. a two stage framework achieved the mean auc of 0.857 inside isic-2017 skin lesion validation set and was 0.04 higher than that of a original inputs, 0.821.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17426,"stochastic network optimization problems entail finding resource allocation policies that are optimum on an average but must be designed inside an online fashion. such problems are ubiquitous inside communication networks, where resources such as energy and bandwidth are divided among nodes to satisfy certain long-term objectives. this paper proposes an asynchronous incremental dual decent resource allocation algorithm that utilizes delayed stochastic {gradients} considering carrying out its updates. a proposed algorithm was well-suited to heterogeneous networks as it allows a computationally-challenged or energy-starved nodes to, at times, postpone a updates. a asymptotic analysis of a proposed algorithm was carried out, establishing dual convergence under both, constant and diminishing step sizes. it was also shown that with constant step size, a proposed resource allocation policy was asymptotically near-optimal. an application involving multi-cell coordinated beamforming was detailed, demonstrating a usefulness of a proposed algorithm.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
9418,"experimental evidence on high-tc cuprates reveals ubiquitous charge density wave (cdw) modulations, which coexist with superconductivity. although a cdw had been predicted by theory, important questions remain about a extent to which a cdw influences lattice and charge degrees of freedom and its characteristics as functions of doping and temperature. these questions are intimately connected to a origin of a cdw and its relation to a mysterious cuprate pseudogap. here, we use ultrahigh resolution resonant inelastic x-ray scattering (rixs) to reveal new cdw character inside underdoped bi2sr2cacu2o8+{\delta} (bi2212). at low temperature, we observe dispersive excitations from an incommensurate cdw that induces anomalously enhanced phonon intensity, unseen with the help of other techniques. near a pseudogap temperature t*, a cdw persists, but a associated excitations significantly weaken and a cdw wavevector shifts, becoming nearly commensurate with the periodicity of four lattice constants. a dispersive cdw excitations, phonon anomaly, and temperature dependent commensuration provide the comprehensive momentum space picture of complex cdw behavior and point to the closer relationship with a pseudogap state.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
14571,"data fusion has become an active research topic inside recent years. growing computational performance has allowed a use of redundant sensors to measure the single phenomenon. while bayesian fusion approaches are common inside general applications, a computer vision field has largely relegated this approach. most object following algorithms have gone towards pure machine learning fusion techniques that tend to lack flexibility. consequently, the more general data fusion scheme was needed. within this work, the hierarchical bayesian fusion idea behind the method was proposed, which outperforms individual trackers by with the help of redundant measurements. a adaptive framework was achieved by relying on each measurement's local statistics and the global softened majority voting. a proposed idea behind the method is validated inside the simulated application and two robotic platforms.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
9563,"it was well known inside many settings that reversible langevin diffusions inside confining potentials converge to equilibrium exponentially fast. adding irreversible perturbations to a drift of the langevin diffusion that maintain a same invariant measure accelerates its convergence to stationarity. many existing works thus advocate a use of such non-reversible dynamics considering sampling. when implementing markov chain monte carlo algorithms (mcmc) with the help of time discretisations of such stochastic differential equations (sdes), one should append a discretization with a usual metropolis-hastings accept-reject step and this was often done inside practice because a accept--reject step eliminates bias. on a other hand, such the step makes a resulting chain reversible. it was not known whether adding a accept-reject step preserves a faster mixing properties of a non-reversible dynamics. inside this paper, we address this gap between theory and practice by analyzing a optimal scaling of mcmc algorithms constructed from proposal moves that are time-step euler discretisations of an irreversible sde, considering high dimensional gaussian target measures. we call a resulting algorithm a ipmala, inside comparison to a classical mala algorithm (here ip was considering irreversible proposal). inside order to quantify how a cost of a algorithm scales with a dimension $n$, we prove invariance principles considering a appropriately rescaled chain. inside contrast to a usual mala algorithm, we show that there could be two regimes asymptotically: (i) the diffusive regime, as inside a mala algorithm and (ii) the ""fluid"" regime where a limit was an ordinary differential equation. we provide concrete examples where a limit was the diffusion, as inside a standard mala, but with provably higher limiting acceptance probabilities. numerical results are also given corroborating a theory.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
19896,"unmanned aerial vehicles (uavs) have attracted significant interest recently inside assisting wireless communication due to their high maneuverability, flexible deployment, and low cost. this paper considers the multi-uav enabled wireless communication system, where multiple uav-mounted aerial base stations (bss) are employed to serve the group of users on a ground. to achieve fair performance among users, we maximize a minimum throughput over all ground users inside a downlink communication by optimizing a multiuser communication scheduling and association jointly with a uavs' trajectory and power control. a formulated problem was the mixed integer non-convex optimization problem that was challenging to solve. as such, we propose an efficient iterative algorithm considering solving it by applying a block coordinate descent and successive convex optimization techniques. specifically, a user scheduling and association, uav trajectory, and transmit power are alternately optimized inside each iteration. inside particular, considering a non-convex uav trajectory and transmit power optimization problems, two approximate convex optimization problems are solved, respectively. we further show that a proposed algorithm was guaranteed to converge to at least the locally optimal solution. to speed up a algorithm convergence and achieve good throughput, the low-complexity and systematic initialization scheme was also proposed considering a uav trajectory design based on a simple circular trajectory and a circle packing scheme. extensive simulation results are provided to demonstrate a significant throughput gains of a proposed design as compared to other benchmark schemes.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
12004,"p2p lending presents as an innovative and flexible alternative considering conventional lending institutions like banks, where lenders and borrowers directly make transactions and benefit each other without complicated verifications. however, due to lack of specialized laws, delegated monitoring and effective managements, p2p platforms may spawn potential risks, such as withdraw failures, investigation involvements and even runaway bosses, which cause great losses to lenders and are especially serious and notorious inside china. although there are abundant public information and data available on a internet related to p2p platforms, challenges of multi-sourcing and heterogeneity matter. inside this paper, we promote the novel deep learning model, omnirank, which comprehends multi-dimensional features of p2p platforms considering risk quantification and produces scores considering ranking. we first construct the large-scale flexible crawling framework and obtain great amounts of multi-source heterogeneous data of domestic p2p platforms since 2007 from a internet. purifications like duplication and noise removal, null handing, format unification and fusion are applied to improve data qualities. then we extract deep features of p2p platforms using text comprehension, topic modeling, knowledge graph and sentiment analysis, which are delivered as inputs to omnirank, the deep learning model considering risk quantification of p2p platforms. finally, according to rankings generated by omnirank, we conduct flourish data visualizations and interactions, providing lenders with comprehensive information supports, decision suggestions and safety guarantees.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4877,"we report the $^{23}$na spin-lattice relaxation rate, $t_1^{-1}$, inside low-silica x zeolite. $t_1^{-1}$ follows multiple bpp-type behavior as the result of thermal motion of sodium cations inside insulating material. a estimated lowest activation energy of 15~mev was much lower than 100~mev observed previously considering sodium motion inside heavily na-loaded samples and was most likely attributed to short-distance jumps of sodium cations between sites within a same supercage.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
13684,"considering population systems modeled by age-structured hyperbolic partial differential equations (pdes) that are bilinear inside a input and evolve with the positive-valued infinite-dimensional state, global stabilization of constant yield set points is achieved inside prior work. seasonal demands inside biotechnological production processes give rise to time-varying yield references. considering a proposed control objective aiming at the global attractivity of desired yield trajectories, multiple non-standard features have to be considered: the non-local boundary condition, the pde state restricted to a positive orthant of a function space and arbitrary restrictive but physically meaningful input constraints. moreover, we provide control lyapunov functionals ensuring an exponentially fast attraction of adequate reference trajectories. to achieve this goal, we make use of a relation between first-order hyperbolic pdes and integral delay equations leading to the decoupling of a input-dependent dynamics and a infinite-dimensional internal one. furthermore, a dynamic control structure does not necessitate exact knowledge of a model parameters or online measurements of a age-profile. with the galerkin-based numerical simulation scheme with the help of a key ideas of a karhunen-lo√®ve-decomposition, we demonstrate a controller's performance.",1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
2804,"we propose the general yet simple theorem describing a convergence of sgd under a arbitrary sampling paradigm. our theorem describes a convergence of an infinite array of variants of sgd, each of which was associated with the specific probability law governing a data selection rule used to form mini-batches. this was a first time such an analysis was performed, and most of our variants of sgd were never explicitly considered inside a literature before. our analysis relies on a recently introduced notion of expected smoothness and does not rely on the uniform bound on a variance of a stochastic gradients. by specializing our theorem to different mini-batching strategies, such as sampling with replacement and independent sampling, we derive exact expressions considering a stepsize as the function of a mini-batch size. with this we should also determine a mini-batch size that optimizes a total complexity, and show explicitly that as a variance of a stochastic gradient evaluated at a minimum grows, so does a optimal mini-batch size. considering zero variance, a optimal mini-batch size was one. moreover, we prove insightful stepsize-switching rules which describe when one should switch from the constant to the decreasing stepsize regime.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
903,"a fully spin polarized composite fermion (cf) fermi sea at half filled lowest landau level has the fermi wave vector $k^*_{\rm f}=\sqrt{4\pi\rho_e}$, where $\rho_e$ was a density of electrons or composite fermions, supporting a notion that a interaction between composite fermions should be treated perturbatively. away from $\nu=1/2$, a area was seen to be consistent with $k^*_{\rm f}=\sqrt{4\pi\rho_e}$ considering $\nu<1/2$ but $k^*_{\rm f}=\sqrt{4\pi\rho_h}$ considering $\nu>1/2$, where $\rho_h$ was a density of holes inside a lowest landau level. this result was consistent with particle-hole symmetry inside a lowest landau level. we investigate inside this article a fermi wave vector of a spin-singlet cf fermi sea (cffs) at $\nu=1/2$, considering which particle-hole symmetry was not the consideration. with the help of a microscopic cf theory, we find that considering a spin-singlet cffs a fermi wave vectors considering up and down spin cffss at $\nu=1/2$ are consistent with $k^{*\uparrow,\downarrow}_{\rm f}=\sqrt{4\pi\rho^{\uparrow,\downarrow}_e}$, where $\rho^{\uparrow}_e=\rho^{\downarrow}_e=\rho_e/2$, which implies that a residual interactions between composite fermions do not cause the non-perturbative correction considering non-fully spin polarized cffs either. our results suggest a natural conjecture that considering arbitrary spin polarization a cf fermi wave vectors are given by $k^{*\uparrow}_{\rm f}=\sqrt{4\pi\rho^{\uparrow}_e}$ and $k^{*\downarrow}_{\rm f}=\sqrt{4\pi\rho^{\downarrow}_e}$.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
5621,"we have carried the detailed analysis on a impact of cosmological redshift inside a non-parametric idea behind the method to automated galaxy morphology classification. we artificially redshifted each galaxy from a efigi 4458 sample (re-centred at $z \sim 0$) simulating sdss, des, lsst and hst instruments set-ups over a range $0 < z < 1.5$. we then traced how a morphometry was degraded inside each $z$ with the help of morfometryka. inside a process, we re-sampled all catalogues to several resolutions and to the diverse signal-to-noise range, allowing us to understand a impact of image sampling and noise on our measurements separately. we summarize by exploring a impact of these effects on our capacity to perform automated galaxy supervised morphological classification by investigating a degradation of our classifier's metrics as the function of redshift considering each instrument. a overall conclusion was that we should make reliable classification with morfometryka considering $z < 0.2$ with sdss, considering $z < 0.5$ with des, considering $z < 0.8$ with lsst and considering at least $z < 1.5$ with hst.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
18214,"random walks are at a heart of many existing deep learning algorithms considering graph data. however, such algorithms have many limitations that arise from a use of random walks, e.g., a features resulting from these methods are unable to transfer to new nodes and graphs as they are tied to node identity. inside this work, we introduce a notion of attributed random walks which serves as the basis considering generalizing existing methods such as deepwalk, node2vec, and many others that leverage random walks. our proposed framework enables these methods to be more widely applicable considering both transductive and inductive learning as well as considering use on graphs with attributes (if available). this was achieved by learning functions that generalize to new nodes and graphs. we show that our proposed framework was effective with an average auc improvement of 16.1% while requiring on average 853 times less space than existing methods on the variety of graphs from several domains.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18762,"a resolvent analysis of mckeon & sharma (2010) recasts a navier-stokes equations into an input/output form inside which a nonlinear term was treated as the forcing that acts upon a linear dynamics to yield the velocity response. a framework has shown promise with regards to producing low-dimensional representations of exact coherent states. previous work has focused on the primitive variable output; here we show the velocity-vorticity formulation of a governing equations along with the helmholtz decomposition of a nonlinear forcing term reveals the simplified input/output form inside a resolvent analysis. this idea behind the method leads to an improved method considering compact representations of exact coherent states considering both forcing and response fields, with the significant reduction inside degrees of freedom inside comparison to a primitive variable approach.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
598,"recommendation was a task of improving customer experience through personalized recommendation based on users' past feedback. inside this paper, we investigate a most common scenario: a user-item (u-i) matrix of implicit feedback. even though many recommendation approaches are designed based on implicit feedback, they attempt to project a u-i matrix into the low-rank latent space, which was the strict restriction that rarely holds inside practice. inside addition, although misclassification costs from imbalanced classes are significantly different, few methods take a cost of classification error into account. to address aforementioned issues, we propose the robust framework by decomposing a u-i matrix into two components: (1) the low-rank matrix that captures a common preference, and (2) the sparse matrix that detects a user-specific preference of individuals. the cost-sensitive learning model was embedded into a framework. specifically, this model exploits different costs inside a loss function considering a observed and unobserved instances. we show that a resulting non-smooth convex objective should be optimized efficiently by an accelerated projected gradient method with closed-form solutions. morever, a proposed algorithm should be scaled up to large-sized datasets after the relaxation. a theoretical result shows that even with the small fraction of 1's inside a u-i matrix $m\in\mathbb{r}^{n\times m}$, a cost-sensitive error of a proposed model was upper bounded by $o(\frac{\alpha}{\sqrt{mn}})$, where $\alpha$ was the bias over imbalanced classes. finally, empirical experiments are extensively carried out to evaluate a effectiveness of our proposed algorithm. encouraging experimental results show that our algorithm outperforms several state-of-the-art algorithms on benchmark recommendation datasets.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7984,"inside this paper, the level-wise mixture model (lmm) was developed by embedding visual hierarchy with deep networks to support large-scale visual recognition (i.e., recognizing thousands or even tens of thousands of object classes), and the bayesian idea behind the method was used to adapt the pre-trained visual hierarchy automatically to a improvements of deep features (that are used considering image and object class representation) when more representative deep networks are learned along a time. our lmm model should provide an end-to-end idea behind the method considering jointly learning: (a) a deep networks to extract more discriminative deep features considering image and object class representation; (b) a tree classifier considering recognizing large numbers of object classes hierarchically; and (c) a visual hierarchy adaptation considering achieving more accurate indexing of large numbers of object classes hierarchically. by supporting joint learning of a tree classifier, a deep networks and a visual hierarchy adaptation, our lmm algorithm should provide an effective idea behind the method considering controlling inter-level error propagation effectively, thus it should achieve better accuracy rates on large-scale visual recognition. our experiments are carried on imagenet1k and imagenet10k image sets, and our lmm algorithm should achieve very competitive results on both a accuracy rates and a computation efficiency as compared with a baseline methods.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13299,"inside this paper, we consider a problem of automatically segmenting neuronal cells inside dual-color confocal microscopy images. this problem was the key task inside various quantitative analysis applications inside neuroscience, such as tracing cell genesis inside danio rerio (zebrafish) brains. deep learning, especially with the help of fully convolutional networks (fcn), has profoundly changed segmentation research inside biomedical imaging. we face two major challenges inside this problem. first, neuronal cells may form dense clusters, making it difficult to correctly identify all individual cells (even to human experts). consequently, segmentation results of a known fcn-type models are not accurate enough. second, pixel-wise ground truth was difficult to obtain. only the limited amount of approximate instance-wise annotation should be collected, which makes a training of fcn models quite cumbersome. we propose the new fcn-type deep learning model, called deep complete bipartite networks (cb-net), and the new scheme considering leveraging approximate instance-wise annotation to train our pixel-wise prediction model. evaluated with the help of seven real datasets, our proposed new cb-net model outperforms a state-of-the-art fcn models and produces neuron segmentation results of remarkable quality",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2725,"considering each integer $k \geq 2$, we apply gluing methods to construct sequences of minimal surfaces embedded inside a round $3$-sphere. we produce two types of sequences, all desingularizing collections of intersecting clifford tori. sequences of a first type converge to the collection of $k$ clifford tori intersecting with maximal symmetry along these two circles. near each of a circles, after rescaling, a sequences converge smoothly on compact subsets to the karcher-scherk tower of order $k$. sequences of a second type desingularize the collection of a same $k$ clifford tori supplemented by an additional clifford torus equidistant from a original two circles of intersection, so that a latter torus orthogonally intersects each of a former $k$ tori along the pair of disjoint orthogonal circles, near which a corresponding rescaled sequences converge to the singly periodic scherk surface. a simpler examples of a first type resemble surfaces constructed by choe and soret \cite{cs} by different methods where a number of handles desingularizing each circle was a same. there was the plethora of new examples which are more complicated and on which a number of handles considering a two circles differs. examples of a second type are new as well.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17801,"we propose the new reinforcement learning algorithm considering partially observable markov decision processes (pomdp) based on spectral decomposition methods. while spectral methods have been previously employed considering consistent learning of (passive) latent variable models such as hidden markov models, pomdps are more challenging since a learner interacts with a environment and possibly changes a future observations inside a process. we devise the learning algorithm running through epochs, inside each epoch we employ spectral techniques to learn a pomdp parameters from the trajectory generated by the fixed policy. at a end of a epoch, an optimization oracle returns a optimal memoryless planning policy which maximizes a expected reward based on a estimated pomdp model. we prove an order-optimal regret bound with respect to a optimal memoryless policy and efficient scaling with respect to a dimensionality of observation and action spaces.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1291,"a upsilon andromedae system is a first multi-planet system discovered orbiting the main sequence star. we describe a detection of water vapor inside a atmosphere of a innermost non-transiting gas giant ups~and~b by treating a star-planet system as the spectroscopic binary with high-resolution, ground-based spectroscopy. we resolve a signal of a planet's motion and break a mass-inclination degeneracy considering this non-transiting planet using deep combined flux observations of a star and a planet. inside total, seven epochs of keck nirspec $l$ band observations, three epochs of keck nirspec short wavelength $k$ band observations, and three epochs of keck nirspec long wavelength $k$ band observations of a ups~and~system were obtained. we perform the multi-epoch cross correlation of a full data set with an atmospheric model. we measure a radial projection of a keplerian velocity ($k_p$ = 55 $\pm$ 9 km/s), true mass ($m_b$ = 1.7 $^{+0.33}_{-0.24}$ $m_j$), and orbital inclination \big($i_b$ = 24 $\pm$ 4$^{\circ}$\big), and determine that a planet's opacity structure was dominated by water vapor at a probed wavelengths. dynamical simulations of a planets inside a ups~and~system with these orbital elements considering ups~and~b show that stable, long-term (100 myr) orbital configurations exist. these measurements will inform future studies of a stability and evolution of a ups~and~system, as well as a atmospheric structure and composition of a hot jupiter.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10174,"the large class of machine learning techniques requires a solution of optimization problems involving spectral functions of parametric matrices, e.g. log-determinant and nuclear norm. unfortunately, computing a gradient of the spectral function was generally of cubic complexity, as such gradient descent methods are rather expensive considering optimizing objectives involving a spectral function. thus, one naturally turns to stochastic gradient methods inside hope that they will provide the way to reduce or altogether avoid a computation of full gradients. however, here the new challenge appears: there was no straightforward way to compute unbiased stochastic gradients considering spectral functions. inside this paper, we develop unbiased stochastic gradients considering spectral-sums, an important subclass of spectral functions. our unbiased stochastic gradients are based on combining randomized trace estimators with stochastic truncation of a chebyshev expansions. the careful design of a truncation distribution allows us to offer distributions that are variance-optimal, which was crucial considering fast and stable convergence of stochastic gradient methods. we further leverage our proposed stochastic gradients to devise stochastic methods considering objective functions involving spectral-sums, and rigorously analyze their convergence rate. a utility of our methods was demonstrated inside numerical experiments.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11472,"relation extraction refers to a task of populating the database with tuples of a form $r(e_1, e_2)$, where $r$ was the relation and $e_1$, $e_2$ are entities. distant supervision was one such technique which tries to automatically generate training examples based on an existing kb such as freebase. this paper was the survey of some of a techniques inside distant supervision which primarily rely on probabilistic graphical models (pgms).",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6482,"recently, there has been increased interest inside fusing multimodal imaging to better understand brain organization. specifically, accounting considering knowledge of anatomical pathways connecting brain regions should lead to desirable outcomes such as increased accuracy inside functional brain network estimates and greater reproducibility of topological features across scanning sessions. despite a clear merits, major challenges persist inside integrative analyses including an incomplete understanding of a structure-function relationship and inaccuracies inside mapping anatomical structures due to deficiencies inside existing imaging technology. clearly advanced network modeling tools are needed to appropriately incorporate anatomical structure inside constructing brain functional networks. we propose the hierarchical bayesian gaussian graphical modeling idea behind the method that estimates a functional networks using sparse precision matrices whose degree of edge-specific shrinkage was informed by anatomical structure and an independent baseline component. a idea behind the method flexibly identifies functional connections supported by structural connectivity knowledge. this enables robust brain network approximation even inside a presence of mis-specified anatomical knowledge, while accommodating heterogeneity inside a structure-function relationship. we implement a idea behind the method using an efficient optimization algorithm yielding maximum the posteriori estimates. extensive numerical studies reveal a clear advantages of our idea behind the method over competing methods inside accurately estimating brain functional connectivity, even when a anatomical knowledge was mis-specified. an application of a idea behind the method to a philadelphia neurodevelopmental cohort (pnc) study reveals gender based connectivity differences across multiple age groups, and higher reproducibility inside a approximation of network metrics compared to alternative methods.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17555,"detection of radio emission from exoplanets should provide information on a star-planet system that was difficult to study otherwise, such as a planetary magnetic field, magnetosphere, rotation period, interior structure, atmospheric dynamics and escape, and any star-planet interactions. such the detection inside a radio domain would open up the whole new field inside a study of exoplanets. however, currently there are no confirmed detections of an exoplanet at radio frequencies. inside this study, we search considering non-thermal radio emission from a 55 cnc system which has 5 known exoplanets. according to theoretical predictions 55 cnc e, a innermost planet, was among a best targets considering this search. we observed considering 18 hours with a low-frequency array (lofar) low band antenna inside a frequency range 26-73 mhz with full-polarization and covered 85% of a orbital phase of 55 cnc e. during a observations four digital beams within a station beam were recorded simultaneously on 55 cnc, nearby ""empty"" sky, the bright radio source, and the pulsar. the pipeline is created to automatically find and mask radio frequency interference, calibrate a time-frequency response of a telescope, and to search considering bursty planetary radio signals inside our data. extensive tests and verifications were carried out on a pipeline. analysis of a first 4 hours of these observations do not contain any exoplanet signal from 55 cnc but we should confirm that our setup was adequate to detect faint astrophysical signals. we find the 3$\sigma$ upper limit considering 55 cnc of 230 mjy with the help of a pulsar to approximate a sensitivity of a observations and 2.6 jy with the help of a time-series difference between a target and sky beam. a full data set was still under-going analysis. inside a near future we will apply our observational technique and pipeline to a most promising exoplanet candidates considering which lofar observations have already been obtained.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
13305,"material recognition enables robots to incorporate knowledge of material properties into their interactions with everyday objects. considering example, material recognition opens up opportunities considering clearer communication with the robot, such as ""bring me a metal coffee mug"", and recognizing plastic versus metal was crucial when with the help of the microwave or oven. however, collecting labeled training data with the robot was often more difficult than unlabeled data. we present the semi-supervised learning idea behind the method considering material recognition that uses generative adversarial networks (gans) with haptic features such as force, temperature, and vibration. our idea behind the method achieves state-of-the-art results and enables the robot to approximate a material class of household objects with ~90% accuracy when 92% of a training data are unlabeled. we explore how well this idea behind the method should recognize a material of new objects and we discuss challenges facing generalization. to motivate learning from unlabeled training data, we also compare results against several common supervised learning classifiers. inside addition, we have released a dataset used considering this work which consists of time-series haptic measurements from the robot that conducted thousands of interactions with 72 household objects.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
17420,"a goal of this paper was to present an end-to-end, data-driven framework to control autonomous mobility-on-demand systems (amod, i.e. fleets of self-driving vehicles). we first model a amod system with the help of the time-expanded network, and present the formulation that computes a optimal rebalancing strategy (i.e., preemptive repositioning) and a minimum feasible fleet size considering the given travel demand. then, we adapt this formulation to devise the model predictive control (mpc) algorithm that leverages short-term demand forecasts based on historical data to compute rebalancing strategies. we test a end-to-end performance of this controller with the state-of-the-art lstm neural network to predict customer demand and real customer data from didi chuxing: we show that this idea behind the method scales very well considering large systems (indeed, a computational complexity of a mpc algorithm does not depend on a number of customers and of vehicles inside a system) and outperforms state-of-the-art rebalancing strategies by reducing a mean customer wait time by up to to 89.6%.",1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1
6708,"on the daily investment decision inside the security market, a price earnings (pe) ratio was one of a most widely applied methods being used as the firm valuation tool by investment experts. unfortunately, recent academic developments inside financial econometrics and machine learning rarely look at this tool. inside practice, fundamental pe ratios are often estimated only by subjective expert opinions. a purpose of this research was to formalize the process of fundamental pe approximation by employing advanced dynamic bayesian network (dbn) methodology. a estimated pe ratio from our model should be used either as the information support considering an expert to make investment decisions, or as an automatic trading system illustrated inside experiments. forward-backward inference and em parameter approximation algorithms are derived with respect to a proposed dbn structure. unlike existing works inside literatures, a economic interpretation of our dbn model was well-justified by behavioral finance evidences of volatility. the simple but practical trading strategy was invented based on a result of bayesian inference. extensive experiments show that our trading strategy equipped with a inferenced pe ratios consistently outperforms standard investment benchmarks.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16195,"we have fabricated oxygen deficient polycrystalline zno films by a rf sputtering deposition method. to systematically investigate a charge transport mechanisms inside these samples, a electrical resistivities have been measured over the wide range of temperature from 300 k down to liquid-helium temperatures. we found that below about 100 k, a variable-range-hopping (vrh) conduction processes govern a charge transport properties. inside particular, a mott vrh conduction process dominates at higher temperatures, while crossing over to a efros-shklovskii (es) vrh conduction process at lower temperatures. a crossover occurred at temperatures as high as the few tens degrees kelvin. moreover, a temperature behavior of resistivity over a entire vrh conduction regime from a mott-type to a es-type process should be well described by the universal scaling law.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
15539,"sparse feature selection was necessary when we fit statistical models, we have access to the large group of features, don't know which are relevant, but assume that most are not. alternatively, when a number of features was larger than a available data a model becomes over parametrized and a sparse feature selection task involves selecting a most informative variables considering a model. when a model was the simple location model and a number of relevant features does not grow with a total number of features, sparse feature selection corresponds to sparse mean estimation. we deal with the simplified mean approximation problem consisting of an additive model with gaussian noise and mean that was inside the restricted, finite hypothesis space. this restriction simplifies a mean approximation problem into the selection problem of combinatorial nature. although a hypothesis space was finite, its size was exponential inside a dimension of a mean. inside limited data settings and when a size of a hypothesis space depends on a amount of data or on a dimension of a data, choosing an approximation set of hypotheses was the desirable approach. choosing the set of hypotheses instead of the single one implies replacing a bias-variance trade off with the resolution-stability trade off. generalization capacity provides the resolution selection criterion based on allowing a learning algorithm to communicate a largest amount of information inside a data to a learner without error. inside this work a theory of approximation set coding and generalization capacity was explored inside order to understand this approach. we then apply a generalization capacity criterion to a simplified sparse mean approximation problem and detail an importance sampling algorithm which at once solves a difficulty posed by large hypothesis spaces and a slow convergence of uniform sampling algorithms.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16879,"aims. eso 243-49 hlx-1, otherwise known as hlx-1, was an intermediate mass black hole (imbh) candidate located 8"" (3.7 kpc) from a centre of a edge-on s0 galaxy eso 243-49. how a black hole came to be associated with this galaxy, and a nature of a environment inside which it resides, are still unclear. with the help of multi-wavelength observations we investigate a nature of a medium surrounding hlx-1, search considering evidence of past mergers with eso 243-49 and constrain parameters of a galaxy. methods. we reduce and analyse integral field unit observations of eso 243-49 that were taken with a muse instrument on a vlt. with the help of complementary multi-wavelength data, including x-shooter, hst, swift, chandra and atca data, we further examine a vicinity of hlx-1. we additionally examine a nature of a host galaxy and approximate a mass of a central supermassive black hole (smbh) inside eso 243-49. results. no evidence considering the recent minor-merger that could result inside a presence of a imbh was discerned, but a data are compatible with the scenario inside which minor mergers may have occurred inside a history of eso 243-49. a muse data reveal the rapidly rotating disc inside a centre of a galaxy, around a smbh. a mass of a smbh at a centre of eso 243-49 was estimated to be 0.5-23 $\times$ 10$^7$ m$_\odot$. studying a spectra of hlx-1, that were taken inside a low/hard state, we determine h$_\alpha$ flux variability to be at least the factor 6, compared to observations taken during a high/soft state. this h$_\alpha$ flux variability over one year indicates that a line originates close to a imbh, excluding a possibility that a line emanates from the surrounding nebula or the star cluster. a large variability associated with a x-ray states of hlx-1 confirms that a h$_\alpha$ line was associated with a object and therefore validates a distance to hlx-1.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5927,"we study the symmetric collaborative dialogue setting inside which two agents, each with private knowledge, must strategically communicate to achieve the common goal. a open-ended dialogue state inside this setting poses new challenges considering existing dialogue systems. we collected the dataset of 11k human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements. to model both structured knowledge and unstructured language, we propose the neural model with dynamic knowledge graph embeddings that evolve as a dialogue progresses. automatic and human evaluations show that our model was both more effective at achieving a goal and more human-like than baseline neural and rule-based models.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7579,"this paper addresses a boundary stabilization of the flexible wing model, both inside bending and twisting displacements, under unsteady aerodynamic loads, and inside presence of the store. a wing dynamics was captured by the distributed parameter system as the coupled euler-bernoulli and timoshenko beam model. a problem was tackled inside a framework of semigroup theory, and the lyapunov-based stability analysis was carried out to assess that a system energy, as well as a bending and twisting displacements, decay exponentially to zero. a effectiveness of a proposed boundary control scheme was evaluated based on simulations.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
6837,"we present an application of deep generative models inside a context of partial-differential equation (pde) constrained inverse problems. we combine the generative adversarial network (gan) representing an the priori model that creates subsurface geological structures and their petrophysical properties, with a numerical solution of a pde governing a propagation of acoustic waves within a earth's interior. we perform bayesian inversion with the help of an approximate metropolis-adjusted langevin algorithm (mala) to sample from a posterior given seismic observations. gradients with respect to a model parameters governing a forward problem are obtained by solving a adjoint of a acoustic wave equation. gradients of a mismatch with respect to a latent variables are obtained by leveraging a differentiable nature of a deep neural network used to represent a generative model. we show that approximate mala sampling allows efficient bayesian inversion of model parameters obtained from the prior represented by the deep generative model, obtaining the diverse set of realizations that reflect a observed seismic response.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15755,"we present an idea behind the method considering a verification of feed-forward neural networks inside which all nodes have the piece-wise linear activation function. such networks are often used inside deep learning and have been shown to be hard to verify considering modern satisfiability modulo theory (smt) and integer linear programming (ilp) solvers. a starting point of our idea behind the method was a addition of the global linear approximation of a overall network behavior to a verification problem that helps with smt-like reasoning over a network behavior. we present the specialized verification algorithm that employs this approximation inside the search process inside which it infers additional node phases considering a non-linear nodes inside a network from partial node phase assignments, similar to unit propagation inside classical sat solving. we also show how to infer additional conflict clauses and safe node fixtures from a results of a analysis steps performed during a search. a resulting idea behind the method was evaluated on collision avoidance and handwritten digit recognition case studies.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5498,"preserving privacy of users was the key requirement of web-scale analytics and reporting applications, and has witnessed the renewed focus inside light of recent data breaches and new regulations such as gdpr. we focus on a problem of computing robust, reliable analytics inside the privacy-preserving manner, while satisfying product requirements. we present pripearl, the framework considering privacy-preserving analytics and reporting, inspired by differential privacy. we describe a overall design and architecture, and a key modeling components, focusing on a unique challenges associated with privacy, coverage, utility, and consistency. we perform an experimental study inside a context of ads analytics and reporting at linkedin, thereby demonstrating a tradeoffs between privacy and utility needs, and a applicability of privacy-preserving mechanisms to real-world data. we also highlight a lessons learned from a production deployment of our system at linkedin.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
18449,"we formulate the supervised learning problem, referred to as continuous ranking, where the continuous real-valued label y was assigned to an observable r.v. x taking its values inside the feature space $\mathcal{x}$ and a goal was to order all possible observations x inside $\mathcal{x}$ by means of the scoring function $s:\mathcal{x}\rightarrow \mathbb{r}$ so that s(x) and y tend to increase or decrease together with highest probability. this problem generalizes bi/multi-partite ranking to the certain extent and a task of finding optimal scoring functions s(x) should be naturally cast as optimization of the dedicated functional criterion, called a iroc curve here, or as maximization of a kendall ${\tau}$ related to a pair (s(x), y ). from a theoretical side, we describe a optimal elements of this problem and provide statistical guarantees considering empirical kendall ${\tau}$ maximization under appropriate conditions considering a class of scoring function candidates. we also propose the recursive statistical learning algorithm tailored to empirical iroc curve optimization and producing the piecewise constant scoring function that was fully described by an oriented binary tree. preliminary numerical experiments highlight a difference inside nature between regression and continuous ranking and provide strong empirical evidence of a performance of empirical optimizers of a criteria proposed.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5677,"this paper proposes the novel type of random forests called the denoising random forests that are robust against noises contained inside test samples. such noise-corrupted samples cause serious damage to a approximation performances of random forests, since unexpected child nodes are often selected and a leaf nodes that a input sample reaches are sometimes far from those considering the clean sample. our main idea considering tackling this problem originates from the binary indicator vector that encodes the traversal path of the sample inside a forest. our proposed method effectively employs this vector by introducing denoising autoencoders into random forests. the denoising autoencoder should be trained with indicator vectors produced from clean and noisy input samples, and non-leaf nodes where incorrect decisions are made should be identified by comparing a input and output of a trained denoising autoencoder. multiple traversal paths with respect to a nodes with incorrect decisions caused by a noises should then be considered considering a estimation.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1065,"we investigate a role of gap characteristics such as anisotropy and inequality of a gaps inside a quasiparticle interferences of iron pnictides with the help of the five-orbital tight-binding model. we examine how a difference inside a sensitivities exhibited by a sign-changing and -preserving $s$-wave superconductivity inside an annular region around ($\pi, 0$), which should be used to determine a sign change of a superconducting gap, gets affected when a gaps are unequal on a electron and hole pocket. inside addition, we also discuss how robust these differentiating features are on changing a quasiparticle energy or when a gap was anisotropic.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
3332,a regularity of weak solutions of the two-dimensional nonlinear sigma model with coarse gravitino was shown. here a gravitino was only assumed to be inside $l^p$ considering some $p>4$. a precise regularity results depend on a value of $p$.,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8531,"we introduce the certain differential graded bialgebra, neither commutative nor cocommutative, that governs perturbations of the differential on complexes supplied with an abstract hodge decomposition. this leads to the conceptual treatment of a homological perturbation lemma and its multiplicative version. as an application we give an explicit form of a decomposition theorem considering $a_\infty$ algebras and $a_\infty$ modules and, more generally, considering twisted objects inside differential graded categories.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
2015,"allometric scaling should reflect underlying mechanisms, dynamics and structures inside complex systems; examples include typical scaling laws inside biology, ecology and urban development. inside this work, we study allometric scaling inside scientific fields. by performing an analysis of a outputs/inputs of various scientific fields, including a numbers of publications, citations, and references, with respect to a number of authors, we find that inside all fields that we have studied thus far, including physics, mathematics and economics, there are allometric scaling laws relating a outputs/inputs and a sizes of scientific fields. furthermore, a exponents of a scaling relations have remained quite stable over a years. we also find that a deviations of individual subfields from a overall scaling laws are good indicators considering ranking subfields independently of their sizes.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
1165,"a existence of the spin-liquid ground state of a $s=1/2$ heisenberg kagome antiferromagnet (kafm) was well established. meanwhile, also considering a $s=1$ heisenberg kafm evidence considering a absence of magnetic long-range order (lro) is found. magnetic lro inside heisenberg kafms should emerge by increasing a spin quantum number $s$ to $s>1$ and considering $s=1$ by an easy-plane anisotropy. inside a present paper we discuss a route to magnetic order inside $s=1/2$ kafms by including an isotropic interlayer coupling (ilc) $j_\perp$ as well as an easy-plane anisotropy inside a kagome layers by with the help of a coupled-cluster method to high orders of approximation. we consider ferro- as well as antiferromagnetic $j_\perp$. to discuss a general question considering a crossover from the purely two-dimensional (2d) to the quasi-2d and finally to the three-dimensional system we consider a simplest model of stacked (unshifted) kagome layers. although a ilc of real kagome compounds was often more sophisticated, such the geometry of a ilc should be relevant considering barlowite. we find that a spin-liquid ground state present considering a strictly 2d $s=1/2$ $xxz$ kafm survives the finite ilc, where a spin-liquid region shrinks monotonously with increasing anisotropy. if a ilc becomes large enough (about 15\% of intralayer coupling considering a isotropic heisenberg case and about 4\% considering a $xy$ limit) magnetic lro should be established, where a $q=0$ symmetry was favorable if $j_\perp$ was of moderate strength. if a strength of a ilc further increases, $\sqrt{3}\times \sqrt{3}$ lro should become favorable against $q=0$ lro.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
11449,"a purpose of this short contribution was to report on a development of the spectral neighbor analysis potential (snap) considering tungsten. we have focused on a characterization of elastic and defect properties of a pure material inside order to support molecular dynamics simulations of plasma-facing materials inside fusion reactors. the parallel genetic algorithm idea behind the method is used to efficiently search considering fitting parameters optimized against the large number of objective functions. inside addition, we have shown that this many-body tungsten potential should be used inside conjunction with the simple helium pair potential to produce accurate defect formation energies considering a w-he binary system.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
2415,"we propose to synthesize feasible caging grasps considering the target object through computing caging loops, the closed curve defined inside a shape embedding space of a object. different from a traditional methods, our idea behind the method decouples caging loops from a surface geometry of target objects through working inside a embedding space. this enables us to synthesize caging loops encompassing multiple topological holes, instead of always tied with one specific handle which could be too small to be graspable by a robot gripper. our method extracts caging loops through the topological analysis of a distance field defined considering a target surface inside a embedding space, based on the rigorous theoretical study on a relation between caging loops and a field topology. due to a decoupling, our method should tolerate incomplete and noisy surface geometry of an unknown target object captured on-the-fly. we implemented our method with the robotic gripper and demonstrate through extensive experiments that our method should synthesize reliable grasps considering objects with complex surface geometry and topology and inside various scales.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
13405,"much of a success of single agent deep reinforcement learning (drl) inside recent years should be attributed to a use of experience replay memories (erm), which allow deep q-networks (dqns) to be trained efficiently through sampling stored state transitions. however, care was required when with the help of erms considering multi-agent deep reinforcement learning (ma-drl), as stored transitions should become outdated because agents update their policies inside parallel [11]. inside this work we apply leniency [23] to ma-drl. lenient agents map state-action pairs to decaying temperature values that control a amount of leniency applied towards negative policy updates that are sampled from a erm. this introduces optimism inside a value-function update, and has been shown to facilitate cooperation inside tabular fully-cooperative multi-agent reinforcement learning problems. we evaluate our lenient-dqn (ldqn) empirically against a related hysteretic-dqn (hdqn) algorithm [22] as well as the modified version we call scheduled-hdqn, that uses average reward learning near terminal states. evaluations take place inside extended variations of a coordinated multi-agent object transportation problem (cmotp) [8] which include fully-cooperative sub-tasks and stochastic rewards. we find that ldqn agents are more likely to converge to a optimal policy inside the stochastic reward cmotp compared to standard and scheduled-hdqn agents.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19657,"a fine details of a large-scale structure inside a local universe provide important empirical benchmarks considering testing cosmological models of structure formation. dwarf galaxies are key object considering such studies. enlarge a sample of known dwarf galaxies inside a local universe. we performed the search considering faint, unresolved low-surface brightness dwarf galaxies inside a m101 group complex, including a region around a major spiral galaxies m101, m51, and m63 lying at the distance 7.0, 8.6, and 9.0 mpc, respectively. a new dwarf galaxy sample should be used inside the first step to test considering significant substructure inside a 2d-distribution and inside the second step to study a spatial distribution of a galaxy complex. with the help of filtering algorithms we surveyed 330 square degrees of imaging data obtained from a sloan digital sky survey. a images were visually inspected. a spatial distribution of known galaxies and candidates is analyzed transforming a system into the m101 eigenframe, with the help of a geometrical alignment of a group. we discovered 15 new dwarf galaxies and carried out surface photometry inside a g and r bands. a similarity of a photometric properties of these dwarfs to those of local group dwarfs suggest membership to a m101 group complex. a sky distribution of a candidates follows a thin planar structure outlined by a known members of a three subgroups. a ~3mpc long filamentary structure has the rms thickness of 67 kpc. a planar structure of a embedded m101 subgroup was even thinner, with rms=46 kpc. a formation of this structure might be due to a expansion of a local void to which it borders. other implications are discussed as well. we show a viability of sdss data to extend a sample of dwarfs inside a local universe and test cosmological models on small scales.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14525,"a shape of the rotating electric solar wind sail under a centrifugal force and solar wind dynamic pressure was modeled to address a sail attitude maintenance and thrust vectoring. a sail rig assumes centrifugally stretched main tethers that extend radially outward from a spacecraft inside a sail spin plane. furthermore, a tips of a main tethers host remote units that are connected by auxiliary tethers at a sail rim. here, we derive a equation of main tether shape and present both the numerical solution and an analytical approximation considering a shape as parametrized both by a ratio of a electric sail force to a centrifugal force and a sail orientation with respect to a solar wind direction. a resulting shape was such that near a spacecraft, a roots of a main tethers form the cone, whereas towards a rim, this coning was flattened by a centrifugal force, and a sail was coplanar with a sail spin plane. our approximation considering a sail shape was parametrized only by a tether root coning angle and a main tether length. with the help of a approximate shape, we obtain a torque and thrust of a electric sail force applied to a sail. as the result, a amplitude of a tether voltage modulation required considering a maintenance of a sail attitude was given as the torque-free solution. a amplitude was smaller than that previously obtained considering the rigid single tether resembling the spherical pendulum. this implies that less thrusting margin was required considering a maintenance of a sail attitude. considering the given voltage modulation, a thrust vectoring was then considered inside terms of a radial and transverse thrust components.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
850,"understanding how qso's uv radiation affects galaxy formation was vital to our understanding of reionization era. with the help of the custom made narrow-band filter, $nb906$, on subaru/suprime-cam, we investigated a number density of ly$\alpha$ emitters (lae) around the qso at z=6.4. to date, this was a highest redshift narrow-band observation, where laes around the luminous qso are investigated. due to a large field-of-view of suprime-cam, our survey area was $\sim$5400~cmpc$^2$, much larger than previously studies at z=5.7 ($\sim$200 cmpc$^2$). inside this field, we previously found the factor of 7 overdensity of lyman break galaxies (lbgs). based on this, we expected to detect $\sim$100 laes down to $nb906$=25 abmag. however, our 6.4 hour exposure found none. a obtained upper limit on a number density of laes was more than an order lower than a blank fields. furthermore, this lower density of laes spans the large scale of 10 $p$mpc across. the simple argument suggests the strong uv radiation from a qso should suppress star-formation inside halos with $m_{vir}<10^{10}m_{\odot}$ within the $p$mpc from a qso, but a deficit at a edge of a field (5 $p$mpc) remains to be explained.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1816,"this paper studies a landscape of empirical risk of deep neural networks by theoretically analyzing its convergence behavior to a population risk as well as its stationary points and properties. considering an $l$-layer linear neural network, we prove its empirical risk uniformly converges to its population risk at a rate of $\mathcal{o}(r^{2l}\sqrt{d\log(l)}/\sqrt{n})$ with training sample size of $n$, a total weight dimension of $d$ and a magnitude bound $r$ of weight of each layer. we then derive a stability and generalization bounds considering a empirical risk based on this result. besides, we establish a uniform convergence of gradient of a empirical risk to its population counterpart. we prove a one-to-one correspondence of a non-degenerate stationary points between a empirical and population risks with convergence guarantees, which describes a landscape of deep neural networks. inside addition, we analyze these properties considering deep nonlinear neural networks with sigmoid activation functions. we prove similar results considering convergence behavior of their empirical risks as well as a gradients and analyze properties of their non-degenerate stationary points. to our best knowledge, this work was a first one theoretically characterizing landscapes of deep learning algorithms. besides, our results provide a sample complexity of training the good deep neural network. we also provide theoretical understanding on how a neural network depth $l$, a layer width, a network size $d$ and parameter magnitude determine a neural network landscapes.",1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0
16323,"recent work has shown that sulfur hazes may arise inside a atmospheres of some giant exoplanets due to a photolysis of h$_{2}$s. we investigate a impact such the haze would have on an exoplanet's geometric albedo spectrum and how it may affect a direct imaging results of wfirst, the planned nasa space telescope. considering temperate (250 k $<$ t$_{\rm eq}$ $<$ 700 k) jupiter--mass planets, photochemical destruction of h$_{2}$s results inside a production of $\sim$1 ppmv of \seight between 100 and 0.1 mbar, which, if cool enough, will condense to form the haze. nominal haze masses are found to drastically alter the planet's geometric albedo spectrum: whereas the clear atmosphere was dark at wavelengths between 0.5 and 1 $\mu$m due to molecular absorption, a addition of the sulfur haze boosts a albedo there to $\sim$0.7 due to scattering. strong absorption by a haze shortward of 0.4 $\mu$m results inside albedos $<$0.1, inside contrast to a high albedos produced by rayleigh scattering inside the clear atmosphere. as the result, a color of a planet shifts from blue to orange. a existence of the sulfur haze masks a molecular signatures of methane and water, thereby complicating a characterization of atmospheric composition. detection of such the haze by wfirst was possible, though discriminating between the sulfur haze and any other highly reflective, high altitude scatterer will require observations shortward of 0.4 $\mu$m, which was currently beyond wfirst's design.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1661,"magnetic skyrmions are particle-like objects with topologically-protected stability which should be set into motion with an applied current. with the help of the particle-based model we simulate current-driven magnetic skyrmions interacting with random quenched disorder and examine a skyrmion velocity fluctuations parallel and perpendicular to a direction of motion as the function of increasing drive. we show that a magnus force contribution to skyrmion dynamics combined with a random pinning produces an isotropic effective shaking temperature. as the result, a skyrmions form the moving crystal at large drives instead of a moving smectic state observed inside systems with the negligible magnus force where a effective shaking temperature was anisotropic. we demonstrate that spectral analysis of a velocity noise fluctuations should be used to identify dynamical phase transitions and to extract information about a different dynamic phases, and show how a velocity noise fluctuations are correlated with changes inside a skyrmion hall angle, transport features, and skyrmion lattice structure.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
6951,"penalized regression approaches are standard tools inside quantitative genetics. it was known that a fit of an \emph{ordinary least squares} (ols) regression was independent of certain transformations of a coding of a predictor variables, and that a standard mixed model \emph{ridge regression best linear unbiased prediction} (rrblup) was neither affected by translations of a variable coding, nor by global scaling. however, it has been reported that an extended version of this mixed model, which incorporates interactions by products of markers as additional predictor variables was affected by translations of a marker coding. inside this work, we identify a cause of this loss of invariance inside the general context of penalized regression on polynomials inside a predictor variables. we show that inside most cases, translating a coding of a predictor variables has an impact on effect estimates, with an exception when only a size of a coefficients of monomials of highest total degree are penalized. a invariance of rrblup should thus be considered as the special case of this setting, with the polynomial of total degree 1, where a size of a fixed effect (total degree 0) was not penalized but all coefficients of monomials of total degree 1 are. a extended rrblup, which includes interactions, was not invariant to translations because it does not only penalize interactions (total degree 2), but also additive effects (total degree 1). our observations are not restricted to ridge regression, but generally valid considering penalized regressions, considering instance also considering a $\ell_1$ penalty of lasso.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1888,"inside textual information extraction and other sequence labeling tasks it was now common to use recurrent neural networks (such as lstm) to form rich embedded representations of long-term input co-occurrence patterns. representation of output co-occurrence patterns was typically limited to the hand-designed graphical model, such as the linear-chain crf representing short-term markov dependencies among successive labels. this paper presents the method that learns embedded representations of latent output structure inside sequence data. our model takes a form of the finite-state machine with the large number of latent states per label (a latent variable crf), where a state-transition matrix was factorized---effectively forming an embedded representation of state-transitions capable of enforcing long-term label dependencies, while supporting exact viterbi inference over output labels. we demonstrate accuracy improvements and interpretable latent structure inside the synthetic but complex task based on conll named entity recognition.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14434,"general human action recognition requires understanding of various visual cues. inside this paper, we propose the network architecture that computes and integrates a most important visual cues considering action recognition: pose, motion, and a raw images. considering a integration, we introduce the markov chain model which adds cues successively. a resulting idea behind the method was efficient and applicable to action classification as well as to spatial and temporal action localization. a two contributions clearly improve a performance over respective baselines. a overall idea behind the method achieves state-of-the-art action classification performance on hmdb51, j-hmdb and ntu rgb+d datasets. moreover, it yields state-of-the-art spatio-temporal action localization results on ucf101 and j-hmdb.",1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10368,"this paper focuses on a application of discriminant analysis to the set of geometrical objects (bodies) characterized by currents. the current was the relevant mathematical object to model geometrical data, like hypersurfaces, through integration of vector fields along them. as the consequence of a choice of the vector-valued reproducing kernel hilbert space (rkhs) as the test space to integrate hypersurfaces, it was possible to consider that hypersurfaces are embedded inside this hilbert space. this embedding enables us to consider classification algorithms of geometrical objects. the method to apply functional discriminant analysis inside a obtained vector-valued rkhs was given. this method was based on a eigenfunction decomposition of a kernel. so, a novelty of this paper was a reformulation of the size and shape classification problem inside functional data analysis terms with the help of a theory of currents and vector-valued rkhs. this idea behind the method was applied to the 3d database obtained from an anthropometric survey of a spanish child population with the potential application to online sales of children's wear.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
8472,"this paper proposes a application of discrete wavelet transform (dwt) to detect a qrs (ecg was characterized by the recurrent wave sequence of p, qrs and t-wave) of an electrocardiogram (ecg) signal. wavelet transform provides localization inside both time and frequency. inside preprocessing stage, dwt was used to remove a baseline wander inside a ecg signal. a performance of a algorithm of qrs detection was evaluated against a standard mit bih (massachusetts institute of technology, beth israel hospital) arrhythmia database. a average qrs complexes detection rate of 98.1 % was achieved.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8362,"we study a time evolution of the thin liquid film coating a outer surface of the sphere inside a presence of gravity, surface tension and thermal gradients. we derive a fourth-order nonlinear partial differential equation that models a thin film dynamics, including marangoni terms arising from a dependence of surface tension on temperature. we consider two different imposed temperature distributions with axial or radial thermal gradients. we analyze a stability of the uniform coating under small perturbations and carry out numerical simulations inside comsol considering the range of parameter values. inside a case of an axial temperature gradient, we find steady states with either uniform film thickness, or with a fluid accumulating at a bottom or near a top of a sphere, depending on a total volume of liquid inside a film, dictating whether gravity or marangoni effects dominate. inside a case of the radial temperature gradient, the stability analysis reveals a most unstable non-axisymmetric modes on an initially uniform coating film.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9852,"we present the new paradigm considering real-time object-oriented slam with the monocular camera. contrary to previous approaches, that rely on object-level models, we construct category-level models from cad collections which are now widely available. to alleviate a need considering huge amounts of labeled data, we develop the rendering pipeline that enables synthesis of large datasets from the limited amount of manually labeled data. with the help of data thus synthesized, we learn category-level models considering object deformations inside 3d, as well as discriminative object features inside 2d. these category models are instance-independent and aid inside a design of object landmark observations that should be incorporated into the generic monocular slam framework. where typical object-slam approaches usually solve only considering object and camera poses, we also approximate object shape on-the-fly, allowing considering the wide range of objects from a category to be present inside a scene. moreover, since our 2d object features are learned discriminatively, a proposed object-slam system succeeds inside several scenarios where sparse feature-based monocular slam fails due to insufficient features or parallax. also, a proposed category-models aid inside object instance retrieval, useful considering augmented reality (ar) applications. we evaluate a proposed framework on multiple challenging real-world scenes and show --- to a best of our knowledge --- first results of an instance-independent monocular object-slam system and a benefits it enjoys over feature-based slam methods.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
2161,"we introduce a first deep reinforcement learning agent that learns to beat atari games with a aid of natural language instructions. a agent uses the multimodal embedding between environment observations and natural language to self-monitor progress through the list of english instructions, granting itself reward considering completing instructions inside addition to increasing a game score. our agent significantly outperforms deep q-networks (dqns), asynchronous advantage actor-critic (a3c) agents, and a best agents posted to openai gym on what was often considered a hardest atari 2600 environment: montezuma's revenge.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1621,"we study a cauchy problem considering the system of equations corresponding to the singular limit of radiative hydrodynamics, namely a 3d radiative compressible euler system coupled to an electromagnetic field. assuming smallness hypotheses considering a data, we prove that a problem admits the unique global smooth solution and study its asymptotics.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8107,"magnetic proximity effect of the topological insulator inside contact with the ferromagnet was reported inside thin film bilayers of 15 nm thick $bisbte_3$ on either 15 or 40 nm thick $srruo_3$ on (100) $srtio_3$ wafers. magneto transport results of a bilayers were compared with those of reference films of 15 nm $bisbte_3$ and 15 or 40 nm $srruo_3$. comparison of a temperature coefficient of resistance [(1/r)$\times$dr/dt which was qualitatively proportional to a magnetization] of a bilayer and reference ferromagnetic film normalized above $t_c$, shows the clear suppression inside a bilayer by about 50% just below $t_c$, indicating the weaker proximity magnetization inside a bilayer. resistance hysteresis loops versus field at 1.85$\pm$0.05 k inside a bilayer and reference films show the clear magnetic proximity effect, where a peak resistance of a bilayer at a coercive field shifts to lower fields by $\sim$30% compared to the hypothetical bilayer of two resistors connected inside parallel with no interaction between a layers. narrowing of a coercive peaks of a bilayers as compared to those of a reference ferromagnetic films by 25-35% is also observed, which represents another signature of a magnetic proximity effect.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
14486,"fuzzy critic-based learning forms the reinforcement learning method based on dynamic programming. inside this paper, an adaptive critic-based neuro-fuzzy system was presented considering an unmanned bicycle. a only information available considering a critic agent was a system feedback which was interpreted as a last action performed by a controller inside a previous state. a signal produced by a critic agent was used along with a error back propagation to tune (online) conclusion parts of a fuzzy inference rules of a adaptive controller. simulations and experiments are conducted to evaluate a performance of a proposed controller. a results demonstrate superior performance of a developed controller inside terms of improved transient response, robustness to model uncertainty and fast online learning.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
3163,"we define a notion of the smooth pseudo-riemannian algebraic variety $(x,g)$ over the field $k$ of characteristic $0$, which was an algebraic analogue of a notion of riemannian manifold and we study, from the model-theoretic perspective, a algebraic differential equation describing a geodesics on $(x,g)$. when $k$ was a field of real numbers, we prove that if a real points of $x$ are zariski-dense inside $x$ and if a real analytification of $(x,g)$ was the compact riemannian manifold with negative curvature, then a algebraic differential equation describing a geodesics on $(x,g)$ was absolutely irreducible and its generic type was orthogonal to a constants.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7633,"we consider a problem of active linear regression with $\ell_2$-bounded noise. inside this context, a learner receives the set of unlabeled data points, chooses the small subset to receive a labels for, and must give an approximate of a function that performs well on fresh samples. we give an algorithm that was simultaneously optimal inside a number of labeled and unlabeled data points, with $o(d)$ labeled samples; previous work required $\omega(d \log d)$ labeled samples regardless of a number of unlabeled samples. our results also apply to learning linear functions from noisy queries, again achieving optimal sample complexities. a techniques extend beyond linear functions, giving improved sample complexities considering learning a family of $k$-fourier-sparse signals with continuous frequencies.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16835,"we establish connections between a problem of learning the two-layer neural network and tensor decomposition. we consider the model with feature vectors $\boldsymbol x \in \mathbb r^d$, $r$ hidden units with weights $\{\boldsymbol w_i\}_{1\le i \le r}$ and output $y\in \mathbb r$, i.e., $y=\sum_{i=1}^r \sigma( \boldsymbol w_i^{\mathsf t}\boldsymbol x)$, with activation functions given by low-degree polynomials. inside particular, if $\sigma(x) = a_0+a_1x+a_3x^3$, we prove that no polynomial-time learning algorithm should outperform a trivial predictor that assigns to each example a response variable $\mathbb e(y)$, when $d^{3/2}\ll r\ll d^2$. our conclusion holds considering the `natural data distribution', namely standard gaussian feature vectors $\boldsymbol x$, and output distributed according to the two-layer neural network with random isotropic weights, and under the certain complexity-theoretic assumption on tensor decomposition. roughly speaking, we assume that no polynomial-time algorithm should substantially outperform current methods considering tensor decomposition based on a sum-of-squares hierarchy. we also prove generalizations of this statement considering higher degree polynomial activations, and non-random weight vectors. remarkably, several existing algorithms considering learning two-layer networks with rigorous guarantees are based on tensor decomposition. our results support a idea that this was indeed a core computational difficulty inside learning such networks, under a stated generative model considering a data. as the side result, we show that under this model learning a network requires accurate learning of its weights, the property that does not hold inside the more general setting.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6674,"we utilize a hsc camira cluster catalog and a photo-$z$ galaxy catalog constructed inside a hsc wide field (s16a), covering $\sim$ 174 deg$^{2}$, to study a star formation activity of galaxies inside different environments over 0.2 $<$ $z$ $<$ 1.1. we probe galaxies down to $i \sim$ 26, corresponding to the stellar mass limit of log$_{10}$(m$_*$/m$_{\odot}$) $\sim$ 8.2 and $\sim$ 8.6 considering star-forming and quiescent populations, respectively, at $z$ $\sim$ 0.2. a existence of a red sequence considering low stellar mass galaxies inside clusters suggests that a environmental quenching persists to halt a star formation inside a low-mass regime. inside addition, star-forming galaxies inside groups or clusters are systematically biased toward lower values of specific star formation rate by 0.1 -- 0.3 dex with respect to those inside a field and a offsets shows no strong redshift evolution over our redshift range, implying the universal slow quenching mechanism acting inside a dense environments since $z$ $\sim$ 1.1. moreover, a environmental quenching dominates a mass quenching inside low mass galaxies, and a quenching dominance reverses inside high mass ones. a transition mass was greater inside clusters than inside groups, indicating that a environmental quenching was more effective considering massive galaxies inside clusters compared to groups.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19892,"we assess a range of validity of sgoldstino-less inflation inside the scenario of low energy supersymmetry breaking. we first analyze a consistency conditions that an effective theory of a inflaton and goldstino superfields should satisfy inside order to be faithfully described by the sgoldstino-less model. enlarging a scope of previous studies, we investigate a case where a effective field theory cut-off, and thus also a sgoldstino mass, are inflaton-dependent. we then introduce the uv complete model where one should realize successfully sgoldstino-less inflation and gauge mediation of supersymmetry breaking, combining a alpha-attractor mechanism and the weakly coupled model of spontaneous breaking of supersymmetry. inside this class of models we find that, given current limits on superpartner masses, a gravitino mass has the lower bound of a order of a mev, i.e. we cannot reach very low supersymmetry breaking scales. on a plus side, we recognize that inside this framework, one should derive a complete superpartner spectrum as well as compute inflation observables, a reheating temperature, and address a gravitino overabundance problem. we then show that further constraints come from collider results and inflation observables. their non trivial interplay seems the staple feature of phenomenological studies of supersymmetric inflationary models.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15978,"a lack of large realistic datasets presents the bottleneck inside online deception detection studies. inside this paper, we apply the data collection method based on social network analysis to quickly identify high-quality deceptive and truthful online reviews from amazon. a dataset contains more than 10,000 deceptive reviews and was diverse inside product domains and reviewers. with the help of this dataset, we explore effective general features considering online deception detection that perform well across domains. we demonstrate that with generalized features - advertising speak and writing complexity scores - deception detection performance should be further improved by adding additional deceptive reviews from assorted domains inside training. finally, reviewer level evaluation gives an interesting insight into different deceptive reviewers' writing styles.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9771,"this paper investigates a validity of kleinberg's axioms considering clustering functions with respect to a quite popular clustering algorithm called $k$-means. while kleinberg's axioms have been discussed heavily inside a past, we concentrate here on a case predominantly relevant considering $k$-means algorithm, that was behavior embedded inside euclidean space. we point at some contradictions and counter intuitiveness aspects of this axiomatic set within $\mathbb{r}^m$ that were evidently not discussed so far. our results suggest that apparently without defining clearly what kind of clusters we expect we will not be able to construct the valid axiomatic system. inside particular we look at a shape and a gaps between a clusters. finally we demonstrate that there exist several ways to reconcile a formulation of a axioms with their intended meaning and that under this reformulation a axioms stop to be contradictory and a real-world $k$-means algorithm conforms to this axiomatic system.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14849,effects of a structural distortion associated with a $\rm oso_6$ octahedral rotation and tilting on a electronic band structure and magnetic anisotropy energy considering a $5d^3$ compound naoso$_3$ are investigated with the help of a density functional theory (dft) and within the three-orbital model. comparison of a essential features of a dft band structures with a three-orbital model considering both a undistorted and distorted structures provides insight into a orbital and directional asymmetry inside a electron hopping terms resulting from a structural distortion. a orbital mixing terms obtained inside a transformed hopping hamiltonian resulting from a octahedral rotations are shown to account considering a fine features inside a dft band structure. staggered magnetization and a magnetic character of states near a fermi energy indicate weak coupling behavior.,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
15597,"kraus maps (completely positive trace preserving maps) arise classically inside quantum information, as they describe a evolution of noncommutative probability measures. we introduce tropical analogues of kraus maps, obtained by replacing a addition of positive semidefinite matrices by the multivalued supremum with respect to a l√∂wner order. we show that non-linear eigenvectors of tropical kraus maps determine piecewise quadratic approximations of a value functions of switched optimal control problems. this leads to the new approximation method, which we illustrate by two applications: 1) approximating a joint spectral radius, 2) computing approximate solutions of hamilton-jacobi pde arising from the class of switched linear quadratic problems studied previously by mceneaney. we report numerical experiments, indicating the major improvement inside terms of scalability by comparison with earlier numerical schemes, owing to a ""lmi-free"" nature of our method.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
6131,"we consider a content delivery problem inside the fading multi-input single-output channel with cache-aided users. we are interested inside a scalability of a equivalent content delivery rate when a number of users, k, was large. analytical results show that, with the help of coded caching and wireless multicasting, without channel state information at a transmitter (csit), linear scaling of a content delivery rate with respect to k should be achieved inside three different ways. first, with quasi-static fading, it should be achieved when a number of transmit antennas grows logarithmically with k. second, even with the fixed number of antennas, we should achieve a linear scaling with the threshold-based user selection requiring only one-bit feedbacks from a users. third, if a multicast transmission should span over multiple independent sub-channels, e.g., inside block fading or multi-carrier systems, linear scaling should be obtained when a product of a number of sub-channels and a number of transmit antennas scales logarithmically with k. when csit was available, we propose the mixed strategy that combines spatial multiplexing and multicasting. numerical results show that, by optimizing a power split between spatial multiplexing and multicasting, we should achieve the significant gain of a content delivery rate with moderate cache size.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
4860,"recent successful applications of convolutional neural networks (cnns) to audio classification and speech recognition have motivated a search considering better input representations considering more efficient training. visual displays of an audio signal, through various time-frequency representations such as spectrograms offer the rich representation of a temporal and spectral structure of a original signal. inside this letter, we compare various popular signal processing methods to obtain this representation, such as short-time fourier transform (stft) with linear and mel scales, constant-q transform (cqt) and continuous wavelet transform (cwt), and assess their impact on a classification performance of two environmental sound datasets with the help of cnns. this study supports a hypothesis that time-frequency representations are valuable inside learning useful features considering sound classification. moreover, a actual transformation used was shown to impact a classification accuracy, with mel-scaled stft outperforming a other discussed methods slightly and baseline mfcc features to the large degree. additionally, we observe that a optimal window size during transformation was dependent on a characteristics of a audio signal and architecturally, 2d convolution yielded better results inside most cases compared to 1d.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1400,"i describe two novel techniques originally devised to select strongly lensed quasar candidates inside wide-field surveys. a first relies on outlier selection inside optical and mid-infrared magnitude space; a second combines mid-infrared colour selection with gaia spatial resolution, to identify multiplets of objects with quasar-like colours. both methods have already been applied successfully to a sdss, atlas and des footprints: besides recovering known lenses from previous searches, they have led to new discoveries, including quadruply lensed quasars, which are rare within a rare-object class of quasar lenses. as the serendipitous by-product, at least four candidate galactic streams inside a south have been identified among foreground contaminants. there was considerable scope considering tailoring a wise-gaia multiplet search to stellar-like objects, instead of quasar-like, and to automatically detect galactic streams.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9192,we obtain non-trivial lower approximate considering a qoutient of two subsets of a interval.,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
15916,"we investigate thermodynamic phase transitions inside a compass model and inside $e_g$ orbital model on an infinite square lattice by variational tensor network renormalization (vtnr) inside imaginary time. a onset of nematic order inside a quantum compass model was estimated at ${\cal t}_c/j=0.0606(4)$. for~the $e_g$ orbital model one finds: ($i$) the very accurate approximate of ${\cal t}_c/j=0.3566\pm 0.0001$ and ($ii$)~the~critical exponents inside a ising universality class. remarkably large difference inside frustration results inside so distinct values of ${\cal t}_c$, while entanglement influences a quality of ${\cal t}_c$ estimation.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
8108,"group factor analysis (gfa) methods have been widely used to infer a common structure and a group-specific signals from multiple related datasets inside various fields including systems biology and neuroimaging. to date, most available gfa models require gibbs sampling or slice sampling to perform inference, which prevents a practical application of gfa to large-scale data. inside this paper we present an efficient collapsed variational inference (cvi) algorithm considering a nonparametric bayesian group factor analysis (ngfa) model built upon an hierarchical beta bernoulli process. our cvi algorithm proceeds by marginalizing out a group-specific beta process parameters, and then approximating a true posterior inside a collapsed space with the help of mean field methods. experimental results on both synthetic and real-world data demonstrate a effectiveness of our cvi algorithm considering a ngfa compared with state-of-the-art gfa methods.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8352,"experience replay was one of a most commonly used approaches to improve a sample efficiency of reinforcement learning algorithms. inside this work, we propose an idea behind the method to select and replay sequences of transitions inside order to accelerate a learning of the reinforcement learning agent inside an off-policy setting. inside addition to selecting appropriate sequences, we also artificially construct transition sequences with the help of information gathered from previous agent-environment interactions. these sequences, when replayed, allow value function information to trickle down to larger sections of a state/state-action space, thereby making a most of a agent's experience. we demonstrate our idea behind the method on modified versions of standard reinforcement learning tasks such as a mountain car and puddle world problems and empirically show that it enables better learning of value functions as compared to other forms of experience replay. further, we briefly discuss some of a possible extensions to this work, as well as applications and situations where this idea behind the method could be particularly useful.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14357,"this rescaled subset of a alternative data release 1 to a tata institute of fundamental physics giant metrewave radio telescope sky survey (tgss-rsadr1) modifies a initial data release of tgss-adr1 (intema et al. 2017) to bring that catalogue to a same flux scale as a extragalactic catalogue from a galactic and extragalactic all-sky murchison widefield array survey (gleam: wayth et al. 2015; hurley-walker et al. 2017). inside this paper we motivate a derivation of correct and complementary flux density scales, introduce the methodology considering correction based on radial basis functions, apply it to tgss-adr1, and create the modified catalogue, tgss-rsadr1. this catalogue comprises 383,589 tgss-adr1 sources with updated flux density and flux density uncertainty values, and covers $\mathrm{declination}\leq+30^\circ$, $|b|\geq10^\circ$, the sky area of 18,800 deg$^2$.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
2483,"high resolution hubble space telescope (hst) image analysis of a moa-2008-blg-310 microlens system indicates that a excess flux at a location of a source found inside a discovery paper cannot primarily be due to a lens star because it does not match a lens-source relative proper motion, $\mu_{\rm rel}$, predicted by a microlens models. this excess flux was most likely to be due to an unrelated star that happens to be located inside close proximity to a source star. two epochs of hst observations indicate proper motion considering this blend star that was typical of the random bulge star, but was not consistent with the companion to a source or lens stars if a flux was dominated by only one star, aside from a lens. we consider models inside which a excess flux was due to the combination of an unrelated star and a lens star, and this yields 95\% confidence level upper limit on a lens star brightness of $i_l > 22.44$ and $v_l >23.62$. the bayesian analysis with the help of the standard galactic model and these magnitude limits yields the host star mass $m_h = 0.21 ^{+0.21}_{-0.09}~ m_\odot$, the planet mass of $m_p = 23.4 ^{+23.9}_{-9.9}~m_\oplus$ at the projected separation of $a_\perp = 1.12^{+0.16}_{-0.17},$au. this result illustrates excess flux inside the high resolution image of the microlens-source system need not be due to a lens. it was important to check that a lens-source relative proper motion was consistent with a microlensing prediction. a high resolution image analysis techniques developed inside this paper should be used to verify a wfirst exoplanet microlensing survey mass measurements.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7281,"inside this work, we propose the novel robot learning framework called neural task programming (ntp), which bridges a idea of few-shot learning from demonstration and neural program induction. ntp takes as input the task specification (e.g., video demonstration of the task) and recursively decomposes it into finer sub-task specifications. these specifications are fed to the hierarchical neural program, where bottom-level programs are callable subroutines that interact with a environment. we validate our method inside three robot manipulation tasks. ntp achieves strong generalization across sequential tasks that exhibit hierarchal and compositional structures. a experimental results show that ntp learns to generalize well to- wards unseen tasks with increasing lengths, variable topologies, and changing objectives.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
12997,"with a advent of big data, nowadays inside many applications databases containing large quantities of similar time series are available. forecasting time series inside these domains with traditional univariate forecasting procedures leaves great potentials considering producing accurate forecasts untapped. recurrent neural networks (rnns), and inside particular long short-term memory (lstm) networks, have proven recently that they are able to outperform state-of-the-art univariate time series forecasting methods inside this context when trained across all available time series. however, if a time series database was heterogeneous, accuracy may degenerate, so that on a way towards fully automatic forecasting methods inside this space, the notion of similarity between a time series needs to be built into a methods. to this end, we present the prediction model that should be used with different types of rnn models on subgroups of similar time series, which are identified by time series clustering techniques. we assess our proposed methodology with the help of lstm networks, the widely popular rnn variant. our method achieves competitive results on benchmarking datasets under competition evaluation procedures. inside particular, inside terms of mean smape accuracy, it consistently outperforms a baseline lstm model and outperforms all other methods on a cif2016 forecasting competition dataset.",1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5103,"we consider irreducible unitary representations $a_i$ of g=so(n+1,1) with a same infinitesimal character as a trivial representation and representations $b_j$ of h=so(n,1) with a same properties and discuss h-equivariant homomorphisms hom_h($a_i,b_j$). considering tempered representations our results confirm a predictions of conjectures by b. gross and d. prasad.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0
1627,"biological networks are the very convenient modelling and visualisation tool to discover knowledge from modern high-throughput genomics and postgenomics data sets. indeed, biological entities are not isolated, but are components of complex multi-level systems. we go one step further and advocate considering a consideration of causal representations of a interactions inside living systems.we present a causal formalism and bring it out inside a context of biological networks, when a data was observational. we also discuss its ability to decipher a causal information flow as observed inside gene expression. we also illustrate our exploration by experiments on small simulated networks as well as on the real biological data set.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19683,"predictive models of student success inside massive open online courses (moocs) are the critical component of effective content personalization and adaptive interventions. inside this article we review a state of a art inside predictive models of student success inside moocs and present the categorization of mooc research according to a predictors (features), prediction (outcomes), and underlying theoretical model. we critically survey work across each category, providing data on a raw data source, feature engineering, statistical model, evaluation method, prediction architecture, and other aspects of these experiments. such the review was particularly useful given a rapid expansion of predictive modeling research inside moocs since a emergence of major mooc platforms inside 2012. this survey reveals several key methodological gaps, which include extensive filtering of experimental subpopulations, ineffective student model evaluation, and a use of experimental data which would be unavailable considering real-world student success prediction and intervention, which was a ultimate goal of such models. finally, we highlight opportunities considering future research, which include temporal modeling, research bridging predictive and explanatory student models, work which contributes to learning theory, and evaluating long-term learner success inside moocs.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1849,"parts of texas, oklahoma, and kansas have experienced increased rates of seismicity inside recent years, providing new datasets of earthquake recordings to develop ground motion prediction models considering this particular region of a central and eastern north america (cena). this paper outlines the framework considering with the help of artificial neural networks (anns) to develop attenuation models from a ground motion recordings inside this region. while attenuation models exist considering a cena, concerns over a increased rate of seismicity inside this region necessitate investigation of ground motions prediction models particular to these states. to do so, an ann-based framework was proposed to predict peak ground acceleration (pga) and peak ground velocity (pgv) given magnitude, earthquake source-to-site distance, and shear wave velocity. inside this framework, approximately 4,500 ground motions with magnitude greater than 3.0 recorded inside these three states (texas, oklahoma, and kansas) since 2005 are considered. results from this study suggest that existing ground motion prediction models developed considering cena do not accurately predict a ground motion intensity measures considering earthquakes inside this region, especially considering those with low source-to-site distances or on very soft soil conditions. a proposed ann models provide much more accurate prediction of a ground motion intensity measures at all distances and magnitudes. a proposed ann models are also converted to relatively simple mathematical equations so that engineers should easily use them to predict a ground motion intensity measures considering future events. finally, through the sensitivity analysis, a contributions of a predictive parameters to a prediction of a considered intensity measures are investigated.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8464,"let $g$ be the higher rank semisimple linear algebraic group over the non-archimedean local field. a simplicial complexes corresponding to any sequence of pairwise non-conjugate irreducible lattices inside $g$ are benjamini-schramm convergent to a bruhat-tits building. convergence of a relative plancherel measures and normalized betti numbers follows. this extends a work of abert, bergeron, biringer, gelander, nokolov, raimbault and samet from real lie groups to linear groups over arbitrary local fields. along a way, various results concerning invariant random subgroups and inside particular the variant of a classical borel density theorem are also extended.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
3795,"existing automatic music generation approaches that feature deep learning should be broadly classified into two types: raw audio models and symbolic models. symbolic models, which train and generate at a note level, are currently a more prevalent approach; these models should capture long-range dependencies of melodic structure, but fail to grasp a nuances and richness of raw audio generations. raw audio models, such as deepmind's wavenet, train directly on sampled audio waveforms, allowing them to produce realistic-sounding, albeit unstructured music. inside this paper, we propose an automatic music generation methodology combining both of these approaches to create structured, realistic-sounding compositions. we consider the long short term memory network to learn a melodic structure of different styles of music, and then use a unique symbolic generations from this model as the conditioning input to the wavenet-based raw audio generator, creating the model considering automatic, novel music. we then evaluate this idea behind the method by showcasing results of this work.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7350,"inside this article we propose the geometric description of arthur packets considering $p$-adic groups with the help of vanishing cycles of perverse sheaves. our idea behind the method was inspired by a 1992 book by adams, barbasch and vogan on a langlands classification of admissible representations of real groups and follows a direction indicated by vogan inside his 1993 paper on a langlands correspondence. with the help of vanishing cycles, we introduce and study the functor from a category of equivariant perverse sheaves on a moduli space of certain langlands parameters to local systems on a regular part of a conormal bundle considering this variety. inside this article we establish a main properties of this functor and show that it plays a role of microlocalization inside a work of adams, barbasch and vogan. we use this to define abv-packets considering pure rational forms of $p$-adic groups and propose the geometric description of a transfer coefficients that appear inside arthur's main local result inside a endoscopic classification of representations. this article includes conjectures modelled on vogan's work, including a prediction that arthur packets are abv-packets considering $p$-adic groups. we gather evidence considering these conjectures by verifying them inside numerous examples.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0
16582,"inside this paper we study nonparametric mean curvature type flows inside $m\times\mathbb{r}$ which are represented as graphs $(x,u(x,t))$ over the domain inside the riemannian manifold $m$ with prescribed contact angle. a speed of $u$ was a mean curvature speed minus an admissible function $\psi(x,u,du)$. long time existence and uniformly convergence are established if $\psi(x,u, du)\equiv 0$ with vertical contact angle and $\psi(x,u,du)=h(x,u)\omega$ with $h_u(x,u)\geq h_0>0$ and $\omega=\sqrt{1+|du|^2}$. their applications include mean curvature type equations with prescribed contact angle boundary condition and a asymptotic behavior of nonparametric mean curvature flows of graphs over the convex domain inside $m^2$ which was the surface with nonnegative ricci curvature.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
642,"among a myriad of desirable properties discussed inside a context of forgetting inside answer set programming (asp), strong persistence naturally captures its essence. recently, it has been shown that it was not always possible to forget the set of atoms from the program while obeying this property, and the precise criterion regarding what should be forgotten has been presented, accompanied by the class of forgetting operators that return a correct result when forgetting was possible. however, it was an open question what to do when we have to forget the set of atoms, but cannot without violating this property. inside this paper, we address this issue and investigate three natural alternatives to forget when forgetting without violating strong persistence was not possible, which turn out to correspond to a different possible relaxations of a characterization of strong persistence. additionally, we discuss their preferable usage, shed light on a relation between forgetting and notions of relativized equivalence established earlier inside a context of asp, and present the detailed study on their computational complexity.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11115,"we consider how to connect the set of disjoint networks to optimize a performance of a resulting composite network. we quantify this performance by a coherence of a composite network, which was defined by an $h_2$ norm of a system. two dynamics are considered: noisy consensus dynamics with and without stubborn agents. considering noisy consensus dynamics without stubborn agents, we derive analytical expressions considering a coherence of composite networks inside terms of a coherence of a individual networks and a structure of their interconnections. we also identify optimal interconnection topologies and give bounds on coherence considering general composite graphs. considering noisy consensus dynamics with stubborn agents, we develop the non-combinatorial algorithm that identifies connecting edges such that a composite network coherence closely approximates a performance of a optimal composite graph.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
19216,"recidivism prediction instruments (rpi's) provide decision makers with an assessment of a likelihood that the criminal defendant will reoffend at the future point inside time. while such instruments are gaining increasing popularity across a country, their use was attracting tremendous controversy. much of a controversy concerns potential discriminatory bias inside a risk assessments that are produced. this paper discusses several fairness criteria that have recently been applied to assess a fairness of recidivism prediction instruments. we demonstrate that a criteria cannot all be simultaneously satisfied when recidivism prevalence differs across groups. we then show how disparate impact should arise when the recidivism prediction instrument fails to satisfy a criterion of error rate balance.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11605,"the large variety of dynamical systems, such as chemical and biomolecular systems, should be seen as networks of nonlinear entities. prediction, control, and identification of such nonlinear networks require knowledge of a state of a system. however, network states are usually unknown, and only the fraction of a state variables are directly measurable. a observability problem concerns reconstructing a network state from this limited information. here, we propose the general optimization-based idea behind the method considering observing a states of nonlinear networks and considering optimally selecting a observed variables. our results reveal several fundamental limitations inside network observability, such as a trade-off between a fraction of observed variables and a observation length on one side, and a approximation error on a other side. we also show that owing to a crucial role played by a dynamics, purely graph- theoretic observability approaches cannot provide conclusions about one's practical ability to approximate a states. we demonstrate a effectiveness of our methods by finding a key components inside biological and combustion reaction networks from which we determine a full system state. our results should lead to a design of novel sensing principles that should greatly advance prediction and control of a dynamics of such networks.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
16926,"we present the new method of generating mixture models considering data with categorical attributes. a keys to this idea behind the method are an entropy-based density metric inside categorical space and annealing of high-entropy/low-density components from an initial state with many components. pruning of low-density components with the help of a entropy-based density allows galileo to consistently find high-quality clusters and a same optimal number of clusters. galileo has shown promising results on the range of test datasets commonly used considering categorical clustering benchmarks. we demonstrate that a scaling of galileo was linear inside a number of records inside a dataset, making this method suitable considering very large categorical datasets.",1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3986,"we revisit a \emph{leaderboard problem} introduced by blum and hardt (2015) inside an effort to reduce overfitting inside machine learning benchmarks. we show that the randomized version of their ladder algorithm achieves leaderboard error o(1/n^{0.4}) compared with a previous best rate of o(1/n^{1/3}). short of proving that our algorithm was optimal, we point out the major obstacle toward further progress. specifically, any improvement to our upper bound would lead to asymptotic improvements inside a general adaptive approximation setting as have remained elusive inside recent years. this connection also directly leads to lower bounds considering specific classes of algorithms. inside particular, we exhibit the new attack on a leaderboard algorithm that both theoretically and empirically distinguishes between our algorithm and previous leaderboard algorithms.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6094,"a goal of language modeling techniques was to capture a statistical and structural properties of natural languages from training corpora. this task typically involves a learning of short range dependencies, which generally model a syntactic properties of the language and/or long range dependencies, which are semantic inside nature. we propose inside this paper the new multi-span architecture, which separately models a short and long context information while it dynamically merges them to perform a language modeling task. this was done through the novel recurrent long-short range context (lsrc) network, which explicitly models a local (short) and global (long) context with the help of two separate hidden states that evolve inside time. this new architecture was an adaptation of a long-short term memory network (lstm) to take into account a linguistic properties. extensive experiments conducted on a penn treebank (ptb) and a large text compression benchmark (ltcb) corpus showed the significant reduction of a perplexity when compared to state-of-the-art language modeling techniques.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
315,"climate projections continue to be marred by large uncertainties, which originate inside processes that need to be parameterized, such as clouds, convection, and ecosystems. but rapid progress was now within reach. new computational tools and methods from data assimilation and machine learning make it possible to integrate global observations and local high-resolution simulations inside an earth system model (esm) that systematically learns from both. here we propose the blueprint considering such an esm. we outline how parameterization schemes should learn from global observations and targeted high-resolution simulations, considering example, of clouds and convection, through matching low-order statistics between esms, observations, and high-resolution simulations. we illustrate learning algorithms considering esms with the simple dynamical system that shares characteristics of a climate system; and we discuss a opportunities a proposed framework presents and a challenges that remain to realize it.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7957,"supply-demand systems inside smart city sectors such as energy, transportation, telecommunication, are subject of unprecedented technological transformations by a internet of things. usually, supply-demand systems involve actors that produce and consume resources, e.g. energy, and they are regulated such that supply meets demand, or demand meets available supply. mismatches of supply and demand may increase operational costs, should cause catastrophic damage inside infrastructure, considering instance power blackouts, and may even lead to social unrest and security threats. long-term, operationally offline and top-down regulatory decision-making by governmental officers, policy makers or system operators may turn out to be ineffective considering matching supply-demand under new dynamics and opportunities that internet of things technologies bring to supply-demand systems, considering instance, interactive cyber-physical systems and software agents running locally inside physical assets to monitor and apply automated control actions inside real-time. e.g. power flow redistributions by smart transformers to improve a smart grid reliability. existing work on online regulatory mechanisms of matching supply-demand either focuses on game-theoretic solutions with assumptions that cannot be easily met inside real-world systems or assume centralized management entities and local access to global information. this paper contributes the generic decentralized self-regulatory framework, which, inside contrast to related work, was shaped around standardized control system concepts and internet of things technologies considering an easier adoption and applicability. a framework involves the decentralized combinatorial optimization mechanism that matches supply-demand under different regulatory scenarios.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
7029,"we formulate a problem of sampling and recovering clustered graph signal as the multi-armed bandit (mab) problem. this formulation lends naturally to learning sampling strategies with the help of a well-known gradient mab algorithm. inside particular, a sampling strategy was represented as the probability distribution over a individual arms of a mab and optimized with the help of gradient ascent. some illustrative numerical experiments indicate that a sampling strategies based on a gradient mab algorithm outperform existing sampling methods.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
12054,"given the complete hypersurface isometrically immersed inside an ambient manifold, inside this paper we provide the lower bound considering a norm of a mean curvature vector field of a immersion assuming that: 1) a ambient manifold admits the killing submersion with unit-length killing vector field. 2)the projection of a image of a immersion was bounded inside a base manifold. 3)the hypersurface was stochastically complete, or a immersion was proper.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19299,"sparse representation of the single measurement vector (smv) has been explored inside the variety of compressive sensing applications. recently, smv models have been extended to solve multiple measurement vectors (mmv) problems, where a underlying signal was assumed to have joint sparse structures. to circumvent a np-hardness of a $\ell_0$ minimization problem, many deterministic mmv algorithms solve a convex relaxed models with limited efficiency. inside this paper, we develop stochastic greedy algorithms considering solving a joint sparse mmv reconstruction problem. inside particular, we propose a mmv stochastic iterative hard thresholding (mstoiht) and mmv stochastic gradient matching pursuit (mstogradmp) algorithms, and we also utilize a mini-batching technique to further improve their performance. convergence analysis indicates that a proposed algorithms are able to converge faster than their smv counterparts, i.e., concatenated stoiht and stogradmp, under certain conditions. numerical experiments have illustrated a superior effectiveness of a proposed algorithms over their smv counterparts.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
932,"a application of deep neural networks considering ranking inside search engines may obviate a need considering a extensive feature engineering common to current learning-to-rank methods. however, we show that combining simple relevance matching features like bm25 with existing deep neural net models often substantially improves a accuracy of these models, indicating that they do not capture essential local relevance matching signals. we describe the novel deep recurrent neural net-based model that we call match-tensor. a architecture of a match-tensor model simultaneously accounts considering both local relevance matching and global topicality signals allowing considering the rich interplay between them when computing a relevance of the document to the query. on the large held-out test set consisting of social media documents, we demonstrate not only that match-tensor outperforms bm25 and other classes of dnns but also that it largely subsumes signals present inside these models.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13992,"theoretical predictions of pressure-induced phase transformations often become long-standing enigmas because of limitations of contemporary available experimental possibilities. hitherto a existence of the non-icosahedral boron allotrope has been one of them. here we report on a first non-icosahedral boron allotrope, which we denoted as {\zeta}-b, with a orthorhombic {\alpha}-ga-type structure (space group cmce) synthesized inside the diamond anvil cell at extreme high-pressure high-temperature conditions (115 gpa and 2100 k). a structure of {\zeta}-b is solved with the help of single-crystal synchrotron x-ray diffraction and its compressional behavior is studied inside a range of very high pressures (115 gpa to 135 gpa). experimental validation of theoretical predictions reveals a degree of our up-to-date comprehension of condensed matter and promotes further development of a solid state physics and chemistry.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
102,"inside this paper, we investigate whether text from the community question answering (qa) platform should be used to predict and describe real-world attributes. we experiment with predicting the wide range of 62 demographic attributes considering neighbourhoods of london. we use a text from qa platform of yahoo! answers and compare our results to a ones obtained from twitter microblogs. outcomes show that a correlation between a predicted demographic attributes with the help of text from yahoo! answers discussions and a observed demographic attributes should reach an average pearson correlation coefficient of \r{ho} = 0.54, slightly higher than a predictions obtained with the help of twitter data. our qualitative analysis indicates that there was semantic relatedness between a highest correlated terms extracted from both datasets and their relative demographic attributes. furthermore, a correlations highlight a different natures of a information contained inside yahoo! answers and twitter. while a former seems to offer the more encyclopedic content, a latter provides information related to a current sociocultural aspects or phenomena.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
14845,"we consider a defocusing nonlinear wave equations (nlw) on a two-dimensional torus. inside particular, we construct invariant gibbs measures considering a renormalized so-called wick ordered nlw. we then prove weak universality of a wick ordered nlw, showing that a wick ordered nlw naturally appears as the suitable scaling limit of non-renormalized nlw with gaussian random initial data.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3874,"catastrophic forgetting was the problem of neural networks that loses a information of a first task after training a second task. here, we propose the method, i.e. incremental moment matching (imm), to resolve this problem. imm incrementally matches a moment of a posterior distribution of a neural network which was trained on a first and a second task, respectively. to make a search space of posterior parameter smooth, a imm procedure was complemented by various transfer learning techniques including weight transfer, l2-norm of a old and a new parameter, and the variant of dropout with a old parameter. we analyze our idea behind the method on the variety of datasets including a mnist, cifar-10, caltech-ucsd-birds, and lifelog datasets. a experimental results show that imm achieves state-of-the-art performance by balancing a information between an old and the new network.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
10858,"constructing simpler models, either stochastic or deterministic, considering exploring a phenomenon of flow reversals inside fluid systems was inside vogue across disciplines. with the help of direct numerical simulations and nonlinear time series analysis, we illustrate that a basic nature of flow reversals inside convecting fluids should depend on a dimensionless parameters describing a system. specifically, we find evidence of low-dimensional determinism inside flow reversals occurring at zero prandtl number, whereas we fail to find such signatures considering reversals at infinite prandtl number. thus, even inside the single system, as one varies a system parameters, one should encounter reversals that are fundamentally different inside nature. consequently, we conclude that the single general low-dimensional deterministic model cannot faithfully characterize flow reversals considering every set of parameter values.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7394,"inside this paper, we introduce the new sparsity-promoting prior, namely, a ""normal product"" prior, and develop an efficient algorithm considering sparse signal recovery under a bayesian framework. a normal product distribution was a distribution of the product of two normally distributed variables with zero means and possibly different variances. like other sparsity-encouraging distributions such as a student's $t$-distribution, a normal product distribution has the sharp peak at origin, which makes it the suitable prior to encourage sparse solutions. the two-stage normal product-based hierarchical model was proposed. we resort to a variational bayesian (vb) method to perform a inference. simulations are conducted to illustrate a effectiveness of our proposed algorithm as compared with other state-of-the-art compressed sensing algorithms.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7105,"person re-identification (re-id) aims at matching images of a same person across disjoint camera views, which was the challenging problem inside multimedia analysis, multimedia editing and content-based media retrieval communities. a major challenge lies inside how to preserve similarity of a same person across video footages with large appearance variations, while discriminating different individuals. to address this problem, conventional methods usually consider a pairwise similarity between persons by only measuring a point to point (p2p) distance. inside this paper, we propose to use deep learning technique to model the novel set to set (s2s) distance, inside which a underline objective focuses on preserving a compactness of intra-class samples considering each camera view, while maximizing a margin between a intra-class set and inter-class set. a s2s distance metric was consisted of three terms, namely a class-identity term, a relative distance term and a regularization term. a class-identity term keeps a intra-class samples within each camera view gathering together, a relative distance term maximizes a distance between a intra-class class set and inter-class set across different camera views, and a regularization term smoothness a parameters of deep convolutional neural network (cnn). as the result, a final learned deep model should effectively find out a matched target to a probe object among various candidates inside a video gallery by learning discriminative and stable feature representations. with the help of a cuhk01, cuhk03, prid2011 and market1501 benchmark datasets, we extensively conducted comparative evaluations to demonstrate a advantages of our method over a state-of-the-art approaches.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7408,"inside a encoder-decoder architecture considering neural machine translation (nmt), a hidden states of a recurrent structures inside a encoder and decoder carry a crucial information about a sentence.these vectors are generated by parameters which are updated by back-propagation of translation errors through time. we argue that propagating errors through a end-to-end recurrent structures are not the direct way of control a hidden vectors. inside this paper, we propose to use word predictions as the mechanism considering direct supervision. more specifically, we require these vectors to be able to predict a vocabulary inside target sentence. our simple mechanism ensures better representations inside a encoder and decoder without with the help of any extra data or annotation. it was also helpful inside reducing a target side vocabulary and improving a decoding efficiency. experiments on chinese-english and german-english machine translation tasks show bleu improvements by 4.53 and 1.3, respectively",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13474,"an objective bayesian idea behind the method to approximate a number of degrees of freedom $(\nu)$ considering a multivariate $t$ distribution and considering a $t$-copula, when a parameter was considered discrete, was proposed. inference on this parameter has been problematic considering a multivariate $t$ and, considering a absence of any method, considering a $t$-copula. an objective criterion based on loss functions which allows to overcome a issue of defining objective probabilities directly was employed. a support of a prior considering $\nu$ was truncated, which derives from a property of both a multivariate $t$ and a $t$-copula of convergence to normality considering the sufficiently large number of degrees of freedom. a performance of a priors was tested on simulated scenarios. a r codes and a replication material are available as the supplementary material of a electronic version of a paper and on real data: daily logarithmic returns of ibm and of a center considering research inside security prices database.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
4675,"clinical measurements that should be represented as time series constitute an important fraction of a electronic health records and are often both uncertain and incomplete. recurrent neural networks are the special class of neural networks that are particularly suitable to process time series data but, inside their original formulation, cannot explicitly deal with missing data. inside this paper, we explore imputation strategies considering handling missing values inside classifiers based on recurrent neural network (rnn) and apply the recently proposed recurrent architecture, a gated recurrent unit with decay, specifically designed to handle missing data. we focus on a problem of detecting surgical site infection inside patients by analyzing time series of their blood sample measurements and we compare a results obtained with different rnn-based classifiers.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15906,"artificial intelligence federates numerous scientific fields inside a aim of developing machines able to assist human operators performing complex treatments -- most of which demand high cognitive skills (e.g. learning or decision processes). central to this quest was to give machines a ability to approximate a likeness or similarity between things inside a way human beings approximate a similarity between stimuli. inside this context, this book focuses on semantic measures: approaches designed considering comparing semantic entities such as units of language, e.g. words, sentences, or concepts and instances defined into knowledge bases. a aim of these measures was to assess a similarity or relatedness of such semantic entities by taking into account their semantics, i.e. their meaning -- intuitively, a words tea and coffee, which both refer to stimulating beverage, will be estimated to be more semantically similar than a words toffee (confection) and coffee, despite that a last pair has the higher syntactic similarity. a two state-of-the-art approaches considering estimating and quantifying semantic similarities/relatedness of semantic entities are presented inside detail: a first one relies on corpora analysis and was based on natural language processing techniques and semantic models while a second was based on more or less formal, computer-readable and workable forms of knowledge such as semantic networks, thesaurus or ontologies. (...) beyond the simple inventory and categorization of existing measures, a aim of this monograph was to convey novices as well as researchers of these domains towards the better understanding of semantic similarity approximation and more generally semantic measures.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13420,"we examine a growth and evolution of quenched galaxies inside a mufasa cosmological hydrodynamic simulations that include an evolving halo mass-based quenching prescription, with galaxy colours computed accounting considering line-of-sight extinction to individual star particles. mufasa reproduces a observed present-day red sequence reasonably well, including its slope, amplitude, and scatter. inside mufasa, a red sequence slope was driven entirely by a steep stellar mass-stellar metallicity relation, which independently agrees with observations. high-mass star-forming galaxies blend smoothly onto a red sequence, indicating a lack of the well-defined green valley at m*>10^10.5 mo. a most massive galaxies quench a earliest and then grow very little inside mass using dry merging; they attain their high masses at earlier epochs when cold inflows more effectively penetrate hot halos. to higher redshifts, a red sequence becomes increasingly contaminated with massive dusty star-forming galaxies; uvj selection subtly but effectively separates these populations. we then examine a evolution of a mass functions of central and satellite galaxies split into passive and star-forming using uvj. massive quenched systems show good agreement with observations out to z~2, despite not including the rapid early quenching mode associated with mergers. however, low-mass quenched galaxies are far too numerous at z<1 inside mufasa, indicating that mufasa strongly over-quenches satellites. the challenge considering hydrodynamic simulations was to devise the quenching model that produces enough early massive quenched galaxies and keeps them quenched to z=0, while not being so strong as to over-quench satellites; mufasa's current scheme fails at a latter.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6107,"detecting test samples drawn sufficiently far away from a training distribution statistically or adversarially was the fundamental requirement considering deploying the good classifier inside many real-world machine learning applications. however, deep neural networks with a softmax classifier are known to produce highly overconfident posterior distributions even considering such abnormal samples. inside this paper, we propose the simple yet effective method considering detecting any abnormal samples, which was applicable to any pre-trained softmax neural classifier. we obtain a class conditional gaussian distributions with respect to (low- and upper-level) features of a deep models under gaussian discriminant analysis, which result inside the confidence score based on a mahalanobis distance. while most prior methods have been evaluated considering detecting either out-of-distribution or adversarial samples, but not both, a proposed method achieves a state-of-the-art performances considering both cases inside our experiments. moreover, we found that our proposed method was more robust inside harsh cases, e.g., when a training dataset has noisy labels or small number of samples. finally, we show that a proposed method enjoys broader usage by applying it to class-incremental learning: whenever out-of-distribution samples are detected, our classification rule should incorporate new classes well without further training deep models.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14800,"the hallmark of human intelligence was a ability to ask rich, creative, and revealing questions. here we introduce the cognitive model capable of constructing human-like questions. our idea behind the method treats questions as formal programs that, when executed on a state of a world, output an answer. a model specifies the probability distribution over the complex, compositional space of programs, favoring concise programs that aid a agent learn inside a current context. we evaluate our idea behind the method by modeling a types of open-ended questions generated by humans who were attempting to learn about an ambiguous situation inside the game. we find that our model predicts what questions people will ask, and should creatively produce novel questions that were not present inside a training set. inside addition, we compare the number of model variants, finding that both question informativeness and complexity are important considering producing human-like questions.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3071,"warehouse logistics robots will work inside different warehouse environments. inside order to enable robots to perceive environment and plan path faster without modifying existing warehouses, we uses monocular camera to achieve an efficient robot integrated system. mapping and path planning a two main tasks presented inside this paper. a direct method visual odometry was applied to localize, and a 3d position of major obstacles inside a environment was calculated. we describe a terrain with occupied grid map, a 3d points are projected onto a robot motion plane, thus accessibility of each grid was determined. based on a terrain information, a optimized a* algorithm was used considering path planning. finally, according to localization and planning, we control a robot to track path. we also develop the path-tracking robot prototype. simulation and experimental results verify a effectiveness and reliability of a proposed method.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
13202,"inside this paper, we gives the complete classification of a global dynamics of two- species lotka-volterra competition models with nonlocal dispersals: where k, p represent nonlocal operators, under a assumptions that a nonlo- cal operators are symmetric, a models admit two semi-trivial steady states and 0<bc<1. inside particular, when both semi-trivial steady states are locally stable, it was proved that there exist infinitely many steady states and a solution with non- negative and nontrivial initial data converges to some steady state. furthermore, we generalize these results to a case that competition coefficients are location-dependent and dispersal strategies are mixture of local and nonlocal dispersals.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15494,"temporal difference learning and residual gradient methods are a most widely used temporal difference based learning algorithms; however, it has been shown that none of their objective functions was optimal w.r.t approximating a true value function $v$. two novel algorithms are proposed to approximate a true value function $v$. this paper makes a following contributions: (1) the batch algorithm that should aid find a approximate optimal off-policy prediction of a true value function $v$. (2) the linear computational cost (per step) near-optimal algorithm that should learn from the collection of off-policy samples. (3) the new perspective of a emphatic temporal difference learning which bridges a gap between off-policy optimality and off-policy stability.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7482,"spin patterns of spiral galaxies should be broadly separated into galaxies with clockwise (z-wise) patterns and galaxies with counterclockwise (s-wise) spin patterns. while a differences between these patterns are visually noticeable, they are the matter of a perspective of a observer, and therefore inside the sufficiently large universe no other differences are expected between galaxies with z-wise and s-wise patterns. here large datasets of spiral galaxies separated by their spin patterns are used to show that spiral galaxies with z-wise spin patterns are photometrically different from spiral galaxies with s-wise patterns. that asymmetry changes based on a direction of observation, such that a observed asymmetry inside one hemisphere was aligned with a inverse observed asymmetry inside a opposite hemisphere. a results are consistent across different sky surveys (sdss and panstarrs) and analysis methods. a proximity of a most probable asymmetry axis to a galactic pole suggests that a asymmetry might be driven by relativistic beaming. annotated data from sdss and panstarrs are publicly available.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3052,"how does the person work out their location with the help of the floorplan? it was probably safe to say that we do not explicitly measure depths to every visible surface and try to match them against different pose estimates inside a floorplan. and yet, this was exactly how most robotic scan-matching algorithms operate. similarly, we do not extrude a 2d geometry present inside a floorplan into 3d and try to align it to a real-world. and yet, this was how most vision-based approaches localise. humans do a exact opposite. instead of depth, we use high level semantic cues. instead of extruding a floorplan up into a third dimension, we collapse a 3d world into the 2d representation. evidence of this was that many of a floorplans we use inside everyday life are not accurate, opting instead considering high levels of discriminative landmarks. inside this work, we use this insight to present the global localisation idea behind the method that relies solely on a semantic labels present inside a floorplan and extracted from rgb images. while our idea behind the method was able to use range measurements if available, we demonstrate that they are unnecessary as we should achieve results comparable to state-of-the-art without them.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
11343,"while jeffreys priors usually are well-defined considering a parameters of mixtures of distributions, they are not available inside closed form. furthermore, they often are improper priors. hence, they have never been used to draw inference on a mixture parameters. a implementation and a properties of jeffreys priors inside several mixture settings are studied. it was shown that a associated posterior distributions most often are improper. nevertheless, a jeffreys prior considering a mixture weights conditionally on a parameters of a mixture components will be shown to have a property of conservativeness with respect to a number of components, inside case of overfitted mixture and it should be therefore used as the default priors inside this context.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
16581,"currently, there starts the research trend to leverage neural architecture considering recommendation systems. though several deep recommender models are proposed, most methods are too simple to characterize users' complex preference. inside this paper, considering the fine-grain analysis, users' ratings are explained from multiple perspectives, based on which, we propose our neural architecture. specifically, our model employs several sequential stages to encode a user and item into hidden representations. inside one stage, a user and item are represented from multiple perspectives and inside each perspective, a representations of user and item put attentions to each other. last, we metric a output representations of final stage to idea behind the method a users' rating. extensive experiments demonstrate that our method achieves substantial improvements against baselines.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19847,"we carry out simulations of gravitationally unstable disks with the help of smoothed particle hydrodynamics(sph) and a novel lagrangian meshless finite mass (mfm) scheme inside a gizmo code (hopkins 2015). our aim was to understand a cause of a non-convergence of a cooling boundary considering fragmentation reported inside a literature. we run sph simulations with two different artificial viscosity implementations, and compare them with mfm, which does not employ any artificial viscosity. with mfm we demonstrate convergence of a critical cooling time scale considering fragmentation at \beta_{crit} =3.. non-convergence persists inside sph codes, although it was significantly mitigated with schemes having reduced artificial viscosity such as inviscid sph (isph) (cullen & dehnen 2010). we show how a non-convergence problem was caused by artificial fragmentation triggered by excessive dissipation of angular momentum inside domains with large velocity derivatives. with increased resolution such domains become more prominent. vorticity lags behind density due to numerical viscous dissipation inside these regions, promoting collapse with longer cooling times. such effect was shown to be dominant over a competing tendency of artificial viscosity to diminish with increasing resolution. when a initial conditions are first relaxed considering several orbits, a flow was more regular, with lower shear and vorticity inside non-axisymmetric regions, aiding convergence. yet mfm was a only method that converges exactly. our findings are of general interest as numerical dissipation using artificial viscosity or advection errors should also occur inside grid-based codes. indeed considering a fargo code values of \beta_{crit} significantly higher than our converged approximate have been reported inside a literature. finally, we discuss implications considering giant planet formation using disk instability.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
617,"we study a problem of approximating a largest root of the real-rooted polynomial of degree $n$ with the help of its top $k$ coefficients and give nearly matching upper and lower bounds. we present algorithms with running time polynomial inside $k$ that use a top $k$ coefficients to approximate a maximum root within the factor of $n^{1/k}$ and $1+o(\tfrac{\log n}{k})^2$ when $k\leq \log n$ and $k>\log n$ respectively. we also prove corresponding information-theoretic lower bounds of $n^{\omega(1/k)}$ and $1+\omega\left(\frac{\log \frac{2n}{k}}{k}\right)^2$, and show strong lower bounds considering noisy version of a problem inside which one was given access to approximate coefficients. this problem has applications inside a context of a method of interlacing families of polynomials, which is used considering proving a existence of ramanujan graphs of all degrees, a solution of a kadison-singer problem, and bounding a integrality gap of a asymmetric traveling salesman problem. all of these involve computing a maximum root of certain real-rooted polynomials considering which a top few coefficients are accessible inside subexponential time. our results yield an algorithm with a running time of $2^{\tilde o(\sqrt[3]n)}$ considering all of them.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2003,"inside this paper we study a variance of a euler totient function (normalized to $\varphi(n)/n$) inside a integers $\mathbb{z}$ and inside a polynomial ring $\mathbb{f}_q[t]$ over the finite field $\mathbb{f}_q$. it turns out that inside $\mathbb{z}$, under some assumptions, a variance of a normalized euler function becomes constant. this was supported by several numerical simulations. surprisingly, inside $\mathbb{f}_q[t]$, $q\rightarrow \infty$, a analogue does not hold: due to the high amount of cancellation, a variance becomes inversely proportional to a size of a interval.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
13737,"the first step to reach theory of mind (tom) abilities (attribution of beliefs to others) inside synthetic agents through sensorimotor interactions, would be to tag sensory data with agent typology and action intentions: autonomous agent x moved an object under a box. we propose the dual arm robotic setup inside which tom could be probed. we then discuss what measures should be extracted from sensorimotor interaction data (based on the correlation analysis) inside a proposed setup that allow to distinguish self than other and other/inanimate from other/active with intentions. we finally discuss what elements are missing inside current cognitive architectures to be able to acquire tom abilities inside synthetic agents from sensorimotor interactions, bottom-up from reactive agent interaction behaviors and top-down from a optimization of social behaviour and cooperation.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
2383,"inside this paper we present the method considering a unsupervised clustering of high-dimensional binary data, with the special focus on electronic healthcare records. we present the robust and efficient heuristic to face this problem with the help of tensor decomposition. we present a reasons why this idea behind the method was preferable considering tasks such as clustering patient records, to more commonly used distance-based methods. we run a algorithm on two datasets of healthcare records, obtaining clinically meaningful results.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15763,"from longitudinal biomedical studies to social networks, graphs have emerged as the powerful framework considering describing evolving interactions between agents inside complex systems. inside such studies, after pre-processing, a data should be represented by the set of graphs, each representing the system's state at different points inside time. a analysis of a system's dynamics depends on a selection of a appropriate analytical tools. after characterizing similarities between states, the critical step lies inside a choice of the distance between graphs capable of reflecting such similarities. while a literature offers the number of distances that one could the priori choose from, their properties have been little investigated and no guidelines regarding a choice of such the distance have yet been provided. inside particular, most graph distances consider that a nodes are exchangeable and do not take into account node identities. accounting considering a alignment of a graphs enables us to enhance these distances' sensitivity to perturbations inside a network and detect important changes inside graph dynamics. thus a selection of an adequate metric was the decisive --yet delicate--practical matter. inside a spirit of goldenberg, zheng and fienberg's seminal 2009 review, a purpose of this article was to provide an overview of commonly-used graph distances and an explicit characterization of a structural changes that they are best able to capture. we use as the guiding thread to our discussion a application of these distances to a analysis of both the longitudinal microbiome dataset and the brain fmri study. we show examples of with the help of permutation tests to detect a effect of covariates on a graphs' variability. synthetic examples provide intuition as to a qualities and drawbacks of a different distances. above all, we provide some guidance considering choosing one distance over another inside certain types of applications.",1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
18881,"the new generative adversarial network was developed considering joint distribution matching. distinct from most existing approaches, that only learn conditional distributions, a proposed model aims to learn the joint distribution of multiple random variables (domains). this was achieved by learning to sample from conditional distributions between a domains, while simultaneously learning to sample from a marginals of each individual domain. a proposed framework consists of multiple generators and the single softmax-based critic, all jointly trained using adversarial learning. from the simple noise source, a proposed framework allows synthesis of draws from a marginals, conditional draws given observations from the subset of random variables, or complete draws from a full joint distribution. most examples considered are considering joint analysis of two domains, with examples considering three domains also presented.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19503,"with the help of the mean-field network formulation of a bass innovation diffusion model and exact results by fotouhi and rabbat on a degree correlations of barabasi-albert networks, we compute a times of a diffusion peak and compare them with those on scale-free networks which have a same scale-free exponent but different assortativity properties. we compare our results with those obtained by caldarelli et al. considering a sis epidemic model with a spectral method applied to adjacency matrices. it turns out that diffusion times on finite barabasi-albert networks are at the minimum. this may be due to the little-known property of these networks: although a value of a assortativity coefficient was close to zero, they look disassortative if one considers only the bounded range of degrees, including a smallest ones, and slightly assortative on a range of a higher degrees. we also find that if a trickle-down character of a diffusion process was enhanced by the larger initial stimulus on a hubs (via the inhomogeneous linear term inside a bass model), a relative difference between a diffusion times considering ba networks and uncorrelated networks was even larger, reaching considering instance a 34% inside the typical case on the network with $10^4$ nodes.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
1297,"3d face reconstruction was the fundamental computer vision problem of extraordinary difficulty. current systems often assume a availability of multiple facial images (sometimes from a same subject) as input, and must address the number of methodological challenges such as establishing dense correspondences across large facial poses, expressions, and non-uniform illumination. inside general these methods require complex and inefficient pipelines considering model building and fitting. inside this work, we propose to address many of these limitations by training the convolutional neural network (cnn) on an appropriate dataset consisting of 2d images and 3d facial models or scans. our cnn works with just the single 2d facial image, does not require accurate alignment nor establishes dense correspondence between images, works considering arbitrary facial poses and expressions, and should be used to reconstruct a whole 3d facial geometry (including a non-visible parts of a face) bypassing a construction (during training) and fitting (during testing) of the 3d morphable model. we achieve this using the simple cnn architecture that performs direct regression of the volumetric representation of a 3d facial geometry from the single 2d image. we also demonstrate how a related task of facial landmark localization should be incorporated into a proposed framework and aid improve reconstruction quality, especially considering a cases of large poses and facial expressions. testing code will be made available online, along with pre-trained models this http url",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2380,"to manage and maintain large-scale cellular networks, operators need to know which sectors underperform at any given time. considering this purpose, they use a so-called hot spot score, which was a result of the combination of multiple network measurements and reflects a instantaneous overall performance of individual sectors. while operators have the good understanding of a current performance of the network and its overall trend, forecasting a performance of each sector over time was the challenging task, as it was affected by both regular and non-regular events, triggered by human behavior and hardware failures. inside this paper, we study a spatio-temporal patterns of a hot spot score and uncover its regularities. based on our observations, we then explore a possibility to use recent measurements' history to predict future hot spots. to this end, we consider tree-based machine learning models, and study their performance as the function of time, amount of past data, and prediction horizon. our results indicate that, compared to a best baseline, tree-based models should deliver up to 14% better forecasts considering regular hot spots and 153% better forecasts considering non-regular hot spots. a latter brings strong evidence that, considering moderate horizons, forecasts should be made even considering sectors exhibiting isolated, non-regular behavior. overall, our work provides insight into a dynamics of cellular sectors and their predictability. it also paves a way considering more proactive network operations with greater forecasting horizons.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1
7325,"inside this paper, we offer and discuss three efficient structural solutions considering a hardware-oriented implementation of discrete quaternion fourier transform basic operations with reduced implementation complexities. a first solution: the scheme considering calculating sq product, a second solution: the scheme considering calculating qt product, and a third solution: the scheme considering calculating sqt product, where s was the so-called i-quaternion, t was an j-quaternion, and q was an usual quaternion. a direct multiplication of two usual quaternions requires 16 real multiplications (or two-operand multipliers inside a case of fully parallel hardware implementation) and 12 real additions (or binary adders). at a same time, our solutions allow to design a computation units, which consume only 6 multipliers plus 6 two input adders considering implementation of sq or qt basic operations and 9 binary multipliers plus 6 two-input adders and 4 four-input adders considering implementation of sqt basic operation.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8804,"achieving a goals inside a title (and others) relies on the cardinality-wise scanning of a ideals of a poset. specifically, a relevant numbers attached to a k+1 element ideals are inferred from a corresponding numbers of a k-element (order) ideals. crucial inside all of this was the compressed representation (using wildcards) of a ideal lattice. a whole scheme invites distributed computation.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17311,"inside this article, a analysis of misspecification is extended to a recently introduced stochastic restricted biased estimators when multicollinearity exists among a explanatory variables. a stochastic restricted ridge estimator (srre), stochastic restricted almost unbiased ridge estimator (sraure), stochastic restricted liu estimator (srle), stochastic restricted almost unbiased liu estimator (sraule), stochastic restricted principal component regression estimator (srpcr), stochastic restricted r-k class estimator (srrk) and stochastic restricted r-d class estimator (srrd) were examined inside a misspecified regression model due to missing relevant explanatory variables when incomplete prior information of a regression coefficients was available. further, a superiority conditions between estimators and their respective predictors were obtained inside a mean square error matrix (msem) sense. finally, the numerical example and the monte carlo simulation study were used to illustrate a theoretical findings.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
13258,"we introduce features considering massive data streams. these stream features should be thought of as ""ordered moments"" and generalize stream sketches from ""moments of order one"" to ""ordered moments of arbitrary order"". inside analogy to classic moments, they have theoretical guarantees such as universality that are important considering learning algorithms.",1,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0
1656,"single-photon avalanche diodes (spad) are affordable photodetectors, capable to collect extremely fast low-energy events, due to their single-photon sensibility. this makes them very suitable considering time-of-flight-based range imaging systems, allowing to reduce costs and power requirements, without sacrifizing much temporal resolution. inside this work we describe the computational model to simulate a behaviour of spad sensors, aiming to provide the realistic camera model considering time-resolved light transport simulation, with applications on prototyping new reconstructions techniques based on spad time-of-flight data. our model accounts considering a major effects of a sensor on a incoming signal. we compare our model against real-world measurements, and apply it to the variety of scenarios, including complex multiply-scattered light transport.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6945,"extracting useful entities and attribute values from illicit domains such as human trafficking was the challenging problem with a potential considering widespread social impact. such domains employ atypical language models, have `long tails' and suffer from a problem of concept drift. inside this paper, we propose the lightweight, feature-agnostic information extraction (ie) paradigm specifically designed considering such domains. our idea behind the method uses raw, unlabeled text from an initial corpus, and the few (12-120) seed annotations per domain-specific attribute, to learn robust ie models considering unobserved pages and websites. empirically, we demonstrate that our idea behind the method should outperform feature-centric conditional random field baselines by over 18\% f-measure on five annotated sets of real-world human trafficking datasets inside both low-supervision and high-supervision settings. we also show that our idea behind the method was demonstrably robust to concept drift, and should be efficiently bootstrapped even inside the serial computing environment.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9084,"providing an efficient strategy to navigate safely through unsignaled intersections was the difficult task that requires determining a intent of other drivers. we explore a effectiveness of deep reinforcement learning to handle intersection problems. with the help of recent advances inside deep rl, we are able to learn policies that surpass a performance of the commonly-used heuristic idea behind the method inside several metrics including task completion time and goal success rate and have limited ability to generalize. we then explore the system's ability to learn active sensing behaviors to enable navigating safely inside a case of occlusions. our analysis, provides insight into a intersection handling problem, a solutions learned by a network point out several shortcomings of current rule-based methods, and a failures of our current deep reinforcement learning system point to future research directions.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
1011,"autonomous vehicles should be able to generate accurate probabilistic predictions considering uncertain behavior of other road users. moreover, reactive predictions are necessary inside highly interactive driving scenarios to answer ""what if i take this action inside a future"" considering autonomous vehicles. there was no existing unified framework to homogenize a problem formulation, representation simplification, and evaluation metric considering various prediction methods, such as probabilistic graphical models (pgm), neural networks (nn) and inverse reinforcement learning (irl). inside this paper, we formulate the probabilistic reaction prediction problem, and reveal a relationship between reaction and situation prediction problems. we employ prototype trajectories with designated motion patterns other than ""intention"" to homogenize a representation so that probabilities corresponding to each trajectory generated by different methods should be evaluated. we also discuss a reasons why ""intention"" was not suitable to serve as the motion indicator inside highly interactive scenarios. we propose to use brier score as a baseline metric considering evaluation. inside order to reveal a fatality of a consequences when a predictions are adopted by decision-making and planning, we propose the fatality-aware metric, which was the weighted brier score based on a criticality of a trajectory pairs of a interacting entities. conservatism and non-defensiveness are defined from a weighted brier score to indicate a consequences caused by inaccurate predictions. modified methods based on pgm, nn and irl are provided to generate probabilistic reaction predictions inside an exemplar scenario of nudging from the highway ramp. a results are evaluated by a baseline and proposed metrics to construct the mini benchmark. analysis on a properties of each method was also provided by comparing a baseline and proposed metric scores.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
2336,"at initialization, artificial neural networks (anns) are equivalent to gaussian processes inside a infinite-width limit, thus connecting them to kernel methods. we prove that a evolution of an ann during training should also be described by the kernel: during gradient descent on a parameters of an ann, a network function $f_\theta$ (which maps input vectors to output vectors) follows a kernel gradient of a functional cost (which was convex, inside contrast to a parameter cost) w.r.t. the new kernel: a neural tangent kernel (ntk). this kernel was central to describe a generalization features of anns. while a ntk was random at initialization and varies during training, inside a infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. this makes it possible to study a training of anns inside function space instead of parameter space. convergence of a training should then be related to a positive-definiteness of a limiting ntk. we prove a positive-definiteness of a limiting ntk when a data was supported on a sphere and a non-linearity was non-polynomial. we then focus on a setting of least-squares regression and show that inside a infinite-width limit, a network function $f_\theta$ follows the linear differential equation during training. a convergence was fastest along a largest kernel principal components of a input data with respect to a ntk, thus suggesting the theoretical motivation considering early stopping. finally we study a ntk numerically, observe its behavior considering wide networks, and compare it to a infinite-width limit.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2561,"as a size of modern data sets exceeds a disk and memory capacities of the single computer, machine learning practitioners have resorted to parallel and distributed computing. given that optimization was one of a pillars of machine learning and predictive modeling, distributed optimization methods have recently garnered ample attention, inside particular when either observations or features are distributed, but not both. we propose the general stochastic algorithm where observations, features, and gradient components should be sampled inside the double distributed setting, i.e., with both features and observations distributed. very technical analyses establish convergence properties of a algorithm under different conditions on a learning rate (diminishing to zero or constant). computational experiments inside spark demonstrate the superior performance of our algorithm versus the benchmark inside early iterations of a algorithm, which was due to a stochastic components of a algorithm.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2697,"with a latest technology of vectoring, dsl data rates inside a order of 100mbps have become the reality that was under field deployment. a key was to cancel crosstalk from other lines, which was also known as multiuser mimo cancellation considering wireless communications. during a dsl system upgrade phase of field deployment, mix of legacy and vectoring-enabled vdsl lines was inevitable and the channel approximation solution considering a entire mix was needed before vectoring should be enforced. this paper describes the practical method considering crosstalk channel approximation considering downstream vectoring, assuming that the vectoring-enabled dslam forces dmt symbol-level timing to be aligned considering all of a lines, but also assuming that a location of synch symbols are aligned only among vectoring-enabled lines. each vectoring-enabled receiver was capable of reporting error samples to vectoring-dslam. a approximation method was not only practical, but also matches a performance of maximum-likelihood estimator considering a selected training sequences.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
2560,"human pose approximation with the help of deep neural networks aims to map input images with large variations into multiple body keypoints which must satisfy the set of geometric constraints and inter-dependency imposed by a human body model. this was the very challenging nonlinear manifold learning process inside the very high dimensional feature space. we believe that a deep neural network, which was inherently an algebraic computation system, was not a most effecient way to capture highly sophisticated human knowledge, considering example those highly coupled geometric characteristics and interdependence between keypoints inside human poses. inside this work, we propose to explore how external knowledge should be effectively represented and injected into a deep neural networks to guide its training process with the help of learned projections that impose proper prior. specifically, we use a stacked hourglass design and inception-resnet module to construct the fractal network to regress human pose images into heatmaps with no explicit graphical modeling. we encode external knowledge with visual features which are able to characterize a constraints of human body models and evaluate a fitness of intermediate network output. we then inject these external features into a neural network with the help of the projection matrix learned with the help of an auxiliary cost function. a effectiveness of a proposed inception-resnet module and a benefit inside guided learning with knowledge projection was evaluated on two widely used benchmarks. our idea behind the method achieves state-of-the-art performance on both datasets.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16745,"we explore a possibility that fast radio bursts are due to a annihilation of cusps on cosmic string loops. we compute a energy released inside a annihilation events inside a radio region, a expected event rate, and a time scale of a bursts. we find that a energy and event rates are sufficiently high and a time scale was sufficiently small to explain a current data. we predict how a event rate will change as a resolution of telescopes improves. since a burst rate depends on a string tension, future data will allow a determination of a tension.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2050,"we consider the registration-based idea behind the method considering localizing sensor networks from range measurements. this was based on a assumption that one should find overlapping cliques spanning a network. that is, considering each sensor, one should identify geometric neighbors considering which all inter-sensor ranges are known. such cliques should be efficiently localized with the help of multidimensional scaling. however, since each clique was localized inside some local coordinate system, we are required to register them inside the global coordinate system. inside other words, our idea behind the method was based on transforming a localization problem into the problem of registration. inside this context, a main contributions are as follows. first, we describe an efficient method considering partitioning a network into overlapping cliques. second, we study a problem of registering a localized cliques, and formulate the necessary rigidity condition considering uniquely recovering a global sensor coordinates. inside particular, we present the method considering efficiently testing rigidity, and the proposal considering augmenting a partitioned network to enforce rigidity. the recently proposed semidefinite relaxation of global registration was used considering registering a cliques. we present simulation results on random and structured sensor networks to demonstrate that a proposed method compares favourably with state-of-the-art methods inside terms of run-time, accuracy, and scalability.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
8951,"scaling bayesian optimization to high dimensions was challenging task as a global optimization of high-dimensional acquisition function should be expensive and often infeasible. existing methods depend either on limited active variables or a additive form of a objective function. we propose the new method considering high-dimensional bayesian optimization, that uses the dropout strategy to optimize only the subset of variables at each iteration. we derive theoretical bounds considering a regret and show how it should inform a derivation of our algorithm. we demonstrate a efficacy of our algorithms considering optimization on two benchmark functions and two real-world applications- training cascade classifiers and optimizing alloy composition.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4205,"this study introduces the procedure to obtain general expressions, $y = f(x)$, subject to linear constraints on a function and its derivatives defined at specified values. these constrained expressions should be used describe functions with embedded specific constraints. a paper first shows how to express a most general explicit function passing through the single point inside three distinct ways: linear, additive, and rational. then, functions with constraints on single, two, or multiple points are introduced as well as those satisfying relative constraints. this capability allows to obtain general expressions to solve linear differential equations with no need to satisfy constraints (the ""subject to:"" conditions) as a constraints are already embedded inside a constrained expression. inside particular, considering expressions passing through the set of points, the generalization of a waring's interpolation form, was introduced. a general form of additive constrained expressions was introduced as well as the procedure to derive its coefficient functions, requiring a inversion of the matrix with dimensions as a number of constraints.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
3685,"a search considering majorana bound state (mbs) has recently emerged as one of a most active research areas inside condensed matter physics, fueled by a prospect of with the help of its non-abelian statistics considering robust quantum computation. the highly sought-after platform considering mbs was two-dimensional topological superconductors, where mbs was predicted to exist as the zero-energy mode inside a core of the vortex. the clear observation of mbs, however, was often hindered by a presence of additional low-lying bound states in a vortex core. by with the help of scanning tunneling microscope on a newly discovered superconducting dirac surface state of iron-based superconductor fete1-xsex (x = 0.45, superconducting transition temperature tc = 14.5 k), we clearly observe the sharp and non-split zero-bias peak in the vortex core. systematic studies of its evolution under different magnetic fields, temperatures, and tunneling barriers strongly suggest that this was a case of tunneling to the nearly pure mbs, separated from non-topological bound states which was moved away from a zero energy due to a high ratio between a superconducting gap and a fermi energy inside this material. this observation offers the new, robust platform considering realizing and manipulating mbss at the relatively high temperature.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
11401,"a low-energy behaviors of gapless double- and triple-weyl fermions caused by a interplay of long-range coulomb interaction and quenched disorder are studied by performing the renormalization group analysis. it was found that an arbitrarily weak disorder drives a double-weyl semimetal to undergo the quantum phase transition into the compressible diffusive metal, independent of a disorder type and a coulomb interaction strength. inside contrast, a nature of a ground state of triple-weyl fermion system relies sensitively on a specific disorder type inside a noninteracting limit: a system was turned into the compressible diffusive metal state by an arbitrarily weak random scalar potential or $z$ component of random vector potential but exhibits stable critical behavior when there was only $x$ or $y$ component of random vector potential. inside case a triple-weyl fermions couple to random scalar potential, a system becomes the diffusive metal inside a weak interaction regime but remains the semimetal if coulomb interaction was sufficiently strong. interplay of coulomb interaction and $x$, or $y$, component of random vector potential leads to the stable infrared fixed point that was likely to be characterized by critical behavior. when coulomb interaction coexists with a $z$ component of random vector potential, a system flows to a interaction-dominated strong coupling regime, which might drive the mott insulating transition. it was thus clear that double- and triple-weyl fermions exhibit distinct low-energy behavior inside response to interaction and disorder. a physical explanation of such distinction was discussed inside detail. a role played by long-range coulomb impurity inside triple-weyl semimetal was also considered.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
5752,"although recurrent neural network (rnn) has been the powerful tool considering modeling sequential data, its performance was inadequate when processing sequences with multiple patterns. inside this paper, we address this challenge by introducing the persistent memory and constructing an adaptive rnn. a persistent memory augmented rnn (termed as prnn) captures a principle patterns inside training sequences and stores them inside an external memory. by leveraging a persistent memory, a proposed method should adaptively update states according to a similarities between encoded inputs and memory slots, leading to the stronger capacity inside assimilating sequences with multiple patterns. content-based addressing was suggested inside memory accessing, and gradient descent was utilized considering implicitly updating a memory. our idea behind the method should be further extended by combining a prior knowledge of data. experiments on several datasets demonstrate a effectiveness of our method.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15233,"social learning, i.e., students learning from each other through social interactions, has a potential to significantly scale up instruction inside online education. inside many cases, such as inside massive open online courses (moocs), social learning was facilitated through discussion forums hosted by course providers. inside this paper, we propose the probabilistic model considering a process of learners posting on such forums, with the help of point processes. different from existing works, our method integrates topic modeling of a post text, timescale modeling of a decay inside post activity over time, and learner topic interest modeling into the single model, and infers this information from user data. our method also varies a excitation levels induced by posts according to a thread structure, to reflect typical notification settings inside discussion forums. we experimentally validate a proposed model on three real-world mooc datasets, with a largest one containing up to 6,000 learners making 40,000 posts inside 5,000 threads. results show that our model excels at thread recommendation, achieving significant improvement over the number of baselines, thus showing promise of being able to direct learners to threads that they are interested inside more efficiently. moreover, we demonstrate analytics that our model parameters should provide, such as a timescales of different topic categories inside the course.",1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
5028,"autonomous robotics and artificial intelligence techniques should be used to support human personnel inside a event of critical incidents. these incidents should pose great danger to human life. some examples of such assistance include: multi-robot surveying of a scene; collection of sensor data and scene imagery, real-time risk assessment and analysis; object identification and anomaly detection; and retrieval of relevant supporting documentation such as standard operating procedures (sops). these incidents, although often rare, should involve chemical, biological, radiological/nuclear or explosive (cbrne) substances and should be of high consequence. real-world training and deployment of these systems should be costly and sometimes not feasible. considering this reason, we have developed the realistic 3d model of the cbrne scenario to act as the testbed considering an initial set of assisting ai tools that we have developed.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
10229,"we propose the novel two-phase idea behind the method to functional network approximation of multi-subject functional magnetic resonance imaging (fmri) data, which applies model-based image segmentation to determine the group-representative connectivity map. inside our approach, we first improve clustering-based independent component analysis (ica) to generate maps of components occurring consistently across subjects, and then approximate a group-representative map through map-mrf (maximum the priori - markov random field) labeling. considering a latter, we provide the novel and efficient variational bayes algorithm. we study a performance of a proposed method with the help of synthesized data following the theoretical model, and demonstrate its viability inside blind extraction of group-representative functional networks with the help of simulated fmri data. we anticipate a proposed method will be applied inside identifying common neuronal characteristics inside the population, and could be further extended to real-world clinical diagnosis.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16379,"autonomous robots increasingly depend on third-party off-the-shelf components and complex machine-learning techniques. this trend makes it challenging to provide strong design-time certification of correct operation. to address this challenge, we present soter, the programming framework that integrates a core principles of runtime assurance to enable a use of uncertified controllers, while still providing safety guarantees. runtime assurance (rta) was an idea behind the method used considering safety-critical systems where design-time analysis was coupled with run-time techniques to switch between unverified advanced controllers and verified simple controllers. inside this paper, we present the runtime assurance programming framework considering modular design of provably-safe robotics software. \tool provides language primitives to declaratively construct the \rta module consisting of an advanced controller (untrusted), the safe controller (trusted), and a desired safety specification (s). if a rta module was well formed then a framework provides the formal guarantee that it satisfies property s. a compiler generates code considering monitoring system state and switching control between a advanced and safe controller inside order to guarantee s. rta allows complex systems to be constructed through a composition of rta modules. to demonstrate a efficacy of our framework, we consider the real-world case-study of building the safe drone surveillance system. our experiments both inside simulation and on actual drones show that rta-enabled rta ensures safety of a system, including when untrusted third-party components have bugs or deviate from a desired behavior.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1
11029,"it has recently been found that bosonic excitations of ordered media, such as phonons or spinons, should exhibit topologically nontrivial band structures. of particular interest are magnon and triplon excitations inside quantum magnets, as they should easily be manipulated by an applied field. here we study triplon excitations inside an s=1/2 quantum spin ladder and show that they exhibit nontrivial topology, even inside a quantum-disordered paramagnetic phase. our analysis reveals that a paramagnetic phase actually consists of two separate regions with topologically distinct triplon excitations. we demonstrate that a topological transition between these two regions should be tuned by an external magnetic field. a winding number that characterizes a topology of a triplons was derived and evaluated. by a bulk-boundary correspondence, we find that a non-zero winding number implies a presence of localized triplon end states. experimental signatures and possible physical realizations of a topological paramagnetic phase are discussed.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
12865,"during a last decade, a information technology industry has adopted the data-driven culture, relying on online metrics to measure and monitor business performance. under a setting of big data, a majority of such metrics approximately follow normal distributions, opening up potential opportunities to model them directly without extra model assumptions and solve big data problems using closed-form formulas with the help of distributed algorithms at the fraction of a cost of simulation-based procedures like bootstrap. however, certain attributes of a metrics, such as their corresponding data generating processes and aggregation levels, pose numerous challenges considering constructing trustworthy approximation and inference procedures. motivated by four real-life examples inside metric development and analytics considering large-scale a/b testing, we provide the practical guide to applying a delta method, one of a most important tools from a classic statistics literature, to address a aforementioned challenges. we emphasize a central role of a delta method inside metric analytics by highlighting both its classic and novel applications.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8273,"species tree reconstruction from genomic data was increasingly performed with the help of methods that account considering sources of gene tree discordance such as incomplete lineage sorting. one popular method considering reconstructing species trees from unrooted gene tree topologies was astral. inside this paper, we derive theoretical sample complexity results considering a number of genes required by astral to guarantee reconstruction of a correct species tree with high probability. we also validate those theoretical bounds inside the simulation study. our results indicate that astral requires $\mathcal{o}(f^{-2} \log n)$ gene trees to reconstruct a species tree correctly with high probability where n was a number of species and f was a length of a shortest branch inside a species tree. our simulations, which are a first to test astral explicitly under a anomaly zone, show trends consistent with a theoretical bounds and also provide some practical insights on a conditions where astral works well.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
7117,"we present the comprehensive statistical analysis of star-forming objects located inside a vicinities of 1 360 bubble structures throughout a galactic plane and their local environments. a compilation of ~70 000 star-forming sources, found inside a proximity of a ionized (hii) regions and detected inside both hi-gal and glimpse surveys, provided the broad overview of a different evolutionary stages of star-formation inside bubbles, from prestellar objects to more evolved young stellar objects (ysos). surface density maps of star-forming objects clearly reveal an evolutionary trend where more evolved star-forming objects are found spatially located near a center, while younger star-forming objects are found at a edge of a bubbles. we derived dynamic ages considering the subsample of 182 hii regions considering which kinematic distances and radio continuum flux measurements were available. we detect ~80% more star-forming sources per unit area inside a direction of bubbles than inside a surrounding fields. we approximate ~10% clump formation efficiency (cfe) of hi-gal clumps inside bubbles, twice a cfe inside fields not affected by feedback. we find higher cfe of protostellar clumps inside younger bubbles, whose density of a bubble shells was higher. we argue that a formation rate from prestellar to protostellar phase was probably higher during a early stages of a bubble expansion. evaluation of a fragmentation time in a shell of bubbles advocates a preexistence of clumps inside a medium before a bubble, as supported by numerical simulations. approximately 23% of a hi-gal clumps are found located inside a direction of the bubble, with 15% considering prestellar clumps and 41% considering protostellar clumps. we argue that a high fraction of protostellar clumps may be due to a acceleration of a star-formation process cause by a feedback of a (hii) bubbles.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9362,"we explore the new research direction inside bayesian variational inference with discrete latent variable priors where we exploit kronecker matrix algebra considering efficient and exact computations of a evidence lower bound (elbo). a proposed ""direct"" idea behind the method has several advantages over its predecessors; (i) it should exactly compute elbo gradients (i.e. unbiased, zero-variance gradient estimates), eliminating a need considering high-variance stochastic gradient estimators and enabling a use of quasi-newton optimization methods; (ii) its training complexity was independent of a number of training points, permitting inference on large datasets; and (iii) its posterior samples consist of sparse and low-precision quantized integers which permit fast inference on hardware limited devices. inside addition, our direct models should exactly compute statistical moments of a parameterized predictive posterior without relying on monte carlo sampling. a direct idea behind the method was not practical considering all likelihoods, however, we identify the popular model structure which was practical, and demonstrate accurate inference with the help of latent variables discretized as extremely low-precision 4-bit quantized integers. while a elbo computations considered inside a numerical studies require over $10^{2352}$ log-likelihood evaluations, we train on datasets with over two-million points inside just seconds.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
13108,"we study periodic monopoles satisfying some mild conditions, called of gck type. particularly, we give the classification of periodic monopoles of gck type inside terms of difference modules with parabolic structure, which was the kind of kobayashi-hitchin correspondence between differential geometric objects and algebraic objects. we also clarify a asymptotic behaviour of periodic monopoles of gck type around infinity.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1063,"an innovative model was presented considering merging of bubbles in the liquid metal. a proposed model was based on forming the thin film (narrow channel) between merging bubbles during growth. rupturing of a film occurs when an oscillation inside velocity and pressure arises in a channel followed by merging of a bubbles. a proposed model based on lattice boltzmann method was capable of simulating merging bubbles inside micro, meso, and macro-scales with no limitation on a number of bubbles. experimental studies reveal the good consistency between modeling results and real conditions.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13851,"we establish sharp hardy-adams inequalities on hyperbolic space $\mathbb{b}^{4}$ of dimension four. namely, we will show that considering any $\alpha>0$ there exists the constant $c_{\alpha}>0$ such that \[ \int_{\mathbb{b}^{4}}(e^{32\pi^{2} u^{2}}-1-32\pi^{2} u^{2})dv=16\int_{\mathbb{b}^{4}}\frac{e^{32\pi^{2} u^{2}}-1-32\pi^{2} u^{2}}{(1-|x|^{2})^{4}}dx\leq c_{\alpha}. \] considering any $u\in c^{\infty}_{0}(\mathbb{b}^{4})$ with \[ \int_{\mathbb{b}^{4}}\left(-\delta_{\mathbb{h}}-\frac{9}{4}\right)(-\delta_{\mathbb{h}}+\alpha)u\cdot udv\leq1. \] as applications, we obtain the sharpened adams inequality on hyperbolic space $\mathbb{b}^{4}$ and an inequality which improves a classical adams' inequality and a hardy inequality simultaneously. a later inequality was inside a spirit of a hardy-trudinger-moser inequality on the disk inside dimension two given by wang and ye [37] and on any convex planar domain by a authors [26]. a tools of fractional laplacian, fourier transform and a plancherel formula on hyperbolic spaces and symmetric spaces play an important role inside our work.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19358,"this paper presents the study of a asymptotic behavior of a solutions considering a history value problem of the viscoelastic wave equation which features the fading memory term as well as the supercritical source term and the frictional damping term: \begin{align*} \begin{cases} u_{tt}- k(0) \delta u - \int_0^{\infty} k'(s) \delta u(t-s) ds +|u_t|^{m-1}u_t =|u|^{p-1}u, \quad \text{ inside } \omega \times (0,t), \\ u(x,t)=u_0(x,t), \quad \text{ inside } \omega \times (-\infty,0], \end{cases} \end{align*} where $\omega$ was the bounded domain inside $\mathbb r^3$ with the dirichl√©t boundary condition and $u_0$ represents a history value. the suitable notion of the potential well was introduced considering a system, and global existence of solutions was justified provided that a history value $u_0$ was taken from the subset of a potential well. also, uniform energy decay rate was obtained which depends on a relaxation kernel $-k'(s)$ as well as a growth rate of a damping term. this manuscript complements our previous work [guo et al. inside j differ equ 257, 3778-3812(2014), j differ equ 262, 1956-1979(2017)] where hadamard well-posedness and a singularity formulation have been studied considering a system. it was worth stressing a special features of a model, namely a source term here has the supercritical growth rate and a memory term accounts to a full past history that goes back to $-\infty$.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17729,"we explore a problem of learning to decompose spatial tasks into segments, as exemplified by a problem of the painting robot covering the large object. inspired by a ability of classical decision tree algorithms to construct structured partitions of their input spaces, we formulate a problem of decomposing objects into segments as the parsing approach. we make a insight that a derivation of the parse-tree that decomposes a object into segments closely resembles the decision tree constructed by id3, which should be done when a ground-truth available. we learn to imitate an expert parsing oracle, such that our neural parser should generalize to parse natural images without ground truth. we introduce the novel deterministic policy gradient update, drag (i.e., deterministically aggrevate) inside a form of the deterministic actor-critic variant of aggrevated, to train our neural parser. from another perspective, our idea behind the method was the variant of a deterministic policy gradient suitable considering a imitation learning setting. a deterministic policy representation offered by training our neural parser with drag allows it to outperform state of a art imitation and reinforcement learning approaches.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
19153,"kagome antiferromagnets are known to be highly frustrated and degenerate when they possess simple, isotropic interactions. we consider a entire class of these magnets when their interactions are spatially anisotropic. we do so by identifying the certain class of systems whose degenerate ground states should be mapped onto a folding motions of the generalized ""spin origami"" two-dimensional mechanical sheet. some such anisotropic spin systems, including cs2zrcu3f12, map onto flat origami sheets, possessing extensive degeneracy similar to isotropic systems. others, such as cs2cecu3f12, should be mapped onto sheets with non-zero gaussian curvature, leading to more mechanically stable corrugated surfaces. remarkably, even such distortions do not always lift a entire degeneracy, instead permitting the large but sub-extensive space of zero-energy modes. we show that considering cs2cecu3f12, due to an additional point group symmetry associated with structure, these modes are 'dirac' line nodes with the double degeneracy protected by the topological invariant. a existence of mechanical analogs thus serves to identify and explicate a robust degeneracy of a spin systems.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
10757,"previous optical and radio observations of a binary millisecond pulsar psr j1640+2224 have come to inconsistent conclusions about a identity of its companion, with some observations suggesting a companion was the low-mass helium-core (he-core) white dwarf (wd), while others indicate it was most likely the high-mass carbon-oxygen (co) wd. binary evolution models predict psr j1640+2224 most likely formed inside the low-mass x-ray binary (lmxb) based on a pulsar's short spin period and long-period, low-eccentricity orbit, inside which case its companion should be the he-core wd with mass about $0.35 - 0.39 \, m_\odot$, depending on metallicity. if it was instead the co wd, that would suggest a system has an unusual formation history. inside this paper we present a first astrometric parallax measurement considering this system from observations made with a very long baseline array (vlba), from which we determine a distance to be $1520^{+170}_{-150}\,\mathrm{pc}$. we use this distance and the reanalysis of archival optical observations originally taken inside 1995 with a wide field planetary camera 2 (wfpc2) on a hubble space telescope (hst) inside order to measure a wd's mass. we also incorporate improvements inside calibration, extinction model, and wd cooling models. we find that a existing observations are not sufficient to tightly constrain a companion mass, but we conclude a wd mass was $>0.4\,m_\odot$ with $>90\%$ confidence. a limiting factor inside our analysis was a low signal-to-noise ratio of a original hst observations.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
974,"spectral topic modeling algorithms operate on matrices/tensors of word co-occurrence statistics to learn topic-specific word distributions. this idea behind the method removes a dependence on a original documents and produces substantial gains inside efficiency and provable topic inference, but at the cost: a model should no longer provide information about a topic composition of individual documents. recently thresholded linear inverse (tli) was proposed to map a observed words of each document back to its topic composition. however, its linear characteristics limit a inference quality without considering a important prior information over topics. inside this paper, we evaluate simple probabilistic inverse (spi) method and novel prior-aware dual decomposition (padd) that was capable of learning document-specific topic compositions inside parallel. experiments show that padd successfully leverages topic correlations as the prior, notably outperforming tli and learning quality topic compositions comparable to gibbs sampling on various data.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8451,"simple online and realtime tracking (sort) was the pragmatic idea behind the method to multiple object tracking with the focus on simple, effective algorithms. inside this paper, we integrate appearance information to improve a performance of sort. due to this extension we are able to track objects through longer periods of occlusions, effectively reducing a number of identity switches. inside spirit of a original framework we place much of a computational complexity into an offline pre-training stage where we learn the deep association metric on the large-scale person re-identification dataset. during online application, we establish measurement-to-track associations with the help of nearest neighbor queries inside visual appearance space. experimental evaluation shows that our extensions reduce a number of identity switches by 45%, achieving overall competitive performance at high frame rates.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11189,"this article consists to give the necessary and sufficient condition of a meromorphic continuity of dirichlet series defined as $\sum_{x\in \mathbf{n}^n} \frac{a_{x}}{p(x)^s}$, where $a_{x}$ was the $q$-automatic sequence of $n$ parameters and $p: \mathbf{c}^n \to \mathbf{c}$ the polynomial, such that $p$ does not have zeros on $\mathbf{q}^{n}_{+}$. and some specific cases of $n=1$ will also be studied inside this article as examples to show a possibility to have an holomorphic continuity on a whole complex plane. some equivalences between infinite products are also built as consequences of these results.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
8935,"we consider the nonparametric bayesian idea behind the method to approximate a diffusion coefficient of the stochastic differential equation given discrete time observations over the fixed time interval. as the prior on a diffusion coefficient, we employ the histogram-type prior with piecewise constant realisations on bins forming the partition of a time interval. specifically, these constants are realizations of independent inverse gamma distributed randoma variables. we justify our idea behind the method by deriving a rate at which a corresponding posterior distribution asymptotically concentrates around a data-generating diffusion coefficient. this posterior contraction rate turns out to be optimal considering approximation of the h√∂lder-continuous diffusion coefficient with smoothness parameter $0<\lambda\leq 1.$ our idea behind the method was straightforward to implement, as a posterior distributions turn out to be inverse gamma again, and leads to good practical results inside the wide range of simulation examples. finally, we apply our method on exchange rate data sets.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
18321,"the multiple instance dictionary learning approach, dictionary learning with the help of functions of multiple instances (dl-fumi), was used to perform beat-to-beat heart rate approximation and to characterize heartbeat signatures from ballistocardiogram (bcg) signals collected with the hydraulic bed sensor. dl-fumi estimates the ""heartbeat concept"" that represents an individual's personal ballistocardiogram heartbeat pattern. dl-fumi formulates heartbeat detection and heartbeat characterization as the multiple instance learning problem to address a uncertainty inherent inside aligning bcg signals with ground truth during training. experimental results show that a estimated heartbeat concept found by dl-fumi was an effective heartbeat prototype and achieves superior performance over comparison algorithms.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
171,"bayesian nonparametrics are the class of probabilistic models inside which a model size was inferred from data. the recently developed methodology inside this field was small-variance asymptotic analysis, the mathematical technique considering deriving learning algorithms that capture much of a flexibility of bayesian nonparametric inference algorithms, but are simpler to implement and less computationally expensive. past work on small-variance analysis of bayesian nonparametric inference algorithms has exclusively considered batch models trained on the single, static dataset, which are incapable of capturing time evolution inside a latent structure of a data. this work presents the small-variance analysis of a maximum the posteriori filtering problem considering the temporally varying mixture model with the markov dependence structure, which captures temporally evolving clusters within the dataset. two clustering algorithms result from a analysis: d-means, an iterative clustering algorithm considering linearly separable, spherical clusters; and sd-means, the spectral clustering algorithm derived from the kernelized, relaxed version of a clustering problem. empirical results from experiments demonstrate a advantages of with the help of d-means and sd-means over contemporary clustering algorithms, inside terms of both computational cost and clustering accuracy.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8417,"we show that gravitational lensing should provide the direct method to probe a nature of dark energy at astrophysical scales. considering lensing system as an isolated astrophysical object, we derive a dark energy contribution to gravitational potential as the repulsive power-law term, containing the generic equation of state parameter $w$. we find that it generates $w$-dependent and position-dependent modification to a conventional light orbital equation of $w=-1$. with post-newtonian approximation, we compute its direct effect considering an isolated lensing system at astrophysical scales and find that a dark energy force should deflect a path of incident light rays. we demonstrate that a dark-energy-induced deflection angle $\delta\alpha_{de}^{}\propto m^{(1+\frac{1}{3w})}$ (with $1+\frac{1}{3w}>0$), which increases with a lensing mass $m$ and consistently approaches zero inside a limit $m \to 0$. this effect was distinctive because dark energy tends to diffuse a rays and generates concave lensing effect. this was inside contrast to a conventional convex lensing effect caused by both visible and dark matter. measuring such concave lensing effect should directly probe a existence and nature of dark energy. we approximate this effect and show that a current gravitational lensing experiments are sensitive to a direct probe of dark energy at astrophysical scales. considering a special case $w=-1$, our independent study favors a previous works that a cosmological constant should affect light bending, although our prediction qualitatively and quantitatively differ from a literature, including our consistent realization of $\delta\alpha_{de}\to 0$ (under $m \to 0$) at a leading order.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14346,"a xo project aims at detecting transiting exoplanets around bright stars from a ground with the help of small telescopes. a original configuration of xo (mccullough et al. 2005) has been changed and extended as described here. a instrumental setup consists of three identical units located at different sites, each composed of two lenses equipped with ccd cameras mounted on a same mount. we observed two strips of a sky covering an area of 520 deg$^2$ considering twice nine months. we build lightcurves considering ~20,000 stars up to magnitude r~12.5 with the help of the custom-made photometric data reduction pipeline. a photometric precision was around 1-2% considering most stars, and a large quantity of data allows us to reach the millimagnitude precision when folding a lightcurves on timescales that are relevant to exoplanetary transits. we search considering periodic signals and identify several hundreds of variable stars and the few tens of transiting planet candidates. follow-up observations are underway to confirm or reject these candidates. we found two close-in gas giant planets so far, inside line with a expected yield.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
2461,"adversarial samples are strategically modified samples, which are crafted with a purpose of fooling the classifier at hand. an attacker introduces specially crafted adversarial samples to the deployed classifier, which are being mis-classified by a classifier. however, a samples are perceived to be drawn from entirely different classes and thus it becomes hard to detect a adversarial samples. most of a prior works have been focused on synthesizing adversarial samples inside a image domain. inside this paper, we propose the new method of crafting adversarial text samples by modification of a original samples. modifications of a original text samples are done by deleting or replacing a important or salient words inside a text or by introducing new words inside a text sample. our algorithm works best considering a datasets which have sub-categories within each of a classes of examples. while crafting adversarial samples, one of a key constraint was to generate meaningful sentences which should at pass off as legitimate from language (english) viewpoint. experimental results on imdb movie review dataset considering sentiment analysis and twitter dataset considering gender detection show a efficiency of our proposed method.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5836,"we study a adjoint of a double layer potential associated with a laplacian (the adjoint of a neumann-poincar√© operator), as the map on a boundary surface $\gamma$ of the domain inside $\mathbb{r}^3$ with conical points. a spectrum of this operator directly reflects a well-posedness of related transmission problems across $\gamma$. inside particular, if a domain was understood as an inclusion with complex permittivity $\epsilon$, embedded inside the background medium with unit permittivity, then a polarizability tensor of a domain was well-defined when $(\epsilon+1)/(\epsilon-1)$ belongs to a resolvent set inside energy norm. we study surfaces $\gamma$ that have the finite number of conical points featuring rotational symmetry. on a energy space, we show that a essential spectrum consists of an interval. on $l^2(\gamma)$, i.e. considering square-integrable boundary data, we show that a essential spectrum consists of the countable union of curves, outside of which a fredholm index should be computed as the winding number with respect to a essential spectrum. we provide explicit formulas, depending on a opening angles of a conical points. we reinforce our study with very precise numerical experiments, computing a energy space spectrum and a spectral measures of a polarizability tensor inside two different examples. our results indicate that a densities of a spectral measures may idea behind the method zero extremely rapidly inside a continuous part of a energy space spectrum.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3097,"social networking accounts considering the significant chunk of interest among various online activities~\cite{smith2009social}. a proclivity of being social, online, has been ingrained inside us so much that we are actively producing content considering a rest of a world to see or take interest inside our whereabouts, our meals, our opinions, photographs etc. yelp (this https url), seamlessly, integrates this very aspect of people inside its portal. it engages people to write reviews about a businesses they have availed a services of, rate them, add photographs, tags, follow other people and their activities, etc. inside this paper we examine and present a co-relation between the user's rating and a influence of a people, that a user follows, on a user considering the particular business. a group of people that a user follows was commonly referred as friends of a user. we also analyze if the user should get influenced, if the business has the certain number of reviews already present or if a reviews have been written by elite reviewers (a reviewer who, according to yelp, has contributed exceptionally inside engaging a community inside a form of consistency inside writing reviews, as well as a quality of a reviews). our analysis, through correlation and regression techniques, was able to prove that a user's rating remains unaffected by a number of people the user is friends with nor does a existing number of reviews and presence of elite reviewers helps inside influencing the user. what shapes the user's rating was a overall experience, that a user had at a restaurant.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
14941,"this paper presents the probabilistic method considering capturing non-monotonic behavior under a biphasic dose-response regime observed inside many biological systems experiencing different types of stress. a proposed method was based on a rolling-pin method introduced earlier to approximate highly nonlinear and non-monotonic joint probability distributions from continuous domain data. we show that a proposed method outperforms a conventional parametric methods inside terms of a error (namely rmse) and it needs fewer parameters to be estimated the priori, while offering high flexibility. a application and performance of a proposed method are shown through an example.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1615,"we present the new rheological model depending on the real parameter $\nu \in [0,1]$ that reduces to a maxwell body considering $\nu=0$ and to a becker body considering $\nu=1$. a corresponding creep law was expressed inside an integral form inside which a exponential function of a becker model was replaced and generalized by the mittag-leffler function of order $\nu$. then, a corresponding non-dimensional creep function and its rate are studied as functions of time considering different values of $\nu$ inside order to visualize a transition from a classical maxwell body to a becker body. based on a hereditary theory of linear viscoelasticity, we also approximate a relaxation function by solving numerically the volterra integral equation of a second kind. inside turn, a relaxation function was shown versus time considering different values of $\nu$ to visualize again a transition from a classical maxwell body to a becker body. furthermore, we provide the full characterization of a new model by computing, inside addition to a creep and relaxation functions, a so-called specific dissipation $q^{-1}$ as the function of frequency, which was of particularly relevance considering geophysical applications",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
15146,"semantic similarity measures are an important part inside natural language processing tasks. however semantic similarity measures built considering general use do not perform well within specific domains. therefore inside this study we introduce the domain specific semantic similarity measure that is created by a synergistic union of word2vec, the word embedding method that was used considering semantic similarity calculation and lexicon based (lexical) semantic similarity methods. we prove that this proposed methodology out performs word embedding methods trained on generic corpus and methods trained on domain specific corpus but do not use lexical semantic similarity methods to augment a results. further, we prove that text lemmatization should improve a performance of word embedding methods.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1594,"people change their physical contacts as the preventive response to infectious disease propagations. yet, only the few mathematical models consider a coupled dynamics of a disease propagation and a contact adaptation process. this paper presents the model where each agent has the default contact neighborhood set, and switches to the different contact set once she becomes alert about infection among her default contacts. since each agent should adopt either of two possible neighborhood sets, a overall contact network switches among 2^n possible configurations. notably, the two-layer network representation should fully model a underlying adaptive, state-dependent contact network. contact adaptation influences a size of a disease prevalence and a epidemic threshold---a characteristic measure of the contact network robustness against epidemics---in the nonlinear fashion. particularly, a epidemic threshold considering a presented adaptive contact network belongs to a solution of the nonlinear perron-frobenius (npf) problem, which does not depend on a contact adaptation rate monotonically. furthermore, a network adaptation model predicts the counter-intuitive scenario where adaptively changing contacts may adversely lead to lower network robustness against epidemic spreading if a contact adaptation was not fast enough. an original result considering the class of npf problems facilitate a analytical developments inside this paper.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
12888,let $f$ be the non-archimedean local field. we study a restriction of an irreducible admissible genuine representations of a two fold metaplectic cover $\widetilde{gl}_{2}(f)$ of $gl_{2}(f)$ to a inverse image inside $\widetilde{gl}_{2}(f)$ of the maximal torus inside $gl_{2}(f)$.,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0
967,"deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (ner). however, this typically requires large amounts of labeled data. inside this work, we demonstrate that a amount of labeled training data should be drastically reduced when deep learning was combined with active learning. while active learning was sample-efficient, it should be computationally expensive since it requires iterative retraining. to speed this up, we introduce the lightweight architecture considering ner, viz., a cnn-cnn-lstm model consisting of convolutional character and word encoders and the long short term memory (lstm) tag decoder. a model achieves nearly state-of-the-art performance on standard datasets considering a task while being computationally much more efficient than best performing models. we carry out incremental active learning, during a training process, and are able to nearly match state-of-the-art performance with just 25\% of a original training data.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8842,"inside order to achieve the dexterous robotic manipulation, we need to equip our robot with the tactile feedback capability, i.e. a ability to drive action based on tactile sensing. inside this paper we specifically address a challenge of tactile servoing, i.e. given a current tactile sensing and the target/goal tactile sensing --for example being memorized from the successful task execution inside a past--, what was a action that will bring a current tactile sensing to move closer towards a target tactile sensing at a next time step. we develop the data-driven idea behind the method to acquire the dynamics model considering tactile servoing by learning from demonstration. moreover, our method represents a tactile sensing information as to lie on the surface --or the 2d manifold-- and perform the manifold learning, making it applicable to any tactile skin geometry. as the proof of concept, we evaluate our method on the robot equipped with the tactile finger. the video demonstrating our idea behind the method should be seen inside this https url",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
4154,"inside recent years crowdsourcing has become a method of choice considering gathering labeled training data considering learning algorithms. standard approaches to crowdsourcing view a process of acquiring labeled data separately from a process of learning the classifier from a gathered data. this should give rise to computational and statistical challenges. considering example, inside most cases there are no known computationally efficient learning algorithms that are robust to a high level of noise that exists inside crowdsourced data, and efforts to eliminate noise through voting often require the large number of queries per example. inside this paper, we show how by interleaving a process of labeling and learning, we should attain computational efficiency with much less overhead inside a labeling cost. inside particular, we consider a realizable setting where there exists the true target function inside $\mathcal{f}$ and consider the pool of labelers. when the noticeable fraction of a labelers are perfect, and a rest behave arbitrarily, we show that any $\mathcal{f}$ that should be efficiently learned inside a traditional realizable pac model should be learned inside the computationally efficient manner by querying a crowd, despite high amounts of noise inside a responses. moreover, we show that this should be done while each labeler only labels the constant number of examples and a number of labels requested per example, on average, was the constant. when no perfect labelers exist, the related task was to find the set of a labelers which are good but not perfect. we show that we should identify all good labelers, when at least a majority of labelers are good.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
9399,"data analytics helps basketball teams to create tactics. however, manual data collection and analytics are costly and ineffective. therefore, we applied the deep bidirectional long short-term memory (blstm) and mixture density network (mdn) approach. this model was not only capable of predicting the basketball trajectory based on real data, but it also should generate new trajectory samples. it was an excellent application to aid coaches and players decide when and where to shoot. its structure was particularly suitable considering dealing with time series problems. blstm receives forward and backward information at a same time, while stacking multiple blstms further increases a learning ability of a model. combined with blstms, mdn was used to generate the multi-modal distribution of outputs. thus, a proposed model can, inside principle, represent arbitrary conditional probability distributions of output variables. we tested our model with two experiments on three-pointer datasets from nba sportvu data. inside a hit-or-miss classification experiment, a proposed model outperformed other models inside terms of a convergence speed and accuracy. inside a trajectory generation experiment, eight model-generated trajectories at the given time closely matched real trajectories.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8739,"smillie (1984) proved an interesting result on a stability of nonlinear, time-invariant, strongly cooperative, and tridiagonal dynamical systems. this result has found many applications inside models from various fields including biology, ecology, and chemistry. smith (1991) has extended smillie's result and proved entrainment inside a case where a vector field was time-varying and periodic. we use a theory of linear totally nonnegative differential systems developed by schwarz (1970) to give the generalization of these two results. this was based on weakening a requirement considering strong cooperativity to cooperativity, and adding an additional observability-type condition.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
8403,"advances inside gnc, particularly from miniaturized control electronics, reaction-wheels and attitude determination sensors make it possible to design surface probes and small robots to perform surface exploration and science on low-gravity environments. these robots would use their reaction wheels to roll, hop and tumble over rugged surfaces. these robots could provide 'google streetview' quality images of off-world surfaces and perform some unique science with the help of penetrometers. these systems should be powered by high-efficiency fuel cells that operate at 60-65 % and utilize hydrogen and oxygen electrolyzed from water. however, one of a major challenges that prevent these probes and robots from performing long duration surface exploration and science was thermal design and control. inside a inner solar system, during a day time, there was often enough solar-insolation to keep these robots warm and power these devices, but during eclipse a temperatures falls well below storage temperature. we have developed the thermal control system that utilizes chemicals to store and dispense heat when needed. a system takes waste products, such as water from these robots and transfers them to the thermochemical storage system. these thermochemical storage systems when mixed with water (a waste product from the pem fuel cell) releases heat. under eclipse, a heat from a thermochemical storage system was released to keep a probe warm enough to survive. inside sunlight, solar photovoltaics are used to electrolyze a water and reheat a thermochemical storage system to release a water. our research has showed thermochemical storage systems are the feasible solution considering use on surface probes and robots considering applications on a moon, mars and asteroids.",1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1
8992,"traditionally, deep learning algorithms update a network weights whereas a network architecture was chosen manually, with the help of the process of trial and error. inside this work, we propose two novel approaches that automatically update a network structure while also learning its weights. a novelty of our idea behind the method lies inside our parameterization where a depth, or additional complexity, was encapsulated continuously inside a parameter space through control parameters that add additional complexity. we propose two methods: inside tunnel networks, this selection was done at a level of the hidden unit, and inside budding perceptrons, this was done at a level of the network layer; updating this control parameter introduces either another hidden unit or another hidden layer. we show a effectiveness of our methods on a synthetic two-spirals data and on two real data sets of mnist and mirflickr, where we see that our proposed methods, with a same set of hyperparameters, should correctly adjust a network complexity to a task complexity.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16677,"personalization was pervasive inside a online space as, when combined with learning, it leads to higher efficiency and revenue by allowing a most relevant content to be served to each user. however, recent studies suggest that such personalization should propagate societal or systemic biases, which has led to calls considering regulatory mechanisms and algorithms to combat inequality. here we propose the rigorous algorithmic framework that allows considering a possibility to control biased or discriminatory personalization with respect to sensitive attributes of users without losing all of a benefits of personalization.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16737,"one of central issues inside iron-based superconductors was a role of structural change to a superconducting transition temperature (t_c). it is found inside fese that a lattice strain leads to the drastic increase inside t_c, accompanied by suppression of nematic order. by angle-resolved photoemission spectroscopy on tensile- or compressive-strained and strain-free fese, we experimentally show that a in-plane strain causes the marked change inside a energy overlap (deltae_{h-e}) between a hole and electron pockets inside a normal state. a change inside deltae_{h-e} modifies a fermi-surface volume, leading to the change inside t_c. furthermore, a strength of nematicity was also found to be characterized by deltae_{h-e}. these results suggest that a key to understanding a phase diagram was a fermiology and interactions linked to a semimetallic band overlap.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
12290,"inside noncentrosymmetric crystals with broken inversion symmetry $\mathcal{i}$, a $i-v$ ($i$: current, $v$: voltage) characteristic was generally expected to depend on a direction of $i$, which was known as nonreciprocal response and, considering example, found inside p-n junction. however, it was the highly nontrivial issue inside translationally invariant systems since a time-reversal symmetry ($\mathcal{t}$) plays an essential role, where a two states at crystal momenta $k$ and $-k$ are connected inside a band structure. therefore, it has been considered that a external magnetic field ($b$) or a magnetic order which breaks a $\mathcal{t}$-symmetry was necessary to realize a nonreciprocal $i-v$ characteristics, i.e., magnetochiral anisotropy. here we theoretically show that a electron correlation inside $\mathcal{i}$-broken multi-band systems should induce nonreciprocal $i-v$ characteristics {\it without} $\mathcal{t}$-breaking. an analog of onsager's relation shows that nonreciprocal current response without $\mathcal{t}$-breaking generally requires two effects: dissipation and interactions. by with the help of nonequilibrium green's functions, we derive general formula of a nonreciprocal response considering two-band systems with onsite interaction. a formula was applied to rice-mele model, the representative 1d model with inversion breaking, and some candidate materials are discussed. this finding offers the coherent understanding of a origin of nonreciprocal $i-v$ characteristics, and will pave the way to design it.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
6529,"simplicial complexes are now the popular alternative to networks when it comes to describing a structure of complex systems, primarily because they encode multi-node interactions explicitly. with this new description comes a need considering principled null models that allow considering easy comparison with empirical data. we propose the natural candidate, a simplicial configuration model. a core of our contribution was an efficient and uniform markov chain monte carlo sampler considering this model. we demonstrate its usefulness inside the short case study by investigating a topology of three real systems and their randomized counterparts (using their betti numbers). considering two out of three systems, a model allows us to reject a hypothesis that there was no organization beyond a local scale.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
19181,"we develop methods to efficiently reconstruct a topology and line parameters of the power grid from a measurement of nodal variables. we propose two compressed sensing algorithms that minimize a amount of necessary measurement resources by exploiting network sparsity, symmetry of connections and potential prior knowledge about a connectivity. a algorithms are reciprocal to established state approximation methods, where nodal variables are estimated from few measurements given a network structure. hence, they enable an advanced grid monitoring where both state and structure of the grid are subject to uncertainties or missing information.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
4360,"we proposed the deep learning method considering interpretable diabetic retinopathy (dr) detection. a visual-interpretable feature of a proposed method was achieved by adding a regression activation map (ram) after a global averaging pooling layer of a convolutional networks (cnn). with ram, a proposed model should localize a discriminative regions of an retina image to show a specific region of interest inside terms of its severity level. we believe this advantage of a proposed deep learning model was highly desired considering dr detection because inside practice, users are not only interested with high prediction performance, but also keen to understand a insights of dr detection and why a adopted learning model works. inside a experiments conducted on the large scale of retina image dataset, we show that a proposed cnn model should achieve high performance on dr detection compared with a state-of-the-art while achieving a merits of providing a ram to highlight a salient regions of a input image.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1823,"this paper was a third chapter of three of a author's undergraduate thesis. inside this paper, we study a convergence of local bulk statistics considering linearized covariance matrices under dyson's brownian motion. we consider deterministic initial data $v$ approximate a dyson brownian motion considering linearized covariance matrices by a wigner flow. with the help of universality results considering a wigner flow, we deduce universality considering a linearized covariance matrices. we deduce bulk universality of averaged bulk correlation functions considering both biregular bipartite graphs and honest covariance matrices. we also deduce the weak level repulsion approximate considering a dyson brownian motion of linearized covariance matrices.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
4638,a present paper deals with a study of ricci solitons on invariant and anti-invariant submanifolds of $(lcs)_n$-manifolds with respect to riemannian connection as well as quarter symmetric metric connection.,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7129,"let $\pi\colon p\to m$ be the principal bundle and $p$ an invariant polynomial of degree r on a lie algebra of a structure group. a theory of chern-simons differential characters was exploited to define an homology map $\chi^{k} : h_{2r-k-1}(m)\times h_{k}(\mathcal{f}/\mathcal{g})\to \mathbb{r}/\mathbb{z}$, considering $k<r-1$, where $\mathcal{f} /\mathcal{g}$ was a moduli space of flat connections of $\pi$ under a action of the subgroup $\mathcal{g}$ of a gauge group. a differential characters of first order are related to a dijkgraaf-witten action considering chern-simons theory. a second order characters are interpreted geometrically as a holonomy of the connection inside the line bundle over $\mathcal{f}/\mathcal{g})$. a relationship with other constructions inside a literature was also analyzed.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2321,"space-borne low-to medium-resolution (r~10^2-10^3) transmission spectroscopy of atmospheres detect a broadest spectral features (alkali doublets, molecular bands, scattering), while high-resolution (r~10^5), ground-based observations probe a sharpest features (cores of a alkali lines, molecular lines).the two techniques differ by:(1) a lsf of ground-based observations was 10^3 times narrower than considering space-borne observations;(2)space-borne transmission spectra probe up to a base of thermosphere, while ground-based observations should reach pressures down to 10^(-11);(3)space-borne observations directly yield a transit depth of a planet, while ground-based observations measure differences inside a radius of a planet at different wavelengths.it was challenging to combine both techniques.we develop the method to compare theoretical models with observations at different resolutions.we introduce pyeta, the line-by-line 1d radiative transfer code to compute transmission spectra at r~10^6 (0.01 a) over the broad wavelength range.an hybrid forward modeling/retrieval optimization scheme was devised to deal with a large computational resources required by modeling the broad wavelength range (0.3-2 $\mu$m) at high resolution.we apply our technique to hd189733b.here, hst observations reveal the flattened spectrum due to scattering by aerosols, while high-resolution ground-based harps observations reveal a sharp cores of sodium lines.we reconcile these results by building models that reproduce simultaneously both data sets, from a troposphere to a thermosphere. we confirm:(1)the presence of scattering by tropospheric aerosols;(2)that a sodium core feature was of thermospheric origin.accounting considering aerosols, a sodium cores indicate t up to 10000k inside a thermosphere.the precise value of a thermospheric temperature was degenerate with a abundance of sodium and altitude of a aerosol deck.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6814,"we consider goodness-of-fit tests with i.i.d. samples generated from the categorical distribution $(p_1,...,p_k)$. considering the given $(q_1,...,q_k)$, we test a null hypothesis whether $p_j=q_{\pi(j)}$ considering some label permutation $\pi$. a uncertainty of label permutation implies that a null hypothesis was composite instead of being singular. inside this paper, we construct the testing procedure with the help of statistics that are defined as indefinite integrals of some symmetric polynomials. this method was aimed directly at a invariance of a problem, and avoids a need of matching a unknown labels. a asymptotic distribution of a testing statistic was shown to be chi-squared, and its power was proved to be nearly optimal under the local alternative hypothesis. various degenerate structures of a null hypothesis are carefully analyzed inside a paper. the two-sample version of a test was also studied.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
13745,"perivascular spaces (pvs) are the recently recognised feature of small vessel disease (svd), also indicating neuroinflammation, and are an important part of a brain's circulation and glymphatic drainage system. quantitative analysis of pvs on magnetic resonance images (mri) was important considering understanding their relationship with neurological diseases. inside this work, we propose the segmentation technique based on a 3d frangi filtering considering extraction of pvs from mri. based on prior knowledge from neuroradiological ratings of pvs, we used ordered logit models to optimise frangi filter parameters inside response to a variability inside a scanner's parameters and study protocols. we optimized and validated our proposed models on two independent cohorts, the dementia sample (n=20) and patients who previously had mild to moderate stroke (n=48). results demonstrate a robustness and generalisability of our segmentation method. segmentation-based pvs burden estimates correlated with neuroradiological assessments (spearman's $\rho$ = 0.74, p $<$ 0.001), suggesting a great potential of our proposed method",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10258,"this paper proposes the new optimal control synthesis algorithm considering multi-robot systems under global temporal logic tasks. existing planning approaches under global temporal goals rely on graph search techniques applied to the product automaton constructed among a robots. inside this paper, we propose the new sampling-based algorithm that builds incrementally trees that approximate a state-space and transitions of a synchronous product automaton. by approximating a product automaton by the tree rather than representing it explicitly, we require much fewer memory resources to store it and motion plans should be found by tracing sequences of parent nodes without a need considering sophisticated graph search methods. this significantly increases a scalability of our algorithm compared to existing optimal control synthesis methods. we also show that a proposed algorithm was probabilistically complete and asymptotically optimal. finally, we present numerical experiments showing that our idea behind the method should synthesize optimal plans from product automata with billions of states, which was not possible with the help of standard optimal control synthesis algorithms or off-the-shelf model checkers.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
4715,"we outline polarization performance calculations and predictions considering a daniel k. inouye solar telescope (dkist) optics and show mueller matrices considering two of a first light instruments. telescope polarization was due to polarization dependent mirror reflectivity and rotations between groups of mirrors as a telescope moves inside altitude and azimuth. a zemax optical modeling software has polarization ray-trace capabilities and predicts system performance given the coating prescription. we develop the model coating formula that approximates measured witness sample polarization properties. estimates show a dkist telescope mueller matrix as functions of wavelength, azimuth, elevation, and field angle considering a cryogenic near infra-red spectro-polarimeter (cryonirsp) and considering a visible spectropolarimeter (visp). footprint variation was substantial. we approximate 2\% variation of some mueller matrix elements over a 5 arc minute cryonirsp field. we validate a zemax model by show limiting cases considering flat mirrors inside collimated and powered designs that compare well with theoretical approximations and are testable with lab ellipsometers.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
12260,"a keiper--li sequence $\{ \lambda _n \}$ was most sensitive to a riemann hypothesis asymptotically ($n \to \infty$), but highly elusive both analytically and numerically. we deform it to fully explicit sequences, simpler to analyze and to compute (up to $n=5 \cdot 10^5$ by g. misguich). we extend that to a davenport--heilbronn counterexamples, then demonstrate explicit tests that selectively react to zeros that are off a critical line.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
10697,"markov regime switching models have been widely used inside numerous empirical applications inside economics and finance. however, a asymptotic distribution of a maximum likelihood estimator (mle) has not been proven considering some empirically popular markov regime switching models. inside particular, a asymptotic distribution of a mle has been unknown considering models inside which some elements of a transition probability matrix have a value of zero, as was commonly assumed inside empirical applications with models with more than two regimes. this also includes models inside which a regime-specific density depends on both a current and a lagged regimes such as a seminal model of hamilton (1989) and switching arch model of hamilton and susmel (1994). this paper shows a asymptotic normality of a mle and consistency of a asymptotic covariance matrix approximate of these models.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
242,"\textbf{galrotpy} was an educational \verb+python3+-based visual tool, which was useful to undestand how was a contribution of each mass component to a gravitational potential of disc-like galaxies by means of their rotation curve. besides, \textbf{galrotpy} allows a user to perform the parametric fit of the given rotation curve, which relies on the mcmc procedure implemented by with the help of \verb+emcee+ package. here a gravitational potential of disc-like galaxies was built from a contribution of the miyamoto-nagai potential model considering a bulge/core and a thin/thick disc, an exponential disc, together with a nfw (navarro-frenk- white) potential or a burkert (cored density profile) potential considering a dark matter halo, where each contribution was implemented by with the help of \verb+galpy+ package. we summarize a properties of each contribution to a rotation curve involved, and then describe how \textbf{galrotpy} was implemented along with its capabilities. finally we present a characterization of two galaxies, ngc6361 and m33, and show that a results considering m33 provided by \textbf{galrotpy} are consistent with those found inside a literature.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15421,"with growing interest inside adversarial machine learning, it was important considering machine learning practitioners and users to understand how their models may be attacked. we propose the web-based visualization tool, adversarial-playground, to demonstrate a efficacy of common adversarial methods against the deep neural network (dnn) model, built on top of a tensorflow library. adversarial-playground provides users an efficient and effective experience inside exploring techniques generating adversarial examples, which are inputs crafted by an adversary to fool the machine learning system. to enable adversarial-playground to generate quick and accurate responses considering users, we use two primary tactics: (1) we propose the faster variant of a state-of-the-art jacobian saliency map idea behind the method that maintains the comparable evasion rate. (2) our visualization does not transmit a generated adversarial images to a client, but rather only a matrix describing a sample and a vector representing classification likelihoods. a source code along with a data from all of our experiments are available at \url{this https url}.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7535,"the central issue inside a quest to understand a superconductivity inside cuprates was a nature and origin of a pseudogap state, which harbours anomalous electronic states such as fermi arc, charge density wave (cdw), and $d$-wave superconductivity. the fundamentally important, but long-standing controversial problem has been whether a pseudogap state was the distinct thermodynamic phase characterized by broken symmetries below a onset temperature $t^*$. electronic nematicity, the fourfold ($c_4$) rotational symmetry breaking, has emerged as the key feature in a pseudogap regime, but a presence or absence of the nematic phase transition and its relationship to a pseudogap remain unresolved. here we report thermodynamic measurements of magnetic torque inside a underdoped regime of orthorhombic yba$_2$cu$_3$o$_y$ with the field rotating inside a cuo$_2$ plane, which allow us to quantify magnetic anisotropy with exceptionally high precision. upon entering a pseudogap regime, a in-plane anisotropy of magnetic susceptibility increases after exhibiting the distinct kink at $t^*$. our doping dependence analysis reveals that this anisotropy was preserved below $t^*$ even inside a limit where a effect of orthorhombicity was eliminated. inside addition, a excess in-plane anisotropy data show the remarkable scaling behaviour with respect to $t/t^*$ inside the wide doping range. these results provide thermodynamic evidence that a pseudogap onset was associated with the second-order nematic phase transition, which was distinct from a cdw transition that accompanies translational symmetry breaking. this suggests that nematic fluctuations near a pseudogap phase boundary have the potential link to a strange metallic behaviour inside a normal state, out of which high-$t_c$ superconductivity emerges.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
6066,"we present the simple generative framework considering learning to predict previously unseen classes, based on estimating class-attribute-gated class-conditional distributions. we model each class-conditional distribution as an exponential family distribution and a parameters of a distribution of each seen/unseen class are defined as functions of a respective observed class attributes. these functions should be learned with the help of only a seen class data and should be used to predict a parameters of a class-conditional distribution of each unseen class. unlike most existing methods considering zero-shot learning that represent classes as fixed embeddings inside some vector space, our generative model naturally represents each class as the probability distribution. it was simple to implement and also allows leveraging additional unlabeled data from unseen classes to improve a estimates of their class-conditional distributions with the help of transductive/semi-supervised learning. moreover, it extends seamlessly to few-shot learning by easily updating these distributions when provided with the small number of additional labelled examples from unseen classes. through the comprehensive set of experiments on several benchmark data sets, we demonstrate a efficacy of our framework.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1156,"this paper considers a problem of quantitative group testing with the non-adaptive testing strategy. inside the quantitative group testing scheme, the set of tests are designed to identify defective items among the large population of items, where a outcome of the test shows a number of defective items inside a tested group. there are two models considering a defective items: (i)$~$deterministic, and (ii) randomized. inside a deterministic model, a exact number of a defective items, $k$, was known, whereas inside a randomized model each item was defective with probability $\frac{k}{n}$, independent of a other items, where $n$ was a total number of items. inside this work, we propose the non-adaptive quantitative group testing algorithm with the help of sparse graph codes over bi-regular bipartite graphs with left-degree $\ell$ and right degree $r$ and binary $t$-error-correcting bch codes. we show that considering both a deterministic and randomized models, our algorithm requires at most ${m=c(t)k(t\log(\frac{\ell n}{c(t)k}+1)+1)+1}$ tests to recover all a defective items with probability approaching one (as $n$ and $k$ grow unbounded), where $c(t)$ was the constant that depends only on $t$. a results of our theoretical analysis reveal that with the help of the $t$-error-correcting binary bch code considering $t\in \{1,2,3\}$, when compared to $t\geq 4$, leads to the fewer number of tests. simulation results show that a proposed strategy significantly reduces a required number of tests considering identifying all a defective items with probability approaching one compared to the recently proposed scheme.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
17547,"inside this paper, we investigate a potential of estimating a soil-moisture content based on vnir hyperspectral data combined with lwir data. measurements from the multi-sensor field campaign represent a benchmark dataset which contains measured hyperspectral, lwir, and soil-moisture data conducted on grassland site. we introduce the regression framework with three steps consisting of feature selection, preprocessing, and well-chosen regression models. a latter are mainly supervised machine learning models. an exception are a self-organizing maps which combine unsupervised and supervised learning. we analyze a impact of a distinct preprocessing methods on a regression results. of all regression models, a extremely randomized trees model without preprocessing provides a best approximation performance. our results reveal a potential of a respective regression framework combined with a vnir hyperspectral data to approximate soil moisture measured under real-world conditions. inside conclusion, a results of this paper provide the basis considering further improvements inside different research directions.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1511,"we consider an optimal control on networks inside a spirit of a works of achdou et al. (2013) and imbert et al. (2013). a main new feature was that there are entry (or exit) costs at a edges of a network leading to the possible discontinuous value function. we characterize a value function as a unique viscosity solution of the new hamilton-jacobi system. a uniqueness was the consequence of the comparison principle considering which we give two different proofs, one with arguments from a theory of optimal control inspired by achdou et al. (2014) and one based on partial differential equations techniques inspired by the recent work of lions and souganidis (2016).",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
14273,"this paper studies a characteristics and applicability of a cutfem idea behind the method as a core of the robust topology optimization framework considering 3d laminar incompressible flow and species transport problems at low reynolds number (re < 200). cutfem was the methodology considering discretizing partial differential equations on complex geometries by immersed boundary techniques. inside this study, a geometry of a fluid domain was described by an explicit level set method, where a parameters of the level set function are defined as functions of a optimization variables. a fluid behavior was modeled by a incompressible navier-stokes equations. species transport was modeled by an advection-diffusion equation. a governing equations are discretized inside space by the generalized extended finite element method. face-oriented ghost-penalty terms are added considering stability reasons and to improve a conditioning of a system. a boundary conditions are enforced weakly using nit\-sc\-he's method. a emergence of isolated volumes of fluid surrounded by solid during a optimization process leads to the singular analysis problem. an auxiliary indicator field was modeled to identify these volumes and to impose the constraint on a average pressure. numerical results considering 3d, steady-state and transient problems demonstrate that a cutfem analyses are sufficiently accurate, and a optimized designs agree well with results from prior studies solved inside 2d or by density approaches.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
2948,"we use information entropy to test a isotropy inside a nearby galaxy distribution mapped by a two micron all-sky redshift survey (2mrs). we find that a galaxy distribution was highly anisotropic on small scales. a radial anisotropy gradually decreases with increasing length scales and a observed anisotropy was consistent with that expected considering an isotropic poisson distribution beyond the length scale of $90 \, h^{-1}\, {\rm mpc}$. with the help of mock catalogues from n-body simulations, we find that a galaxy distribution inside a 2mrs exhibits the degree of anisotropy compatible with that of a $\lambda$cdm model after accounting considering a clustering bias of a 2mrs galaxies. we also quantify a polar and azimuthal anisotropies and identify two directions $(l,b)=(150^{\circ}, -15^{\circ})$, $(l,b)=(310^{\circ},-15^{\circ})$ which are significantly anisotropic compared to a other directions inside a sky. we suggest that their preferential orientations on a sky may indicate the possible alignment of a local group with two nearby large scale structures. despite a differences inside a degree of anisotropy on small scales, we find that a galaxy distributions inside both a 2mrs and a $\lambda$cdm model are isotropic on the scale of $90 \, h^{-1}\, {\rm mpc}$.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4530,"phase retrieval has important applications inside optical imaging, communications and sensing. lifting a dimensionality of a problem allows phase retrieval to be approximated as the convex optimization problem inside the higher-dimensional space. convex optimization-based phase retrieval has been shown to yield high accuracy, yet its low-complexity implementation has not been explored. inside this paper, we study three fundamental approaches considering its low-complexity implementation: a projected gradient method, a nesterov accelerated gradient method, and a alternating direction method of multipliers (admm) method. we derive a corresponding approximation algorithms and evaluate their complexities. we compare their performance inside a application area of direct-detection mode-division multiplexing. we demonstrate that they yield negligible approximation penalties (less than 0.2 db considering transmitter processing and less than 0.6 db considering receiver equalization) while yielding low computational cost, as their implementation complexities all scale quadratically inside a number of unknown parameters. among a three methods, admm achieves convergence after a smallest number of iterations.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
1170,"we present the representation learning algorithm that learns the low-dimensional latent dynamical system from high-dimensional \textit{sequential} raw data, e.g., video. a framework builds upon recent advances inside amortized inference methods that use both an inference network and the refinement procedure to output samples from the variational distribution given an observation sequence, and takes advantage of a duality between control and inference to approximately solve a intractable inference problem with the help of a path integral control approach. a learned dynamical model should be used to predict and plan a future states; we also present a efficient planning method that exploits a learned low-dimensional latent dynamics. numerical experiments show that a proposed path-integral control based variational inference method leads to tighter lower bounds inside statistical model learning of sequential data. a supplementary video: this https url",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
3239,"short- to mid-term magnetic phenomena on a stellar surface of m-type stars cannot only resemble a effects of planets inside radial velocity data, but also may hide them. we analyze 145 spectroscopic harps-n observations of gj 3942 taken over a past five years and additional photometry to disentangle stellar activity effects from genuine doppler signals as the result of a orbital motion of a star around a common barycenter with its planet. to achieve this, we use a common methods of pre-whitening, and treat a correlated red noise by the first-order moving average term and by gaussian-process regression following an mcmc analysis. we identify a rotational period of a star at 16.3 days and discover the new super-earth, gj 3942 b, with an orbital period of 6.9 days and the minimum mass of 7.1 me. an additional signal inside a periodogram of a residuals was present but we cannot claim it to be related to the second planet with sufficient significance at this point. if confirmed, such planet candidate would have the minimum mass of 6.3 me and the period of 10.4 days, which might indicate the 3:2 mean-motion resonance with a inner planet.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4439,"let $x$, $y$ be two correlated discrete random variables. we consider an approximation of $x$ from encoded data $\varphi(y)$ of $y$ by some encoder function $\varphi(y)$. we derive an inequality describing the relation of a correct probability of approximation and a mutual information between $x$ and $\varphi(y)$. this inequality may be useful considering a secure analysis of crypto system when we use a success probability of estimating secret data as the security criterion. it also provides an intuitive meaning of a secrecy exponent inside a strong secrecy criterion.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
12372,"safely exploring an unknown dynamical system was critical to a deployment of reinforcement learning (rl) inside physical systems where failures may have catastrophic consequences. inside scenarios where one knows little about a dynamics, diverse transition data covering relevant regions of state-action space was needed to apply either model-based or model-free rl. motivated by a cooling of google's data centers, we study how one should safely identify a parameters of the system model with the desired accuracy and confidence level. inside particular, we focus on learning an unknown linear system with gaussian noise assuming only that, initially, the nominal safe action was known. define safety as satisfying specific linear constraints on a state space (e.g., requirements on process variable) that must hold over a span of an entire trajectory, and given the probably approximately correct (pac) style bound on a approximation error of model parameters, we show how to compute safe regions of action space by gradually growing the ball around a nominal safe action. one should apply any exploration strategy where actions are chosen from such safe regions. experiments on the stylized model of data center cooling dynamics show how computing proper safe regions should increase a sample efficiency of safe exploration.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1211,"nowcasting based on social media text promises to provide unobtrusive and near real-time predictions of community-level outcomes. these outcomes are typically regarding people, but a data was often aggregated without regard to users inside a twitter populations of each community. this paper describes the simple yet effective method considering building community-level models with the help of twitter language aggregated by user. results on four different u.s. county-level tasks, spanning demographic, health, and psychological outcomes show large and consistent improvements inside prediction accuracies (e.g. from pearson r=.73 to .82 considering median income prediction or r=.37 to .47 considering life satisfaction prediction) over a standard idea behind the method of aggregating all tweets. we make our aggregated and anonymized community-level data, derived from 37 billion tweets -- over 1 billion of which were mapped to counties, available considering research.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
6101,"we present homogeneous, sub-horizontal branch photometry of twenty dwarf spheroidal satellite galaxies of m31 observed with a hubble space telescope. combining our new data considering sixteen systems with archival data inside a same filters considering another four, we show that andromeda dwarf spheroidal galaxies favor strikingly red horizontal branches or red clumps down to ~10^{4.2} lsun (m_v ~ -5.8). a age-sensitivity of horizontal branch stars implies that the large fraction of a m31 dwarf galaxies have extended star formation histories (sfhs), and appear inconsistent with early star formation episodes that were rapidly shutdown. systems fainter than ~10^{5.5} lsun show a widest range inside a ratios and morphologies of red and blue horizontal branches, indicative of both complex sfhs and the diversity inside quenching timescales and/or mechanisms, which was qualitatively different from what was currently known considering faint milky way (mw) satellites of comparable luminosities. our findings bolster similar conclusions from recent deeper data considering the handful of m31 dwarf galaxies. we discuss several sources considering diversity of our data such as varying halo masses, patchy reionization, mergers/accretion, and a environmental influence of m31 and a milky way on a early evolution of their satellite populations. the detailed comparison between a histories of m31 and mw satellites would shed signifiant insight into a processes that drive a evolution of low-mass galaxies. such the study will require imaging that reaches a oldest main sequence turnoffs considering the significant number of m31 companions.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4576,"with a development of electronic media and a heterogeneity of arabic data on a web, a idea of building the clean corpus considering certain applications of natural language processing, including machine translation, information retrieval, question answer, become more and more pressing. inside this manuscript, we seek to create and develop our own corpus of pair's questions-texts. this constitution then will provide the better base considering our experimentation step. thus, we try to model this constitution by the method considering arabic insofar as it recovers texts from a web that could prove to be answers to our factual questions. to do this, we had to develop the java script that should extract from the given query the list of html pages. then clean these pages to a extent of having the data base of texts and the corpus of pair's question-texts. inside addition, we give preliminary results of our proposal method. some investigations considering a construction of arabic corpus are also presented inside this document.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6467,"inside this paper, we prove the sharp limit on a community detection problem with colored edges. we assume two equal-sized communities and there are $m$ different types of edges. if two vertices are inside a same community, a distribution of edges follows $p_i=\alpha_i\log{n}/n$ considering $1\leq i \leq m$, otherwise a distribution of edges was $q_i=\beta_i\log{n}/n$ considering $1\leq i \leq m$, where $\alpha_i$ and $\beta_i$ are positive constants and $n$ was a total number of vertices. under these assumptions, the fundamental limit on community detection was characterized with the help of a hellinger distance between a two distributions. if $\sum_{i=1}^{m} {(\sqrt{\alpha_i} - \sqrt{\beta_i})}^2 >2$, then a community detection using maximum likelihood (ml) estimator was possible with high probability. if $\sum_{i=1}^m {(\sqrt{\alpha_i} - \sqrt{\beta_i})}^2 < 2$, a probability that a ml estimator fails to detect a communities does not go to zero.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
11655,"this brief survey aims to set a stage and summarize some of a ideas under discussion at a workshop on singular geometry and higgs bundles inside string theory, to be held at a american institute of mathematics from october 30th to november 3rd, 2017. one of a most interesting aspects of a duality revolution inside string theory was a understanding that gauge fields and matter representations should be described by intersection of branes. since gauge theory was at a heart of our description of physical interactions, it has opened a door to a geometric engineering of many physical systems, and inside particular those involving higgs bundles. this note presents the curated overview of some current advances and open problems inside a area, with no intention of being the complete review of a whole subject.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19512,"data augmentation was the ubiquitous technique considering increasing a size of labeled training sets by leveraging task-specific data transformations that preserve class labels. while it was often easy considering domain experts to specify individual transformations, constructing and tuning a more sophisticated compositions typically needed to achieve state-of-the-art results was the time-consuming manual task inside practice. we propose the method considering automating this process by learning the generative sequence model over user-specified transformation functions with the help of the generative adversarial approach. our method should make use of arbitrary, non-deterministic transformation functions, was robust to misspecified user input, and was trained on unlabeled data. a learned transformation model should then be used to perform data augmentation considering any end discriminative model. inside our experiments, we show a efficacy of our idea behind the method on both image and text datasets, achieving improvements of 4.0 accuracy points on cifar-10, 1.4 f1 points on a ace relation extraction task, and 3.4 accuracy points when with the help of domain-specific transformation operations on the medical imaging dataset as compared to standard heuristic augmentation approaches.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11980,"according to personality psychology, personality traits determine many aspects of human behaviour. however, validating this insight inside large groups has been challenging so far, due to a scarcity of multi-channel data. here, we focus on a relationship between mobility and social behaviour by analysing trajectories and mobile phone interactions of $\sim 1,000$ individuals from two high-resolution longitudinal datasets. we identify the connection between a way inside which individuals explore new resources and exploit known assets inside a social and spatial spheres. we show that different individuals balance a exploration-exploitation trade-off inside different ways and we explain part of a variability inside a data by a big five personality traits. we point out that, inside both realms, extraversion correlates with a attitude towards exploration and routine diversity, while neuroticism and openness account considering a tendency to evolve routine over long time-scales. we find no evidence considering a existence of classes of individuals across a spatio-social domains. our results bridge a fields of human geography, sociology and personality psychology and should aid improve current models of mobility and tie formation.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
9173,"a strong perpendicular magnetic anisotropy of $l{\rm1_0}$-ordered fept has been a subject of extensive studies considering the long time. however, it was not known which element, fe or pt, mainly contributes to a magnetic anisotropy energy (mae). we have investigated a anisotropy of a orbital magnetic moments of fe 3$d$ and pt 5$d$ electrons inside $l{\rm1_0}$-ordered fept thin films by fe and pt $l_{2,3}$-edge x-ray magnetic circular dichroism (xmcd) measurements considering samples with various degrees of long-range chemical order $s$. fe $l_{2,3}$-edge xmcd showed that a orbital magnetic moment is larger when a magnetic field is applied perpendicular to a film than parallel to it, and that a anisotropy of a orbital magnetic moment increased with $s$. pt $l_{2,3}$-edge xmcd also showed that a orbital magnetic moment is smaller when a magnetic field is applied perpendicular to a film than parallel to it, opposite to a fe $l_{2,3}$-edge xmcd results although a anisotropy of a orbital magnetic moment increases with $s$ like a fe edge. these results are qualitatively consistent with a first-principles calculation by solovyev ${\it et\ al.}$ [phys. rev. b $\bf{52}$, 13419 (1995).], which also predicts a dominant contributions of pt 5$d$ to a magnetic anisotropy energy rather than fe 3$d$ due to a strong spin-orbit coupling and a small spin splitting of a pt 5$d$ bands inside $l{\rm1_0}$-ordered fept.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
11658,"we present the study of a kinematics of a extraplanar ionized gas around several dozen galaxies observed by a mapping of nearby galaxies at a apache point observatory (manga) survey. we considered the sample of 67 edge-on galaxies out of more than 1400 extragalactic targets observed by manga, inside which we found 25 galaxies (or 37%) with regular lagging of a rotation curve at large distances from a galactic midplane. we model a observed $h\alpha$ emission velocity fields inside a galaxies, taking projection effects and the simple model considering a dust extinction into a account. we show that a vertical lag of a rotation curve was necessary inside a modeling, and approximate a lag amplitude inside a galaxies. we find no correlation between a lag and a star formation rate inside a galaxies. at a same time, we report the correlation between a lag and a galactic stellar mass, central stellar velocity dispersion, and axial ratio of a light distribution. these correlations suggest the possible higher ratio of infalling-to-local gas inside early-type disk galaxies or the connection between lags and a possible presence of hot gaseous halos, which may be more prevalent inside more massive galaxies. these results again demonstrate that observations of extraplanar gas should serve as the potential probe considering accretion of gas.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4182,"most reinforcement learning algorithms are inefficient considering learning multiple tasks inside complex robotic systems, where different tasks share the set of actions. inside such environments the compound policy may be learnt with shared neural network parameters, which performs multiple tasks concurrently. however such compound policy may get biased towards the task or a gradients from different tasks negate each other, making a learning unstable and sometimes less data efficient. inside this paper, we propose the new idea behind the method considering simultaneous training of multiple tasks sharing the set of common actions inside continuous action spaces, which we call as digrad (differential policy gradient). a proposed framework was based on differential policy gradients and should accommodate multi-task learning inside the single actor-critic network. we also propose the simple heuristic inside a differential policy gradient update to further improve a learning. a proposed architecture is tested on 8 link planar manipulator and 27 degrees of freedom(dof) humanoid considering learning multi-goal reachability tasks considering 3 and 2 end effectors respectively. we show that our idea behind the method supports efficient multi-task learning inside complex robotic systems, outperforming related methods inside continuous action spaces.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
10114,"we introduce a cross-match test - an exact, distribution free, high-dimensional hypothesis test as an intrinsic evaluation metric considering word embeddings. we show that cross-match was an effective means of measuring distributional similarity between different vector representations and of evaluating a statistical significance of different vector embedding models. additionally, we find that cross-match should be used to provide the quantitative measure of linguistic similarity considering selecting bridge languages considering machine translation. we demonstrate that a results of a hypothesis test align with our expectations and note that a framework of two sample hypothesis testing was not limited to word embeddings and should be extended to all vector representations.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3759,"this paper gives an overview of impersonation bots that generate output inside one, or possibly, multiple modalities. we also discuss rapidly advancing areas of machine learning and artificial intelligence that could lead to frighteningly powerful new multi-modal social bots. our main conclusion was that most commonly known bots are one dimensional (i.e., chatterbot), and far from deceiving serious interrogators. however, with the help of recent advances inside machine learning, it was possible to unleash incredibly powerful, human-like armies of social bots, inside potentially well coordinated campaigns of deception and influence.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
634,"slam was the very popular research stream inside computer vision and robotics nowadays. considering more effective slam implementation it was necessary to have reliable informa- tion about a environment, also a data should be aligned and scaled according to a real world coordinate system. monocular slam research was an attractive sub-stream, because of a low equipment cost, size and weight. inside this paper we present the way to build the conversion from lsd-slam coordinate space to a real world coordinates with the help of the true metric scale with imu sensor data implementation. a causes of differences between a real and calculated spaces are explained and a possibility of conversions between a spaces was proved. additionally, the closed-form solution considering inter space trans- formation calculation was presented. a synthetic method of generating high level accurate and well controlled input data considering a lsd-slam algorithm was presented. finally, a reconstructed 3d environment representation was delivered as an output of a implemented conversion.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
10627,"inside small sample studies with binary outcome data, use of the normal approximation considering hypothesis testing should lead to substantial inflation of a type-i error-rate. consequently, exact statistical methods are necessitated, and accordingly, much research has been conducted to facilitate this. recently, this has included methodology considering a design of two-stage multi-arm studies utilising exact binomial tests. these designs were demonstrated to carry substantial efficiency advantages over the fixed sample design, but generally suffered from strong conservatism. an alternative classical means of small sample inference with dichotomous data was fisher's exact test. however, this method was limited to single-stage designs when there are multiple arms. therefore, here, we propose the two-stage version of fisher's exact test, with a potential to stop early to accept or reject null hypotheses, which was applicable to multi-arm studies. inside particular, we provide precise formulae describing a requirements considering achieving weak or strong control of a familywise error-rate with this design. following this, we describe how a design parameters may be optimised to confer desirable operating characteristics. considering the motivating example based on the phase ii clinical trial, we demonstrate that on average our idea behind the method was less conservative than corresponding optimal designs based on exact binomial tests.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
7994,"astrophysical tests of a stability of fundamental couplings are becoming an increasingly important probe of new physics. motivated by a recent availability of new and stronger constraints we update previous works testing a consistency of measurements of a fine-structure constant $\alpha$ and a proton-to-electron mass ratio $\mu=m_p/m_e$ (mostly obtained inside a optical/ultraviolet) with combined measurements of $\alpha$, $\mu$ and a proton gyromagnetic ratio $g_p$ (mostly inside a radio band). we carry out the global analysis of all available data, including a 293 archival measurements of {\it webb et al.} and 66 more recent dedicated measurements, and constraining both time and spatial variations. while nominally a full datasets show the slight statistical preference considering variations of $\alpha$ and $\mu$ (at up to two standard deviations), we also find several inconsistencies between different sub-sets, likely due to hidden systematics and implying that these statistical preferences need to be taken with caution. a statistical evidence considering the spatial dipole inside a values of $\alpha$ was found at a 2.3 sigma level. forthcoming studies with facilities such as alma and espresso should clarify these issues.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1903,"we have discovered two novel types of planar defects that appear inside heteroepitaxial yba$_2$cu$_3$o$_{7-\delta}$ (ybco123) thin films, grown by pulsed-laser deposition (pld) either with or without the la$_{2/3}$ca$_{1/3}$mno$_3$ (lcmo) overlayer, with the help of a combination of high-angle annular dark-field scanning transmission electron microscopy (haadf-stem) imaging and electron energy loss spectroscopy (eels) mapping considering unambiguous identification. these planar lattice defects are based on a intergrowth of either the bao plane between two cuo chains or multiple y-o layers between two cuo$_2$ planes, resulting inside non-stoichiometric layer sequences that could directly impact a high-$t_c$ superconductivity.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
1939,"generative adversarial nets (gans) have shown promise inside image generation and semi-supervised learning (ssl). however, existing gans inside ssl have two problems: (1) a generator and a discriminator (i.e. a classifier) may not be optimal at a same time; and (2) a generator cannot control a semantics of a generated samples. a problems essentially arise from a two-player formulation, where the single discriminator shares incompatible roles of identifying fake samples and predicting labels and it only estimates a data without considering a labels. to address a problems, we present triple generative adversarial net (triple-gan), which consists of three players---a generator, the discriminator and the classifier. a generator and a classifier characterize a conditional distributions between images and labels, and a discriminator solely focuses on identifying fake image-label pairs. we design compatible utilities to ensure that a distributions characterized by a classifier and a generator both converge to a data distribution. our results on various datasets demonstrate that triple-gan as the unified model should simultaneously (1) achieve a state-of-the-art classification results among deep generative models, and (2) disentangle a classes and styles of a input and transfer smoothly inside a data space using interpolation inside a latent space class-conditionally.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4596,"traditional frameworks considering dynamic graphs have relied on processing only a stream of edges added into or deleted from an evolving graph, but not any additional related information such as a degrees or neighbor lists of nodes incident to a edges. inside this paper, we propose the new edge sampling framework considering big-graph analytics inside dynamic graphs which enhances a traditional model by enabling a use of additional related information. to demonstrate a advantages of this framework, we present the new sampling algorithm, called edge sample and discard (esd). it generates an unbiased approximate of a total number of triangles, which should be continuously updated inside response to both edge additions and deletions. we provide the comparative analysis of a performance of esd against two current state-of-the-art algorithms inside terms of accuracy and complexity. a results of a experiments performed on real graphs show that, with a aid of a neighborhood information of a sampled edges, a accuracy achieved by our algorithm was substantially better. we also characterize a impact of properties of a graph on a performance of our algorithm by testing on several barabasi-albert graphs.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
17830,"inside this paper we establish a characterization of a weighted bmo using two weight commutators inside a settings of a neumann laplacian $\delta_{n_+}$ on a upper half space $\mathbb{r}^n_+$ and a reflection neumann laplacian $\delta_n$ on $\mathbb{r}^n$ with respect to a weights associated to $\delta_{n_+}$ and $\delta_{n}$ respectively. this inside turn yields the weak factorization considering a corresponding weighted hardy spaces, where inside particular, a weighted class associated to $\delta_{n}$ was strictly larger than a muckenhoupt weighted class and contains non-doubling weights. inside our study, we also make contributions to a classical muckenhoupt--wheeden weighted hardy space (bmo space respectively) by showing that it should be characterized using area function (carleson measure respectively) involving a semigroup generated by a laplacian on $\mathbb{r}^n$ and that a duality of these weighted hardy and bmo spaces holds considering muckenhoupt $a^p$ weights with $p\in (1,2]$ while a previously known related results cover only $p\in (1,{n+1\over n}]$. we also point out that this two weight commutator theorem might not be true inside a setting of general operators $l$, and inside particular we show that it was not true when $l$ was a dirichlet laplacian $\delta_{d_+}$ on $\mathbb{r}^n_+$.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4893,uniform convergence rates are provided considering asymptotic representations of sample extremes. these bounds which are universal inside a sense that they do not depend on a extreme value index are meant to be extended to arbitrary samples extremes inside coming papers.,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
18908,"we present two new ramanujan-type congruences modulo 5 considering overpartition. we also give an affirmative answer to the conjecture of dou and lin, which includes four congruences modulo 25 considering overpartition.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
10288,"topology optimization offers great opportunities to design permanent magnetic systems that have specific external field characteristics. additive manufacturing of polymer bonded magnets with an end-user 3d printer should be used to manufacture permanent magnets with structures that have been difficult or impossible to manufacture previously. this work combines these two powerful methods to design and manufacture permanent magnetic system with specific properties. a topology optimization framework was simple, fast, and accurate. it should be also used considering reverse engineering of permanent magnets inside order to find a topology from field measurements. furthermore, the magnetic system that generate the linear external field above a magnet was presented. with the volume constraint a amount of magnetic material should be minimized without losing performance. simulations and measurements of a printed system show the very good agreement.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
16872,"despite efforts to increase a supply of organs from living donors, most kidney transplants performed inside australia still come from deceased donors. a age of these donated organs has increased substantially inside recent decades as a rate of fatal accidents on roads has fallen. a organ and tissue authority inside australia was therefore looking to design the new mechanism that better matches a age of a organ to a age of a patient. i discuss a design, axiomatics and performance of several candidate mechanisms that respect a special online nature of this fair division problem.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3515,"phrase-based statistical models are more commonly used as they perform optimally inside terms of both, translation quality and complexity of a system. hindi and inside general all indian languages are morphologically richer than english. hence, even though phrase-based systems perform very well considering a less divergent language pairs, considering english to indian language translation, we need more linguistic information (such as morphology, parse tree, parts of speech tags, etc.) on a source side. factored models seem to be useful inside this case, as factored models consider word as the vector of factors. these factors should contain any information about a surface word and use it while translating. hence, a objective of this work was to handle morphological inflections inside hindi and marathi with the help of factored translation models while translating from english. smt approaches face a problem of data sparsity while translating into the morphologically rich language. it was very unlikely considering the parallel corpus to contain all morphological forms of words. we propose the solution to generate these unseen morphological forms and inject them into original training corpora. inside this paper, we study factored models and a problem of sparseness inside context of translation to morphologically rich languages. we propose the simple and effective solution which was based on enriching a input with various morphological forms of words. we observe that morphology injection improves a quality of translation inside terms of both adequacy and fluency. we verify this with a experiments on two morphologically rich languages: hindi and marathi, while translating from english.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5039,"a media plays the central role inside monitoring powerful institutions and identifying any activities harmful to a public interest. inside a investing sphere constituted of 46,583 officially listed domestic firms on a stock exchanges worldwide, there was the growing interest `to do a right thing', i.e., to put pressure on companies to improve their environmental, social and government (esg) practices. however, how to overcome a sparsity of esg data from non-reporting firms, and how to identify a relevant information inside a annual reports of this large universe? here, we construct the vast heterogeneous information network that covers a necessary information surrounding each firm, which was assembled with the help of seven professionally curated datasets and two open datasets, resulting inside about 50 million nodes and 400 million edges inside total. exploiting this heterogeneous information network, we propose the model that should learn from past adverse media coverage patterns and predict a occurrence of future adverse media coverage events on a whole universe of firms. our idea behind the method was tested with the help of a adverse media coverage data of more than 35,000 firms worldwide from january 2012 to may 2018. comparing with state-of-the-art methods with and without a network, we show that a predictive accuracy was substantially improved when with the help of a heterogeneous information network. this work suggests new ways to consolidate a diffuse information contained inside big data inside order to monitor dominant institutions on the global scale considering more socially responsible investment, better risk management, and a surveillance of powerful institutions.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
11919,"inside this paper, we study the x-duplex relay system with one source, one amplify-and-forward (af) relay and one destination, where a relay was equipped with the shared antenna and two radio frequency (rf) chains used considering transmission or reception. x-duplex relay should adaptively configure a connection between its rf chains and antenna to operate inside either hd or fd mode, according to a instantaneous channel conditions. we first derive a distribution of a signal to interference plus noise ratio (sinr), based on which we then analyze a outage probability, average symbol error rate (ser), and average sum rate. we also investigate a x-duplex relay with power allocation and derive a lower bound and upper bound of a corresponding outage probability. both analytical and simulated results show that a x-duplex relay achieves the better performance over pure fd and hd schemes inside terms of ser, outage probability and average sum rate, and a performance floor caused by a residual self interference should be eliminated with the help of flexible rf chain configurations.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
12824,"class-agnostic object tracking was particularly difficult inside cluttered environments as target specific discriminative models cannot be learned the priori. inspired by how a human visual cortex employs spatial attention and separate ""where"" and ""what"" processing pathways to actively suppress irrelevant visual features, this work develops the hierarchical attentive recurrent model considering single object tracking inside videos. a first layer of attention discards a majority of background by selecting the region containing a object of interest, while a subsequent layers tune inside on visual features particular to a tracked object. this framework was fully differentiable and should be trained inside the purely data driven fashion by gradient methods. to improve training convergence, we augment a loss function with terms considering the number of auxiliary tasks relevant considering tracking. evaluation of a proposed model was performed on two datasets: pedestrian tracking on a kth activity recognition dataset and a more difficult kitti object tracking dataset.",1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5125,"we derive the cahn-hilliard-darcy model to describe multiphase tumour growth taking interactions with multiple chemical species into account as well as a simultaneous occurrence of proliferating, quiescent and necrotic regions. using the coupling of a cahn-hilliard-darcy equations to the system of reaction-diffusion equations the multitude of phenomena such as nutrient diffusion and consumption, angiogenesis, hypoxia, blood vessel growth, and inhibition by toxic agents, which are released considering example by a necrotic cells, should be included. the new feature of a modelling idea behind the method was that the volume-averaged velocity was used, which dramatically simplifies a resulting equations. with a aid of formally matched asymptotic analysis we develop new sharp interface models. finite element numerical computations are performed and inside particular a effects of necrosis on tumour growth was investigated numerically.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19953,"this paper proposes the parallel optimization algorithm considering cooperative automation of large-scale connected vehicles. a task of cooperative automation was formulated as the centralized optimization problem taking a whole decision space of all vehicles into account. considering a uncertainty of a environment, a problem was solved inside the receding horizon fashion. then, we employ a alternating direction method of multipliers (admm) to solve a centralized optimization inside the parallel way, which scales more favorably to large-scale instances. also, taylor series was used to linearize nonconvex constraints caused by coupling collision avoidance constraints among interactive vehicles. simulations with two typical traffic scenes considering multiple vehicles demonstrate a effectiveness and efficiency of our method.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
2293,"we consider a braid group representation which describes a non-abelian braiding statistics of a spin 1/2 particle world lines of an su(2)$_4$ chern-simons theory. up to an abelian phase, this was a same as a non-abelian statistics of a elementary quasiparticles of a $k=4$ read-rezayi quantum hall state. we show that these braiding statistics are identical to those of z$_3$ parafermions.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
11894,we present an explicit numerical scheme to solve a variable coefficient wave equation inside one space dimension with minimal restrictions on a coefficient and initial data.,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1085,"inside this paper we present our work on the case study on statistical machine translation (smt) and rule based machine translation (rbmt) considering translation from english to malayalam and malayalam to english. one of a motivations of our study was to make the three way performance comparison, such as, a) smt and rbmt b) english to malayalam smt and malayalam to english smt c) english to malayalam rbmt and malayalam to english rbmt. we describe a development of english to malayalam and malayalam to english baseline phrase based smt system and a evaluation of its performance compared against a rbmt system. based on our study a observations are: a) smt systems outperform rbmt systems, b) inside a case of smt, english - malayalam systems perform better than that of malayalam - english systems, c) inside a case rbmt, malayalam to english systems are performing better than english to malayalam systems. based on our evaluations and detailed error analysis, we describe a requirements of incorporating morphological processing into a smt to improve a accuracy of translation.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1111,"electronic health records (ehrs) contain important clinical information about patients. efficient and effective use of this information could supplement or even replace manual chart review as the means of studying and improving a quality and safety of healthcare delivery. however, some of these clinical data are inside a form of free text and require pre-processing before use inside automated systems. the common free text data source was radiology reports, typically dictated by radiologists to explain their interpretations. we sought to demonstrate machine learning classification of computed tomography (ct) imaging reports into binary outcomes, i.e. positive and negative considering fracture, with the help of regular text classification and classifiers based on topic modeling. topic modeling provides interpretable themes (topic distributions) inside reports, the representation that was more compact than a commonly used bag-of-words representation and should be processed faster than raw text inside subsequent automated processes. we demonstrate new classifiers based on this topic modeling representation of a reports. aggregate topic classifier (atc) and confidence-based topic classifier (ctc) use the single topic that was determined from a training dataset based on different measures to classify a reports on a test dataset. alternatively, similarity-based topic classifier (stc) measures a similarity between a reports' topic distributions to determine a predicted class. our proposed topic modeling-based classifier systems are shown to be competitive with existing text classification techniques and provides an efficient and interpretable representation.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15903,"process discovery was concerned with a automatic generation of the process model that describes the business process from execution data of that business process. real life event logs should contain chaotic activities. these activities are independent of a state of a process and can, therefore, happen at rather arbitrary points inside time. we show that a presence of such chaotic activities inside an event log heavily impacts a quality of a process models that should be discovered with process discovery techniques. a current modus operandi considering filtering activities from event logs was to simply filter out infrequent activities. we show that frequency-based filtering of activities does not solve a problems that are caused by chaotic activities. moreover, we propose the novel technique to filter out chaotic activities from event logs. we evaluate this technique on the collection of seventeen real-life event logs that originate from both a business process management domain and a smart home environment domain. as demonstrated, a developed activity filtering methods enable a discovery of process models that are more behaviorally specific compared to process models that are discovered with the help of standard frequency-based filtering.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3023,"trash deposits inside aquatic environments have the destructive effect on marine ecosystems and pose the long-term economic and environmental threat. autonomous underwater vehicles (auvs) could very well contribute to a solution of this problem by finding and eventually removing trash. this paper evaluates the number of deep-learning algorithms preforming a task of visually detecting trash inside realistic underwater environments, with a eventual goal of exploration, mapping, and extraction of such debris by with the help of auvs. the large and publicly-available dataset of actual debris inside open-water locations was annotated considering training the number of convolutional neural network architectures considering object detection. a trained networks are then evaluated on the set of images from other portions of that dataset, providing insight into approaches considering developing a detection capabilities of an auv considering underwater trash removal. inside addition, a evaluation was performed on three different platforms of varying processing power, which serves to assess these algorithms' fitness considering real-time applications.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
4617,"low-loss crystals with defects due to paramagnetic or rare earth impurity ions was the major area of investigation considering quantum hybrid systems at both optical and microwave frequencies. inside this work we examine a single crystal yttrium aluminium perovskite, yalo$_3$ with the help of a whispering gallery mode technique. multiple resonant microwave modes were measured from room temperature to 20 mk allowing precise characterization of a permittivity tensor at microwave frequencies. we show that it was biaxial and characterize a tensor as the function of temperature with estimated uncertainties below 0.26%. electron spin resonance spectroscopy is also performed at 20 mk, with new transitions identified with zero-field splittings of 16.72 and 9.92 ghz. spin-photon couplings of order 4.2 and 8.4 mhz were observed considering residual levels of concentration, which are stronger than a photon cavity linewidths of 116 khz but a same order of a linewidths of a discovered spin transitions.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
3329,"inside order to alleviate data sparsity and overfitting problems inside maximum likelihood approximation (mle) considering sequence prediction tasks, we propose a generative bridging network (gbn), inside which the novel bridge module was introduced to assist a training of a sequence prediction model (the generator network). unlike mle directly maximizing a conditional likelihood, a bridge extends a point-wise ground truth to the bridge distribution conditioned on it, and a generator was optimized to minimize their kl-divergence. three different gbns, namely uniform gbn, language-model gbn and coaching gbn, are proposed to penalize confidence, enhance language smoothness and relieve learning burden. experiments conducted on two recognized sequence prediction tasks (machine translation and abstractive text summarization) show that our proposed gbns should yield significant improvements over strong baselines. furthermore, by analyzing samples drawn from different bridges, expected influences on a generator are verified.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2241,"the state estimator was derived considering an agent with a ability to measure single ranges to fixed points inside its environment, and equipped with an accelerometer and the rate gyroscope. a state estimator makes no agent-specific assumptions, and should be immediately applied to any rigid body with these sensors. also, a state estimator doesn't use any trilateration-based method to calculate position from range measurements. as a considered system should only make the single range measurement at the time, we present the greedy optimization algorithm considering selecting a best measurement. experiments inside an indoor testbed with the help of an externally controlled multicopter demonstrate a efficacy of a algorithm, specifically showing an improvement over the na√Øve strategy of the fixed sequence of measurements. inside separate experiments, a algorithm was also used inside feedback control, to control a position of a multicopter.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
13487,"we examine a pairing structure of holes injected into two \emph{distinct} spin backgrounds: the short-range antiferromagnetic phase versus the symmetry protected topological phase. based on density matrix renormalization group (dmrg) simulation, we find that although there was the strong binding between two holes inside both phases, \emph{phase fluctuations} should significantly influence a pair-pair correlation depending on a spin-spin correlation inside a background. here a phase fluctuation was identified as an intrinsic string operator nonlocally controlled by a spins. we show that while a pairing amplitude was generally large, a coherent cooper pairing should be substantially weakened by a phase fluctuation inside a symmetry-protected topological phase, inside contrast to a short-range antiferromagnetic phase. it provides an example of the non-bcs mechanism considering pairing, inside which a paring phase coherence was determined by a underlying spin state self-consistently, bearing an interesting resemblance to a pseudogap physics inside a cuprate.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
10111,"learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on a unique opportunities offered by code's known syntax. considering example, long-range dependencies induced by with the help of a same variable or function inside distant locations are often not considered. we propose to use graphs to represent both a syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures. inside this work, we present how to construct graphs from source code and how to scale gated graph neural networks training to such large graphs. we evaluate our method on two tasks: varnaming, inside which the network attempts to predict a name of the variable given its usage, and varmisuse, inside which a network learns to reason about selecting a correct variable that should be used at the given program location. our comparison to methods that use less structured program representations shows a advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve a varmisuse task inside many cases. additionally, our testing showed that varmisuse identifies the number of bugs inside mature open-source projects.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
588,"we present inside this paper the new algorithm considering urban traffic light control with mixed traffic (communicating and non communicating vehicles) and mixed infrastructure (equipped and unequipped junctions). we call equipped junction here the junction with the traffic light signal (tls) controlled by the road side unit (rsu). on such the junction, a rsu manifests its connectedness to equipped vehicles by broadcasting its communication address and geographical coordinates. a rsu builds the map of connected vehicles approaching and leaving a junction. a algorithm allows a rsu to select the traffic phase, based on a built map. a selected traffic phase was applied by a tls; and both equipped and unequipped vehicles must respect it. a traffic management was inside feedback on a traffic demand of communicating vehicles. we simulated a vehicular traffic as well as a communications. a two simulations are combined inside the closed loop with visualization and monitoring interfaces. several indicators on vehicular traffic (mean travel time, ended vehicles) and ieee 802.11p communication performances (end-to-end delay, throughput) are derived and illustrated inside three dimension maps. we then extended a traffic control to the urban road network where we also varied a number of equipped junctions. other indicators are shown considering road traffic performances inside a road network case, where high gains are experienced inside a simulation results.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
7584,"4most was the new wide-field, high-multiplex spectroscopic survey facility considering a vista telescope of eso. starting inside 2022, 4most will deploy more than 2400 fibres inside the 4.1 square degree field-of-view with the help of the positioner based on a tilting spine principle. inside this ontribution we give an outline of a major science goals we wish to achieve with 4most inside a area of galactic archeology. a 4most galactic archeology surveys have been designed to address long-standing and far-reaching problems inside galactic science. they are focused on our major themes: 1) near-field cosmology tests, 2) chemo-dynamical characterisation of a major milky way stellar components, 3) a galactic halo and beyond, and 4) discovery and characterisation of extremely metal-poor stars. inside addition to the top-level description of a galactic surveys we provide information about how a community will be able to join 4most using the call considering public spectroscopic surveys that eso will launch.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
4901,"real-time traffic flow prediction should not only provide travelers with reliable traffic information so that it should save people's time, but also assist a traffic management agency to manage traffic system. it should greatly improve a efficiency of a transportation system. traditional traffic flow prediction approaches usually need the large amount of data but still give poor performances. with a development of deep learning, researchers begin to pay attention to artificial neural networks (anns) such as rnn and lstm. however, these anns are very time-consuming. inside our research, we improve a deep residual network and build the dynamic model which previous researchers hardly use. we firstly integrate a input and output of a $i^{th}$ layer to a input of a $i+1^{th}$ layer and prove that each layer will fit the simpler function so that a error rate will be much smaller. then, we use a concept of online learning inside our model to update pre-trained model during prediction. our result shows that our model has higher accuracy than some state-of-the-art models. inside addition, our dynamic model should perform better inside practical applications.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
12333,"we present the novel solution to a camera pose approximation problem, where rotation and translation of the camera between two views are estimated from matched feature points inside a images. a camera pose approximation problem was traditionally solved using algorithms that are based on a essential matrix or a euclidean homography. with six or more feature points inside general positions inside a space, essential matrix based algorithms should recover the unique solution. however, such algorithms fail when points are on critical surfaces (e.g., coplanar points) and homography should be used instead. by formulating a problem inside quaternions and decoupling a rotation and translation estimation, our proposed algorithm works considering all point configurations. with the help of both simulated and real world images, we compare a approximation accuracy of our algorithm with some of a most commonly used algorithms. our method was shown to be more robust to noise and outliers. considering a benefit of community, we have made a implementation of our algorithm available online and free.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
4391,"considering a first time, we have reported inside this study an ab initio investigation on elastic properties, debye temperature, mulliken population, vickers hardness, and charge density of superconducting scrhp and scirp phosphides. a optimized cell parameters show fair agreement with experimental results. a elastic constants and moduli, poisson's as well as pugh's ratio and elastic anisotropy factors have also been calculated to understand a mechanical behavior of these ternary compounds. their mechanical stability was confirmed using a calculated elastic constants. a calculated values considering poisson's and pugh's ratio indicate a ductile nature of these compounds. scirp was expected to be elastically more anisotropic than scrhp. a estimated value of debye temperature predicts that scrhp was thermally more conductive than scirp and a phonon frequency inside scrhp was higher than that inside scirp. a hardness of scrhp was lower due to a presence of anti-bonding rh-rh inside scrhp. a investigated electronic structure predict that a metallic conductivity of scrhp reduces significantly when rh was replaced with ir. a main contribution to a total density of states (tdos) at fermi-level (ef) comes from a d-electrons of sc and rh/ir inside both compounds. these two ternary compounds are characterized mainly by metallic and covalent bonding with little ionic contribution. as far as superconductivity was concerned, a matrix elements of electron-phonon interaction are noticeably enhanced inside scirp compared to that inside scrhp.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0
9132,"a representation theorem by zomorodian and carlsson has been a starting point of a study of persistent homology under a lens of algebraic representation theory. inside this work, we give the more accurate statement of a original theorem and provide the complete and self-contained proof. furthermore, we generalize a statement from a case of linear sequences of $r$-modules to $r$-modules indexed over more general monoids. this generalization subsumes a representation theorem of multidimensional persistence as the special case.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
17257,"a character vocabulary should be very large inside non-alphabetic languages such as chinese and japanese, which makes neural network models huge to process such languages. we explored the model considering sentiment classification that takes a embeddings of a radicals of a chinese characters, i.e, hanzi of chinese and kanji of japanese. our model was composed of the cnn word feature encoder and the bi-directional rnn document feature encoder. a results achieved are on par with a character embedding-based models, and close to a state-of-the-art word embedding-based models, with 90% smaller vocabulary, and at least 13% and 80% fewer parameters than a character embedding-based models and word embedding-based models respectively. a results suggest that a radical embedding-based idea behind the method was cost-effective considering machine learning on chinese and japanese.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8380,"we study a problem of variable selection considering linear models under a high-dimensional asymptotic setting, where a number of observations $n$ grows at a same rate as a number of predictors $p$. we consider two-stage variable selection techniques (tvs) inside which a first stage uses bridge estimators to obtain an approximate of a regression coefficients, and a second stage simply thresholds this approximate to select a ""important"" predictors. a asymptotic false discovery proportion (afdp) and true positive proportion (atpp) of these tvs are evaluated. we prove that considering the fixed attp, inside order to obtain the smaller afdp, one should pick the bridge estimator with smaller asymptotic mean square error inside a first stage of tvs. based on such principled discovery, we present the sharp comparison of different tvs, using an in-depth investigation of a approximation properties of bridge estimators. rather than ""order-wise"" error bounds with loose constants, our analysis focuses on precise error characterization. various interesting signal-to-noise ratio and sparsity settings are studied. our results offer new and thorough insights into high-dimensional variable selection. considering instance, we prove that the tvs with ridge inside its first stage outperforms tvs with other bridge estimators inside large noise settings; two-stage lasso becomes inferior when a signal was rare and weak. as the by-product, we show our proposed two-stage methods outperform some standard variable selection techniques, such as lasso and sure independence screening, under certain conditions.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
15369,"we examine a impact of baryon acoustic oscillation (bao) scale measurements on a discrepancy between a value of a hubble constant ($h_0$) inferred from a local distance ladder and from planck cosmic microwave background (cmb) data. while a bao data alone cannot constrain $h_0$, we show that combining a latest bao results with wmap, atacama cosmology telescope (act), or south pole telescope (spt) cmb data produces values of $h_0$ that are $2.4-3.1\sigma$ lower than a distance ladder, independent of planck, and that this downward pull is less apparent inside some earlier analyses that used only angle-averaged bao scale constraints rather than full anisotropic information. at a same time, a combination of bao and cmb data also disfavors a lower values of $h_0$ preferred by a planck high-multipole temperature power spectrum. combining galaxy and lyman-$\alpha$ forest (ly$\alpha$) bao with the precise approximate of a primordial deuterium abundance produces $h_0=66.98\pm1.18$ km s$^{-1}$ mpc$^{-1}$ considering a flat $\lambda$cdm model. this value was completely independent of cmb anisotropy constraints and was $3.0\sigma$ lower than a latest distance ladder constraint, although $2.4\sigma$ tension also exists between a galaxy bao and ly$\alpha$ bao. these results show that it was not possible to explain a $h_0$ disagreement solely with the systematic error specific to a planck data. a fact that tensions remain even after a removal of any single data set makes this intriguing puzzle all a more challenging to resolve.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1019,"modern threats have emerged from a prevalence of social networks. hostile actors, such as extremist groups or foreign governments, utilize these networks to run propaganda campaigns with different aims. considering extremists, these campaigns are designed considering recruiting new members or inciting violence. considering foreign governments, a aim may be to create instability inside rival nations. proper social network counter-measures are needed to combat these threats. here we present one important counter-measure: penetrating social networks. this means making target users connect with or follow agents deployed inside a social network. once such connections are established with a targets, a agents should influence them by sharing content which counters a influence campaign. inside this work we study how to penetrate the social network, which we call a follow-back problem. a goal here was to find the policy that maximizes a number of targets that follow a agent. we conduct an empirical study to understand what behavioral and network features affect a probability of the target following an agent. we find that a degree of a target and a size of a mutual neighborhood of a agent and target inside a network affect this probability. based on our empirical findings, we then propose the model considering targets following an agent. with the help of this model, we solve a follow-back problem exactly on directed acyclic graphs and derive the closed form expression considering a expected number of follows an agent receives under a optimal policy. we then formulate a follow-back problem on an arbitrary graph as an integer program. to evaluate our integer program based policies, we conduct simulations on real social network topologies inside twitter. we find that our polices result inside more effective network penetration, with significant increases inside a expected number of targets that follow a agent.",1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
15134,"with a advent of large labelled datasets and high-capacity models, a performance of machine vision systems has been improving rapidly. however, a technology has still major limitations, starting from a fact that different vision problems are still solved by different models, trained from scratch or fine-tuned on a target data. a human visual system, inside stark contrast, learns the universal representation considering vision inside a early life of an individual. this representation works well considering an enormous variety of vision problems, with little or no change, with a major advantage of requiring little training data to solve any of them.",1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8297,"detect facial keypoints was the critical element inside face recognition. however, there was difficulty to catch keypoints on a face due to complex influences from original images, and there was no guidance to suitable algorithms. inside this paper, we study different algorithms that should be applied to locate keyponits. specifically: our framework (1)prepare a data considering further investigation (2)using pca and lbp to process a data (3) apply different algorithms to analysis data, including linear regression models, tree based model, neural network and convolutional neural network, etc. finally we will give our conclusion and further research topic. the comprehensive set of experiments on dataset demonstrates a effectiveness of our framework.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3557,a effectiveness of the robot manipulation to the large extent was determined by a speed of making this or that movement needed considering carrying out a task. accordingly to this a problem of optimal robot control was often subdivided into two subproblems solved separately. inside an autonomous regime a trajectory planning was fulfilled considering providing a robot movement time close to a minimal.,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
9131,"the standard idea behind the method considering assessing a performance of partition or mixture models was to create synthetic data sets with the pre-specified clustering structure, and assess how well a model reveals this structure. the common format was that subjects are assigned to different clusters, with variable observations simulated so that subjects within a same cluster have similar profiles, allowing considering some variability. inside this manuscript, we consider observations from nominal, ordinal and interval categorical variables. theoretical and empirical results are utilized to explore a dependence structure between a variables, inside relation to a clustering structure considering a subjects. the novel idea behind the method was proposed that allows to control a marginal association or correlation structure of a variables, and to specify exact correlation values. practical examples are shown and additional theoretical results are derived considering interval data, commonly observed inside cohort studies, including observations that emulate single nucleotide polymorphisms. we compare the synthetic dataset to the real one, to demonstrate similarities and differences.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
5582,"we investigate a potential existence of the superconducting phase inside $5d$ mott insulators with an eye to hole doped sr$_2$iro$_4$. with the help of the mean-field method, the mixed singlet-triplet superconductivity, $d + p$, was observed due to a antisymmetric exchange originating from the quasi-spin-orbit-coupling. our calculation on ribbon geometry shows possible existence of a topologically protected edge states, because of nodal structure of a superconducting gap. these edge modes are spin polarized and emerge as zero-energy flat bands, supporting the symmetry protected majorana states, verified by evaluation of winding number and $\mathbb{z}_2$ topological invariant. at a end, the possible experimental idea behind the method considering observation of these edge states and determination of a superconducting gap symmetry are discussed based on a quasi-particle interference (qpi) technique.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
6673,"this paper presents the novel hierarchical idea behind the method considering a simultaneous tracking of multiple targets inside the video. we use the network flow idea behind the method to link detections inside low-level and tracklets inside high-level. at each step of a hierarchy, a confidence of candidates was measured by with the help of the new scoring system, confrank, that considers a quality and a quantity of its neighborhood. a output of a first stage was the collection of safe tracklets and unlinked high-confidence detections. considering each individual detection, we determine if it belongs to an existing or was the new tracklet. we show a effect of our framework to recover missed detections and reduce switch identity. a proposed tracker was referred to as tvod considering multi-target tracking with the help of a visual tracker and generic object detector. we achieve competitive results with lower identity switches on several datasets comparing to state-of-the-art.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19678,"experiments are presented that demonstrate how liquid-infused surfaces should reduce turbulent drag significantly inside taylor-couette flow. a test liquid is water, and a test surface is composed of square microscopic grooves measuring 100 $\mu$m to 800 $\mu$m, filled with alkane liquids with viscosities from 0.3 to 1.4 times that of water. we achieve drag reduction exceeding 35\%, four times higher than previously reported considering liquid-infused surfaces inside turbulent flow. a level of drag reduction increased with viscosity ratio, groove width, fluid area fraction, and reynolds number. a optimum groove width is given by $w^+ \approx 35$.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3439,"we study scenarios inside which a baryon asymmetry was generated from a decay of the particle whose mass originates from a spontaneous breakdown of the symmetry. this was realized inside many models, including low-scale leptogenesis and theories with classical scale invariance. symmetry breaking inside a early universe proceeds through the phase transition that gives a parent particle the time-dependent mass, which provides an additional departure from thermal equilibrium that could modify a efficiency of baryogenesis from out-of-equilibrium decays. we characterize a effects of various types of phase transitions and show that an enhancement inside a baryon asymmetry from decays was possible if a phase transition was of a second order, although such models are typically fine-tuned. we also stress a role of new annihilation modes that deplete a parent particle abundance inside models realizing such the phase transition, reducing a efficacy of baryogenesis. the proper treatment of baryogenesis inside such models therefore requires a inclusion of a effects we study inside this paper.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15939,"when electrons are confined inside the two dimensional (2d) system, typical quantum mechanical phenomena such as landau quantization should be detected. graphene systems, including a single atomic layer and few-layer stacked crystals, are ideal 2d materials considering studying the variety of quantum mechanical problems. inside this article, we review a experimental progress inside a unusual landau quantized behaviors of dirac fermions inside monolayer and multilayer graphene by with the help of scanning tunneling microscopy(stm) and scanning tunneling spectroscopy(sts). through sts measurement of a strong magnetic fields, distinct landau-level spectra and rich level splitting phenomena are observed inside different graphene layers. these unique properties provide an effective method considering identifying a number of layers, as well as a stacking orders, and investigating a fundamentally physical phenomena of graphene. moreover, inside a presence of the strain and charged defects, a landau quantization of graphene should be significantly modified, leading to unusual spectroscopic and electronic properties.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
6667,"robotic surgery and novel surgical instrumentation present great potentials towards safer, more accurate and consistent minimally invasive surgery. however, their adoption was dependent to a access to training facilities and extensive surgical training. robotic instruments require different dexterity skills compared to open or laparoscopic. surgeons, therefore, are required to invest significant time by attending extensive training programs. contrary, hands on experiences represent an additional operational cost considering hospitals as a availability of robotic systems considering training purposes was limited. all these technological and financial barriers considering surgeons and hospitals hinder a adoption of robotic surgery. inside this paper, we present the mobile dexterity training kit to develop basic surgical techniques within an affordable setting. a system could be used to train basic surgical gestures and to develop a motor skills needed considering manoeuvring robotic instruments. our work presents a architecture and components needed to create the simulated environment considering training sub-tasks as well as the design considering portable mobile manipulators that should be used as master controllers of different instruments. the preliminary study results demonstrate usability and skills development with this system.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
15889,"principal components regression (pcr) was the traditional tool considering dimension reduction inside linear regression that has been both criticized and defended. one concern about pcr was that obtaining a leading principal components tends to be computationally demanding considering large data sets. while random projections do not possess a optimality properties of a leading principal subspace, they are computationally appealing and thus have become increasingly popular inside recent years. inside this paper, we present an analysis showing that considering random projections satisfying the johnson-lindenstrauss embedding property, a prediction error inside subsequent regression was close to that of pcr, at a expense of requiring the slightly large number of random projections than principal components. column sub-sampling constitutes an even cheaper way of randomized dimension reduction outside a class of johnson-lindenstrauss transforms. we provide numerical results based on synthetic and real data as well as basic theory revealing differences and commonalities inside terms of statistical performance.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0
13247,"we prove that if a difference of two sufficiently smooth solutions of a three-dimensional zakharov-kuznetsov equation $$\partial_{t}u+\partial_{x}\triangle u+u\partial_{x}u=0 \text{,}\quad (x,y,z)\in\mathbb r^3, \;t\in[0,1],$$ decays as $e^{-a(x^2+y^2+z^2)^{3/4}}$ at two different times, considering some $a>0$ large enough, then both solutions coincide.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4885,"quantum critical points (qcps) are widely accepted as the source of the diverse set of collective quantum phases of matter. the central question was how a order parameters of phases near the qcp interact and determine a fundamental character of a critical dynamics which drive a quantum critical behavior. one of a most interesting proposals considering a quantum critical behavior that occurs inside correlated electron systems was that a behavior may arise from local, as opposed to long wavelength, critical fluctuations of a order parameter. a local criticality was believed to give rise to energy over temperature ($e/t$) scaling of a dynamic susceptibility with the fractional exponent near a quantum critical point (qcp). here we show that $e/t$ scaling was indeed observed considering cecu$_{6-x}$ag$_x$ but on closer inspection, a fluctuations should be separated into two components, implying that multiple order parameters play an important role inside a unconventional critical behavior. additionally, when a fluctuations corresponding to a magnetically ordered side of a phase diagram are separated, they are found to be three dimensional and to obey a scaling behavior expected considering long wavelength fluctuations near an itinerant antiferromagnetic qcp.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
1645,"sensors are routinely mounted on robots to acquire various forms of measurements inside spatio-temporal fields. locating features within these fields and reconstruction (mapping) of a dense fields should be challenging inside resource-constrained situations, such as when trying to locate a source of the gas leak from the small number of measurements. inside such cases, the model of a underlying complex dynamics should be exploited to discover informative paths within a field. we use the fluid simulator as the model, to guide inference considering a location of the gas leak. we perform localization using minimization of a discrepancy between observed measurements and gas concentrations predicted by a simulator. our method was able to account considering dynamically varying parameters of wind flow (e.g., direction and strength), and its effects on a observed distribution of gas. we develop algorithms considering off-line inference as well as considering on-line path discovery using active sensing. we demonstrate a efficiency, accuracy and versatility of our algorithm with the help of experiments with the physical robot conducted inside outdoor environments. we deploy an unmanned air vehicle (uav) mounted with the co2 sensor to automatically seek out the gas cylinder emitting co2 using the nozzle. we evaluate a accuracy of our algorithm by measuring a error inside a inferred location of a nozzle, based on which we show that our proposed idea behind the method was competitive with respect to state of a art baselines.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
2661,"inside this work, we address a low-complexity construction of shortened and punctured polar codes from the unified view. while several independent puncturing and shortening designs were attempted inside a literature, our goal was the unique, low-complexity construction encompassing both techniques inside order to achieve any code length and rate. we observe that our solution significantly reduces a construction complexity as compared to state-of-the-art solutions while providing the block error rate performance comparable to constructions that are highly optimized considering specific lengths and rates. this makes a constructed polar codes highly suitable considering practical application inside future communication systems requiring the large set of polar codes with different lengths and rates.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
19888,"objective detection of specific patterns inside statistical distributions, like groupings or gaps or abrupt transitions between different subsets, was the task with the rich range of applications inside astronomy: milky way stellar population analysis, investigations of a exoplanets diversity, solar system minor bodies statistics, extragalactic studies, etc. we adapt a powerful technique of a wavelet transforms to this generalized task, making the strong emphasis on a assessment of a patterns detection significance. among other things, our method also involves optimal minimum-noise wavelets and minimum-noise reconstruction of a distribution density function. based on this development, we construct the self-closed algorithmic pipeline aimed to process statistical samples. it was currently applicable to single-dimensional distributions only, but it was flexible enough to undergo further generalizations and development.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
17575,"crowdsourcing was an important avenue considering collecting machine learning data, but crowdsourcing should go beyond simple data collection by employing a creativity and wisdom of crowd workers. yet crowd participants are unlikely to be experts inside statistics or predictive modeling, and it was not clear how well non-experts should contribute creatively to a process of machine learning. here we study an end-to-end crowdsourcing algorithm where groups of non-expert workers propose supervised learning problems, rank and categorize those problems, and then provide data to train predictive models on those problems. problem proposal includes and extends feature engineering because workers propose a entire problem, not only a input features but also a target variable. we show that workers without machine learning experience should collectively construct useful datasets and that predictive models should be learned on these datasets. inside our experiments, a problems proposed by workers covered the broad range of topics, from politics and current events to problems capturing health behavior, demographics, and more. workers also favored questions showing positively correlated relationships, which has interesting implications given many supervised learning methods perform as well with strong negative correlations. proper instructions are crucial considering non-experts, so we also conducted the randomized trial to understand how different instructions may influence a types of problems proposed by workers. inside general, shifting a focus of machine learning tasks from designing and training individual predictive models to problem proposal allows crowdsourcers to design requirements considering problems of interest and then guide workers towards contributing to a most suitable problems.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7746,"macroscopic models considering systems involving diffusion, short-range repulsion, and long-range attraction have been studied extensively inside a last decades. inside this paper we extend a analysis to the system considering two species interacting with each other according to different inner- and intra-species attractions. under suitable conditions on this self- and crosswise attraction an interesting effect should be observed, namely phase separation into neighbouring regions, each of which contains only one of a species. we prove that a intersection of a support of a stationary solutions of a continuum model considering a two species has zero lebesgue measure, while a support of a sum of a two densities was simply connected. preliminary results indicate a existence of phase separation, i.e. spatial sorting of a different species. the detailed analysis inside one spatial dimension follows. a existence and shape of segregated stationary solutions was shown using a krein-rutman theorem. moreover, considering small repulsion/nonlinear diffusion, also uniqueness of these stationary states was proved.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16489,"a need considering large annotated image datasets considering training convolutional neural networks (cnns) has been the significant impediment considering their adoption inside computer vision applications. we show that with transfer learning an effective object detector should be trained almost entirely on synthetically rendered datasets. we apply this strategy considering detecting pack- aged food products clustered inside refrigerator scenes. our cnn trained only with 4000 synthetic images achieves mean average precision (map) of 24 on the test set with 55 distinct products as objects of interest and 17 distractor objects. the further increase of 12% inside a map was obtained by adding only 400 real images to these 4000 synthetic images inside a training set. the high degree of photorealism inside a synthetic images is not essential inside achieving this performance. we analyze factors like training data set size and 3d model dictionary size considering their influence on detection performance. additionally, training strategies like fine-tuning with selected layers and early stopping which affect transfer learning from synthetic scenes to real scenes are explored. training cnns with synthetic datasets was the novel application of high-performance computing and the promising idea behind the method considering object detection applications inside domains where there was the dearth of large annotated image data.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9807,"how many bits of information are required to pac learn the class of hypotheses of vc dimension $d$? a mathematical setting we follow was that of bassily et al. (2018), where a value of interest was a mutual information $\mathrm{i}(s;a(s))$ between a input sample $s$ and a hypothesis outputted by a learning algorithm $a$. we introduce the class of functions of vc dimension $d$ over a domain $\mathcal{x}$ with information complexity at least $\omega\left(d\log \log \frac{|\mathcal{x}|}{d}\right)$ bits considering any consistent and proper algorithm (deterministic or random). bassily et al. proved the similar (but quantitatively weaker) result considering a case $d=1$. a above result was inside fact the special case of the more general phenomenon we explore. we define a notion of information complexity of the given class of functions $\mathcal{h}$. intuitively, it was a minimum amount of information that an algorithm considering $\mathcal{h}$ must retain about its input to ensure consistency and properness. we prove the direct sum result considering information complexity inside this context; roughly speaking, a information complexity sums when combining several classes.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
9179,"motivated by the model proposed by peng et al. [advances inside coll. and interf. sci. 206 (2014)] considering break-up of tear films on human eyes, we study a dynamics of the generalized thin film model. a governing equations form the fourth-order coupled system of nonlinear parabolic pde considering a film thickness and salt concentration subject to non-conservative effects representing evaporation. we analytically prove a global existence of solutions to this model with mobility exponents inside several different ranges and a results are then validated against pde simulations. we also numerically capture other interesting dynamics of a model, including finite-time rupture-shock phenomenon due to a instabilities caused by locally elevated evaporation rates, convergence to equilibrium and infinite-time thinning.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1914,"it was well known that a problem of computing a feedback capacity of the stationary gaussian channel should be recast as an infinite-dimensional optimization problem; moreover, necessary and sufficient conditions considering a optimality of the solution to this optimization problem have been characterized, and based on this characterization, an explicit formula considering a feedback capacity has been given considering a case that a noise was the first-order autoregressive moving-average gaussian process. inside this paper, we further examine a above-mentioned infinite-dimensional optimization problem. we prove that unless a gaussian noise was white, its optimal solution was unique, and we propose an algorithm to recursively compute a unique optimal solution, which was guaranteed to converge inside theory and features an efficient implementation considering the suboptimal solution inside practice. furthermore, considering a case that a noise was the k-th order autoregressive moving-average gaussian process, we give the relatively more explicit formula considering a feedback capacity; more specifically, a feedback capacity was expressed as the simple function evaluated at the solution to the system of polynomial equations, which was amenable to numerical computation considering a cases k=1, 2 and possibly beyond.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
6632,"inside social media, recommender systems are responsible considering directing a users to relevant content. inside order to enhance a users' engagement, recommender systems adapt their output to a expected reactions of a users, which are inside turn affected by a recommended contents. inside this work, we model the single user that interacts with an online news aggregator, with a purpose of making explicit a feedback loop between a evolution of a user's opinion and a personalised recommendation of contents. we assume that a user has the scalar opinion on the certain issue: this opinion was influenced by all received news, which are characterized by the binary position on a issue at hand. a user has the confirmation bias, that is, the preference considering news that confirm her current opinion. at a same time, we assume that a recommender has a goal of maximizing a number of user's clicks (as the measure of her engagement): inside order to fulfil its goal, a recommender has to compromise between exploring a user's preferences and exploiting them. after defining suitable metrics considering a effectiveness of a recommender systems and considering its impact on a opinion, we perform both extensive numerical simulations and the mathematical analysis of a model. we find that personalised contents and confirmation bias do affect a evolution of opinions: a extent of these effects was inherently related to a effectiveness of a recommender. we also show that by tuning a amount of randomness inside a recommendation algorithm, one should reduce a impact of a recommendation system on a opinions.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
8524,"output-agreement mechanisms such as esp game have been widely used inside human computation to obtain reliable human-generated labels. inside this paper, we argue that the ""time-limited"" output-agreement mechanism should be used to create the fast and robust crowd-powered component inside interactive systems, particularly dialogue systems, to extract key information from user utterances on a fly. our experiments on amazon mechanical turk with the help of a airline travel information system (atis) dataset showed that a proposed idea behind the method achieves high-quality results with an average response time shorter than 9 seconds.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1777,"although there was an extensive statistical literature showing a disadvantages of discretizing continuous variables, categorization was the common practice inside clinical research which results inside substantial loss of information. the large collection of methods inside cancer phase i clinical trial design establishes dose of the new agent as the discrete variable. the noteworthy exception was a escalation with overdose control (ewoc) design, where doses should be defined either as continuous or as the grid of discrete doses. the monte carlo simulation study is performed to compare a operating characteristics of continuous and discrete dose ewoc designs. four equally spaced grids with different interval lengths were considered. a loss of information is measured by several operating characteristics easier considering clinicians to interpret, inside addition to a usual statistical measures of bias and mean squared error. based on a simulations, if there was not enough knowledge about a true mtd value as commonly happens inside phase i clinical trials, continuous dose scheme arises as an attractive option.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
11461,"inside this paper, we propose the mixture of probabilistic partial canonical correlation analysis (mppcca) that extracts a causal patterns from two multivariate time series. causal patterns refer to a signal patterns within interactions of two elements having multiple types of mutually causal relationships, rather than the mixture of simultaneous correlations or a absence of presence of the causal relationship between a elements. inside multivariate statistics, partial canonical correlation analysis (pcca) evaluates a correlation between two multivariates after subtracting a effect of a third multivariate. pcca should calculate a granger causal- ity index (which tests whether the time-series should be predicted from an- other time-series), but was not applicable to data containing multiple partial canonical correlations. after introducing a mppcca, we propose an expectation-maxmization (em) algorithm that estimates a parameters and latent variables of a mppcca. a mppcca was expected to ex- tract multiple partial canonical correlations from data series without any supervised signals to split a data as clusters. a method is then eval- uated inside synthetic data experiments. inside a synthetic dataset, our method estimated a multiple partial canonical correlations more accurately than a existing method. to determine a types of patterns detectable by a method, experiments were also conducted on real datasets. a method estimated a communication patterns inside motion-capture data. a mp- pcca was applicable to various type of signals such as brain signals, human communication and nonlinear complex multibody systems.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0
19971,"graph-based methods are known to be successful inside many machine learning and pattern classification tasks. these methods consider semi-structured data as graphs where nodes correspond to primitives (parts, interest points, segments, etc.) and edges characterize a relationships between these primitives. however, these non-vectorial graph data cannot be straightforwardly plugged into off-the-shelf machine learning algorithms without the preliminary step of -- explicit/implicit -- graph vectorization and embedding. this embedding process should be resilient to intra-class graph variations while being highly discriminant. inside this paper, we propose the novel high-order stochastic graphlet embedding (sge) that maps graphs into vector spaces. our main contribution includes the new stochastic search procedure that efficiently parses the given graph and extracts/samples unlimitedly high-order graphlets. we consider these graphlets, with increasing orders, to model local primitives as well as their increasingly complex interactions. inside order to build our graph representation, we measure a distribution of these graphlets into the given graph, with the help of particular hash functions that efficiently assign sampled graphlets into isomorphic sets with the very low probability of collision. when combined with maximum margin classifiers, these graphlet-based representations have positive impact on a performance of pattern comparison and recognition as corroborated through extensive experiments with the help of standard benchmark databases.",1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16189,"tasks such as search and recommendation have become increas- ingly important considering e-commerce to deal with a information over- load problem. to meet a diverse needs of di erent users, person- alization plays an important role. inside many large portals such as taobao and amazon, there are the bunch of di erent types of search and recommendation tasks operating simultaneously considering person- alization. however, most of current techniques address each task separately. this was suboptimal as no information about users shared across di erent tasks. inside this work, we propose to learn universal user representations across multiple tasks considering more e ective personalization. inside partic- ular, user behavior sequences (e.g., click, bookmark or purchase of products) are modeled by lstm and attention mechanism by integrating all a corresponding content, behavior and temporal information. user representations are shared and learned inside an end-to-end setting across multiple tasks. bene ting from better information utilization of multiple tasks, a user representations are more e ective to re ect their interests and are more general to be transferred to new tasks. we refer this work as deep user perception network (dupn) and conduct an extensive set of o ine and online experiments. across all tested ve di erent tasks, our dupn consistently achieves better results by giving more e ective user representations. moreover, we deploy dupn inside large scale operational tasks inside taobao. detailed implementations, e.g., incre- mental model updating, are also provided to address a practical issues considering a real world applications.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18018,"deep neural networks (dnns) have achieved superior performance inside various prediction tasks, but should be very vulnerable to adversarial examples or perturbations. therefore, it was crucial to measure a sensitivity of dnns to various forms of perturbations inside real applications. we introduce the novel perturbation manifold and its associated influence measure to quantify a effects of various perturbations on dnn classifiers. such perturbations include various external and internal perturbations to input samples and network parameters. a proposed measure was motivated by information geometry and provides desirable invariance properties. we demonstrate that our influence measure was useful considering four model building tasks: detecting potential 'outliers', analyzing a sensitivity of model architectures, comparing network sensitivity between training and test sets, and locating vulnerable areas. experiments show reasonably good performance of a proposed measure considering a popular dnn models resnet50 and densenet121 on cifar10 and mnist datasets.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17280,"a purpose of this study is to do the dataset distribution analysis, the classification performance analysis, and the topical analysis concerning what people are tweeting about four disease characteristics: symptoms, transmission, prevention, and treatment. the combination of natural language processing and machine learning techniques were used to determine what people are tweeting about zika. specifically, the two-stage classifier system is built to find relevant tweets on zika, and then categorize these into a four disease categories. tweets inside each disease category were then examined with the help of latent dirichlet allocation (lda) to determine a five main tweet topics considering each disease characteristic. results 1,234,605 tweets were collected. tweets by males and females were similar (28% and 23% respectively). a classifier performed well on a training and test data considering relevancy (f=0.87 and 0.99 respectively) and disease characteristics (f=0.79 and 0.90 respectively). five topics considering each category were found and discussed with the focus on a symptoms category. through this process, we demonstrate how misinformation should be discovered so that public health officials should respond to a tweets with misinformation.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
12660,"we establish the new connection between metric diophantine approximation and a parametric geometry of numbers by proving the variational principle facilitating a computation of a hausdorff and packing dimensions of many sets of interest inside diophantine approximation. inside particular, we show that a hausdorff and packing dimensions of a set of singular $m\times n$ matrices are both equal to $mn \big(1-\frac1{m+n}\big)$, thus proving the conjecture of kadyrov, kleinbock, lindenstrauss, and margulis (preprint 2014) as well as answering the question of bugeaud, cheung, and chevallier (preprint 2016). we introduce a notion of the $template$, which generalizes a notion of the $rigid$ $system$ (roy, 2015) to a setting of matrix approximation. our main theorem takes a following form: considering any class of templates $\mathcal f$ closed under finite perturbations, a hausdorff and packing dimensions of a set of matrices whose successive minima functions are members of $\mathcal f$ (up to finite perturbation) should be written as a suprema over $\mathcal f$ of certain natural functions on a space of templates. besides implying kklm's conjecture, this theorem has many other applications including computing a hausdorff and packing dimensions of a set of points witnessing the conjecture of starkov (2000), and of a set of points witnessing the conjecture of schmidt (1983).",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
1696,"we derive a effective action considering a collective spin modes inside iron-based superconductors. we show that, due to a orbital-selective nature of spin fluctuations, a magnetic and nematic instabilities are controlled by a degrees of orbital nesting between electron and hole pockets. within the prototypical three-pockets model a hole-electron orbital mismatch was found to boost spin-nematic order. this explains a enhancement of a nematic order inside fese as compared to 122 compounds, and its suppression under pressure, where a emergence of a second hole pocket compensates a orbital mismatch of a three-pockets configuration.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
1542,"we provide new results considering noise-tolerant and sample-efficient learning algorithms under $s$-concave distributions. a new class of $s$-concave distributions was the broad and natural generalization of log-concavity, and includes many important additional distributions, e.g., a pareto distribution and $t$-distribution. this class has been studied inside a context of efficient sampling, integration, and optimization, but much remains unknown about a geometry of this class of distributions and their applications inside a context of learning. a challenge was that unlike a commonly used distributions inside learning (uniform or more generally log-concave distributions), this broader class was not closed under a marginalization operator and many such distributions are fat-tailed. inside this work, we introduce new convex geometry tools to study a properties of $s$-concave distributions and use these properties to provide bounds on quantities of interest to learning including a probability of disagreement between two halfspaces, disagreement outside the band, and a disagreement coefficient. we use these results to significantly generalize prior results considering margin-based active learning, disagreement-based active learning, and passive learning of intersections of halfspaces. our analysis of geometric properties of $s$-concave distributions might be of independent interest to optimization more broadly.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5056,"an individual's decisions are often guided by those of his or her peers, i.e., neighbors inside the social network. presumably, being privy to a experiences of others aids inside learning and decision making, but how much advantage does an individual gain by observing her neighbors? such problems make appearances inside sociology and economics and, inside this paper, we present the novel model to capture such decision-making processes and appeal to a classical multi-armed bandit framework to analyze it. each individual, inside addition to her own actions, should observe a actions and rewards obtained by her neighbors, and should use all of this information inside order to minimize her own regret. we provide algorithms considering this setting, both considering stochastic and adversarial bandits, and show that their regret smoothly interpolates between a regret inside a classical bandit setting and that of a full-information setting as the function of a neighbors' exploration. inside a stochastic setting a additional information must simply be incorporated into a usual approximation of a rewards, while inside a adversarial setting this was attained by constructing the new unbiased estimator considering a rewards and appropriately bounding a amount of additional information provided by a neighbors. these algorithms are optimal up to log factors; despite a fact that a agents act independently and selfishly, this implies that it was an approximate nash equilibria considering all agents to use our algorithms. further, we show using empirical simulations that our algorithms, often significantly, outperform existing algorithms that one could apply to this setting.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0
1765,"an important objective considering analyzing real-world graphs was to achieve scalable performance on large, streaming graphs. the challenging and relevant example was a graph partition problem. as the combinatorial problem, graph partition was np-hard, but existing relaxation methods provide reasonable approximate solutions that should be scaled considering large graphs. competitive benchmarks and challenges have proven to be an effective means to advance state-of-the-art performance and foster community collaboration. this paper describes the graph partition challenge with the baseline partition algorithm of sub-quadratic complexity. a algorithm employs rigorous bayesian inferential methods based on the statistical model that captures characteristics of a real-world graphs. this strong foundation enables a algorithm to address limitations of well-known graph partition approaches such as modularity maximization. this paper describes various aspects of a challenge including: (1) a data sets and streaming graph generator, (2) a baseline partition algorithm with pseudocode, (3) an argument considering a correctness of parallelizing a bayesian inference, (4) different parallel computation strategies such as node-based parallelism and matrix-based parallelism, (5) evaluation metrics considering partition correctness and computational requirements, (6) preliminary timing of the python-based demonstration code and a open source c++ code, and (7) considerations considering partitioning a graph inside streaming fashion. data sets and source code considering a algorithm as well as metrics, with detailed documentation are available at graphchallenge.org.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
2679,"direct policy gradient methods considering reinforcement learning and continuous control problems are the popular idea behind the method considering the variety of reasons: 1) they are easy to implement without explicit knowledge of a underlying model 2) they are an ""end-to-end"" approach, directly optimizing a performance metric of interest 3) they inherently allow considering richly parameterized policies. the notable drawback was that even inside a most basic continuous control problem (that of linear quadratic regulators), these methods must solve the non-convex optimization problem, where little was understood about their efficiency from both computational and statistical perspectives. inside contrast, system identification and model based planning inside optimal control theory have the much more solid theoretical footing, where much was known with regards to their computational and statistical properties. this work bridges this gap showing that (model free) policy gradient methods globally converge to a optimal solution and are efficient (polynomially so inside relevant problem dependent quantities) with regards to their sample and computational complexities.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
13344,"the class of reaction-diffusion virus dynamics models with intracellular state-dependent delay and the general non-linear infection rate functional response was investigated. we are interested inside classical solutions with lipschitz in-time initial functions which are adequate to a discontinuous change of parameters due to, considering example, drug administration. a lyapunov functions technique was used to analyse stability of interior infection equilibria which describe a cases of the chronic disease.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9848,"event-based cameras have shown great promise inside the variety of situations where frame based cameras suffer, such as high speed motions and high dynamic range scenes. however, developing algorithms considering event measurements requires the new class of hand crafted algorithms. deep learning has shown great success inside providing model free solutions to many problems inside a vision community, but existing networks have been developed with frame based images inside mind, and there does not exist a wealth of labeled data considering events as there does considering images considering supervised training. to these points, we present ev-flownet, the novel self-supervised deep learning pipeline considering optical flow approximation considering event based cameras. inside particular, we introduce an image based representation of the given event stream, which was fed into the self-supervised neural network as a sole input. a corresponding grayscale images captured from a same camera at a same time as a events are then used as the supervisory signal to provide the loss function at training time, given a estimated flow from a network. we show that a resulting network was able to accurately predict optical flow from events only inside the variety of different scenes, with performance competitive to image based networks. this method not only allows considering accurate approximation of dense optical flow, but also provides the framework considering a transfer of other self-supervised methods to a event-based domain.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
1163,"we establish some identities of euler related sums. by with the help of these identities, we discuss a closed form representations of sums of harmonic numbers and reciprocal parametric binomial coefficients through parametric harmonic numbers, shifted harmonic numbers and riemann zeta function with positive integer arguments. inside particular we investigate products of quadratic and cubic harmonic numbers and reciprocal parametric binomial coefficients. some illustrative special cases as well as immediate consequences of a main results are also considered.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
18345,"let $g$ be the finite abelian group. we show that its davenport constant $d(g)$ satisfies $d(g)\leq \exp(g)+\frac{|g|}{\exp(g)}-1$, provided that $\exp(g)\geq\sqrt{|g|}$, and $d(g)\leq 2\sqrt{|g|}-1$, if $\exp(g)<\sqrt{|g|}$. this proves the conjecture by balasubramanian and a first named author.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
2098,"from a datum of an integer partition and the classical lie algebra, one should define the markov chain on an associated multiplicative graph. considering each classical family a, c, b, d, we thus obtain the sequence of markov chain which was indexed by a rank of a considered algebra. inside this article we show that, considering each type, a transition kernel of a markov chain has the limit when a rank tends to infinity. moreover, a limit kernel does not depend on a considered type.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
14154,"this paper presents sampling-based speech parameter generation with the help of moment-matching networks considering deep neural network (dnn)-based speech synthesis. although people never produce exactly a same speech even if we try to express a same linguistic and para-linguistic information, typical statistical speech synthesis produces completely a same speech, i.e., there was no inter-utterance variation inside synthetic speech. to give synthetic speech natural inter-utterance variation, this paper builds dnn acoustic models that make it possible to randomly sample speech parameters. a dnns are trained so that they make a moments of generated speech parameters close to those of natural speech parameters. since a variation of speech parameters was compressed into the low-dimensional simple prior noise vector, our algorithm has lower computation cost than direct sampling of speech parameters. as a first step towards generating synthetic speech that has natural inter-utterance variation, this paper investigates whether or not a proposed sampling-based generation deteriorates synthetic speech quality. inside evaluation, we compare speech quality of conventional maximum likelihood-based generation and proposed sampling-based generation. a result demonstrates a proposed generation causes no degradation inside speech quality.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4634,"we propose the convolution neural network based algorithm considering simultaneously diagnosing diabetic retinopathy and highlighting suspicious regions. our contributions are two folds: 1) the network termed zoom-in-net which mimics a zoom-in process of the clinician to examine a retinal images. trained with only image-level supervisions, zoomin-net should generate attention maps which highlight suspicious regions, and predicts a disease level accurately based on both a whole image and its high resolution suspicious patches. 2) only four bounding boxes generated from a automatically learned attention maps are enough to cover 80% of a lesions labeled by an experienced ophthalmologist, which shows good localization ability of a attention maps. by clustering features at high response locations on a attention maps, we discover meaningful clusters which contain potential lesions inside diabetic retinopathy. experiments show that our algorithm outperform a state-of-the-art methods on two datasets, eyepacs and messidor.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16878,"a magnetic oscillations inside ybco high-temperature superconductors have been widely studied over a last decade and consist of three equidistant low frequencies with the central frequency several times more intense than its two shoulders. this remains the puzzle inside spite of numerous attempts to explain a corresponding small fermi-surface pockets. furthermore a arpes data indicate only four fermi-arcs with bilayer splitting, and show no sign of such small areas inside a fermi surface. here we argue that a magnetic oscillations measured inside under-doped bilayer high temperature superconductors, inside particular yba$_{2}$cu$_{3}$o$_{6+\delta }$, provide the measure of a interplanar electronic coupling rather than a areas of fine-grain reconstruction of a fermi surfaces coming from induced charge density waves. this identification was based on a relative intensities of a different peaks, as well as their angular dependence, which points to an effective fermi surface that was larger than a oscillation frequencies, and was compatible with several indications from arpes. a dominance of such frequencies with respect to a fundamental frequencies from a fermi surface was natural considering the strongly correlated quasi-two dimensional electronic systems where non-linear mixings of frequencies are more resistant to sample inhomogeneity.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
11707,"this paper proposes the novel idea behind the method to stereo visual odometry without stereo matching. it was particularly robust inside scenes of repetitive high-frequency textures. referred to as dsvo (direct stereo visual odometry), it operates directly on pixel intensities, without any explicit feature matching, and was thus efficient and more accurate than a state-of-the-art stereo-matching-based methods. it applies the semi-direct monocular visual odometry running on one camera of a stereo pair, tracking a camera pose and mapping a environment simultaneously; a other camera was used to optimize a scale of monocular visual odometry. we evaluate dsvo inside the number of challenging scenes to evaluate its performance and present comparisons with a state-of-the-art stereo visual odometry algorithms.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
17776,"a simulation of certain flow problems requires the means considering modeling the free fluid surface; examples being viscoelastic die swell or fluid sloshing inside tanks. inside the finite-element context, this type of problem can, among many other options, be dealt with with the help of an interface-tracking idea behind the method with a deforming-spatial-domain/stabilized-space-time (dsd/sst) formulation. the difficult issue that was connected with this type of idea behind the method was a determination of the suitable coupling mechanism between a fluid velocity at a boundary and a displacement of a boundary mesh nodes. inside order to avoid large mesh distortions, one goal was to keep a nodal movements as small as possible; but of course still compliant with a no-penetration boundary condition. standard displacement techniques are full velocity, velocity inside the specific coordinate direction, and velocity inside normal direction. inside this work, we investigate how a interface-tracking idea behind the method should be combined with isogeometric analysis considering a spatial discretization. if nurbs basis functions of sufficient order are used considering both a geometry and a solution, both the continuous normal vector as well as a velocity are available on a entire boundary. this circumstance allows a weak imposition of a no-penetration boundary condition. we compare this option with an alternative that relies on strong imposition at discrete points. furthermore, we examine several coupling methods between a fluid equations, boundary conditions, and equations considering a adjustment of interior control point positions.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8968,"robots that succeed inside factories stumble to complete a simplest daily task humans take considering granted, considering a change of environment makes a task exceedingly difficult. aiming to teach robot perform daily interactive manipulation inside the changing environment with the help of human demonstrations, we collected our own data of interactive manipulation. a dataset focuses on position, orientation, force, and torque of objects manipulated inside daily tasks. a dataset includes 1,593 trials of 32 types of daily motions and 1,596 trials of pouring alone, as well as helper code. we present our dataset to facilitate a research on task-oriented interactive manipulation.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
12762,"a topic of fake news has drawn attention both from a public and a academic communities. such misinformation has a potential of affecting public opinion, providing an opportunity considering malicious parties to manipulate a outcomes of public events such as elections. because such high stakes are at play, automatically detecting fake news was an important, yet challenging problem that was not yet well understood. nevertheless, there are three generally agreed upon characteristics of fake news: a text of an article, a user response it receives, and a source users promoting it. existing work has largely focused on tailoring solutions to one particular characteristic which has limited their success and generality. inside this work, we propose the model that combines all three characteristics considering the more accurate and automated prediction. specifically, we incorporate a behavior of both parties, users and articles, and a group behavior of users who propagate fake news. motivated by a three characteristics, we propose the model called csi which was composed of three modules: capture, score, and integrate. a first module was based on a response and text; it uses the recurrent neural network to capture a temporal pattern of user activity on the given article. a second module learns a source characteristic based on a behavior of users, and a two are integrated with a third module to classify an article as fake or not. experimental analysis on real-world data demonstrates that csi achieves higher accuracy than existing models, and extracts meaningful latent representations of both users and articles.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0
12621,"when sum-of-squares (sos) programs are recast as semidefinite programs (sdps) with the help of a standard monomial basis, a constraint matrices inside a sdp possess the structural property that we call \emph{partial orthogonality}. inside this paper, we leverage partial orthogonality to develop the fast first-order method, based on a alternating direction method of multipliers (admm), considering a solution of a homogeneous self-dual embedding of sdps describing sos programs. precisely, we show how the ""diagonal plus low rank"" structure implied by partial orthogonality should be exploited to project efficiently a iterates of the recent admm algorithm considering generic conic programs onto a set defined by a affine constraints of a sdp. a resulting algorithm, implemented as the new package inside a solver cdcs, was tested on the range of large-scale sos programs arising from constrained polynomial optimization problems and from lyapunov stability analysis of polynomial dynamical systems. these numerical experiments demonstrate a effectiveness of our idea behind the method compared to common state-of-the-art solvers.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
3960,"domain similarity measures should be used to gauge adaptability and select suitable data considering transfer learning, but existing approaches define ad hoc measures that are deemed suitable considering respective tasks. inspired by work on curriculum learning, we propose to \emph{learn} data selection measures with the help of bayesian optimization and evaluate them across models, domains and tasks. our learned measures outperform existing domain similarity measures significantly on three tasks: sentiment analysis, part-of-speech tagging, and parsing. we show a importance of complementing similarity with diversity, and that learned measures are -- to some degree -- transferable across models, domains, and even tasks.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6337,"mobile robots are often needed considering long duration missions. these include search and rescue, sentry, repair, surveillance and entertainment. current power supply technology limit walking and climbing robots from many such missions. internal combustion engines have high noise and emit toxic exhaust while rechargeable batteries have low energy densities and high rates of self-discharge. inside theory, fuel cells do not have such limitations. inside particular proton exchange membrane (pems) should provide very high energy densities, are clean and quiet. however, pem fuel cells are found to be unreliable due to performance degradation. this should be mitigated by protecting a fuel cell inside the fuel-cell battery hybrid configuration with the help of filtering electronics that ensure a fuel cell was isolated from electrical noise and the battery to isolate it from power surges. simulation results are presented considering the hoap 2 humanoid robot that suggests the fuel cell powered hybrid power supply superior to conventional batteries.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1
1485,"with a wide application of machine learning algorithms to a real world, class imbalance and concept drift have become crucial learning issues. class imbalance happens when a data categories are not equally represented, i.e., at least one category was minority compared to other categories. it should cause learning bias towards a majority class and poor generalization. concept drift was the change inside a underlying distribution of a problem, and was the significant issue specially when learning from data streams. it requires learners to be adaptive to dynamic changes. class imbalance and concept drift should significantly hinder predictive performance, and a problem becomes particularly challenging when they occur simultaneously. this challenge arises from a fact that one problem should affect a treatment of a other. considering example, drift detection algorithms based on a traditional classification error may be sensitive to a imbalanced degree and become less effective; and class imbalance techniques need to be adaptive to changing imbalance rates, otherwise a class receiving a preferential treatment may not be a correct minority class at a current moment. therefore, a mutual effect of class imbalance and concept drift should be considered during algorithm design. a aim of this workshop was to bring together researchers from a areas of class imbalance learning and concept drift inside order to encourage discussions and new collaborations on solving a combined issue of class imbalance and concept drift. it provides the forum considering international researchers and practitioners to share and discuss their original work on addressing new challenges and research issues inside class imbalance learning, concept drift, and a combined issues of class imbalance and concept drift. a proceedings include 8 papers on these topics.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
10033,"a robustness of complex networks under targeted attacks was deeply connected to a resilience of complex systems, i.e., a ability to make appropriate responses to a attacks. inside this article, we investigated a state-of-the-art targeted node attack algorithms and demonstrate that they become very inefficient when a cost of a attack was taken into consideration. inside this paper, we made explicit assumption that a cost of removing the node was proportional to a number of adjacent links that are removed, i.e., higher degree nodes have higher cost. finally, considering a case when it was possible to attack links, we propose the simple and efficient edge removal strategy named hierarchical power iterative normalized cut (hpi-ncut).the results on real and artificial networks show that a hpi-ncut algorithm outperforms all a node removal and link removal attack algorithms when a cost of a attack was taken into consideration. inside addition, we show that on sparse networks, a complexity of this hierarchical power iteration edge removal algorithm was only $o(n\log^{2+\epsilon}(n))$.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15006,"transformer life assessment and failure diagnostics have always been important problems considering electric utility companies. ambient temperature and load profile are a main factors which affect aging of a transformer insulation, and consequently, a transformer lifetime. a ieee std. c57.911995 provides the model considering calculating a transformer loss of life based on ambient temperature and transformer's loading. inside this paper, this standard was used to develop the data-driven static model considering hourly approximation of a transformer loss of life. among various machine learning methods considering developing this static model, a adaptive network-based fuzzy inference system (anfis) was selected. numerical simulations demonstrate a effectiveness and a accuracy of a proposed anfis method compared with other relevant machine learning based methods to solve this problem.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
10667,"inside the context where the decision has to be taken collectively by several agents, a social choice problem consists inside deciding whether there exists the socially acceptable rule that aggregates a individual preferences of a agents into the social one. we analyze this problem considering sets of alternatives defined by equality constraints and obtain the solution that, inside sharp contrast to a classical ones, was expressed inside an elementary language and ultimately reduces a social choice problem to the standard constrained optimization problem. by considering the toy example we shall be able to interpret this general result inside terms of a rationality (in a sense of economics) of a design of a set of alternatives rather than, surprisingly, that of a agents involved inside a actual social choice problem.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
1452,"temporal networks are increasingly being used to model a interactions of complex systems. most studies require a temporal aggregation of edges (or events) into discrete time steps to perform analysis. inside this article we describe the static, lossless, and unique representation of the temporal network, a temporal event graph (teg). a teg describes a temporal network inside terms of both a inter-event time and two-event temporal motif distributions. by considering these distributions inside unison we provide the new method to characterise a behaviour of individuals and collectives inside temporal networks as well as providing the natural decomposition of a network. we illustrate a utility of a teg by providing examples on both synthetic and real temporal networks.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
1061,"instanton bundles on $\mathbb{p}^3$ have been at a core of a research inside algebraic geometry during a last thirty years. motivated by a recent extension of their definition to other fano threefolds of picard number one, we develop a theory of instanton bundles on a complete flag variety $f:=f(0,1,2)$ of point-lines on $\mathbb{p}^2$. after giving considering them two different monadic presentations, we use it to show that a moduli space $mi_f(k)$ of instanton bundles of charge $k$ was the geometric git quotient with the generically smooth component of dim $8k-3$. finally we study their locus of jumping conics.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8738,"we give the fully explicit description of lie algebra derivatives (generalizing raising and lowering operators) considering representations of sl(3,r) inside terms of the basis of wigner functions. this basis was natural from a point of view of principal series representations, as well as computations inside a analytic theory of automorphic forms (e.g., with whittaker functions). a method was based on a clebsch-gordan multiplication rule considering wigner functions, and applies to other lie groups whose maximal compact subgroup was isogenous to the product of su(2) and u(1) factors. as an application, we give the complete and explicit description of a k-type structure of certain cohomological representations.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0
15188,"liver lesion segmentation was an important step considering liver cancer diagnosis, treatment planning and treatment evaluation. lits (liver tumor segmentation challenge) provides the common testbed considering comparing different automatic liver lesion segmentation methods. we participate inside this challenge by developing the deep convolutional neural network (dcnn) method. a particular dcnn model works inside 2.5d inside that it takes the stack of adjacent slices as input and produces a segmentation map corresponding to a center slice. a model has 32 layers inside total and makes use of both long range concatenation connections of u-net [1] and short-range residual connections from resnet [2]. a model is trained with the help of a 130 lits training datasets and achieved an average dice score of 0.67 when evaluated on a 70 test ct scans, which ranked first considering a lits challenge at a time of a isbi 2017 conference.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11081,"we propose significance-offset convolutional neural network, the deep convolutional network architecture considering regression of multivariate asynchronous time series. a model was inspired by standard autoregressive (ar) models and gating mechanisms used inside recurrent neural networks. it involves an ar-like weighting system, where a final predictor was obtained as the weighted sum of adjusted regressors, while a weights are datadependent functions learnt through the convolutional network. a architecture is designed considering applications on asynchronous time series and was evaluated on such datasets: the hedge fund proprietary dataset of over 2 million quotes considering the credit derivative index, an artificially generated noisy autoregressive series and uci household electricity consumption dataset. a proposed architecture achieves promising results as compared to convolutional and recurrent neural networks.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17401,"inside this paper we provide an integral representation of a fractional laplace-beltrami operator considering general riemannian manifolds which has several interesting applications. we give two different proofs, inside two different scenarios, of essentially a same result. one of them deals with compact manifolds with or without boundary, while a other idea behind the method treats a case of riemannian manifolds without boundary whose ricci curvature was uniformly bounded below.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14603,"astrophysics and cosmology are rich with data. a advent of wide-area digital cameras on large aperture telescopes has led to ever more ambitious surveys of a sky. data volumes of entire surveys the decade ago should now be acquired inside the single night and real-time analysis was often desired. thus, modern astronomy requires big data know-how, inside particular it demands highly efficient machine learning and image analysis algorithms. but scalability was not a only challenge: astronomy applications touch several current machine learning research questions, such as learning from biased data and dealing with label and measurement noise. we argue that this makes astronomy the great domain considering computer science research, as it pushes a boundaries of data analysis. inside a following, we will present this exciting application area considering data scientists. we will focus on exemplary results, discuss main challenges, and highlight some recent methodological advancements inside machine learning and image analysis triggered by astronomical applications.",1,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0
16450,"we develop the new perturbation theory (pt) treatment that should describe gravitational dynamics of large-scale structure after shell-crossing inside a one-dimensional cosmological case. starting with cold initial conditions, a motion of matter distribution follows at early stages a single-stream regime, which can, inside one dimension, be described exactly by a first-order lagrangian perturbation, i.e. a zel'dovich solution. however, a single-stream flow no longer holds after shell-crossing and the proper account of a multi-stream flow was essential considering post-collapse dynamics. inside this paper, extending previous work by colombi (2015, mnras 446, 2902), we present the perturbative description considering a multi-stream flow after shell-crossing inside the cosmological setup. inside addition, we introduce an adaptive smoothing scheme to deal with a bulk properties of phase-space structures. a filtering scales inside this scheme are linked to a next-crossing time inside a post-collapse region, estimated from our pt calculations. our pt treatment combined with adaptive smoothing was illustrated inside several cases. predictions are compared to simulations and we find that post-collapse pt with adaptive smoothing reproduces a power spectrum and phase-space structures remarkably well even at small scales, where zel'dovich solution substantially deviates from simulations.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10908,"we improve montgomery's $\omega$-results considering $|\zeta(\sigma+it)|$ inside a strip $1/2<\sigma<1$ and give inside particular lower bounds considering a maximum of $|\zeta(\sigma+it)|$ on $\sqrt{t}\le t \le t$ that are uniform inside $\sigma$. we give similar lower bounds considering a maximum of $|\sum_{n\le x} n^{-1/2-it}|$ on intervals of length much larger than $x$. we rely on our recent work on lower bounds considering maxima of $|\zeta(1/2+it)|$ on long intervals, as well as work of soundararajan, g√°l, and others. a paper aims at displaying and clarifying a conceptually different combinatorial arguments that show up inside various parts of a proofs.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
9086,"inside this article we extend work of shanks and washington on cyclic extensions, and elliptic curves associated to a simplest cubic fields. inside particular, we give families of examples of hyperelliptic curves $c: y^2=f(x)$ defined over $\mathbb{q}$, with $f(x)$ of degree $p$, where $p$ was the sophie germain prime, such that a rank of a mordell--weil group of a jacobian $j/\mathbb{q}$ of $c$ was bounded by a genus of $c$ and a $2$-rank of a class group of a (cyclic) field defined by $f(x)$, and exhibit examples where this bound was sharp.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
19555,"we present single imputation method considering missing values which borrows a idea of data depth---a measure of centrality defined considering an arbitrary point of the space with respect to the probability distribution or data cloud. this consists inside iterative maximization of a depth of each observation with missing values, and should be employed with any properly defined statistical depth function. considering each single iteration, imputation reverts to optimization of quadratic, linear, or quasiconcave functions that are solved analytically by linear programming or a nelder-mead method. as it accounts considering a underlying data topology, a procedure was distribution free, allows imputation close to a data geometry, should make prediction inside situations where local imputation (k-nearest neighbors, random forest) cannot, and has attractive robustness and asymptotic properties under elliptical symmetry. it was shown that the special case---when with the help of a mahalanobis depth---has direct connection to well-known methods considering a multivariate normal model, such as iterated regression and regularized pca. a methodology was extended to multiple imputation considering data stemming from an elliptically symmetric distribution. simulation and real data studies show good results compared with existing popular alternatives. a method has been implemented as an r-package. supplementary materials considering a article are available online.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
7231,"opioid related deaths are increasing dramatically inside recent years, and opioid epidemic was worsening inside a united states. combating opioid epidemic becomes the high priority considering both a u.s. government and local governments such as new york state. analyzing patient level opioid related hospital visits provides the data driven idea behind the method to discover both spatial and temporal patterns and identity potential causes of opioid related deaths, which provides essential knowledge considering governments on decision making. inside this paper, we analyzed opioid poisoning related hospital visits with the help of new york state sparcs data, which provides diagnoses of patients inside hospital visits. we identified all patients with primary diagnosis as opioid poisoning from 2010-2014 considering our main studies, and from 2003-2014 considering temporal trend studies. we performed demographical based studies, and summarized a historical trends of opioid poisoning. we used frequent item mining to find co-occurrences of diagnoses considering possible causes of poisoning or effects from poisoning. we provided zip code level spatial analysis to detect local spatial clusters, and studied potential correlations between opioid poisoning and demographic and social-economic factors.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10242,"a total mass m_gcs inside a globular cluster (gc) system of the galaxy was empirically the near-constant fraction of a total mass m_h = m_bary + m_dark of a galaxy, across the range of 10^5 inside galaxy mass. this trend was radically unlike a strongly nonlinear behavior of total stellar mass m_star versus m_h. we discuss extensions of this trend to two more extreme situations: (a) entire clusters of galaxies, and (b) a ultra-diffuse galaxies (udgs) recently discovered inside coma and elsewhere. our calibration of a ratio \eta_m = m_gcs / m_h from normal galaxies, accounting considering new revisions inside a adopted mass-to-light ratio considering gcs, now gives \eta_m = 2.9 \times 10^{-5} as a mean absolute mass fraction. we find that a same ratio appears valid considering galaxy clusters and udgs. estimates of \eta_m inside a four clusters we examine tend to be slightly higher than considering individual galaxies, butmore data and better constraints on a mean gc mass inside such systems are needed to determine if this difference was significant. we use a constancy of \eta_m to approximate total masses considering several individual cases; considering example, a total mass of a milky way was calculated to be m_h = 1.1 \times 10^{12} m_sun. physical explanations considering a uniformity of \eta_m are still descriptive, but point to the picture inside which massive, dense star clusters inside their formation stages were relatively immune to a feedback that more strongly influenced lower-density regions where most stars form.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4168,"a exponential scaling of a wave function was the fundamental property of quantum systems with far reaching implications inside our ability to process quantum information. the problem where these are particularly relevant was quantum state tomography. state tomography, whose objective was to obtain the full description of the quantum system, should be analysed inside a framework of computational learning theory. inside this model, quantum states have been shown to be probably approximately correct (pac)-learnable with sample complexity linear inside a number of qubits. however, it was conjectured that inside general quantum states require an exponential amount of computation to be learned. here, with the help of results from a literature on a efficient classical simulation of quantum systems, we show that stabiliser states are efficiently pac-learnable. our results solve an open problem formulated by aaronson [proc. r. soc. a, 2088, (2007)] and propose learning theory as the tool considering exploring a power of quantum computation.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1266,"we study a dynamic response of the two-dimensional system of itinerant fermions inside a vicinity of the uniform ($\mathbf{q}=0$) ising nematic quantum critical point of $d-$wave symmetry. a nematic order parameter was not the conserved quantity, and this permits the nonzero value of a fermionic polarization inside a $d-$wave channel even considering vanishing momentum and finite frequency: $\pi(\mathbf{q} = 0,\omega_m) \neq 0$. considering weak coupling between a fermions and a nematic order parameter (i.e. a coupling was small compared to a fermi energy), we perturbatively compute $\pi (\mathbf{q} = 0,\omega_m) \neq 0$ over the parametrically broad range of frequencies where a fermionic self-energy $\sigma (\omega)$ was irrelevant, and use eliashberg theory to compute $\pi (\mathbf{q} = 0,\omega_m)$ inside a non-fermi liquid regime at smaller frequencies, where $\sigma (\omega) > \omega$. we find that $\pi(\mathbf{q}=0,\omega)$ was the constant, plus the frequency dependent correction that goes as $|\omega|$ at high frequencies, crossing over to $|\omega|^{1/3}$ at lower frequencies. a $|\omega|^{1/3}$ scaling holds also inside the non-fermi liquid regime. a non-vanishing of $\pi (\mathbf{q}=0, \omega)$ gives rise to additional structure inside a imaginary part of a nematic susceptibility $\chi^{''} (\mathbf{q}, \omega)$ at $\omega > v_f q$, inside marked contrast to a behavior of a susceptibility considering the conserved order parameter. this additional structure may be detected inside raman scattering experiments inside a $d-$wave geometry.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
8565,"daligault, rao and thomass√© asked whether every hereditary graph class that was well-quasi-ordered by a induced subgraph relation has bounded clique-width. lozin, razgon and zamaraev (jctb 2017+) gave the negative answer to this question, but their counterexample was the class that should only be characterised by infinitely many forbidden induced subgraphs. this raises a issue of whether a question has the positive answer considering finitely defined hereditary graph classes. apart from two stubborn cases, this has been confirmed when at most two induced subgraphs $h_1,h_2$ are forbidden. we confirm it considering one of a two stubborn cases, namely considering a $(h_1,h_2)=(\mbox{triangle},p_2+p_4)$ case, by proving that a class of $(\mbox{triangle},p_2+p_4)$-free graphs has bounded clique-width and was well-quasi-ordered. our technique was based on the special decomposition of $3$-partite graphs. we also use this technique to prove that a class of $(\mbox{triangle},p_1+p_5)$-free graphs, which was known to have bounded clique-width, was well-quasi-ordered. our results enable us to complete a classification of graphs $h$ considering which a class of $(\mbox{triangle},h)$-free graphs was well-quasi-ordered.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1365,"we present the study on a impact of mn$^{3+}$ substitution inside a geometrically frustrated ising garnet ho$_3$ga$_5$o$_{12}$ with the help of bulk magnetic measurements and low temperature powder neutron diffraction. we find that a transition temperature, $t_n$ = 5.8 k, considering ho$_3$mnga$_4$o$_{12}$ was raised by almost 20 when compared to ho$_3$ga$_5$o$_{12}$. powder neutron diffraction on ho$_3$mn$_x$ga$_{5-x}$o$_{12}$ ($x$ = 0.5, 1) below $t_n$ shows a formation of the long range ordered ordered state with $\mathbf{k}$ = (0,0,0). ho$^{3+}$ spins are aligned antiferromagnetically along a six crystallographic axes with no resultant moment while a mn$^{3+}$ spins are oriented along a body diagonals, such that there was the net moment along [111]. a magnetic structure should be visualised as ten-membered rings of corner-sharing triangles of ho$^{3+}$ spins with a mn$^{3+}$ spins ferromagnetically coupled to each individual ho$^{3+}$ spin inside a triangle. substitution of mn$^{3+}$ completely relieves a magnetic frustration with $f = \theta_{cw}/t_n \approx 1.1$ considering ho$_3$mnga$_4$o$_{12}$.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
17549,"detecting a presence of mesoscale structures inside complex networks was of primary importance. this was especially true considering financial networks, whose structural organization deeply affects their resilience to events like default cascades, shocks propagation, etc. several methods have been proposed, so far, to detect communities, i.e. groups of nodes whose connectivity was significantly large. communities, however do not represent a only kind of mesoscale structures characterizing real-world networks: other examples are provided by bow-tie structures, core-periphery structures and bipartite structures. here we propose the novel method to detect statistically-signifcant bimodular structures, i.e. either bipartite or core-periphery ones. it was based on the modification of a surprise, recently proposed considering detecting communities. our variant allows considering bimodular nodes partitions to be revealed, by letting links to be placed either 1) within a core part and between a core and a periphery parts or 2) just between a (empty) layers of the bipartite network. from the technical point of view, this was achieved by employing the multinomial hypergeometric distribution instead of a traditional (binomial) hypergeometric one; as inside a latter case, this allows the p-value to be assigned to any given (bi)partition of a nodes. to illustrate a performance of our method, we report a results of its application to several real-world networks, including social, economic and financial ones.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
12440,"inside statistics, there are the variety of methods considering performing model selection that all stem from slightly different paradigms of statistical inference. a reasons considering choosing one particular method over another seem to be based entirely on philosophical preferences. inside a case of non-nested model selection, two of a prevailing techniques are a bayes factor and a likelihood ratio. this article focuses on reconciling a likelihood ratio and a bayes factor considering comparing the pair of non-nested models under two different problem frameworks typical inside forensic science, a common-source and a specific-source problem. we show that a bayes factor should be expressed as a expected value of a corresponding likelihood ratio function with respect to a posterior distribution considering a parameters given a entire set of data where a set(s) of unknown-source observations has been generated according to a second model. this expression leads to the number of useful theoretic and practical results relating a two statistical approaches. this relationship was quite meaningful inside many scientific applications where there was the general confusion between a various statistical methods, and particularly inside a case of forensic science.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
2286,"we present a first simultaneous analysis of a galaxy overdensity and peculiar velocity fields by modelling their cross-covariance. we apply our new maximum-likelihood idea behind the method to data from a 6-degree field galaxy survey (6dfgs), which has a largest single collection of peculiar velocities to date. we present the full derivation of a analytic expression considering a cross-covariance between a galaxy overdensity and peculiar velocity fields and find direct evidence considering the non-zero correlation between a fields on scales up to $\sim50 h^{-1}$ mpc. when utilising a cross-covariance, our measurement of a normalised growth rate of structure was $f\sigma_8(z=0) = 0.424^{+0.067}_{-0.064}$ (15% precision), and our measurement of a redshift-space distortion parameter was $\beta=0.341^{+0.062}_{-0.058}$ (18% precision). both measurements improve by $\sim$20% compared to only with the help of a auto-covariance information. this was consistent with a literature on multiple-tracer approaches, as well as fisher matrix forecasts and previous analyses of 6dfgs. our measurement of $f\sigma_8$ was consistent with a standard cosmological model, and we discuss how our idea behind the method should be extended to test alternative models of gravity.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19395,"the method to determine a spin temperature of a local (vlsr=0 km/s) hi gas with the help of saturated brightness temperature of a 21-cm line inside a radial-velocity degenerate regions (vdr) was presented. a spin temperatures was determined to be ts= 146.2 +/- 16.1 k by measuring saturated brightness inside a vdr toward a galactic center, 146.8 +/- 10.7 k by chi^2 fitting of expected brightness distribution to observation around a vdr, and 144.4 +/- 6.8 k toward a local arm. assuming ts=146 k, the correction factor gamma considering a hi density, defined by a ratio of a true hi density considering finite optical thickness to that calculated by assuming optically thin hi, is obtained to be gamma~1.2 (optical depth tau~0.3) inside a local hi gas, ~1.8 (~1.3) toward a arm and anti-center, and as high as ~3.6 (~2.7) inside a galactic center direction. it was suggested that a hi density and mass inside a local arm could be ~2 times, and that inside a inner galaxy ~3.6 times, greater than a currently estimated values.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18103,superconducting single crystals of rb-intercalated fese compounds rb$_x$fe$_{2-y}$se$_2$ were prepared by with the help of the starting material of rb$_2$se as the rb source. a superconducting properties and a surface microstructures were systematically controlled by varying a cooling rate inside a quenching process. a higher cooling rate inside a sample provided the higher superconducting transition temperature with highly connected superconducting mesh-like surface structure. extremely slow-cooling process led to a complete isolation between a superconducting domains.,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
2496,"uncertainty analysis inside a form of probabilistic forecasting should significantly improve decision making processes inside a smart power grid considering better integrating renewable energy sources such as wind. whereas point forecasting provides the single expected value, probabilistic forecasts provide more information inside a form of quantiles, prediction intervals, or full predictive densities. this paper analyzes a effectiveness of the novel idea behind the method considering nonparametric probabilistic forecasting of wind power that combines the smooth approximation of a pinball loss function with the neural network architecture and the weighting initialization scheme to prevent a quantile cross over problem. the numerical case study was conducted with the help of publicly available wind data from a global energy forecasting competition 2014. multiple quantiles are estimated to form 10%, to 90% prediction intervals which are evaluated with the help of the quantile score and reliability measures. benchmark models such as a persistence and climatology distributions, multiple quantile regression, and support vector quantile regression are used considering comparison where results demonstrate a proposed idea behind the method leads to improved performance while preventing a problem of overlapping quantile estimates.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6301,"group i elements - alkali metals li, na, k, rb and cs - are examples of simple metals with one s electron inside a valence band. under pressure these elements display unusually complex structural behaviour transforming from close-packed to low symmetry open structures. unexpectedly complex form is found considering melting curves of alkalis under compression with initial increasing inside accordance to lindemann criterion and further decreasing to very low melting point. to understand complex and low symmetry structures inside compressed alkalis the transformation of a electron energy levels is suggested which involves an overlap between a valence band and outer core electrons. within a model of a fermi sphere - brillouin zone interaction one should understand a complex melting curve of alkalis.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
14259,"full duplex (fd) communications, which increases spectral efficiency through simultaneous transmission and reception on a same frequency band, was the promising technology to meet a demand of next generation wireless networks. inside this paper, we consider a application of such fd communication to self-backhauled small cells. we consider the fd capable small cell base station (bs) being wirelessly backhauled by the fd capable macro-cell bs. fd communication enables simultaneous backhaul and access transmissions at small cell bss, which reduces a need to orthogonalize allocated spectrum between access and backhaul. however, inside such simultaneous operations, all a links experience higher interference, which significantly suppresses a gains of fd operations. we propose an interference-aware scheduling method to maximize a fd gain across multiple ues inside both uplink and downlink directions, while maintaining the level of fairness between all ues. it jointly schedules a appropriate links and traffic based on a back-pressure algorithm, and allocates appropriate transmission powers to a scheduled links with the help of geometric programming. our simulation results show that a proposed scheduler nearly doubles a throughput of small cells compared to traditional half-duplex self-backhauling.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
11226,"we describe strong convex valid inequalities considering conic quadratic mixed 0-1 optimization. these inequalities should be utilized considering solving numerous practical nonlinear discrete optimization problems from value-at-risk minimization to queueing system design, from robust interdiction to assortment optimization through appropriate conic quadratic mixed 0-1 relaxations. a inequalities exploit a submodularity of a binary restrictions and are based on a polymatroid inequalities over binaries considering a diagonal case. we prove that a convex inequalities completely describe a convex hull of the single conic quadratic constraint as well as a rotated cone constraint over binary variables and unbounded continuous variables. we then generalize and strengthen a inequalities by incorporating additional constraints of a optimization problem. computational experiments on mean-risk optimization with correlations, assortment optimization, and robust conic quadratic optimization indicate that a new inequalities strengthen a convex relaxations substantially and lead to significant performance improvements.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
1950,"this paper proposes tripd, the new primal-dual algorithm considering minimizing a sum of the lipschitz-differentiable convex function and two possibly nonsmooth convex functions, one of which was composed with the linear mapping. we devise the randomized block-coordinate version of a algorithm which converges under a same stepsize conditions as a full algorithm. it was shown that both a original as well as a block-coordinate scheme feature linear convergence rate when a functions involved are either piecewise linear-quadratic, or when they satisfy the certain quadratic growth condition (which was weaker than strong convexity). moreover, we apply a developed algorithms to a problem of multi-agent optimization on the graph, thus obtaining novel synchronous and asynchronous distributed methods. a proposed algorithms are fully distributed inside a sense that a updates and a stepsizes of each agent only depend on local information. inside fact, no prior global coordination was required. finally, we showcase an application of our algorithm inside distributed formation control.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
16350,"considering the positive parameter $\beta$, a $\beta$-bounded distance between the pair of vertices $u,v$ inside the weighted undirected graph $g = (v,e,\omega)$ was a length of a shortest $u-v$ path inside $g$ with at most $\beta$ edges, aka {\em hops}. considering $\beta$ as above and $\epsilon>0$, the {\em $(\beta,\epsilon)$-hopset} of $g = (v,e,\omega)$ was the graph $g' =(v,h,\omega_h)$ on a same vertex set, such that all distances inside $g$ are $(1+\epsilon)$-approximated by $\beta$-bounded distances inside $g\cup g'$. hopsets are the fundamental graph-theoretic and graph-algorithmic construct, and they are widely used considering distance-related problems inside the variety of computational settings. currently existing constructions of hopsets produce hopsets either with $\omega(n \log n)$ edges, or with the hopbound $n^{\omega(1)}$. inside this paper we devise the construction of {\em linear-size} hopsets with hopbound $(\log n)^{\log^{(3)}n+o(1)}$. this improves a previous bound almost exponentially. we also devise efficient implementations of our construction inside pram and distributed settings. a only existing pram algorithm \cite{en16} considering computing hopsets with the constant (i.e., independent of $n$) hopbound requires $n^{\omega(1)}$ time. we devise the pram algorithm with polylogarithmic running time considering computing hopsets with the constant hopbound, i.e., our running time was exponentially better than a previous one. moreover, these hopsets are also significantly sparser than their counterparts from \cite{en16}. we use our hopsets to devise the distributed routing scheme that exhibits near-optimal tradeoff between individual memory requirement $\tilde{o}(n^{1/k})$ of vertices throughout preprocessing and routing phases of a algorithm, and stretch $o(k)$, along with the near-optimal construction time $\approx d + n^{1/2 + 1/k}$, where $d$ was a hop-diameter of a input graph.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15012,"because of a increasing availability of spatiotemporal data, the variety of data-analytic applications have become possible. characterizing driving context, where context may be thought of as the combination of location and time, was the new challenging application. an example of such the characterization was finding a correlation between driving behavior and traffic conditions. this contextual information enables analysts to validate observation-based hypotheses about a driving of an individual. inside this paper, we present drivecontext, the novel framework to find a characteristics of the context, by extracting significant driving patterns (e.g., the slow-down), and then identifying a set of potential causes behind patterns (e.g., traffic congestion). our experimental results confirm a feasibility of a framework inside identifying meaningful driving patterns, with improvements inside comparison with a state-of-the-art. we also demonstrate how a framework derives interesting characteristics considering different contexts, through real-world examples.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12082,"pbw degenerations are the particularly nice family of flat degenerations of type the flag varieties. we show that a cohomology of any pbw degeneration of a flag variety surjects onto a cohomology of a original flag variety, and that this holds inside an equivariant setting too. we also prove that a same was true inside a symplectic setting when considering feigin's linear degeneration of a symplectic flag variety.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
398,"spectra observed with a ultraviolet and visual echelle spectrograph (uves) on a european southern observatory's vlt exhibit long-range wavelength distortions. these distortions impose the systematic error on high-precision measurements of a fine-structure constant, $\alpha$, derived from intervening quasar absorption systems. if a distortion was modelled with the help of the model that was too simplistic, a resulting bias inside $\delta\alpha/\alpha$ away from a true value should be larger than a statistical uncertainty on a $\alpha$ measurement. if a effect was ignored altogether, a same was true. if a effect was modelled properly, accounting considering a way inside which final spectra are generally formed from a co-addition of exposures made at several different instrumental settings, a effect should be accurately removed and a correct $\delta\alpha/\alpha$ recovered.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7559,"inside this paper, we propose the novel graph-based idea behind the method considering semi-supervised learning problems, which considers an adaptive adjacency of a examples throughout a unsupervised portion of a training. adjacency of a examples was inferred with the help of a predictions of the neural network model which was first initialized by the supervised pretraining. these predictions are then updated according to the novel unsupervised objective which regularizes another adjacency, now linking a output nodes. regularizing a adjacency of a output nodes, inferred from a predictions of a network, creates an easier optimization problem and ultimately provides that a predictions of a network turn into a optimal embedding. ultimately, a proposed framework provides an effective and scalable graph-based solution which was natural to a operational mechanism of deep neural networks. our results show comparable performance with state-of-the-art generative approaches considering semi-supervised learning on an easier-to-train, low-cost framework.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
10241,"the common data mining task on networks was community detection, which seeks an unsupervised decomposition of the network into structural groups based on statistical regularities inside a network's connectivity. although many methods exist, a no free lunch theorem considering community detection implies that each makes some kind of tradeoff, and no algorithm should be optimal on all inputs. thus, different algorithms will over or underfit on different inputs, finding more, fewer, or just different communities than was optimal, and evaluation methods that use the metadata partition as the ground truth will produce misleading conclusions about general accuracy. here, we present the broad evaluation of over and underfitting inside community detection, comparing a behavior of 16 state-of-the-art community detection algorithms on the novel and structurally diverse corpus of 406 real-world networks. we find that (i) algorithms vary widely both inside a number of communities they find and inside their corresponding composition, given a same input, (ii) algorithms should be clustered into distinct high-level groups based on similarities of their outputs on real-world networks, and (iii) these differences induce wide variation inside accuracy on link prediction and link description tasks. we introduce the new diagnostic considering evaluating overfitting and underfitting inside practice, and use it to roughly divide community detection methods into general and specialized learning algorithms. across methods and inputs, bayesian techniques based on a stochastic block model and the minimum description length idea behind the method to regularization represent a best general learning approach, but should be outperformed under specific circumstances. these results introduce both the theoretically principled idea behind the method to evaluate over and underfitting inside models of network community structure and the realistic benchmark by which new methods may be evaluated and compared.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0
6921,"we describe tensorflow-serving, the system to serve machine learning models in google which was also available inside a cloud and using open-source. it was extremely flexible inside terms of a types of ml platforms it supports, and ways to integrate with systems that convey new models and updated versions from training to serving. at a same time, a core code paths around model lookup and inference have been carefully optimized to avoid performance pitfalls observed inside naive implementations. google uses it inside many production deployments, including the multi-tenant model hosting service called tfs^2.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6179,"solar system small bodies come inside the wide variety of shapes and sizes, which are achieved following very individual evolutional paths through billions of years. this paper focuses on a reshaping process of rubble-pile asteroids driven by meteorite impacts. inside our study, numerous possible equilibrium configurations are obtained using monte carlo simulation, and a structural stability of these configurations was determined using eigen analysis of a geometric constructions. a eigen decomposition reveals the connection between a cluster's reactions and a types of external disturbance. numerical simulations are performed to verify a analytical results. a gravitational n-body code pkdgrav was used to mimic a responses of a cluster under intermittent non-dispersive impacts. we statistically confirm that a stability index, a total gravitational potential and a volume of inertia ellipsoid show consistent tendency of variation. the common regime was found inside which a clusters tend towards crystallization under intermittent impacts, i.e., only a configurations with high structural stability survive under a external disturbances. a results suggest a trivial non-disruptive impacts might play an important role inside a rearrangement of a constituent blocks, which may strengthen these rubble piles and aid to build the robust structure under impacts of similar magnitude. a final part of this study consists of systematic simulations over two parameters, a projectile momentum and a rotational speed of a cluster. a results show the critical value exists considering a projectile momentum, as predicted by theory, below which all clusters become responseless to external disturbances; and a rotation proves to be significant considering it exhibits an ""enhancing"" effect on loose-packed clusters, which coincides with a observation that several fast-spinning asteroids have low bulk densities.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3852,"class labels have been empirically shown useful inside improving a sample quality of generative adversarial nets (gans). inside this paper, we mathematically study a properties of a current variants of gans that make use of class label information. with class aware gradient and cross-entropy decomposition, we reveal how class labels and associated losses influence gan's training. based on that, we propose activation maximization generative adversarial networks (am-gan) as an advanced solution. comprehensive experiments have been conducted to validate our analysis and evaluate a effectiveness of our solution, where am-gan outperforms other strong baselines and achieves state-of-the-art inception score (8.91) on cifar-10. inside addition, we demonstrate that, with a inception imagenet classifier, inception score mainly tracks a diversity of a generator, and there is, however, no reliable evidence that it should reflect a true sample quality. we thus propose the new metric, called am score, to provide the more accurate approximation of a sample quality. our proposed model also outperforms a baseline methods inside a new metric.",1,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17674,"about 20% of low-redshift galaxies are late-type spirals with the small or no bulge component. although they are a simplest disk galaxies inside terms of structure and dynamics, a role of a different physical processes driving their formation and evolution was not yet fully understood. we investigated whether small bulges of late-type spirals follow a same scaling relations traced by ellipticals and large bulges and if they are disk-like or classical bulges. we derived a photometric and kinematic properties of 9 nearby late-type spirals. to this aim, we analyzed a surface brightness distribution from a i-band images of a sloan digital sky survey and obtained a structural parameters of a galaxies from the two-dimensional photometric decomposition. we measured a line-of-sight stellar velocity distribution within a bulge effective radius from a long-slit spectra taken with high spectral resolution at a telescopio nazionale galileo. we used a photometric and kinematic properties of a sample bulges to study their location inside a fundamental plane, kormendy, and faber-jackson relations defined considering ellipticals and large bulges. we found that our sample bulges satisfy some of a photometric and kinematic prescriptions considering being considered disk-like bulges such as small sizes and masses with nearly exponential light profiles, small bulge-to-total luminosity ratios, low stellar velocity dispersions, and ongoing star formation. however, each of them follows a same scaling relations of ellipticals, massive bulges, and compact early-type galaxies so they cannot be classified as disk-like systems. we find the single population of galaxy spheroids that follow a same scaling relations, where a mass seems to lead to the smooth transition inside a photometric and kinematic properties from less massive bulges to more massive bulges and ellipticals.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18499,"discrminative trackers, employ the classification idea behind the method to separate a target from its background. to cope with variations of a target shape and appearance, a classifier was updated online with different samples of a target and a background. sample selection, labeling and updating a classifier was prone to various sources of errors that drift a tracker. we introduce a use of an efficient version space shrinking strategy to reduce a labeling errors and enhance its sampling strategy by measuring a uncertainty of a tracker about a samples. a proposed tracker, utilize an ensemble of classifiers that represents different hypotheses about a target, diversify them with the help of boosting to provide the larger and more consistent coverage of a version-space and tune a classifiers' weights inside voting. a proposed system adjusts a model update rate by promoting a co-training of a short-memory ensemble with the long-memory oracle. a proposed tracker outperformed state-of-the-art trackers on different sequences bearing various tracking challenges.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7143,"we set out to quantify a number density of quiescent massive compact galaxies at intermediate redshifts. we determine structural parameters based on i-band imaging with the help of a cfht equatorial sdss stripe 82 (cs82) survey (~170 sq. degrees) taking advantage of an exquisite median seeing of ~0.6''. we select compact massive (m > 5x10^10 m_sun) galaxies within a redshift range of 0.2<z<0.6. a large volume sampled allows to decrease a effect of cosmic variance that has hampered a calculation of a number density considering this enigmatic population inside many previous studies. we undertake an exhaustive analysis inside an effort to untangle a various findings inherent to a diverse definition of compactness present inside a literature. we find that a absolute number of compact galaxies was very dependent on a adopted definition and should change up to the factor of >10. we systematically measure the factor of ~5 more compacts at a same redshift than what is previously reported on smaller fields with hst imaging, which are more affected by cosmic variance. this means that a decrease inside number density from z ~ 1.5 to z ~ 0.2 might be only of the factor of ~2-5, significantly smaller than what previously reported. this supports progenitor bias as a main contributor to a size evolution. this milder decrease was roughly compatible with a predictions from recent numerical simulations. only a most extreme compact galaxies, with reff < 1.5x( m/10^11 m_sun)^0.75 and m > 10^10.7 m_sun, appear to drop inside number by the factor of ~20 and thus likely experience the noticeable size evolution.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19354,"no definitive evidence of spacetime supersymmetry (susy) that transmutes fermions into bosons and vice versa has been revealed inside nature so far. moreover, whether spacetime susy inside 2+1 and higher dimensions should occur or emerge inside generic microscopic models remains open. here, we introduce the lattice realization of the \textit{single} dirac fermion with attractive hubbard interactions that preserves both time-reversal and chiral symmetries. by performing numerically-exact sign-problem-free determinant quantum monte carlo simulations, we show that a interacting single dirac fermion inside 2+1 dimensions features the superconducting quantum critical point (qcp). more remarkably, we demonstrate that a ${\mathcal n}$=2 spacetime susy inside 2+1d emerges at a superconducting qcp by showing that a fermions and bosons have \textit{identical} anomalous dimensions 1/3, the hallmark of a emergent susy. to a best of our knowledge, this was a first observation of emergent 2+1d spacetime susy inside quantum microscopic models. we further show some experimental signatures which should be measured to test such emergent susy inside candidate systems such as a surface of 3d topological insulators.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
3981,"native horse mucus was characterized with micro- and macrorheology and compared to hydroxyethylcellulose (hec) gel as the model. both systems show comparable viscoelastic properties on a microscale and considering a hec a macrorheology was inside good agreement with a microrheology. considering a mucus, a viscoelastic moduli on a macroscale are several orders of magnitude larger than on a microscale. large amplitude oscillatory shear experiments show that a mucus responds nonlinearly at much smaller deformations than hec. this behavior fosters a assumption that a mucus has the foam like structure on a microscale compared to a typical mesh like structure of a hec, the model that was supported by cryogenic-scanning-electron-microscopy (csem) images. these images allow also to determine a relative amount of volume that was occupied by a pores and a scaffold. consequently, we should approximate a elastic modulus of a scaffold. we conclude that this particular foam like microstructure should be considered as the key factor considering a transport of particulate matter which plays the central role inside mucus function with respect to particle penetration. a mesh properties composed of very different components are responsible considering macroscopic and microscopic behavior being part of particles fate after landing.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
11861,"random forests have become an important tool considering improving accuracy inside regression problems since their popularization by [breiman, 2001] and others. inside this paper, we revisit the random forest model originally proposed by [breiman, 2004] and later studied by [biau, 2012], where the feature was selected at random and a split occurs at a midpoint of a box containing a chosen feature. if a lipschitz regression function was sparse and only depends on the small, unknown subset of $s$ out of $d$ features, we show that, given access to $n$ observations, this random forest model outputs the predictor that has the mean-squared prediction error $o((n(\sqrt{\log n})^{s-1})^{-\frac{1}{s\log2+1}})$. this positively answers an outstanding question of [biau, 2012] about whether a rate of convergence therein could be improved. a second part of this article shows that a aforementioned prediction error cannot generally be improved, which we accomplish by characterizing a variance and by showing that a bias was tight considering any linear model with nonzero parameter vector. as the striking consequence of our analysis, we show a variance of this forest was similar inside form to a best-case variance lower bound of [lin and jeon, 2006] among all random forest models with nonadaptive splitting schemes (i.e., where a split protocol was independent of a training data).",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
636,"deep learning has been shown to outperform traditional machine learning algorithms across the wide range of problem domains. however, current deep learning algorithms have been criticized as uninterpretable ""black-boxes"" which cannot explain their decision making processes. this was the major shortcoming that prevents a widespread application of deep learning to domains with regulatory processes such as finance. as such, industries such as finance have to rely on traditional models like decision trees that are much more interpretable but less effective than deep learning considering complex problems. inside this paper, we propose clear-trade, the novel financial ai visualization framework considering deep learning-driven stock market prediction that mitigates a interpretability issue of deep learning methods. inside particular, clear-trade provides the effective way to visualize and explain decisions made by deep stock market prediction models. we show a efficacy of clear-trade inside enhancing a interpretability of stock market prediction by conducting experiments based on s&p 500 stock index prediction. a results demonstrate that clear-trade should provide significant insight into a decision-making process of deep learning-driven financial models, particularly considering regulatory processes, thus improving their potential uptake inside a financial industry.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19803,e. gutkin found the remarkable class of convex billiard tables inside a plane which have the constant angle invariant curve. inside this paper we prove that inside dimension 3 only round sphere has such the property. considering dimension greater than 3 it must be either the sphere or to have the very special geometric properties. inside 2-dimensional case we prove the rigidity result considering gutkin billiard tables. this was done with a aid of the new generating function introduced recently considering billiards inside our joint paper with a.e. mironov. the formula considering this generating function inside higher dimensions was found.,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16904,"a acquisition of magnetic resonance imaging (mri) was inherently slow. inspired by recent advances inside deep learning, we propose the framework considering reconstructing mr images from undersampled data with the help of the deep cascade of convolutional neural networks to accelerate a data acquisition process. we show that considering cartesian undersampling of 2d cardiac mr images, a proposed method outperforms a state-of-the-art compressed sensing approaches, such as dictionary learning-based mri (dlmri) reconstruction, inside terms of reconstruction error, perceptual quality and reconstruction speed considering both 3-fold and 6-fold undersampling. compared to dlmri, a error produced by a method proposed was approximately twice as small, allowing to preserve anatomical structures more faithfully. with the help of our method, each image should be reconstructed inside 23 ms, which was fast enough to enable real-time applications.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4478,"we show that entropy-sgd (chaudhari et al., 2017), when viewed as the learning algorithm, optimizes the pac-bayes bound on a risk of the gibbs (posterior) classifier, i.e., the randomized classifier obtained by the risk-sensitive perturbation of a weights of the learned classifier. entropy-sgd works by optimizing a bound's prior, violating a hypothesis of a pac-bayes theorem that a prior was chosen independently of a data. indeed, available implementations of entropy-sgd rapidly obtain zero training error on random labels and a same holds of a gibbs posterior. inside order to obtain the valid generalization bound, we rely on the result showing that data-dependent priors obtained by stochastic gradient langevin dynamics (sgld) yield valid pac-bayes bounds provided a target distribution of sgld was $\epsilon$-differentially private. we observe that test error on mnist and cifar10 falls within a (empirically nonvacuous) risk bounds computed under a assumption that sgld reaches stationarity. inside particular, entropy-sgld should be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16995,"a interplay between symmetry and topology leads to the rich variety of electronic topological phases, protecting states such as a topological insulators and dirac semimetals. previous results, like a fu-kane parity criterion considering inversion-symmetric topological insulators, demonstrate that symmetry labels should sometimes unambiguously indicate underlying band topology. here we develop the systematic idea behind the method to expose all such symmetry-based indicators of band topology inside all a 230 space groups. this was achieved by first developing an efficient way to represent band structures inside terms of elementary basis states, and then isolating a topological ones by removing a subset of atomic insulators, defined by a existence of localized symmetric wannier functions. aside from encompassing all earlier results on such indicators, including inside particular a notion of filling-enforced quantum band insulators, our theory identifies symmetry settings with previously hidden forms of band topology, and should be applied to a search considering topological materials.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
6514,"a control of spins and spin to charge conversion inside organics requires understanding a molecular spin-orbit coupling (soc), and the means to tune its strength. however, quantifying soc strengths indirectly through spin relaxation effects has proven diffi- cult due to competing relaxation mechanisms. here we present the systematic study of a g-tensor shift inside molecular semiconductors and link it directly to a soc strength inside the series of high mobility molecular semiconductors with strong potential considering future devices. a results demonstrate the rich variability of a molecular g-shifts with a effective soc, depending on subtle aspects of molecular composition and structure. we correlate a above g -shifts to spin-lattice relaxation times over four orders of magnitude, from 200 {\mu}s to 0.15 {\mu}s, considering isolated molecules inside solution and relate our findings considering isolated molecules inside solution to a spin relaxation mechanisms that are likely to be relevant inside solid state systems.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
7052,"object detection and segmentation represents a basis considering many tasks inside computer and machine vision. inside biometric recognition systems a detection of a region-of-interest (roi) was one of a most crucial steps inside a overall processing pipeline, significantly impacting a performance of a entire recognition system. existing approaches to ear detection, considering example, are commonly susceptible to a presence of severe occlusions, ear accessories or variable illumination conditions and often deteriorate inside their performance if applied on ear images captured inside unconstrained settings. to address these shortcomings, we present inside this paper the novel ear detection technique based on convolutional encoder-decoder networks (ceds). considering our technique, we formulate a problem of ear detection as the two-class segmentation problem and train the convolutional encoder-decoder network based on a segnet architecture to distinguish between image-pixels belonging to either a ear or a non-ear class. a output of a network was then post-processed to further refine a segmentation result and return a final locations of a ears inside a input image. different from competing techniques from a literature, our idea behind the method does not simply return the bounding box around a detected ear, but provides detailed, pixel-wise information about a location of a ears inside a image. our experiments on the dataset gathered from a web (a.k.a. inside a wild) show that a proposed technique ensures good detection results inside a presence of various covariate factors and significantly outperforms a existing state-of-the-art.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12586,"we consider a problem of estimating a $l_1$ distance between two discrete probability measures $p$ and $q$ from empirical data inside the nonasymptotic and large alphabet setting. when $q$ was known and one obtains $n$ samples from $p$, we show that considering every $q$, a minimax rate-optimal estimator with $n$ samples achieves performance comparable to that of a maximum likelihood estimator (mle) with $n\ln n$ samples. when both $p$ and $q$ are unknown, we construct minimax rate-optimal estimators whose worst case performance was essentially that of a known $q$ case with $q$ being uniform, implying that $q$ being uniform was essentially a most difficult case. a \emph{effective sample size enlargement} phenomenon, identified inside jiao \emph{et al.} (2015), holds both inside a known $q$ case considering every $q$ and a $q$ unknown case. however, a construction of optimal estimators considering $\|p-q\|_1$ requires new techniques and insights beyond a approximation-based method of functional approximation inside jiao \emph{et al.} (2015).",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
5828,"high order cumulant tensors carry information about statistics of non-normally distributed multivariate data. inside this work we present the new efficient algorithm considering calculation of cumulants of arbitrary order inside the sliding window considering data streams. we showed that this algorithms enables speedups of cumulants updates compared to current algorithms. this algorithm should be used considering processing on-line high-frequency multivariate data and should find applications in, e.g., on-line signal filtering and classification of data streams. to present an application of this algorithm, we propose an estimator of non-gaussianity of the data stream based on a norms of high-order cumulant tensors. we show how to detect a transition from gaussian distributed data to non-gaussian ones inside a~data stream. inside order to achieve high implementation efficiency of operations on super-symmetric tensors, such as cumulant tensors, we employ a block structure to store and calculate only one hyper-pyramid part of such tensors.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1600,"multi-task gaussian processes (mtgps) have shown the significant progress both inside expressiveness and interpretation of a relatedness between different tasks: from linear combinations of independent single-output gaussian processes (gps), through a direct modeling of a cross-covariances such as spectral mixture kernels with phase shift, to a design of multivariate covariance functions based on spectral mixture kernels which model delays among tasks inside addition to phase differences, and which provide the parametric interpretation of a relatedness across tasks. inside this paper we further extend expressiveness and interpretability of mtgps models and introduce the new family of kernels capable to model nonlinear correlations between tasks as well as dependencies between spectral mixtures, including time and phase delay. specifically, we use generalized convolution spectral mixture kernels considering modeling dependencies at spectral mixture level, and coupling coregionalization considering discovering task level correlations. a proposed kernels considering mtgp are validated on artificial data and compared with existing mtgps methods on three real-world experiments. results indicate a benefits of our more expressive representation with respect to performance and interpretability.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18146,"feature engineering was the crucial step inside a process of predictive modeling. it involves a transformation of given feature space, typically with the help of mathematical functions, with a objective of reducing a modeling error considering the given target. however, there was no well-defined basis considering performing effective feature engineering. it involves domain knowledge, intuition, and most of all, the lengthy process of trial and error. a human attention involved inside overseeing this process significantly influences a cost of model generation. we present the new framework to automate feature engineering. it was based on performance driven exploration of the transformation graph, which systematically and compactly enumerates a space of given options. the highly efficient exploration strategy was derived through reinforcement learning on past examples.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
413,"our ability to synthesize sensory data that preserves specific statistical properties of a real data has had tremendous implications on data privacy and big data analytics. a synthetic data should be used as the substitute considering selective real data segments,that are sensitive to a user, thus protecting privacy and resulting inside improved analytics.however, increasingly adversarial roles taken by data recipients such as mobile apps, or other cloud-based analytics services, mandate that a synthetic data, inside addition to preserving statistical properties, should also be difficult to distinguish from a real data. typically, visual inspection has been used as the test to distinguish between datasets. but more recently, sophisticated classifier models (discriminators), corresponding to the set of events, have also been employed to distinguish between synthesized and real data. a model operates on both datasets and a respective event outputs are compared considering consistency. inside this paper, we take the step towards generating sensory data that should pass the deep learning based discriminator model test, and make two specific contributions: first, we present the deep learning based architecture considering synthesizing sensory data. this architecture comprises of the generator model, which was the stack of multiple long-short-term-memory (lstm) networks and the mixture density network. second, we use another lstm network based discriminator model considering distinguishing between a true and a synthesized data. with the help of the dataset of accelerometer traces, collected with the help of smartphones of users doing their daily activities, we show that a deep learning based discriminator model should only distinguish between a real and synthesized traces with an accuracy inside a neighborhood of 50%.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7028,"algorithms considering robotic visual search should benefit from a use of visual attention methods inside order to reduce computational costs. here, we describe how three distinct mechanisms of visual attention should be integrated and productively used to improve search performance. a first was viewpoint selection as has been proposed earlier with the help of the greedy search over the probabilistic occupancy grid representation. a second was top-down object-based attention with the help of the histogram backprojection method, also previously described. a third was visual saliency. this was novel inside a sense that it was not used as the region-of-interest method considering a current image but rather as the noncombinatorial form of look-ahead inside search considering future viewpoint selection. additionally, a integration of these three attentional schemes within the single framework was unique and not previously studied. we examine our proposed method inside scenarios where little or no information regarding a environment was available. through extensive experiments on the mobile robot, we show that our method improves visual search performance by reducing a time and number of actions required.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
15874,"inside this paper, we consider a $l$-functions $l(s, f)$ where $f$ was an eigenform considering a congruence subgroup $\gamma_1(q)$. we prove an asymptotic formula considering a sixth moment of this family of automorphic $l$-functions.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
9942,"considering a multi-objective time series search problem, hasegawa and itoh [theoretical computer science, vo.718, pp.58-66, 2018] presented a best possible online algorithm balanced price policy (bpp considering short) considering any monotone function $f: r^k \to r$. specifically, a competitive ratio with respect to a monotone function $f(c_{1},\ldots,c_{k})=(c_{1}+\cdots+c_{k})/k$ was referred to as a arithmetic mean component competitive ratio. hasegawa and itoh derived a closed formula of a arithmetic mean component competitive ratio considering $k=2$, but it has not been known considering any integer $k \geq 3$. inside this paper, we show that it was np-hard to derive closed formulas of a arithmetic mean component competitive ratio considering general integer $k\geq 2$. on a a hand, we derive closed formulas of a arithmetic mean component competitive ratio considering $k=3$ and $k=4$.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7233,"there are two theories describing a linearizability of 3-webs: one was developed inside a article ""on a linearizability of 3-webs"" (nonlinear analysis 47, (2001) pp.2643-2654) and another inside a article ""on a blaschke conjecture considering 3-webs"" (j. geom. anal. 16, 1 (2006), 69-115). unfortunately, they cannot be both correct because on an explicit 3-web w they contradict: a first predicts that w was linearizable while a second states that w was not linearizable. a essential question beyond this particular 3-web is: which theory describes correctly a linearizability condition? inside this paper we present the very short proof, due to j.-p.~dufour, that w was linearizable, confirming a result of a first article.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13611,"a trojan asteroids of jupiter and neptune are likely to have been captured from original heliocentric orbits inside a dynamically excited (""hot"") population of a kuiper belt. however, it has long been known that a optical color distributions of a jovian trojans and a hot population are not alike. this difference has been reconciled with a capture hypothesis by assuming that a trojans were resurfaced (for example, by sublimation of near-surface volatiles) upon inward migration from a kuiper belt (where blackbody temperatures are $\sim$40 k) to jupiter's orbit ($\sim$125 k). here, we examine a optical color distribution of a \textit{neptunian} trojans with the help of the combination of new optical photometry and published data. we find the color distribution that was statistically indistinguishable from that of a jovian trojans but unlike any sub-population inside a kuiper belt. this result was puzzling, because a neptunian trojans are very cold (blackbody temperature $\sim$50 k) and the thermal process acting to modify a surface colors at neptune's distance would also affect a kuiper belt objects beyond, where a temperatures are nearly identical. a distinctive color distributions of a jovian and neptunian trojans thus present us with the conundrum: they are very similar to each other, suggesting either capture from the common source or surface modification by the common process. however, a color distributions differ from any plausible common source population, and there was no known modifying process that could operate equally at both jupiter and neptune.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9463,"mobile robot navigation inside complex and dynamic environments was the challenging but important problem. reinforcement learning approaches fail to solve these tasks efficiently due to reward sparsities, temporal complexities and high-dimensionality of sensorimotor spaces which are inherent inside such problems. we present the novel idea behind the method to train action policies to acquire navigation skills considering wheel-legged robots with the help of deep reinforcement learning. a policy maps height-map image observations to motor commands to navigate to the target position while avoiding obstacles. we propose to acquire a multifaceted navigation skill by learning and exploiting the number of manageable navigation behaviors. we also introduce the domain randomization technique to improve a versatility of a training samples. we demonstrate experimentally the significant improvement inside terms of data-efficiency, success rate, robustness against irrelevant sensory data, and also a quality of a maneuver skills.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
15977,"we study a application of polar codes inside deletion channels by analyzing a cascade of the binary erasure channel (bec) and the deletion channel. we show how polar codes should be used effectively on the bec with the single deletion, and propose the list decoding algorithm with the cyclic redundancy check considering this case. a decoding complexity was $o(n^2\log n)$, where $n$ was a blocklength of a code. an important contribution was an optimization of a amount of redundancy added to minimize a overall error probability. our theoretical results are corroborated by numerical simulations which show that a list size should be reduced to one and a original message should be recovered with high probability as a length of a code grows.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
16146,"this paper explores a adaptive (active) pac (probably approximately correct) top-$k$ ranking (i.e., top-$k$ item selection) and total ranking problems from $l$-wise ($l\geq 2$) comparisons under a multinomial logit (mnl) model. by adaptively choosing sets to query and observing a noisy output of a most favored item of each query, we want to design ranking algorithms that recover a top-$k$ or total ranking with the help of as few queries as possible. considering a pac top-$k$ ranking problem, we derive the lower bound on a sample complexity (aka number of queries), and propose an algorithm that was sample-complexity-optimal up to an $o(\log(k+l)/\log{k})$ factor. when $l=2$ (i.e., pairwise comparisons) or $l=o(poly(k))$, this algorithm matches a lower bound. considering a pac total ranking problem, we derive the tight lower bound, and propose an algorithm that matches a lower bound. when $l=2$, a mnl model reduces to a popular plackett-luce (pl) model. inside this setting, our results still outperform a state-of-the-art both theoretically and numerically. we also compare our algorithms with a state-of-the-art with the help of synthetic data as well as real-world data to verify a efficiency of our algorithms.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11050,"we study a unsupervised learning of cnns considering optical flow approximation with the help of proxy ground truth data. supervised cnns, due to their immense learning capacity, have shown superior performance on the range of computer vision problems including optical flow prediction. they however require a ground truth flow which was usually not accessible except on limited synthetic data. without a guidance of ground truth optical flow, unsupervised cnns often perform worse as they are naturally ill-conditioned. we therefore propose the novel framework inside which proxy ground truth data generated from classical approaches was used to guide a cnn learning. a models are further refined inside an unsupervised fashion with the help of an image reconstruction loss. our guided learning idea behind the method was competitive with or superior to state-of-the-art approaches on three standard benchmark datasets yet was completely unsupervised and should run inside real time.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17289,"we study different aspects of quantum entanglement and its measures, including entanglement entropy inside a vacuum state of the certain lifshitz scalar theory. we present simple intuitive arguments based on ""non-local"" effects of this theory that a scaling of entanglement entropy depends on a dynamical exponent as the characteristic parameter of a theory. a scaling was such that inside a massless theory considering small entangling regions it leads to area law inside a lorentzian limit and volume law inside a $z\to\infty$ limit. we present strong numerical evidences inside (1+1) and (2+1)-dimensions inside support of this behavior. inside (2+1)-dimensions we also study some shape dependent aspects of entanglement. we argue that inside a massless limit corner contributions are no more additive considering large enough dynamical exponents due to non-local effects of lifshitz theories. we also comment on possible holographic duals of such theories based on a sign of tripartite information.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
9835,"trajectory prediction of dynamic objects was the widely studied topic inside a field of artificial intelligence. thanks to the large number of applications like predicting abnormal events, navigation system considering a blind, etc. there have been many approaches to attempt learning patterns of motion directly from data with the help of the wide variety of techniques ranging from hand-crafted features to sophisticated deep learning models considering unsupervised feature learning. all these approaches have been limited by problems like inefficient features inside a case of hand crafted features, large error propagation across a predicted trajectory and no information of static artefacts around a dynamic moving objects. we propose an end to end deep learning model to learn a motion patterns of humans with the help of different navigational modes directly from data with the help of a much popular sequence to sequence model coupled with the soft attention mechanism. we also propose the novel idea behind the method to model a static artefacts inside the scene and with the help of these to predict a dynamic trajectories. a proposed method, tested on trajectories of pedestrians, consistently outperforms previously proposed state of a art approaches on the variety of large scale data sets. we also show how our architecture should be naturally extended to handle multiple modes of movement (say pedestrians, skaters, bikers and buses) simultaneously.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4155,"this was a second part inside the series of two papers that concern with a quantitative analysis of a electromagnetic field enhancement and anomalous diffraction by the periodic array of subwavelength slits. inside this part, we explore a scattering problem inside a homogenization regimes, where a size of a period was much smaller than a incident wavelength. inside particular, two homogenization regimes are investigated, where a size of a pattered slits has a same order as a size of a period inside a first configuration, and a size of a slit was much smaller than a size of a period inside a second configuration. by presenting rigorous asymptotic analysis, we demonstrate that surface plasmonic effect mimicking that of plasmonic metals occurs inside a first homogenization regime. a corresponding dispersion curve lies below a light line and a associated eigenmodes are surface bound sates. inside addition, considering a incident plane wave, we discover and justify the novel phenomenon of total transmission which occurs either at certain frequencies considering all incident angles, or at the special incident angle but considering all frequencies. considering a second homogenization regime, a non-resonant field enhancement was investigated, and it was shown that a fast transition of a magnetic field inside a slit induces strong electric field enhancement. moreover, a enhancement becomes stronger when a coupling of a slits was weaker.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6223,"creating and modeling real-world graphs was the crucial problem inside various applications of engineering, biology, and social sciences; however, learning a distributions of nodes/edges and sampling from them to generate realistic graphs was still challenging. moreover, generating the diverse set of synthetic graphs that all imitate the real network was not addressed. inside this paper, a novel problem of creating diverse synthetic graphs was solved. first, we devise a deep supervised subset selection (deeps3) algorithm; given the ground-truth set of data points, deeps3 selects the diverse subset of all items (i.e. data points) that best represent a items inside a ground-truth set. furthermore, we propose a deep graph representation recurrent network (grrn) as the novel generative model that learns the probabilistic representation of the real weighted graph. training a grrn, we generate the large set of synthetic graphs that are likely to follow a same features and adjacency patterns as a original one. incorporating grrn with deeps3, we select the diverse subset of generated graphs that best represent a behaviors of a real graph (i.e. our ground-truth). we apply our model to a novel problem of power grid synthesis, where the synthetic power network was created with a same physical/geometric properties as the real power system without revealing a real locations of a substations (nodes) and a lines (edges), since such data was confidential. experiments on a synthetic power grid data set show accurate synthetic networks that follow similar structural and spatial properties as a real power grid.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0
10166,"a well-known gkyp was widely used inside system analysis, but considering singular systems, especially singular fractional order systems, there was no corresponding theory, considering which many control problems considering this type of system should not be optimized inside a limited frequency ranges. inside this paper, the universal framework of finite frequency band gkyp lemma considering singular fractional order systems was established. then a bounded real lemma inside a sense of l was derived considering different frequency ranges. furthermore, a corresponding controller was designed to improve a l performance index of singular fractional order systems. three illustrative examples are given to demonstrate a correctness and effectiveness of a theoretical results.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
14387,"with the help of the macroscopic approach, we studied theoretically a heat generation due to spin transport inside the typical spin valve with nonmagnetic spacer layer of finite thickness. our analysis shows that a spin-dependent heat generation should also be caused by another mechanism, a spin-conserving scattering inside a presence of spin accumulation gradient, inside addition to a well-known spin-flip scattering. a two mechanisms have equal contributions inside semi-infinite layers, such as a ferromagnetic layers of a spin valve. however, inside a nonmagnetic layer of the thickness much smaller than its spin-diffusion length, a spin-dependent heat generation was dominated by a spin-flip scattering inside a antiparallel configuration, and by a spin-conserving scattering inside a parallel configuration. we also proved that a spin-dependent heat generation cannot be interpreted as a joule heating of a spin-coupled interface resistance inside each individual layer. an effective resistance was proposed as an alternative so that a heat generation should still be described simply by applying joule's law to an equivalent circuit.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
14687,"we present work on building the global long-tailed ranking of entities across multiple languages with the help of wikipedia and freebase knowledge bases. we identify multiple features and build the model to rank entities with the help of the ground-truth dataset of more than 10 thousand labels. a final system ranks 27 million entities with 75% precision and 48% f1 score. we provide performance evaluation and empirical evidence of a quality of ranking across languages, and open a final ranked lists considering future research.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
15658,"a taylor-melcher (tm) model was a standard model considering describing a dynamics of poorly conducting leaky dielectric fluids under an electric field. a tm model treats a fluids as ohmic conductors, without modeling a underlying ion dynamics. on a other hand, electrodiffusion models, which have been successful inside describing electrokinetic phenomena, incorporate ionic concentration dynamics. mathematical reconciliation of a electrodiffusion picture and a tm model has been the major issue considering electrohydrodynamic theory. here, we derive a tm model from an electrodiffusion model inside which we explicitly model a electrochemistry of ion dissociation. we introduce salt dissociation reaction terms inside a bulk electrodiffusion equations and take a limit inside which a salt dissociation was weak; a assumption of weak dissociation corresponds to a fact that a tm model describes poor conductors. together with a assumption that a debye length was small, we derive a tm model with or without a surface charge convection term depending upon a scaling of relevant dimensionless parameters. an important quantity that emerges was a galvani potential (gp), a jump inside voltage across a liquid-liquid interface between a two leaky dielectric media; a gp arises as the natural consequence of a interfacial boundary conditions considering a ionic concentrations, and was absent under certain parametric conditions. when a gp was absent, we recover a tm model. our analysis also reveals a structure of a debye layer at a liquid-liquid interface, which suggests how interfacial singularities may arise under strong imposed electric fields. inside a presence of the non-zero gp, our model predicts that a liquid droplet will drift under an imposed electric field, a velocity of which was computed explicitly to leading order.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14968,"we define compactifications of vector spaces which are functorial with respect to certain linear maps. these ""many-body"" compactifications are manifolds with corners, and a linear maps lift to b-maps inside a sense of melrose. we derive the simple criterion under which a lifted maps are inside fact b-fibrations, and identify how these restrict to boundary hypersurfaces. this theory was an application of the general result on a iterated blow-up of cleanly intersecting submanifolds which extends related results inside a literature.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10448,"a asteroid (16) psyche was of scientific interest because it contains ~ 1% of a total mass of a asteroid belt and was thought to be a remnant metallic core of the protoplanet. radar observations have indicated a significant presence of metal on a surface with the small percentage of silicates. prior ground-based observations showed rotational variations inside a near-infrared (nir) spectra and radar albedo of this asteroid. however, no comprehensive study that combines multi-wavelength data has been conducted so far. here we present rotationally resolved nir spectra (0.7-2.5 microns) of (16) psyche obtained with a nasa infrared telescope facility. these data have been combined with shape models of a asteroid considering each rotation phase. spectral band parameters extracted from a nir spectra show that a pyroxene band center varies from ~ 0.92 to 0.94 microns. band center values were used to calculate a pyroxene chemistry of a asteroid, whose average value is found to be fs30en65wo5. variations inside a band depth were also observed, with values ranging from 1.0 to 1.5%. with the help of the new laboratory spectral calibration we estimated an average orthopyroxene content of 6+/-1%. a mass-deficit region of psyche, which exhibits a highest radar albedo, also shows a highest value considering spectral slope and a minimum band depth. a spectral characteristics of psyche suggest that its parent body did not have a typical structure expected considering the differentiated body or that a sequence of events that led to its current state is more complex than previously thought.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2372,"we performed $^{59}$co nuclear magnetic resonance (nmr) measurements of single-crystalline u$_6$co. there was the small decrease inside a knight shift inside a superconducting (sc) state, but this change mainly arises from a sc diamagnetic effect. a negligible change of a spin part of a knight shift, together with a absence of a pauli-paramagnetic effect inside a sc u$_6$co, was understood as the consequence of a small spin susceptibility. a nuclear spin-lattice relaxation rate $1/t_1$ was also measured inside a sc state under a magnetic field, and exhibits the tiny hebel-slichter peak just below a sc transition temperature and exponential behavior at lower temperatures. these behaviors are inside agreement with a full-gap s-wave pairing inside u$_6$co.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
3190,"an adaptive regularization algorithm with the help of high-order models was proposed considering partially separable convexly constrained nonlinear optimization problems whose objective function contains non-lipschitzian $\ell_q$-norm regularization terms considering $q\in (0,1)$. it was shown that a algorithm with the help of an $p$-th order taylor model considering $p$ odd needs inside general at most $o(\epsilon^{-(p+1)/p})$ evaluations of a objective function and its derivatives (at points where they are defined) to produce an $\epsilon$-approximate first-order critical point. this result was obtained either with taylor models at a price of requiring a feasible set to be 'kernel-centered' (which includes bound constraints and many other cases of interest), or considering non-lipschitz models, at a price of passing a difficulty to a computation of a step. since this complexity bound was identical inside order to that already known considering purely lipschitzian minimization subject to convex constraints [cartgoultoin2016], a new result shows that introducing non-lipschitzian singularities inside a objective function may not affect a worst-case evaluation complexity order. a result also shows that with the help of a problem's partially separable structure (if present) does not affect complexity order either. the final (worse) complexity bound was derived considering a case where taylor models are used with the general convex feasible set.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
19372,opportunistic spectrum access was one of a emerging techniques considering maximizing throughput inside congested bands and was enabled by predicting idle slots inside spectrum. we propose the kernel-based reinforcement learning idea behind the method coupled with the novel budget-constrained sparsification technique that efficiently captures a environment to find a best channel access actions. this idea behind the method allows learning and planning over a intrinsic state-action space and extends well to large state spaces. we apply our methods to evaluate coexistence of the reinforcement learning-based radio with the multi-channel adversarial radio and the single-channel csma-ca radio. numerical experiments show a performance gains over carrier-sense systems.,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14611,"autonomous navigation through unknown environments was the challenging task that entails real-time localization, perception, planning, and control. uavs with this capability have begun to emerge inside a literature with advances inside lightweight sensing and computing. although a planning methodologies vary from platform to platform, many algorithms adopt the hierarchical planning architecture where the slow, low-fidelity global planner guides the fast, high-fidelity local planner. however, inside unknown environments, this idea behind the method should lead to erratic or unstable behavior due to a interaction between a global planner, whose solution was changing constantly, and a local planner; the consequence of not capturing higher-order dynamics inside a global plan. this work proposes the planning framework inside which multi-fidelity models are used to reduce a discrepancy between a local and global planner. our idea behind the method uses high-, medium-, and low-fidelity models to compose the path that captures higher-order dynamics while remaining computationally tractable. inside addition, we address a interaction between the fast planner and the slower mapper by considering a sensor data not yet fused into a map during a collision check. this novel mapping and planning framework considering agile flights was validated inside simulation, showing replanning times of 5-40 ms inside cluttered environments, the value that was 3-30 times faster than similar state-of-the-art planning algorithms.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
16730,"when it comes to cluster massive data, response time, disk access and quality of formed classes becoming major issues considering companies. it was inside this context that we have come to define the clustering framework considering large scale heterogeneous data that contributes to a resolution of these issues. a proposed framework was based on, firstly, a descriptive analysis based on mca, and secondly, a mapreduce paradigm inside the large scale environment. a results are encouraging and prove a efficiency of a hybrid deployment on response quality and time component as on qualitative and quantitative data.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8848,"recent breakthroughs inside neural architectural search (nas) have achieved state-of-the-art performance inside many tasks such as image classification and language understanding. however, most existing works only optimize considering model accuracy and largely ignore other important factors imposed by a underlying hardware and devices, such as latency and energy, when making inference. inside this paper, we first introduce a problem of nas and provide the survey on recent works. then we deep dive into two recent advancements on extending nas into multiple-objective frameworks: monas and dpp-net. both monas and dpp-net are capable of optimizing accuracy and other objectives imposed by devices, searching considering neural architectures that should be best deployed on the wide spectrum of devices: from embedded systems and mobile devices to workstations. experimental results are poised to show that architectures found by monas and dpp-net achieves pareto optimality w.r.t a given objectives considering various devices.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5007,"today's economy, production activity, and our life are sustained by social and technological network infrastructures, while new threats of network attacks by destructing loops have been found recently inside network science. we inversely take into account a weakness, and propose the new design principle considering incrementally growing robust networks. a networks are self-organized by enhancing interwoven long loops. inside particular, we consider a range-limited approximation of linking by intermediations inside the few hops, and show a strong robustness inside a growth without degrading efficiency of paths. moreover, we demonstrate that a tolerance of connectivity was reformable even from extremely vulnerable real networks according to our proposed growing process with some investment. these results may indicate the prospective direction to a future growth of our network infrastructures.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
14041,"nonlinear dynamics on graphs has rapidly become the topical issue with many physical applications, ranging from nonlinear optics to bose-einstein condensation. whenever inside the physical experiment the ramified structure was involved, it should prove useful to approximate such the structure by the metric graph, or network. considering a schroedinger equation it turns out that a sixth power inside a nonlinear term of a energy was critical inside a sense that below that power a constrained energy was lower bounded irrespectively of a value of a mass (subcritical case). on a other hand, if a nonlinearity power equals six, then a lower boundedness depends on a value of a mass: below the critical mass, a constrained energy was lower bounded, beyond it, it was not. considering powers larger than six a constrained energy functional was never lower bounded, so that it was meaningless to speak about ground states (supercritical case). these results are a same as inside a case of a nonlinear schrodinger equation on a real line. inside fact, as regards a existence of ground states, a results considering systems on graphs differ, inside general, from a ones considering systems on a line even inside a subcritical case: inside a latter case, whenever a constrained energy was lower bounded there always exist ground states (the solitons, whose shape was explicitly known), whereas considering graphs a existence of the ground state was not guaranteed. considering a critical case, our results show the phenomenology much richer than a analogous on a line.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2924,"we generalise notions of gorenstein homological algebra considering rings to a context of arbitrary abelian categories. a results are strongest considering module categories of rngs with enough idempotents. we also reformulate a notion of frobenius extensions of noetherian rings into the setting which allows considering direct generalisation to arbitrary abelian categories. a abstract theory was then applied to a bgg category o considering lie superalgebras, which should now be seen as the ""frobenius extension"" of a corresponding category considering a underlying lie algebra and was therefore ""gorenstein"". inside particular we obtain new and more general formulae considering a serre functors and instigate a theory of gorenstein extension groups.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
9224,"save considering some special cases, current training methods considering generative adversarial networks (gans) are at best guaranteed to converge to the `local nash equilibrium` (lne). such lnes, however, should be arbitrarily far from an actual nash equilibrium (ne), which implies that there are no guarantees on a quality of a found generator or classifier. this paper proposes to model gans explicitly as finite games inside mixed strategies, thereby ensuring that every lne was an ne. with this formulation, we propose the solution method that was proven to monotonically converge to the resource-bounded nash equilibrium (rb-ne): by increasing computational resources we should find better solutions. we empirically demonstrate that our method was less prone to typical gan problems such as mode collapse, and produces solutions that are less exploitable than those produced by gans and mgans, and closely resemble theoretical predictions about nes.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
13396,"we present the detailed study of a low temperature and high magnetic field phases inside a chemical substitution series uru$_2$si$_{2-x}$p$_x$ with the help of electrical transport and magnetization inside pulsed magnetic fields up to 65t. within a hidden order region (0 $\ x$$\ $ 0.035) a high field ordering was robust even as a hidden order temperature was suppressed. earlier work shows that considering 0.035 $\ x$ $\ $ 0.26 there was the kondo lattice with the no-ordered state that was replaced by antiferromagnetism considering 0.26 $\ x$ 0.5. we observe the simplified continuation of a high field ordering inside a no-order $x$-region and an enhancement of a high field state upon a destruction of a antiferromagnetism with magnetic field. these results closely resemble what was seen considering uru$_{2-x}$rh$_x$si$_2$\footnote{the concentration inside this paper was defined as uru$_{2-x}$rh$_x$si$_2$ while a chemical formula inside a literature was given as u(ru$_{1-x}$rh$_x$)$_2$si$_2$ [24-26]}, from which we infer that charge tuning uniformly controls a ground state of uru$_2$si$_2$, regardless of whether s/p or d-electrons are replaced. this provides guidance considering determining a specific factors that lead to hidden order versus magnetism inside this family of materials.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
18002,"a exquisite sensitivity of a advanced ligo detectors has enabled a detection of multiple gravitational wave signals. a sophisticated design of these detectors mitigates a effect of most types of noise. however, advanced ligo data streams are contaminated by numerous artifacts known as glitches: non-gaussian noise transients with complex morphologies. given their high rate of occurrence, glitches should lead to false coincident detections, obscure and even mimic gravitational wave signals. therefore, successfully characterizing and removing glitches from advanced ligo data was of utmost importance. here, we present a first application of deep transfer learning considering glitch classification, showing that knowledge from deep learning algorithms trained considering real-world object recognition should be transferred considering classifying glitches inside time-series based on their spectrogram images. with the help of a gravity spy dataset, containing hand-labeled, multi-duration spectrograms obtained from real ligo data, we demonstrate that this method enables optimal use of very deep convolutional neural networks considering classification given small training datasets, significantly reduces a time considering training a networks, and achieves state-of-the-art accuracy above 98.8%, with perfect precision-recall on 8 out of 22 classes. furthermore, new types of glitches should be classified accurately given few labeled examples with this technique. once trained using transfer learning, we show that a convolutional neural networks should be truncated and used as excellent feature extractors considering unsupervised clustering methods to identify new classes based on their morphology, without any labeled examples. therefore, this provides the new framework considering dynamic glitch classification considering gravitational wave detectors, which are expected to encounter new types of noise as they undergo gradual improvements to attain design sensitivity.",1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0
14686,"inside this work, we expand a weighted delta-tracking routine to include the treatment considering scattering. a weighted delta-tracking routine adds survival biasing to normal delta-tracking, improving problem figure of merit. inside a original formulation of this method, only absorption events were considered. we have expanded a method to include scattering and investigated a method's effectiveness with two test cases: the pressurized water reactor pin cell and the fast reactor pin cell. we compare a figure of merit considering calculating infinite flux and total cross-section while incrementally changing a amount of weighted delta-tracking used. we find that this new wdt routine has strong potential to improve a efficiency of fast reactor calculations, and may be useful considering light water reactor calculations.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16990,"to a naked eye, turbulent flows exhibit whirls of many different sizes. to each size, or scale, corresponds the fraction of a total energy resulting from the cascade inside five dimensions: scale, time and three-dimensional space. understanding this process was critical to modeling strategies of geophysical and industrial flows. by tracking a flow regions containing energy inside different scales, we have evidenced a statistical predominance of the cross-scale link whereby fluid lumps of energy at scale $\delta$ appear within lumps of scale $2\delta$ and die within those of scale $\delta/2$. our idea behind the method uncovers a energy cascade inside the simple water-like fluid, offering new insights to turbulence models while paving a way to similar analysis inside conducting fluids, quantum fluids and plasmas.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6091,"we develop polynomial-time heuristic methods to solve unimodular quadratic programs (uqps) approximately, which are known to be np-hard. inside a uqp framework, we maximize the quadratic function of the vector of complex variables with unit modulus. several problems inside active sensing and wireless communication applications boil down to uqp. with this motivation, we present three new heuristic methods with polynomial-time complexity to solve a uqp approximately. a first method was called dominant-eigenvector-matching; here a solution was picked that matches a complex arguments of a dominant eigenvector of a hermitian matrix inside a uqp formulation. we also provide the performance guarantee considering this method. a second method, the greedy strategy, was shown to provide the performance guarantee of (1-1/e) with respect to a optimal objective value given that a objective function possesses the property called string submodularity. a third heuristic method was called row-swap greedy strategy, which was an extension to a greedy strategy and utilizes certain properties of a uqp to provide the better performance than a greedy strategy at a expense of an increase inside computational complexity. we present numerical results to demonstrate a performance of these heuristic methods, and also compare a performance of these methods against the standard heuristic method called semidefinite relaxation.",1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
7340,"we present the spectroscopic study of a dynamics of a ionized and neutral gas throughout a lagoon nebula (m8), with the help of vlt/flames data from a gaia-eso survey. we explore a connections between a nebular gas and a stellar population of a associated star cluster ngc6530. we characterize through spectral fitting emission lines of h-alpha, [n ii] and [s ii] doublets, [o iii], and absorption lines of sodium d doublet, with the help of data from a flames/giraffe and uves spectrographs, on more than 1000 sightlines towards a entire face of a lagoon nebula. gas temperatures are derived from line-width comparisons, densities from a [s ii] doublet ratio, and ionization parameter from h-alpha/[n ii] ratio. although doubly-peaked emission profiles are rarely found, line asymmetries often imply multiple velocity components along a line of sight. this was especially true considering a sodium absorption, and considering a [o iii] lines. spatial maps considering density and ionization are derived, and compared to other known properties of a nebula and of its massive stars 9 sgr, herschel 36 and hd 165052 which are confirmed to provide most of a ionizing flux. a detailed velocity fields across a nebula show several expanding shells, related to a cluster ngc6530, a o stars 9 sgr and herschel 36, and a massive protostar m8east-ir. a origins of kinematical expansion and ionization of a ngc6530 shell appear to be different. we are able to put constrains on a line-of-sight (relative or absolute) distances between some of these objects and a molecular cloud. a large obscuring band running through a middle of a nebula was being compressed by both sides, which might explain its enhanced density. we also find an unexplained large-scale velocity gradient across a entire nebula. at larger distances, a transition from ionized to neutral gas was studied with the help of a sodium lines.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13311,"spin-polarized field-effect transistor (spin-fet), where the dielectric layer was generally employed considering a electrical gating as a traditional fet, stands out as the seminal spintronic device under a miniaturization trend of electronics. it would be fundamentally transformative if optical gating is used considering spin-fet. we report the new type of spin-polarized field-effect transistor (spin-fet) with optical gating, which was fabricated by partial exposure of a (la,sr)mno3 channel to light-emitting diode (led) light. a manipulation of a channel conductivity was ascribed to a enhanced scattering of a spin-polarized current by photon-excited antiparallel aligned spins. and a photon-gated spin-fet shows strong light power dependence and reproducible enhancement of resistance under light illumination, indicting well-defined conductivity cycling features. our finding would enrich a concept of spin-fet and promote a use of optical means inside spintronics considering low power consumption and ultrafast data processing.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
1755,"it was pointed out that a generalized lambert series $\displaystyle\sum_{n=1}^{\infty}\frac{n^{n-2h}}{e^{n^{n}x}-1}$ studied by kanemitsu, tanigawa and yoshimoto should be found on page $332$ of ramanujan's lost notebook inside the slightly more general form. we extend an important transformation of this series obtained by kanemitsu, tanigawa and yoshimoto by removing restrictions on a parameters $n$ and $h$ that they impose. from our extension we deduce the beautiful new generalization of ramanujan's famous formula considering odd zeta values which, considering $n$ odd and $m>0$, gives the relation between $\zeta(2m+1)$ and $\zeta(2nm+1)$. the result complementary to a aforementioned generalization was obtained considering any even $n$ and $m\in\mathbb{z}$. it generalizes the transformation of wigert and should be regarded as the formula considering $\zeta\left(2m+1-\frac{1}{n}\right)$. applications of these transformations include the generalization of a transformation considering a logarithm of dedekind eta-function $\eta(z)$, zudilin- and rivoal-type results on transcendence of certain values, and the transcendence criterion considering euler's constant $\gamma$.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
9864,"inside this paper we compare a candidates to be spectral minimal partitions considering two criteria: a maximum and a average of a first eigenvalue on each subdomains of a partition. we analyze inside detail a square, a disk and a equilateral triangle. with the help of numerical simulations, we propose candidates considering a max, prove that most of them should not be optimal considering a sum and then exhibit better candidates considering a sum.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
16850,"we consider a motion of the classical colored spinless particle under a influence of an external yang-mills potential $a$ on the compact manifold with boundary of dimension $\geq 3$. we show that under suitable convexity assumptions, we should recover a potential $a$, up to gauge transformations, from a lens data of a system, namely, scattering data plus travel times between boundary points.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6392,"we investigate a influence of a barrier thickness of co$_{40}$fe$_{40}$b$_{20}$ based magnetic tunnel junctions on a laser-induced tunnel magneto-seebeck effect. varying a barrier thickness from 1nm to 3nm, we find the distinct maximum inside a tunnel magneto-seebeck effect considering 2.6nm barrier thickness. this maximum was independently measured considering two barrier materials, namely mgal$_2$o$_4$ and mgo. additionally, samples with an mgal$_2$o$_4$ barrier exhibit the high thermovoltage of more than 350$\mu$v inside comparison to 90$\mu$v considering a mtjs with mgo barrier when heated with a maximum laser power of 150mw. our results allow considering a fabrication of improved stacks when dealing with temperature differences across magnetic tunnel junctions considering future applications inside spin caloritronics, a emerging research field that combines spintronics and themoelectrics.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
16727,"inverse reinforcement learning (irl) was a task of learning the single reward function given the markov decision process (mdp) without defining a reward function, and the set of demonstrations generated by humans/experts. however, inside practice, it may be unreasonable to assume that human behaviors should be explained by one reward function since they may be inherently inconsistent. also, demonstrations may be collected from various users and aggregated to infer and predict user's behaviors. inside this paper, we introduce a non-parametric behavior clustering irl algorithm to simultaneously cluster demonstrations and learn multiple reward functions from demonstrations that may be generated from more than one behaviors. our method was iterative: it alternates between clustering demonstrations into different behavior clusters and inverse learning a reward functions until convergence. it was built upon a expectation-maximization formulation and non-parametric clustering inside a irl setting. further, to improve a computation efficiency, we remove a need of completely solving multiple irl problems considering multiple clusters during a iteration steps and introduce the resampling technique to avoid generating too many unlikely clusters. we demonstrate a convergence and efficiency of a proposed method through learning multiple driver behaviors from demonstrations generated from the grid-world environment and continuous trajectories collected from autonomous robot cars with the help of a gazebo robot simulator.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6120,"different measures have been proposed to predict whether individuals will adopt the new behavior inside online social networks, given a influence produced by their neighbors. inside this paper, we show one should achieve significant improvement over these standard measures, extending them to consider the pair of time constraints. these constraints provide the better proxy considering social influence, showing the stronger correlation to a probability of influence as well as a ability to predict influence.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
16441,"machine learning techniques are being increasingly used as flexible non-linear fitting and prediction tools inside a physical sciences. fitting functions that exhibit multiple solutions as local minima should be analysed inside terms of a corresponding machine learning landscape. methods to explore and visualise molecular potential energy landscapes should be applied to these machine learning landscapes to gain new insight into a solution space involved inside training and a nature of a corresponding predictions. inside particular, we should define quantities analogous to molecular structure, thermodynamics, and kinetics, and relate these emergent properties to a structure of a underlying landscape. this perspective aims to describe these analogies with examples from recent applications, and suggest avenues considering new interdisciplinary research.",1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14549,"we consider a problem of testing, on a basis of the $p$-variate gaussian random sample, a null hypothesis ${\cal h}_0: {\pmb \theta}_1= {\pmb \theta}_1^0$ against a alternative ${\cal h}_1: {\pmb \theta}_1 \neq {\pmb \theta}_1^0$, where ${\pmb \theta}_1$ was a ""first"" eigenvector of a underlying covariance matrix and ${\pmb \theta}_1^0$ was the fixed unit $p$-vector. inside a classical setup where eigenvalues $\lambda_1>\lambda_2\geq \ldots\geq \lambda_p$ are fixed, a anderson (1963) likelihood ratio test (lrt) and a hallin, paindaveine and verdebout (2010) le cam optimal test considering this problem are asymptotically equivalent under a null hypothesis, thus also under sequences of contiguous alternatives. we show that this equivalence does not survive asymptotic scenarios where $\lambda_{n1}/\lambda_{n2}=1+o(r_n)$ with $r_n=o(1/\sqrt{n})$. considering such scenarios, a le cam optimal test still asymptotically meets a nominal level constraint, whereas a lrt severely overrejects a null hypothesis. consequently, a former test should be favored over a latter one whenever a two largest sample eigenvalues are close to each other. by relying on a le cam's asymptotic theory of statistical experiments, we study a non-null and optimality properties of a le cam optimal test inside a aforementioned asymptotic scenarios and show that a null robustness of this test was not obtained at a expense of power. our asymptotic investigation was extensive inside a sense that it allows $r_n$ to converge to zero at an arbitrary rate. while we restrict to single-spiked spectra of a form $\lambda_{n1}>\lambda_{n2}=\ldots=\lambda_{np}$ to make our results as striking as possible, we extend our results to a more general elliptical case. finally, we present an illustrative real data example.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
2834,"autonomous systems will play an essential role inside many applications across diverse domains including space, marine, air, field, road, and service robotics. they will assist us inside our daily routines and perform dangerous, dirty and dull tasks. however, enabling robotic systems to perform autonomously inside complex, real-world scenarios over extended time periods (i.e. weeks, months, or years) poses many challenges. some of these have been investigated by sub-disciplines of artificial intelligence (ai) including navigation & mapping, perception, knowledge representation & reasoning, planning, interaction, and learning. a different sub-disciplines have developed techniques that, when re-integrated within an autonomous system, should enable robots to operate effectively inside complex, long-term scenarios. inside this paper, we survey and discuss ai techniques as 'enablers' considering long-term robot autonomy, current progress inside integrating these techniques within long-running robotic systems, and a future challenges and opportunities considering ai inside long-term autonomy.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
18668,"a russia-based internet research agency (ira) carried out the broad information campaign inside a u.s. before and after a 2016 presidential election. a organization created an expansive set of internet properties: web domains, facebook pages, and twitter bots, which received traffic using purchased facebook ads, tweets, and search engines indexing their domains. we investigate a scope of ira activities inside 2017, joining data from facebook and twitter with logs from a internet explorer 11 and edge browsers and a bing.com search engine. a studies demonstrate both a ease with which malicious actors should harness social media and search engines considering propaganda campaigns, and a ability to track and understand such activities by fusing content and activity resources from multiple internet services. we show how cross-platform analyses should provide an unprecedented lens on attempts to manipulate opinions and elections inside democracies.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
10984,"it was the well-known fact that adding noise to a input data often improves network performance. while a dropout technique may be the cause of memory loss, when it was applied to recurrent connections, tikhonov regularization, which should be regarded as a training with additive noise, avoids this issue naturally, though it implies regularizer derivation considering different architectures. inside case of feedforward neural networks this was straightforward, while considering networks with recurrent connections and complicated layers it leads to some difficulties. inside this paper, the tikhonov regularizer was derived considering long-short term memory (lstm) networks. although it was independent of time considering simplicity, it considers interaction between weights of a lstm unit, which inside theory makes it possible to regularize a unit with complicated dependences by with the help of only one parameter that measures a input data perturbation. a regularizer that was proposed inside this paper has three parameters: one to control a regularization process, and other two to maintain computation stability while a network was being trained. a theory developed inside this paper should be applied to get such regularizers considering different recurrent neural networks with hadamard products and lipschitz continuous functions.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
9637,"we investigate a superconducting-gap anisotropy inside one of a recently discovered bis$_2$-based superconductors, ndo$_{0.71}$f$_{0.29}$bis$_2$ ($t_c$ $\sim$ 5 k), with the help of laser-based angle-resolved photoemission spectroscopy. whereas a previously discovered high-$t_c$ superconductors such as copper oxides and iron-based superconductors, which are believed to have unconventional superconducting mechanisms, have $3d$ electrons inside their conduction bands, a conduction band of bis$_2$-based superconductors mainly consists of bi 6$p$ electrons, and thus a conventional superconducting mechanism might be expected. contrary to this expectation, we observe the strongly anisotropic superconducting gap. this result strongly suggests that a pairing mechanism considering ndo$_{0.71}$f$_{0.29}$bis$_2$ was unconventional one and we attribute a observed anisotropy to competitive or cooperative multiple paring interactions.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0
17211,"over the field of prime characteristic $p>2$, we prove that a cohomology rings of some pointed hopf algebras of dimension $p^3$ are finitely generated. these are hopf algebras arising inside a ongoing classification of finite dimensional hopf algebras inside positive characteristic, and include bosonizations of nichols algebras of jordan type inside the general setting as well as their liftings when $p=3$. our techniques are applications of twisted tensor product resolutions and anick resolutions inside combination with may spectral sequences.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
2419,"we have performed high-resolution powder x-ray diffraction measurements on the sample of $^{242}$pucoga$_{5}$, a heavy-fermion superconductor with a highest critical temperature $t_{c}$ = 18.7 k. a results show that a tetragonal symmetry of its crystallographic lattice was preserved down to 2 k. marginal evidence was obtained considering an anomalous behaviour below $t_{c}$ of a $a$ and $c$ lattice parameters. a observed thermal expansion was isotropic down to 150 k, and becomes anisotropic considering lower temperatures. this gives the $c/a$ ratio that decreases with increasing temperature to become almost constant above $\sim$150 k. a volume thermal expansion coefficient $\alpha_{v}$ has the jump at $t_{c}$, the factor $\sim$20 larger than a change predicted by a ehrenfest relation considering the second order phase transition. a volume expansion deviates from a curve expected considering a conventional anharmonic behaviour described by the simple gr√ºneisen-einstein model. a observed differences are about ten times larger than a statistical error bars but are too small to be taken as an indication considering a proximity of a system to the valence instability that was avoided by a superconducting state.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
728,"large-batch training approaches have enabled researchers to utilize large-scale distributed processing and greatly accelerate deep-neural net (dnn) training. considering example, by scaling a batch size from 256 to 32k, researchers have been able to reduce a training time of resnet50 on imagenet from 29 hours to 2.2 minutes (ying et al., 2018). inside this paper, we propose the new idea behind the method called linear-epoch gradual-warmup (legw) considering better large-batch training. with legw, we are able to conduct large-batch training considering both cnns and rnns with a sqrt scaling scheme. legw enables sqrt scaling scheme to be useful inside practice and as the result we achieve much better results than a linear scaling learning rate scheme. considering lstm applications, we are able to scale a batch size by the factor of 64 without losing accuracy and without tuning a hyper-parameters. considering cnn applications, legw was able to achieve a same accuracy even as we scale a batch size to 32k. legw works better than previous large-batch auto-tuning techniques. legw achieves the 5.3x average speedup over a baselines considering four lstm-based applications on a same hardware. we also provide some theoretical explanations considering legw.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11908,"inside this paper we study canonical scalar field models with the varying second slow-roll parameter, that allow transitions between constant-roll eras. inside a models with two constant-roll eras it was possible to avoid fine-tunings inside a initial conditions of a scalar field. we mainly focus on a stability of a resulting solutions and we also investigate if these solutions are attractors of a cosmological system. we shall calculate a resulting scalar potential and by with the help of the numerical approach, we examine a stability and attractor properties of a solutions. as we show, a first constant-roll era was dynamically unstable towards linear perturbations and a cosmological system was driven by a attractor solution to a final constant-roll era. as we demonstrate, it was possible to have the nearly-scale invariant power spectrum of primordial curvature perturbations inside some cases, however this was strongly model dependent and depends on a rate of a final constant-roll era. finally, we present inside brief a essential features of the model that allows oscillations between constant-roll eras.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7194,"we study a classic set cover problem inside a streaming model: a sets that comprise a instance are revealed one by one inside the stream and a goal was to solve a problem by making one or few passes over a stream while maintaining the sublinear space $o(mn)$ inside a input size; here $m$ denotes a number of a sets and $n$ was a universe size. notice that inside this model, we are mainly concerned with a space requirement of a algorithms and thus do not restrict their computation time. our main result was the resolution of a space-approximation tradeoff considering a streaming set cover problem: we show that any $\alpha$-approximation algorithm considering a set cover problem requires $\widetilde{\omega}(mn^{1/\alpha})$ space, even if it was allowed polylog${(n)}$ passes over a stream, and even if a sets are arriving inside the random order inside a stream. this space-approximation tradeoff matches a best known bounds achieved by a recent algorithm of har-peled et.al. (pods 2016) that requires only $o(\alpha)$ passes over a stream inside an adversarial order, thus settling a space complexity of approximating a set cover problem inside data streams inside the quite robust manner. additionally, our idea behind the method yields tight lower bounds considering a space complexity of $(1- \epsilon)$-approximating a streaming maximum coverage problem studied inside several recent works.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6156,"one of a most well-studied young stellar associations, taurus-auriga, will be observed by a extended kepler mission, k2, inside a spring of 2017. k2 campaign 13 (c13) will be the unique opportunity to study many stars inside this young association at high photometric precision and cadence. with the help of observations from a kilodegree extremely little telescope (kelt) survey, we identify ""dippers"", stochastic variables, and periodic variables among k2 c13 target stars. this release of a kelt data (lightcurve data inside e-tables) provides a community with long-time baseline observations to assist inside a understanding of a more exotic variables inside a association. transient-like phenomena on timescales of months to years are known characteristics inside a light curves of young stellar objects, making contextual pre- and post-k2 observations critical to understanding their underlying processes. we are providing the comprehensive set of a kelt light curves considering known taurus-auriga stars inside k2 c13. a combined data sets from k2 and kelt should permit the broad array of investigations related to star formation, stellar variability, and protoplanetary environments.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14545,"information should propagate among online social network (osn) users at the high speed, which makes a osns become important platforms considering viral marketing. although a viral marketing related problems inside osns have been extensively studied inside a past decade, a existing works all assume known propagation rates and are not able to solve a scenario when a rates may dynamically increase considering popular topics. inside this paper, we propose the novel model, dynamic influence propagation (dip), which allows propagation rates to change during a diffusion and should be used considering describing information propagation inside osns more realistically. based on dip, we define the new research problem: threshold activation problem under dip (tap-dip). tap-dip was more generalized than tap and should be used considering studying a dip model. however, it adds another layer of complexity over a already \#p-hard tap problem. despite it hardness, we are able to approximate tap-dip with $o(\log|v|)$ ratio. our solution consists of two major parts: 1) a lipschitz optimization technique and 2) the novel solution to a general version of tap, a multi-tap problem. we experimentally test our solution with the help of various real osn datasets, and demonstrate that our solution not only generates high-quality yet much smaller seed sets when being aware of a rate increase, but also was scalable. inside addition, considering dip or not has the significant difference inside seed set selection.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
8856,"this paper presents an automated method considering 3d character skeleton extraction that should be applied considering generic 3d shapes. our work was motivated by a skeleton-based prior work on automatic rigging focused on skeleton extraction and should automatically aligns a extracted structure to fit a 3d shape of a given 3d mesh. a body mesh should be subsequently skinned based on a extracted skeleton and thus enables rigging process. inside a experiment, we apply public dataset to drive a estimated skeleton from different body shapes, as well as a real data obtained from 3d scanning systems. satisfactory results are obtained compared to a existing approaches.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18045,"the special class of standard gaussian autoregressive hilbertian processes of order one (gaussian arh(1) processes), with bounded linear autocorrelation operator, which does not satisfy a usual hilbert-schmidt assumption, was considered. to compensate a slow decay of a diagonal coefficients of a autocorrelation operator, the faster decay velocity of a eigenvalues of a trace autocovariance operator of a innovation process was assumed. as usual, a eigenvectors of a autocovariance operator of a arh(1) process are considered considering projection, since, here, they are assumed to be known. diagonal componentwise classical and bayesian approximation of a autocorrelation operator was studied considering prediction. a asymptotic efficiency and equivalence of both estimators was proved, as well as of their associated componentwise arh(1) plugin predictors. the simulation study was undertaken to illustrate a theoretical results derived.",0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
13149,"context: high-resolution spectroscopy across spatially resolved stellar surfaces aims at obtaining spectral-line profiles that are free from rotational broadening; a gradual changes of these profiles from disk center toward a stellar limb reveal properties of atmospheric fine structure, which are possible to model with 3-d hydrodynamics. aims: previous such studies have only been carried out considering a sun but are now extended to other stars. inside this work, profiles of photospheric spectral lines are retrieved across a disk of a planet-hosting star hd209458 (g0v). methods: during exoplanet transit, stellar surface portions successively become hidden and differential spectroscopy provides spectra of small surface segments temporarily hidden behind a planet. a method is elaborated inside paper i, with observable signatures quantitatively predicted from hydrodynamic simulations. results: from observations of hd209458 with spectral resolution r=80,000, photospheric fei line profiles are obtained at several center-to-limb positions, reaching adequately high s/n after averaging over numerous similar lines conclusions: retrieved line profiles are compared to synthetic line profiles. hydrodynamic 3-d models predict, and current observations confirm, that photospheric absorption lines become broader and shallower toward a stellar limb, reflecting that horizontal velocities inside stellar granulation are greater than vertical velocities. additional types of 3-d signatures will become observable with a highest resolution spectrometers at large telescopes.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
1850,"we propose the quantum field theory description of a x-cube model of fracton topological order. a field theory was not (and cannot be) the topological quantum field theory (tqft), since unlike a x-cube model, tqfts are invariant (i.e. symmetric) under continuous spacetime transformations. however, a theory was instead invariant under the certain subgroup of a conformal group. we describe how braiding statistics and ground state degeneracy are reproduced by a field theory, and how a a x-cube hamiltonian and field theory should be minimally coupled to matter fields. we also show that even on the manifold with trivial topology, spatial curvature should induce the ground state degeneracy that was stable to arbitrary local perturbations! our formalism may allow considering a description of other fracton field theories, where a only necessary input was an equation of motion considering the charge density.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
3700,"a crystal and magnetic structures of stoichiometric zncr2se4 have been investigated with the help of synchrotron x-ray and neutron powder diffraction, muon spin relaxation (musr) and inelastic neutron scattering. synchrotron x-ray diffraction shows the spin-lattice distortion from a cubic spinel to the tetragonal i41/amd lattice below tn = 21 k, where powder neutron diffraction confirms a formation of the helical magnetic structure with magnetic moment of 3.04(3) {\mu}b at 1.5 k; close to that expected considering high-spin cr3+. musr measurements show prominent local spin correlations that are established at temperatures considerably higher (< 100 k) than a onset of long range magnetic order. a stretched exponential nature of a relaxation inside a local spin correlation regime suggests the wide distribution of depolarizing fields. below tn, unusually fast (> 100 {\mu}s-1) muon relaxation rates are suggestive of rapid site hopping of a muons inside static field. inelastic neutron scattering measurements show the gapless mode at an incommensurate propagation vector of k = (0 0 0.4648(2)) inside a low temperature magnetic ordered phase that extends to 0.8 mev. a dispersion was modelled by the two parameter hamiltonian, containing ferromagnetic nearest neighbor and antiferromagnetic next nearest neighbor interactions with the jnnn/jnn = -0.337.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0
2391,"onset of vacuum arcing near the metal surface was often associated with nanoscale asperities, which may dynamically appear due to different processes ongoing inside a surface and subsurface layers inside a presence of high electric fields. thermally activated processes, as well as plastic deformation caused by tensile stress due to an applied electric field, are usually not accessible by atomistic simulations because of long time needed considering these processes to occur. on a other hand, finite element methods, able to describe a process of plastic deformations inside materials at realistic stresses, do not include surface properties. a latter are particularly important considering a problems where a surface plays crucial role inside a studied process, as considering instance, inside case of plastic deformations at the nanovoid. inside a current study by means of molecular dynamics and finite element simulations we analyse a stress distribution inside single crystal copper containing the nanovoid buried deep under a surface. we have developed the methodology to incorporate a surface effects into a solid mechanics framework by utilizing elastic properties of crystals, pre-calculated with the help of molecular dynamic simulations. a method leads to computationally efficient stress calculations and should be easily implemented inside commercially available finite element software, making it an attractive analysis tool.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
2041,this paper gives a exact solution inside terms of a karhunen-lo√®ve expansion to the fractional stochastic partial differential equation on a unit sphere $\mathbb{s}^{2}\subset \mathbb{r}^{3}$ with fractional brownian motion as driving noise and with random initial condition given by the fractional stochastic cauchy problem. the numerical approximation to a solution was given by truncating a karhunen-lo√®ve expansion. we show a convergence rates of a truncation errors inside degree and a mean square approximation errors inside time. numerical examples with the help of an isotropic gaussian random field as initial condition and simulations of evolution of cosmic microwave background (cmb) are given to illustrate a theoretical results.,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
11321,"the number of improvements have been added to a existing analytical model of hysteresis loop defined inside parametric form. inside particular, three phase shifts are included inside a model, which permits to tilt a hysteresis loop smoothly by a required angle at a split point as well as to smoothly change a curvature of a loop. as the result, a error of approximation of the hysteresis loop by a improved model does not exceed 1%, which was several times less than a error of a existing model. a improved model was capable of approximating most of a known types of rate-independent symmetrical hysteresis loops encountered inside a practice of physical measurements. a model allows building smooth, piecewise-linear, hybrid, minor, mirror-reflected, inverse, reverse, double and triple loops. one of a possible applications of a model developed was linearization of the probe microscope piezoscanner. a improved model should be found useful considering a tasks of simulation of scientific instruments that contain hysteresis elements.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
14953,"here, we explore some peculiar orbital features of a recently discovered asteroid a/2017 u1, which was the clear outlier when considering a average value of a eccentricity of known hyperbolic comets. as considering a orientation of its orbit inside space, a orbital plane of a/2017 u1 seems to be away from any obvious clusters present considering this population. a orbital nodes of a/2017 u1 are well away from a paths of a planets of a solar system and a sun. all these orbital properties appear to confirm a/2017 u1 as a first known interstellar asteroid.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16011,"we consider the problem of information structure design inside team decision problems and team games. we propose simple, scalable greedy algorithms considering adding the set of extra information links to optimize team performance and resilience to non-cooperative and adversarial agents. we show using the simple counterexample that a set function mapping additional information links to team performance was inside general not supermodular. although this implies that a greedy algorithm was not accompanied by worst-case performance guarantees, we illustrate through numerical experiments that it should produce effective and often optimal or near optimal information structure modifications.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
8263,"within a shoving model of a glass transition, a relaxation time and a viscosity are related to a local cage rigidity. this idea behind the method should be extended down to a atomic-level inside terms of a interatomic interaction, or potential of mean-force. we applied this idea behind the method to both real metallic glass-formers and model lennard-jones glasses. a main outcome of this analysis was that inside metallic glasses a thermal expansion contribution was mostly independent of composition and was uncorrelated with a interatomic repulsion: as the consequence, a fragility increases upon increasing a interatomic repulsion steepness. inside a lennard-jones glasses, a scenario was opposite: thermal expansion and interatomic repulsion contributions are strongly correlated, and a fragility decreases upon increasing a repulsion steepness. this framework allows one to tell apart systems where ""soft atoms make strong glasses"" from those where, instead, ""soft atoms make fragile glasses"". hence, it opens up a way considering a rational, atomistic tuning of a fragility and viscosity of widely different glass-forming materials all a way from strong to fragile.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
14046,"it was of high interest considering the company to identify customers expected to bring a largest profit inside a upcoming period. knowing as much as possible about each customer was crucial considering such predictions. however, their demographic data, preferences, and other information that might be useful considering building loyalty programs was often missing. additionally, modeling relations among different customers as the network should be beneficial considering predictions at an individual level, as similar customers tend to have similar purchasing patterns. we address this problem by proposing the robust framework considering structured regression on deficient data inside evolving networks with the supervised representation learning based on neural features embedding. a new method was compared to several unstructured and structured alternatives considering predicting customer behavior (e.g. purchasing frequency and customer ticket) on user networks generated from customer databases of two companies from different industries. a obtained results show $4\%$ to $130\%$ improvement inside accuracy over alternatives when all customer information was known. additionally, a robustness of our method was demonstrated when up to $80\%$ of demographic information is missing where it is up to several folds more accurate as compared to alternatives that are either ignoring cases with missing values or learn their feature representation inside an unsupervised manner.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11832,"this work provides the framework considering the workspace aware online grasp planner. this framework greatly improves a performance of standard online grasp planning algorithms by incorporating the notion of reachability into a online grasp planning process. offline, the database of hundreds of thousands of unique end-effector poses were queried considering feasability. at runtime, our grasp planner uses this database to bias a hand towards reachable end-effector configurations. a bias keeps a grasp planner inside accessible regions of a planning scene so that a resulting grasps are tailored to a situation at hand. this results inside the higher percentage of reachable grasps, the higher percentage of successful grasp executions, and the reduced planning time. we also present experimental results with the help of simulated and real environments.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
1031,"inside this work we compare different batch construction methods considering mini-batch training of recurrent neural networks. while popular implementations like tensorflow and mxnet suggest the bucketing idea behind the method to improve a parallelization capabilities of a recurrent training process, we propose the simple ordering strategy that arranges a training sequences inside the stochastic alternatingly sorted way. we compare our method to sequence bucketing as well as various other batch construction strategies on a chime-4 noisy speech recognition corpus. a experiments show that our alternated sorting idea behind the method was able to compete both inside training time and recognition performance while being conceptually simpler to implement.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1302,"this paper presents some useful mathematical results involved inside football table prediction. inside addition, some empirical results indicate that an alternative methodology considering football table prediction may produce high quality forecasts with far less resource usage than conventional methods.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10326,"inside this paper, we study a problem of learning image classification models with label noise. existing approaches depending on human supervision are generally not scalable as manually identifying correct or incorrect labels was time-consuming, whereas approaches not relying on human supervision are scalable but less effective. to reduce a amount of human supervision considering label noise cleaning, we introduce cleannet, the joint neural embedding network, which only requires the fraction of a classes being manually verified to provide a knowledge of label noise that should be transferred to other classes. we further integrate cleannet and conventional convolutional neural network classifier into one framework considering image classification learning. we demonstrate a effectiveness of a proposed algorithm on both of a label noise detection task and a image classification on noisy data task on several large-scale datasets. experimental results show that cleannet should reduce label noise detection error rate on held-out classes where no human supervision available by 41.5% compared to current weakly supervised methods. it also achieves 47% of a performance gain of verifying all images with only 3.2% images verified on an image classification task. source code and dataset will be available at kuanghuei.github.io/cleannetproject.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15073,"connectionist temporal classification has recently attracted the lot of interest as it offers an elegant idea behind the method to building acoustic models (ams) considering speech recognition. a ctc loss function maps an input sequence of observable feature vectors to an output sequence of symbols. output symbols are conditionally independent of each other under ctc loss, so the language model (lm) should be incorporated conveniently during decoding, retaining a traditional separation of acoustic and linguistic components inside asr. considering fixed vocabularies, weighted finite state transducers provide the strong baseline considering efficient integration of ctc ams with n-gram lms. character-based neural lms provide the straight forward solution considering open vocabulary speech recognition and all-neural models, and should be decoded with beam search. finally, sequence-to-sequence models should be used to translate the sequence of individual sounds into the word string. we compare a performance of these three approaches, and analyze their error patterns, which provides insightful guidance considering future research and development inside this important area.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11569,"autonomous science was the field of study which aims to extend a autonomy of exploration robots from low level functionality, such as on-board perception and obstacle avoidance, to science autonomy, which allows scientists to specify missions at task level. this will enable more remote and extreme environments such as deep ocean and other planets to be studied, leading to significant science discoveries. this paper presents an idea behind the method to extend a high level autonomy of robots by enabling them to model and reason about scientific knowledge on-board. we achieve this by with the help of bayesian networks to encode scientific knowledge and adapting monte carlo tree search techniques to reason about a network and plan informative sensing actions. a resulting knowledge representation and reasoning framework was anytime, handles large state spaces and robust to uncertainty making it highly applicable to field robotics. we apply a idea behind the method to the mars exploration mission inside which a robot was required to plan paths and decide when to use its sensing modalities to study the scientific latent variable of interest. extensive simulation results show that our idea behind the method has significant performance benefits over alternative methods. we also demonstrate a practicality of our idea behind the method inside an analog martian environment where our experimental rover, continuum, plans and executes the science mission autonomously.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
11138,"this paper provides approximation and inference methods considering the structural function, such as conditional average treatment effect (cate), based on modern machine learning (ml) tools. we assume that such function should be represented as an expectation g(x) of the signal y conditional on x that depends on an unknown nuisance function. inside addition to cate, examples of such functions include regression function with partially missing outcome and conditional average partial derivative. we approximate g(x) by the linear form that was the product of the vector of a approximating basis functions p(x) and a best linear predictor (blp), which we refer to the pseudo-target. plugging inside a first-stage approximate of a nuisance function into a signal, we approximate blp using ordinary least squares. we deliver the high-quality approximate of a pseudo-target function that features (a) the pointwise gaussian approximation, (b) the simultaneous gaussian approximation, and (c) optimal rate of simultaneous convergence. inside a case, a misspecification error of a linear form decays sufficiently fast, these approximations automatically hold considering a target function g(x) instead of the pseudo-target. a first stage nuisance parameter was allowed to be high-dimensional and was estimated by modern ml tools, such as neural networks, shrinkage estimators, and random forest. with the help of our method, we approximate a average price elasticity conditional on income with the help of yatchew and no (2001) data and provide simultaneous confidence bands considering a target regression function.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0
16010,"existence, uniqueness and monotonic behavior of a solution of classical flow distribution problem considering hydraulic networks with pressure-dependent closure relations is proved. structure and properties of inverse maxwell matrix of a problem were investigated",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10054,"a problem of estimating the high-dimensional sparse vector $\boldsymbol{\theta} \in \mathbb{r}^n$ from an observation inside i.i.d. gaussian noise was considered. a performance was measured with the help of squared-error loss. an empirical bayes shrinkage estimator, derived with the help of the bernoulli-gaussian prior, was analyzed and compared with a well-known soft-thresholding estimator. we obtain concentration inequalities considering a stein's unbiased risk approximate and a loss function of both estimators. a results show that considering large $n$, both a risk approximate and a loss function concentrate on deterministic values close to a true risk. depending on a underlying $\boldsymbol{\theta}$, either a proposed empirical bayes (ebayes) estimator or soft-thresholding may have smaller loss. we consider the hybrid estimator that attempts to pick a better of a soft-thresholding estimator and a ebayes estimator by comparing their risk estimates. it was shown that: i) a loss of a hybrid estimator concentrates on a minimum of a losses of a two competing estimators, and ii) a risk of a hybrid estimator was within order $\frac{1}{\sqrt{n}}$ of a minimum of a two risks. simulation results are provided to support a theoretical results. finally, we use a ebayes and hybrid estimators as denoisers inside a approximate message passing (amp) algorithm considering compressed sensing, and show that their performance was superior to a soft-thresholding denoiser inside the wide range of settings.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0
12393,"long-term visual localization inside outdoor environment was the challenging problem, especially faced with a cross-seasonal, bi-directional tasks and changing environment. inside this paper we propose the novel visual inertial localization framework that localizes against a lidar-built map. based on a geometry information of a laser map, the hybrid bundle adjustment framework was proposed, which estimates a poses of a cameras with respect to a prior laser map as well as optimizes a state variables of a online visual inertial odometry system simultaneously. considering more accurate cross-modal data association, a laser map was optimized with the help of multi-session laser and visual data to extract a salient and stable subset considering localization. to validate a efficiency of a proposed method, we collect data inside south part of our campus inside different seasons, along a same and opposite-direction route. inside all sessions of localization data, our proposed method gives satisfactory results, and shows a superiority of a hybrid bundle adjustment and map optimization.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
6884,"the parametric spectral approximation problem inside a style of byrnes, georgiou, and lindquist is posed inside \cite{fpz-10}, but a existence of the solution is only proved inside the special case. based on their results, we show that the solution indeed exists given an arbitrary matrix-valued prior density. a main tool inside our proof was a topological degree theory.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
18945,"despite a improvements inside humanoids robots over a last decades, they are still far behind compared to human locomotor abilities. their performance limitations should be partially attributed to a hardware, but a primary constraint has been a understanding of bipedal dynamics. based on a recently developed model of potential energy considering bipedal structures, this work proposes the task-space planner considering human-like straight locomotion. a proposed architecture was based on potential energy model and employs locomotor strategies from human data as the reference behaviour. a model generates centre of mass (com) trajectories, foot swing trajectories and a base of support (bos). their calculation relies on a knowledge of a desired speed, initial posture, height, weight, number of steps and a angle between a foot and a ground during heel-strike. a data show that a proposed architecture should generate behaviour inside line with human walking strategies considering both a com and a foot swing. although a planned trajectory was not smooth compared to human trajectories, a proposed model significantly reduces a error inside a approximation of a com vertical trajectory. moreover, a proposed planner should generate the single stride inside less than 140 ms and sequences of 10 strides inside less than 600 ms, it allows an online task-space planning considering locomotion. lastly, a proposed architecture was also supported by analogies with current theories on human motor control of locomotion.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
359,"we derive two versions of an effective model to describe dynamical effects of a yukawa interaction among dirac electrons inside a plane. such short-range interaction was obtained by introducing the mass term considering a intermediate particle, which may be either scalar or an abelian gauge field, both of them inside (3+1) dimensions. thereafter, we consider that a matter field propagates only inside (2+1) dimensions, whereas a bosonic field was free to propagate out of a plane. within these assumptions, we apply the mechanism considering dimensional reduction, which yields an effective model inside (2+1) dimensions. inside particular, considering a gauge-field case, we use a stueckelberg mechanism inside order to preserve gauge invariance. we refer to this version as nonlocal-proca quantum electrodynamics (npqed). considering both scalar and gauge cases, a effective models reproduce a usual $e^{-m r}/r$ yukawa interaction inside a static limit. by means of perturbation theory at one loop, we calculate a mass renormalization of a dirac field. our model was the generalization of pseudoquantum electrodynamics (pqed), which was the gauge-field model that provides the coulomb interaction considering two-dimensional electrons. possibilities of application to fermi-bose mixtures inside mixed dimensions, with the help of cold atoms, are briefly discussed.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
16469,"today all kind of information was getting digitized and along with all this digitization, a huge archive of various kinds of documents was being digitized too. we know that, optical character recognition was a method through which, newspapers and other paper documents convert into digital resources. but, it was the fact that this method works on texts only. as the result, if we try to process any document which contains non-textual zones, then we will get garbage texts as output. that was why; inside order to digitize documents properly they should be prepossessed carefully. and while preprocessing, segmenting document inside different regions according to a category properly was most important. but, a optical character recognition processes available considering bangla language have no such algorithm that should categorize the newspaper/book page fully. so we worked to decompose the document into its several parts like headlines, sub headlines, columns, images etc. and if a input was skewed and rotated, then a input is also deskewed and de-rotated. to decompose any bangla document we found out a edges of a input image. then we find out a horizontal and vertical area of every pixel where it lies in. later on a input image is cut according to these areas. then we pick each and every sub image and found out their height-width ratio, line height. then according to these values a sub images were categorized. to deskew a image we found out a skew angle and de skewed a image according to this angle. to de-rotate a image we used a line height, matra line, pixel ratio of matra line.",1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17168,"physical and chemical properties of 2d material are highly sensitive to its structures whose regularity are seldom investigated, here we proposed the simple mechanical model whose covalent bonds are connected by angle springs, with which we gave insight into stability, bending stiffness and some other properties of 2d structures. it was found that flat, chair and washboard possess larger existent possibilities, which are consistent with existing 2d materials' structures. this model was the tool to evaluate existent possibilities of periodic 2d structures from mechanical viewpoint.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
7930,"the reinforcement learning agent that needs to pursue different goals across episodes requires the goal-conditional policy. inside addition to their potential to generalize desirable behavior to unseen goals, such policies may also enable higher-level planning based on subgoals. inside sparse-reward environments, a capacity to exploit information about a degree to which an arbitrary goal has been achieved while another goal is intended appears crucial to enable sample efficient learning. however, reinforcement learning agents have only recently been endowed with such capacity considering hindsight. inside this paper, we demonstrate how hindsight should be introduced to policy gradient methods, generalizing this idea to the broad class of successful algorithms. our experiments on the diverse selection of sparse-reward environments show that hindsight leads to the remarkable increase inside sample efficiency.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
11337,"we study non-compact manifolds whose sectional curvature tends to $ -\infty $. by the theorem of donelly/li this implies pure discrete spectrum of a laplacian. we prove upper bounds considering a eigenvalues $ \lambda_{k} $ of a laplacian inside terms of $ k^{2} $, $ k\geq 0 $. this stands inside clear contrast to laplacians on graphs where such the bound fails to be true inside general.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5093,"classical methods considering x-ray computed tomography are based on a assumption that a x-ray source intensity was known, but inside practice, a intensity was measured and thus uncertain. under normal operating conditions, when a exposure time was sufficiently high, this kind of uncertainty typically has the negligible effect on a reconstruction quality. however, inside time- or dose-limited applications such as dynamic ct, this uncertainty may cause severe and systematic artifacts known as ring artifacts. by carefully modeling a measurement process and by taking uncertainties into account, we derive the new convex model that leads to improved reconstructions despite poor quality measurements. we demonstrate a effectiveness of a methodology based on simulated and real data sets.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12597,"we establish the geometric condition guaranteeing exact copositive relaxation considering a nonconvex quadratic optimization problem under two quadratic and several linear constraints, and present sufficient conditions considering global optimality inside terms of generalized karush-kuhn-tucker multipliers. a copositive relaxation was tighter than a usual lagrangian relaxation. we illustrate this by providing the whole class of quadratic optimization problems that enjoys exactness of copositive relaxation while a usual lagrangian duality gap was infinite. finally, we also provide verifiable conditions under which both a usual lagrangian relaxation and a copositive relaxation are exact considering an extended cdt (two-ball trust-region) problem. importantly, a sufficient conditions should be verified by solving linear optimization problems.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
13502,"we examine a problem of learning and planning on high-dimensional domains with long horizons and sparse rewards. recent approaches have shown great successes inside many atari 2600 domains. however, domains with long horizons and sparse rewards, such as montezuma's revenge and venture, remain challenging considering existing methods. methods with the help of abstraction (dietterich 2000; sutton, precup, and singh 1999) have shown to be useful inside tackling long-horizon problems. we combine recent techniques of deep reinforcement learning with existing model-based approaches with the help of an expert-provided state abstraction. we construct toy domains that elucidate a problem of long horizons, sparse rewards and high-dimensional inputs, and show that our algorithm significantly outperforms previous methods on these domains. our abstraction-based idea behind the method outperforms deep q-networks (mnih et al. 2015) on montezuma's revenge and venture, and exhibits backtracking behavior that was absent from previous methods.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11886,"approximation of functions of $ d $ variables was considered with the help of ridge combinations of a form $ \textstyle\sum_{k=1}^m c_{1,k} \phi(\textstyle\sum_{j=1}^d c_{0,j,k}x_j-b_k) $ where a activation function $ \phi $ was the function with bounded value and derivative. these include single-hidden layer neural networks, polynomials, and sinusoidal models. from the sample of size $ n $ of possibly noisy values at random sites $ x \in b = [-1,1]^d $, a minimax mean square error was examined considering functions inside a closure of a $ \ell_1 $ hull of ridge functions with activation $ \phi $. it was shown to be of order $ d/n $ to the fractional power (when $ d $ was of smaller order than $ n $), and to be of order $ (\log d)/n $ to the fractional power (when $ d $ was of larger order than $ n $). dependence on constraints $ v_0 $ and $ v_1 $ on a $ \ell_1 $ norms of inner parameter $ c_0 $ and outer parameter $ c_1 $, respectively, was also examined. also, lower and upper bounds on a fractional power are given. a heart of a analysis was development of information-theoretic packing numbers considering these classes of functions.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7492,"typically, discrete choice modelers develop ever-more advanced models and approximation methods. compared to a impressive progress inside model development and estimation, model-checking techniques have lagged behind. often, choice modelers use only crude methods to assess how well an estimated model represents reality. such methods usually stop at checking parameter signs, model elasticities, and ratios of model coefficients. inside this paper, i greatly expand a discrete choice modelers' assessment toolkit by introducing model checking procedures based on graphical displays of predictive simulations. overall, my contributions are as follows. methodologically, i introduce the general and 'semi-automatic' algorithm considering checking discrete choice models using predictive simulations. by combining new graphical displays with existing plots, i introduce methods considering checking one's data against one's model inside terms of a model's predicted distributions of choices (p(y)), choices given explanatory variables (p(y|x)), and explanatory variables given choices (p(x|y)). empirically, i demonstrate my proposed methods by checking a models from brownstone and train (1998). through this case study, i show that my proposed methods should point out lack-of-model-fit inside one's models and suggest concrete model improvements that substantively change a results of one's policy analysis. moreover, a case study highlights the practical trade-off between precision and robustness inside model checking.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6890,"knowledge bases (kb), both automatically and manually constructed, are often incomplete --- many valid facts should be inferred from a kb by synthesizing existing information. the popular idea behind the method to kb completion was to infer new relations by combinatory reasoning over a information found along other paths connecting the pair of entities. given a enormous size of kbs and a exponential number of paths, previous path-based models have considered only a problem of predicting the missing relation given two entities or evaluating a truth of the proposed triple. additionally, these methods have traditionally used random paths between fixed entity pairs or more recently learned to pick paths between them. we propose the new algorithm minerva, which addresses a much more difficult and practical task of answering questions where a relation was known, but only one entity. since random walks are impractical inside the setting with combinatorially many destinations from the start node, we present the neural reinforcement learning idea behind the method which learns how to navigate a graph conditioned on a input query to find predictive paths. empirically, this idea behind the method obtains state-of-the-art results on several datasets, significantly outperforming prior methods.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14311,"we consider a challenging problem of statistical inference considering exponential-family random graph models based on the single observation of the random graph with complex dependence. to facilitate statistical inference, we consider random graphs with additional structure inside a form of block structure. we have shown elsewhere that when a block structure was known, it facilitates consistency results considering $m$-estimators of canonical and curved exponential-family random graph models with complex dependence, such as transitivity. inside practice, a block structure was known inside some applications (e.g., multilevel networks), but was unknown inside others. when a block structure was unknown, a first and foremost question was whether it should be recovered with high probability based on the single observation of the random graph with complex dependence. a main consistency results of a paper show that it was possible to do so provided a number of blocks grows as fast as inside high-dimensional stochastic block models. these results confirm that exponential-family random graph models with block structure constitute the promising direction of statistical network analysis.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
4424,"a extended falicov-kimball model was analyzed exactly inside a ground state at half filling inside a limit of large dimensions. inside a model a on-site and a intersite density-density interactions between all particles are included. we determined a model's phase diagram and found the discontinuous transition between two different charge-ordered phases. our analytical calculations show that a ground state of a system was insulating considering any nonzero values of a interaction couplings. we also show that a dynamical mean-field theory and a static broken-symmetry hartree-fock mean-field approximation give a same results considering a model at zero temperature. inside addition, we prove with the help of analytical expressions that at infinitesimally small, but finite, temperatures a system should be metallic.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
17236,"symmetric nonnegative matrix factorization (symnmf) has important applications inside data analytics problems such as document clustering, community detection and image segmentation. inside this paper, we propose the novel nonconvex variable splitting method considering solving symnmf. a proposed algorithm was guaranteed to converge to a set of karush-kuhn-tucker (kkt) points of a nonconvex symnmf problem. furthermore, it achieves the global sublinear convergence rate. we also show that a algorithm should be efficiently implemented inside parallel. further, sufficient conditions are provided which guarantee a global and local optimality of a obtained solutions. extensive numerical results performed on both synthetic and real data sets suggest that a proposed algorithm converges quickly to the local minimum solution.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0
1720,"high-energy astrophysical events that cause galaxy-scale extinctions have been proposed as the way to explain or mollify a fermi paradox, by making a universe at earlier times more dangerous considering evolving life, and reducing its present-day prevalence. here, we present an anthropic argument that the more dangerous early universe should have a opposite effect, actually increasing estimates considering a amount of visible extragalactic life at a present cosmic time. this occurs when civilizations are assumed to expand and displace possible origination sites considering a evolution of life, and estimates are made by assuming that humanity has appeared at the typical time. a effect was not seen if advanced life was assumed to always remain stationary, with no displacement of habitable worlds.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10684,"we investigate task clustering considering deep-learning based multi-task and few-shot learning inside the many-task setting. we propose the new method to measure task similarities with cross-task transfer performance matrix considering a deep learning scenario. although this matrix provides us critical information regarding similarity between tasks, its asymmetric property and unreliable performance scores should affect conventional clustering methods adversely. additionally, a uncertain task-pairs, i.e., a ones with extremely asymmetric transfer scores, may collectively mislead clustering algorithms to output an inaccurate task-partition. to overcome these limitations, we propose the novel task-clustering algorithm by with the help of a matrix completion technique. a proposed algorithm constructs the partially-observed similarity matrix based on a certainty of cluster membership of a task-pairs. we then use the matrix completion algorithm to complete a similarity matrix. our theoretical analysis shows that under mild constraints, a proposed algorithm will perfectly recover a underlying ""true"" similarity matrix with the high probability. our results show that a new task clustering method should discover task clusters considering training flexible and superior neural network models inside the multi-task learning setup considering sentiment classification and dialog intent classification tasks. our task clustering idea behind the method also extends metric-based few-shot learning methods to adapt multiple metrics, which demonstrates empirical advantages when a tasks are diverse.",1,0,0,1,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17631,"following a model introduced by aguech, lasmar and mahmoud [probab. engrg. inform. sci. 21 (2007) 133-141], a weighted depth of the node inside the labelled rooted tree was a sum of all labels on a path connecting a node to a root. we analyze weighted depths of nodes with given labels, a last inserted node, nodes ordered as visited by a depth first search process, a weighted path length and a weighted wiener index inside the random binary search tree. we establish three regimes of nodes depending on whether a second order behaviour of their weighted depths follows from fluctuations of a keys on a path, a depth of a nodes, or both. finally, we investigate the random distribution function on a unit interval arising as scaling limit considering weighted depths of nodes with at most one child.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11913,"topological nodal-line semimetals are characterized by one-dimensional lines of band crossing inside a brillouin zone. inside contrast to nodal points, nodal lines should be inside topologically nontrivial configurations. inside this paper, we study a simplest topologically nontrivial forms of nodal line, namely, the single nodal line taking a shape of the knot inside a brillouin zone. we introduce the generic construction considering various ""nodal-knot semimetals"", which yields a simplest trefoil nodal knot and other more complicated nodal knots inside a brillouin zone. a knotted-unknotted transitions by nodal-line reconnections are also studied. our work brings a knot theory to a subject of topological semimetals.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
13650,"we introduce the novel regression framework which simultaneously models a quantile and a expected shortfall (es) of the response variable given the set of covariates. this regression was based on the strictly consistent loss function considering a pair quantile and es, which allows considering m- and z-estimation of a joint regression parameters. we show consistency and asymptotic normality considering both estimators under weak regularity conditions. a underlying loss function depends on two specification functions, whose choice affects a properties of a resulting estimators. we find that a z-estimator was numerically unstable and thus, we rely on m-estimation of a model parameters. extensive simulations verify a asymptotic properties and analyze a small sample behavior of a m-estimator considering different specification functions. this joint regression framework allows considering various applications including estimating, forecasting, and backtesting es, which was particularly relevant inside light of a recent introduction of es into a basel accords.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
18835,"linear parameter-varying (lpv) systems with piecewise differentiable parameters was the class of lpv systems considering which no proper analysis conditions have been obtained so far. to fill this gap, we propose an idea behind the method based on a theory of hybrid systems. a underlying idea was to reformulate a considered lpv system as an equivalent hybrid system that will incorporate, through the suitable state augmentation, information on both a dynamics of a state of a system and a considered class of parameter trajectories. then, with the help of the result pertaining on a stability of hybrid systems, two stability conditions are established and shown to naturally generalize and unify a well-known quadratic and robust stability criteria together. a obtained conditions being infinite-dimensional, the relaxation idea behind the method based on sum of squares programming was used inside order to obtain tractable finite-dimensional conditions. a idea behind the method was finally illustrated on two examples from a literature.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
18434,"reasoning about a pose, i.e. position and orientation of objects was one of a cornerstones of robotic manipulation under uncertainty. inside the number of joint research projects our group was developing the robotic perception system that perceives and models an unprepared kitchen scenario with many objects. since no single sensor or measurement provides sufficient information, the technique was needed to fuse the number of uncertain estimates of a pose, i.e. estimates with the widely stretched probability density function ($pdf$). a most frequently used approaches to describe a $pdfs$ are sample based description and multivariate normal (gaussian) distributions. sample based descriptions inside 6d should describe basically any type of $pdfs$, but they require the large number of samples and there are no analytic formulae to fuse several $pdfs$. considering gaussian distributions these formulae exist, but a gaussian distributions are unimodal and don't model widely spread distributions well. inside this paper we present the framework considering probabilistic modeling of 6d poses that combines a expressive power of a sample based description with a conciseness and algorithmic power of a gaussian models. as parameterization of a 6d poses we select a dual quaternions, i.e. any pose was represented by two quaternions. a orientation part of the pose was described by the unit quaternion. a translation part was described by the purely imaginary quaternion. the basic probability density function over a poses was constructed by selecting the tangent point on a 3d sphere representing unit quaternions and taking a cartesian set product of a tangent space with a 3d space of translations. inside this 6d euclidean space the 6d gaussian distribution was defined. projecting this gaussian back to a unit sphere and renormalizing induces the distribution over 6d poses, called the projected gaussian.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
13585,"dead time affects many of a instruments used inside x-ray astronomy, by producing the strong distortion inside power density spectra. this should make it difficult to model a aperiodic variability of a source or look considering quasi-periodic oscillations. whereas inside some instruments the simple the priori correction considering dead-time-affected power spectra was possible, this was not a case considering others such as nustar, where a dead time was non-constant and long (~2.5 ms). bachetti et al. 2015 suggested a cospectrum obtained from light curves of independent detectors within a same instrument as the possible way out, but this solution has always only been the partial one: a measured rms is still affected by dead time, because a width of a power distribution of a cospectrum is modulated by dead time inside the frequency-dependent way. inside this letter we suggest the new, powerful method to normalize cospectra and, with some caveats, even power density spectra. our idea behind the method uses a difference of a fourier amplitudes from two independent detectors to characterize and filter out a effect of dead time. this method was crucially important considering a accurate modelling of periodograms derived from instruments affected by dead time on board current missions like nustar and astrosat, but also future missions such as ixpe",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
2921,"this paper continues the series inside which we predict a main observable characteristics of exoplanets based on their formation. inside paper i we described our global planet formation and evolution model. inside paper ii we studied a planetary mass-radius relationship. here we present an extensive study of a statistics of planetary luminosities during both formation and evolution. our results should be compared with individual directly imaged (proto)planets as well as statistical results from surveys. we calculated three synthetic planet populations assuming different efficiencies of a accretional heating by gas and planetesimals. we describe a temporal evolution of a planetary mass-luminosity relation. we study a shock and internal luminosity during formation. we predict the statistical version of a post-formation mass versus entropy ""tuning fork"" diagram. we find high nominal post-formation luminosities considering hot and cold gas accretion. individual formation histories should still lead to the factor of the few spread inside a post-formation luminosity at the given mass. however, if a gas and planetesimal accretional heating was unknown, a post-formation luminosity may exhibit the spread of as much as 2-3 orders of magnitude at the fixed mass covering cold, warm, and hot states. as the key result we predict the flat log-luminosity distribution considering giant planets, and the steep increase towards lower luminosities due to a higher occurrence rate of low-mass planets. future surveys may detect this upturn. during formation an approximate of a planet mass may be possible considering cold gas accretion if a gas accretion rate should be estimated. due to a ""core-mass effect"" planets that underwent cold gas accretion should still have high post-formation entropies. once a number of directly imaged exoplanets with known ages and luminosities increases, a observed distributions may be compared with our predictions.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2218,"determination of binding affinity of proteins inside a formation of protein complexes requires sophisticated, expensive and time-consuming experimentation which should be replaced with computational methods. most computational prediction techniques require protein structures which limit their applicability to protein complexes with known structures. inside this work, we explore sequence based protein binding affinity prediction with the help of machine learning. our paper highlights a fact that a generalization performance of even a state of a art sequence-only predictor of binding affinity was far from satisfactory and that a development of effective and practical methods inside this domain was still an open problem. we also propose the novel sequence-only predictor of binding affinity called island which gives better accuracy than existing methods over a same validation set as well as on external independent test dataset. the cloud-based webserver implementation of island and its python code are available at a url: this http url.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11252,"consider the $q$-weil polynomial $f$ of degree $2g$. with the help of an equidistribution assumption that was too strong to be true, we define and compute the product of local relative densities of matrices inside $\rm{gsp}_{2g}(\mathbb{f}_\ell)$ with characteristic polynomial $f\mod\ell$ when $g$ was an odd prime. this infinite product was closely related to the ratio of class numbers. when $g=3$ we conjecture that a product gives a size of an isogeny class of principally polarized abelian threefolds.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
4613,"motivated by a recent theoretical and experimental progress inside a heavy fermion system ucoge, we study ferromagnetic chiral superconductors inside a presence of magnetic domains. within mean field approximations, it was shown that chiral superconducting domains are naturally induced by a ferromagnetic domains. a domain wall current flows inside a opposite direction to a naively expected one as inside $^3$he-a phase due to contributions from ""unpaired electrons"". consequently, a domain wall current flows inside a same direction with that of surface currents when a magnetic domain wall lies parallel to a sample surface, and therefore they contribute to a net current along a whole sample. we find that, due to a non-cancellation between a domain wall current and surface current, the fulde-ferrell-like superconducting state should be stabilized inside an anisotropic sample considering all a temperatures below a superconducting transition temperature.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
19234,"a born-infeld (bi) model was the nonlinear correction of maxwell's equations. by adding a energy and poynting vector as additional variables, it should be augmented as the 10$\times$10 system of hyperbolic conservation laws, called a augmented bi (abi) equations. a author found that, through the quadratic change of a time variable, a abi system gives the simple energy dissipation model that combines darcy's law and magnetohydrodynamics (mhd). with the help of a concept of ""relative entropy"" (or ""modulated energy""), borrowed from a theory of hyperbolic systems of conservation laws, we introduce the notion of generalized solutions, that we call dissipative solutions. considering given initial conditions, a set of generalized solutions was not empty, convex, and compact. smooth solutions to a dissipative system are always unique inside this setting.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14648,"we introduce instancewise feature selection as the methodology considering model interpretation. our method was based on learning the function to extract the subset of features that are most informative considering each given example. this feature selector was trained to maximize a mutual information between selected features and a response variable, where a conditional distribution of a response variable given a input was a model to be explained. we develop an efficient variational approximation to a mutual information, and show a effectiveness of our method on the variety of synthetic and real data sets with the help of both quantitative metrics and human evaluation.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
887,"on a basis of the previously established scalar-tensor extension of a $\lambda$cdm model we develop an effective fluid idea behind the method considering a matter growth function. this extended $\lambda$cdm (henceforth $e_{\phi}\lambda$cdm) cosmology takes into account deviations from a standard model both using the modified background expansion and by a inclusion of geometric anisotropic stresses as well as of perturbations of a geometric dark-energy equivalent. a background dynamics was governed by an explicit analytic expression considering a hubble rate inside which modifications of a standard model are given inside terms of the single constant parameter [1]. to close a system of fluid-dynamical perturbation equations we introduce two phenomenological parameters through which a anisotropic stress was related both to a total energy density perturbation of a cosmic substratum and to relative perturbations inside a effective two-component system. we quantify a impact of deviations from a standard background, of anisotropic stresses and of non-vanishing perturbations of a effective dark-energy component on a matter growth rate function $f \sigma_8$ and confront a results with recent redshift-space distortion (rsd) measurements.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3030,"we formulate an optimization problem to control the large number of automated guided vehicles inside the plant without collision. a formulation consists of binary variables. the quadratic cost function over these variables enables us to utilize certain solvers on digital computers and recently developed purpose-specific hardware such as d-wave 2000q and a fujitsu digital annealer. inside a present study, we consider an actual plant inside japan, inside which vehicles run, and assess efficiency of our formulation considering optimizing a vehicles using several solvers. we confirm that our formulation should be the powerful idea behind the method considering performing smooth control while avoiding collisions between vehicles, as compared to the conventional method. inside addition, comparative experiments performed with the help of several solvers reveal that d-wave 2000q should be useful as the rapid solver considering generating the plan considering controlling a vehicles inside the short time although it deals only with the small number of vehicles, while the digital computer should rapidly solve a corresponding optimization problem even with the large number of binary variables.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
12905,"inside this note we extend a theory of twists of elliptic curves as presented inside various standard texts considering characteristic not equal to two or three to a remaining characteristics. considering this, we make explicit use of a correspondence between a twists and a galois cohomology set $h^1\big(\operatorname{g}_{\overline{k}/k}, \operatorname{aut}_{\overline{k}}(e)\big)$. a results are illustrated by examples.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
3272,"amorph utilizes the new bayesian statistical idea behind the method to interpreting x-ray diffraction results of samples with both crystalline and amorphous components. amorph fits x-ray diffraction patterns with the mixture of narrow and wide components, simultaneously inferring all of a model parameters and quantifying their uncertainties. a program simulates background patterns previously applied manually, providing reproducible results, and significantly reducing inter- and intra-user biases. this idea behind the method allows considering a quantification of amorphous and crystalline materials and considering a characterization of a amorphous component, including properties such as a centre of mass, width, skewness, and nongaussianity of a amorphous component. results demonstrate a applicability of this program considering calculating amorphous contents of volcanic materials and independently modeling their properties inside compositionally variable materials.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15607,"one of a major open challenges inside self-driving cars was a ability to detect cars and pedestrians to safely navigate inside a world. deep learning-based object detector approaches have enabled great advances inside with the help of camera imagery to detect and classify objects. but considering the safety critical application, such as autonomous driving, a error rates of a current state of a art are still too high to enable safe operation. moreover, a characterization of object detector performance was primarily limited to testing on prerecorded datasets. errors that occur on novel data go undetected without additional human labels. inside this letter, we propose an automated method to identify mistakes made by object detectors without ground truth labels. we show that inconsistencies inside a object detector output between the pair of similar images should be used as hypotheses considering false negatives (e.g., missed detections) and with the help of the novel set of features considering each hypothesis, an off-the-shelf binary classifier should be used to find valid errors. inside particular, we study two distinct cues - temporal and stereo inconsistencies - with the help of data that are readily available on most autonomous vehicles. our method should be used with any camera-based object detector and we illustrate a technique on several sets of real world data. we show that the state-of-the-art detector, tracker, and our classifier trained only on synthetic data should identify valid errors on kitti tracking dataset with an average precision of 0.94. we also release the new tracking dataset with 104 sequences totaling 80,655 labeled pairs of stereo images along with ground truth disparity from the game engine to facilitate further research. a dataset and code are available at this https url",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
19223,"a erd≈ës-ginzburg-ziv constant of an abelian group $g$, denoted $\mathfrak{s}(g)$, was a smallest $k\in\mathbb{n}$ such that any sequence of elements of $g$ of length $k$ contains the zero-sum subsequence of length $\exp(g)$. inside this paper, we use a partition rank, which generalizes a slice rank, to prove that considering any odd prime $p$, \[ \mathfrak{s}\left(\mathbb{f}_{p}^{n}\right)\leq(p-1)2^{p}\left(j(p)\cdot p\right)^{n} \] where $0.8414<j(p)<0.91837$ was a constant appearing inside ellenberg and gijswijt's bound on arithmetic progression-free subsets of $\mathbb{f}_{p}^{n}$. considering large $n$, and $p>3$, this was a first exponential improvement to a trivial bound. we also provide the near optimal result conditional on a conjecture that $\left(\mathbb{z}/k\mathbb{z}\right)^{n}$ satisfies property $d$, showing that inside this case \[ \mathfrak{s}\left(\left(\mathbb{z}/k\mathbb{z}\right)^{n}\right)\leq(k-1)4^{n}+k. \]",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
7130,"the novel deep learning architecture (xmasnet) based on convolutional neural networks is developed considering a classification of prostate cancer lesions, with the help of a 3d multiparametric mri data provided by a prostatex challenge. end-to-end training is performed considering xmasnet, with data augmentation done through 3d rotation and slicing, inside order to incorporate a 3d information of a lesion. xmasnet outperformed traditional machine learning models based on engineered features, considering both train and test data. considering a test data, xmasnet outperformed 69 methods from 33 participating groups and achieved a second highest auc (0.84) inside a prostatex challenge. this study shows a great potential of deep learning considering cancer imaging.",1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17094,"we verify a brou√©-malle-rouquier (bmr) freeness considering cyclotomic hecke algebras associated with complex reflection groups $g_{17}$, $g_{18}$, $g_{19}$ inside a shephard-todd classification. together with results of ariki, ariki-koike, brou√©-malle, marin, marin-pfeiffer and chavli, this settled affirmatively a bmr freeness conjecture. our verification was inspired by bergman's diamond lemma and requires 1gb memory and 5 days calculation on the pc.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
17244,"a risks of publishing privacy-sensitive data have received considerable attention recently. several de-anonymization attacks have been proposed to re-identify individuals even if data anonymization techniques were applied. however, there was no theoretical quantification considering relating a data utility that was preserved by a anonymization techniques and a data vulnerability against de-anonymization attacks. inside this paper, we theoretically analyze a de-anonymization attacks and provide conditions on a utility of a anonymized data (denoted by anonymized utility) to achieve successful de-anonymization. to a best of our knowledge, this was a first work on quantifying a relationships between anonymized utility and de-anonymization capability. unlike previous work, our quantification analysis requires no assumptions about a graph model, thus providing the general theoretical guide considering developing practical de-anonymization/anonymization techniques. furthermore, we evaluate state-of-the-art de-anonymization attacks on the real-world facebook dataset to show a limitations of previous work. by comparing these experimental results and a theoretically achievable de-anonymization capability derived inside our analysis, we further demonstrate a ineffectiveness of previous de-anonymization attacks and a potential of more powerful de-anonymization attacks inside a future.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
10924,"capture inside mean motion resonance has been observed inside a solar system considering small objects with prograde as well as retrograde orbits of moderate inclinations. however, no example of an object with the nearly polar orbit is known to be inside resonance with the planet. inside this letter, we report that a nearly-polar transneptunian object (471325), nicknamed niku, was inside the 7:9 resonance with neptune, with the mean lifetime inside resonance of 16 +- 11 million years. while entrance and exit inside a 7:9 resonance was caused by close encounters with neptune a resonant configuration provides the temporary protection mechanism against disruptive close encounters with this planet. a other nearly polar transneptunian objects do not seem to be inside resonance with a planets with a possible exception of 2008 kv42, also known as drac, that has the small chance of being inside a 8:13 resonance with neptune.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16046,"recently, optional stopping has been the subject of debate inside a bayesian psychology community. rouder (2014) argues that optional stopping was no problem considering bayesians, and even recommends a use of optional stopping inside practice, as do wagenmakers et al. (2012). this article addresses a question whether optional stopping was problematic considering bayesian methods, and specifies under which circumstances and inside which sense it was and was not. by slightly varying and extending rouder's (2014) experiment, we illustrate that, as soon as a parameters of interest are equipped with default or pragmatic priors - which means, inside most practical applications of bayes factor hypothesis testing - resilience to optional stopping should break down. we distinguish between four types of default priors, each having their own specific issues with optional stopping, ranging from no-problem-at-all (type 0 priors) to quite severe (type ii and iii priors).",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
9266,"propensity score matching (psm) was a de-facto standard considering estimating causal effects inside observational studies. we show that psm and its implementations are susceptible to several major drawbacks and illustrate these findings with the help of the case study with $17,427$ patients. we derive four formal properties an optimal statistical matching algorithm should meet, and propose deterministic balancing score exact matching (dbsem) which meets a aforementioned properties considering an exact matching. furthermore, we investigate one of a main problems of psm, that was that common psm results inside one valid set of matched pairs or the bootstrapped psm inside the selection of possible valid sets of matched pairs. considering exact matchings we provide a mathematical proof, that dbsem, as the result, delivers a expected value of all valid sets of matched pairs considering a investigated dataset.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13734,"the class of algorithms considering a solution of discrete material optimization problems inside electromagnetic applications was discussed. a idea behind a algorithm was similar to that of a sequential programming. however, inside each major iteration the model was established on a basis of an appropriately parametrized material tensor. a resulting nonlinear parametrization was treated on a level of a sub-problem, considering which, globally optimal solutions should be computed due to a block separability of a model. although global optimization of non-convex design problems was generally prohibitive, the well chosen combination of analytic solutions along with standard global optimization techniques leads to the very efficient algorithm considering most relevant material parametrizations. the global convergence result considering a overall algorithm was established. a effectiveness of a idea behind the method inside terms of both computation time and solution quality was demonstrated by numerical examples, including a optimal design of cloaking layers considering the nano-particle and a identification of multiple materials with different optical properties inside the matrix.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
12318,"this paper concerns a inverse source problems considering a time-harmonic elastic and electromagnetic wave equations. a goal was to determine a external force and a electric current density from boundary measurements of a radiated wave field, respectively. a problems are challenging due to a ill-posedness and complex model systems. uniqueness and stability are established considering both of a inverse source problems. based on either continuous or discrete multi-frequency data, the unified increasing stability theory was developed. a stability estimates consist of two parts: a lipschitz type data discrepancy and a high frequency tail of a source functions. as a upper bound of frequencies increases, a latter decreases and thus becomes negligible. a increasing stability results reveal that ill-posedness of a inverse problems should be overcome by with the help of multi-frequency data. a method was based on integral equations and analytical continuation, and requires a dirichlet data only. a analysis employs asymptotic expansions of green's tensors and a transparent boundary conditions by with the help of a dirichlet-to-neumann maps. inside addition, considering a first time, a stability was established on a inverse source problems considering both a navier and maxwell equations.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10633,"we solve the system of ordinary differential equations with an unknown functional form of the sink (reaction rate) term. we assume that a measurements (time series) of state variables are partially available, and we use recurrent neural network to ""learn"" a reaction rate from this data. this was achieved by including the discretized ordinary differential equations as part of the recurrent neural network training problem. we extend tensorflow's recurrent neural network architecture to create the simple but scalable and effective solver considering a unknown functions, and apply it to the fedbatch bioreactor simulation problem. use of techniques from recent deep learning literature enables training of functions with behavior manifesting over thousands of time steps. our networks are structurally similar to recurrent neural networks, but differences inside design and function require modifications to a conventional wisdom about training such networks.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
13242,"suppose that alice and bob are located inside distant laboratories, which are connected by an ideal quantum channel. suppose further that they share many copies of the quantum state $\rho_{abe}$, such that alice possesses a $a$ systems and bob a $be$ systems. inside our model, there was an identifiable part of bob's laboratory that was insecure: the third party named eve has infiltrated bob's laboratory and gained control of a $e$ systems. alice, knowing this, would like use their shared state and a ideal quantum channel to communicate the message inside such the way that bob, who has access to a whole of his laboratory ($be$ systems), should decode it, while eve, who has access only to the sector of bob's laboratory ($e$ systems) and a ideal quantum channel connecting alice to bob, cannot learn anything about alice's transmitted message. we call this task a conditional one-time pad, and inside this paper, we prove that a optimal rate of secret communication considering this task was equal to a conditional quantum mutual information $i(a;b|e)$ of their shared state. we thus give a conditional quantum mutual information an operational meaning that was different from those given inside prior works, using state redistribution, conditional erasure, or state deconstruction. we also generalize a model and method inside several ways, one of which demonstrates that a negative tripartite interaction information $-i_{3}(a;b;e) = i(a;be)-i(a;b)-i(a;e)$ of the tripartite state $\rho_{abe}$ was an achievable rate considering the secret-sharing task, i.e., a case inside which alice's message should be secure from someone possessing only a $ab$ or $ae$ systems but should be decodable by someone possessing all systems $a$, $b$, and $e$.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
12416,"we examine a question to what extent prospective detection of dark matter by direct and indirect- detection experiments could shed light on what fraction of dark matter is generated thermally using a freeze-out process inside a early universe. by simulating putative signals that could be seen inside a near future and with the help of them to reconstruct wimp dark matter properties, we show that, inside the model- independent idea behind the method this could only be achieved inside the thin sliver of a parameter space. however, with additional theoretical input a hypothesis about a thermal freeze-out as a dominant mechanism considering generating dark matter should potentially be verified. we illustrate this with two examples: an effective field theory of dark matter with the vector messenger and the higgsino or wino dark matter within a mssm.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17503,"human parsing has recently attracted the lot of research interests due to its huge application potentials. however existing datasets have limited number of images and annotations, and lack a variety of human appearances and a coverage of challenging cases inside unconstrained environment. inside this paper, we introduce the new benchmark ""look into person (lip)"" that makes the significant advance inside terms of scalability, diversity and difficulty, the contribution that we feel was crucial considering future developments inside human-centric analysis. this comprehensive dataset contains over 50,000 elaborately annotated images with 19 semantic part labels, which are captured from the wider range of viewpoints, occlusions and background complexity. given these rich annotations we perform detailed analyses of a leading human parsing approaches, gaining insights into a success and failures of these methods. furthermore, inside contrast to a existing efforts on improving a feature discriminative capability, we solve human parsing by exploring the novel self-supervised structure-sensitive learning approach, which imposes human pose structures into parsing results without resorting to extra supervision (i.e., no need considering specifically labeling human joints inside model training). our self-supervised learning framework should be injected into any advanced neural networks to aid incorporate rich high-level knowledge regarding human joints from the global perspective and improve a parsing results. extensive evaluations on our lip and a public pascal-person-part dataset demonstrate a superiority of our method.",1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6093,"inside this paper, we study a effects of general relativistic corrections on a observed galaxy power spectrum inside thawing class of cubic galileon model with linear potential that preserves a shift symmetry. inside this scenario, a observed galaxy power spectrum differs from a standard matter power spectrum mainly due to redshift space distortion (rsd) factor and relativistic effects. a rsd term enhances a matter power spectrum both at larger and smaller scales whereas a relativistic terms further enhance a matter power spectrum only at larger scales. inside comparison with $\lambda$cdm, a observed galaxy power spectrum was always suppressed at large scales inside this scenario although this suppression was always small compared to a canonical quintessence scenario.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5208,"understanding and predicting a popularity of online items was an important open problem inside social media analysis. considerable progress has been made recently inside data-driven predictions, and inside linking popularity to external promotions. however, a existing methods typically focus on the single source of external influence, whereas considering many types of online content such as youtube videos or news articles, attention was driven by multiple heterogeneous sources simultaneously - e.g. microblogs or traditional media coverage. here, we propose rnn-mas, the recurrent neural network considering modeling asynchronous streams. it was the sequence generator that connects multiple streams of different granularity using joint inference. we show rnn-mas not only to outperform a current state-of-the-art youtube popularity prediction system by 17%, but also to capture complex dynamics, such as seasonal trends of unseen influence. we define two new metrics: promotion score quantifies a gain inside popularity from one unit of promotion considering the youtube video; a loudness level captures a effects of the particular user tweeting about a video. we use a loudness level to compare a effects of the video being promoted by the single highly-followed user (in a top 1% most followed users) against being promoted by the group of mid-followed users. we find that results depend on a type of content being promoted: superusers are more successful inside promoting howto and gaming videos, whereas a cohort of regular users are more influential considering activism videos. this work provides more accurate and explainable popularity predictions, as well as computational tools considering content producers and marketers to allocate resources considering promotion campaigns.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0
12582,"inspired by a seminal work on stein variational inference and stein variational policy gradient, we derived the method to generate samples from a posterior variational parameter distribution by \textit{explicitly} minimizing a kl divergence to match a target distribution inside an amortize fashion. consequently, we applied this varational inference technique into vanilla policy gradient, trpo and ppo with bayesian neural network parameterizations considering reinforcement learning problems.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
227,"robotic assistants inside the home environment are expected to perform various complex tasks considering their users. one particularly challenging task was pouring drinks into cups, which considering successful completion, requires a detection and tracking of a liquid level during the pour to determine when to stop. inside this paper, we present the novel idea behind the method to autonomous pouring that tracks a liquid level with the help of an rgb-d camera and adapts a rate of pouring based on a liquid level feedback. we thoroughly evaluate our system on various types of liquids and under different conditions, conducting over 250 pours with the pr2 robot. a results demonstrate that our idea behind the method was able to pour liquids to the target height with an accuracy of the few millimeters.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
19550,"matrix factorisation methods decompose multivariate observations as linear combinations of latent feature vectors. a indian buffet process (ibp) provides the way to model a number of latent features required considering the good approximation inside terms of regularised reconstruction error. previous work has focussed on latent feature vectors with independent entries. we extend a model to include nondiagonal latent covariance structures representing characteristics such as smoothness. this was done by . with the help of simulations we demonstrate that under appropriate conditions the smoothness prior helps to recover a true latent features, while denoising more accurately. we demonstrate our method on the real neuroimaging dataset, where computational tractability was the sufficient challenge that a efficient strategy presented here was essential.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8630,"thermal effects are already important inside currently operating interferometric gravitational wave detectors. planned upgrades of these detectors involve increasing optical power to combat quantum shot noise. we consider a ramifications of this increased power considering one particular class of laser beams--wide, flat-topped, mesa beams. inside particular we model the single mesa beam fabry-perot cavity having thermoelastically deformed mirrors. we calculate a intensity profile of a fundamental cavity eigenmode inside a presence of thermal perturbations, and a associated changes inside thermal noise. we also outline an idealized method of correcting considering such effects. at each stage we contrast our results with those of the comparable gaussian beam cavity. although we focus on mesa beams a techniques described are applicable to any azimuthally symmetric system.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
13564,"inside reinforcement learning an agent interacts with a environment by taking actions and observing a next state and reward. when sampled probabilistically, these state transitions, rewards, and actions should all induce randomness inside a observed long-term return. traditionally, reinforcement learning algorithms average over this randomness to approximate a value function. inside this paper, we build on recent work advocating the distributional idea behind the method to reinforcement learning inside which a distribution over returns was modeled explicitly instead of only estimating a mean. that is, we examine methods of learning a value distribution instead of a value function. we give results that close the number of gaps between a theoretical and algorithmic results given by bellemare, dabney, and munos (2017). first, we extend existing results to a approximate distribution setting. second, we present the novel distributional reinforcement learning algorithm consistent with our theoretical formulation. finally, we evaluate this new algorithm on a atari 2600 games, observing that it significantly outperforms many of a recent improvements on dqn, including a related distributional algorithm c51.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
12413,"although timely sepsis diagnosis and prompt interventions inside intensive care unit (icu) patients are associated with reduced mortality, early clinical recognition was frequently impeded by non-specific signs of infection and failure to detect signs of sepsis-induced organ dysfunction inside the constellation of dynamically changing physiological data. a goal of this work was to identify patient at risk of life-threatening sepsis utilizing the data-centered and machine learning-driven approach. we derive the mortality risk predictive dynamic bayesian network (dbn) guided by the customized sepsis knowledgebase and compare a predictive accuracy of a derived dbn with a sepsis-related organ failure assessment (sofa) score, a quick sofa (qsofa) score, a simplified acute physiological score (saps-ii) and a modified early warning score (mews) tools. the customized sepsis ontology is used to derive a dbn node structure and semantically characterize temporal features derived from both structured physiological data and unstructured clinical notes. we assessed a performance inside predicting mortality risk of a dbn predictive model and compared performance to other models with the help of receiver operating characteristic (roc) curves, area under curve (auroc), calibration curves, and risk distributions. a derived dataset consists of 24,506 icu stays from 19,623 patients with evidence of suspected infection, with 2,829 patients deceased at discharge. a dbn auroc is found to be 0.91, which outperformed a sofa (0.843), qsofa (0.66), mews (0.73), and saps-ii (0.77) scoring tools. continuous net reclassification index and integrated discrimination improvement analysis supported a superiority dbn. compared with conventional rule-based risk scoring tools, a sepsis knowledgebase-driven dbn algorithm offers improved performance considering predicting mortality of infected patients inside icus.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19544,"let $m$ be the complex manifold of dimension $n$ with smooth connected boundary $x$. assume that $\overline m$ admits the holomorphic $s^1$-action preserving a boundary $x$ and a $s^1$-action was transversal and cr on $x$. we show that a $\overline\partial$-neumann laplacian on $m$ was transversally elliptic and as the consequence, a $m$-th fourier component of a $q$-th dolbeault cohomology group $h^q_m(\overline m)$ was finite dimensional, considering every $m\in\mathbb z$ and every $q=0,1,\ldots,n$. this enables us to define $\sum^{n}_{j=0}(-1)^j{\rm dim\,}h^q_m(\overline m)$ a $m$-th fourier component of a euler characteristic on $m$ and to study large $m$-behavior of $h^q_m(\overline m)$. inside this paper, we establish an index formula considering $\sum^{n}_{j=0}(-1)^j{\rm dim\,}h^q_m(\overline m)$ and morse inequalities considering $h^q_m(\overline m)$.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9395,"a task of multi-step ahead prediction inside language models was challenging considering a discrepancy between training and testing. at test time, the language model was required to make predictions given past predictions as input, instead of a past targets that are provided during training. this difference, known as exposure bias, should lead to a compounding of errors along the generated sequence at test time. inside order to improve generalization inside neural language models and address compounding errors, we propose the curriculum learning based method that gradually changes an initially deterministic teacher policy to the gradually more stochastic policy, which we refer to as \textit{nearest-neighbor replacement sampling}. the chosen input at the given timestep was replaced with the sampled nearest neighbor of a past target with the truncated probability proportional to a cosine similarity between a original word and its top $k$ most similar words. this allows a teacher to explore alternatives when a teacher provides the sub-optimal policy or when a initial policy was difficult considering a learner to model. a proposed strategy was straightforward, online and requires little additional memory requirements. we report our main findings on two language modelling benchmarks and find that a proposed idea behind the method performs particularly well when used inside conjunction with scheduled sampling, that too attempts to mitigate compounding errors inside language models.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16796,"a bcs-type second-order phase transition of the classical langmuir wave system was identified theoretically and numerically. a transition takes place between two states: langmuir wave turbulence (lwt) and langmuir wave supercontinuum (lwsc), a latter of which exhibits broad power spectra with significant spatiotemporal coherence when the certain number of plasmons (plasma wave quanta) are excited inside a system. inside a lwt-lwsc transition, a modulational instability and resulting creation of plasmon pairs are a classical counterparts of a cooper instability and creation of cooper pairs inside superconducting states. a bose-einstein condensation of cooper pairs inside superconducting states was replaced by a kuramoto oscillator-entrainment of plasmon pairs inside the lwsc. a order parameter of a lwsc state, which was defined as a mean field of a plasmon pairs, takes on the significant value, which clearly indicates that the macroscopic number of plasmon pairs occupy the single momentum state with an identical phase inside a lwsc. a emergence of spatiotemporal coherence and a decrease inside a phase randomization are considered as development of long-range order and spontaneous symmetry breaking, respectively, indicating that a lwt-lwsc transition was the second order phase transition phenomenon. by this transition, u(1) symmetry of lwt was broken. a lwsc is, inside terms of plasma physics, the so-called bernstein-greene-kruskal mode characterized by its undamped nature.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10395,"inside this paper, we discuss how the suitable family of tensor kernels should be used to efficiently solve nonparametric extensions of $\ell^p$ regularized learning methods. our main contribution was proposing the fast dual algorithm, and showing that it allows to solve a problem efficiently. our results contrast recent findings suggesting kernel methods cannot be extended beyond hilbert setting. numerical experiments confirm a effectiveness of a method.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0
11898,"hypothetical exomoons around close-in giant planets may migrate inwards and/or outwards inside virtue of a interplay of a star, planet and moon tidal interactions. these processes could be responsible considering a disruption of lunar systems, a collision of moons with planets or could provide the mechanism considering a formation of exorings. several models have been developed to determine a fate of exomoons when subject to a tidal effects of their host planet. none of them have taken into account a key role that planetary evolution could play inside this process. inside this paper we put together numerical models of exomoon tidal-induced orbital evolution, results of planetary evolution and interior structure models, to study a final fate of exomoons around evolving close-in gas giants. we have found that planetary evolution significantly affects not only a time-scale of exomoon migration but also its final fate. thus, if any change inside planetary radius, internal mass distribution and rotation occurs inside time-scales lower or comparable to orbital evolution, exomoon may only migrate outwards and prevent tidal disruption or the collision with a planet. if exomoons are discovered inside a future around close-in giant planets, our results may contribute to constraint planetary evolution and internal structure models.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
637,"suppose that we have the compact k√§hler manifold $x$ with the very ample line bundle $\mathcal{l}$. we prove that any positive definite hermitian form on a space $h^0 (x,\mathcal{l})$ of holomorphic sections should be written as an $l^2$-inner product with respect to an appropriate hermitian metric on $\mathcal{l}$. we apply this result to show that a fubini--study map, which associates the hermitian metric on $\mathcal{l}$ to the hermitian form on $h^0 (x,\mathcal{l})$, was injective.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10719,"inside this paper, we present the novel idea behind the method to identify a generators and states responsible considering a small-signal stability of power networks. to this end, a newly developed notion of information transfer between a states of the dynamical system was used. inside particular, with the help of a concept of information transfer, which characterizes influence between a various states and the linear combination of states of the dynamical system, we identify a generators and states which are responsible considering causing instability of a power network. while characterizing influence from state to state, information transfer should also describe influence from state to modes thereby generalizing a well-known notion of participation factor while at a same time overcoming some of a limitations of a participation factor. a developed framework was applied to study a three bus system identifying various cause of instabilities inside a system. a simulation study was extended to ieee 39 bus system.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
19918,"background: considering newborn infants inside critical care, continuous monitoring of brain function should aid identify infants at-risk of brain injury. quantitative features allow the consistent and reproducible idea behind the method to eeg analysis, but only when all implementation aspects are clearly defined. methods: we detail quantitative features frequently used inside neonatal eeg analysis and present the matlab software package together with exact implementation details considering all features. a feature set includes stationary features that capture amplitude and frequency characteristics and features of inter-hemispheric connectivity. a software, the neonatal eeg feature set inside matlab (neural), was open source and freely available. a software also includes the pre-processing stage with the basic artefact removal procedure. conclusions: neural provides the common platform considering quantitative analysis of neonatal eeg. this will support reproducible research and enable comparisons across independent studies. these features present summary measures of a eeg that should also be used inside automated methods to determine brain development and health of a newborn inside critical care.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6332,"we report superconductive iridium pnictides baxir4x12 (x = as and p) with the filled skutterudite structure, demonstrating that ba filling dramatically alters their electronic properties and induces the nonmetal-to-metal transition with increasing a ba content x. a highest superconducting transition temperatures are 4.8 and 5.6 k observed considering baxir4as12 and baxir4p12, respectively. a superconductivity inside baxir4x12 should classified into a bardeen-cooper-schrieffer type with intermediate coupling.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
5981,we study constrained 2-dimensional navier-stokes equations driven by the multiplicative gaussian noise inside a stratonovich form. inside a deterministic case [4] we showed a existence of global solutions only on the two dimensional torus and thus we concentrated on such the case here. we prove a existence of the martingale solution and later with the help of schmalfuss idea [20] we show a pathwise uniqueness of a solutions. we also establish a existence of the strong solution with the help of the yamada-watanabe type result from ondrej√°t [17].,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10094,"this paper describes the novel activity to model a dynamics of the jupiter-mass, trans-neptunian planet of the highly eccentric orbit. despite the history rooted inside modern astronomy, ""planet x"", the hypothesised hidden planet lurking inside our outer solar system, has often been touted by conspiracy theorists as a cause of past mass extinction events on earth, as well as other modern-day doomsday scenarios. frequently dismissed as pseudoscience by astronomers, these stories continue to draw a attention of a public by provoking mass media coverage. targeted at junior undergraduate levels, this activity allows students to debunk some of a myths surrounding planet x by with the help of simulation software to demonstrate that such the large-mass planet with extreme eccentricity would be unable to enter our solar system unnoticed, let alone maintain the stable orbit.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
12682,"we present an idea behind the method towards convex optimization that relies on the novel scheme which converts online adaptive algorithms into offline methods. inside a offline optimization setting, our derived methods are shown to obtain favourable adaptive guarantees which depend on a harmonic sum of a queried gradients. we further show that our methods implicitly adapt to a objective's structure: inside a smooth case fast convergence rates are ensured without any prior knowledge of a smoothness parameter, while still maintaining guarantees inside a non-smooth setting. our idea behind the method has the natural extension to a stochastic setting, resulting inside the lazy version of sgd (stochastic gd), where minibathces are chosen \emph{adaptively} depending on a magnitude of a gradients. thus providing the principled idea behind the method towards choosing minibatch sizes.",1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0
17198,"making a right decision inside traffic was the challenging task that was highly dependent on individual preferences as well as a surrounding environment. therefore it was hard to model solely based on expert knowledge. inside this work we use deep reinforcement learning to learn maneuver decisions based on the compact semantic state representation. this ensures the consistent model of a environment across scenarios as well as the behavior adaptation function, enabling on-line changes of desired behaviors without re-training. a input considering a neural network was the simulated object list similar to that of radar or lidar sensors, superimposed by the relational semantic scene description. a state as well as a reward are extended by the behavior adaptation function and the parameterization respectively. with little expert knowledge and the set of mid-level actions, it should be seen that a agent was capable to adhere to traffic rules and learns to drive safely inside the variety of situations.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
6103,"we present a discovery and measurements of the gravitationally lensed supernova (sn) behind a galaxy cluster moo j1014+0038. based on multi-band hubble space telescope and very large telescope (vlt) photometry of a supernova, and vlt spectroscopy of a host galaxy, we find the 97.5% probability that this sn was the sn ia, and the 2.5% chance of the cc sn. our typing algorithm combines a shape and color of a light curve with a expected rates of each sn type inside a host galaxy. with the redshift of 2.2216, this was a highest redshift sn ia discovered with the spectroscopic host-galaxy redshift. the further distinguishing feature was that a lensing cluster, at redshift 1.23, was a most distant to date to have an amplified sn. a sn lies inside a middle of a color and light-curve shape distributions found at lower redshift, disfavoring strong evolution to z = 2.22. we approximate an amplification due to gravitational lensing of 2.8+0.6-0.5 (1.10 +- 0.23 mag)---compatible with a value estimated from a weak-lensing-derived mass and a mass-concentration relation from lambdacdm simulations---making it a most amplified sn ia discovered behind the galaxy cluster.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10000,"over a past couple of years, anecdotal evidence has emerged linking coordinated campaigns by state-sponsored actors with efforts to manipulate public opinion on a web, often around major political events, through dedicated accounts, or ""trolls."" although they are often involved inside spreading disinformation on social media, there was little understanding of how these trolls operate, what type of content they disseminate, and most importantly their influence on a information ecosystem. inside this paper, we shed light on these questions by analyzing 27k tweets posted by 1k twitter users identified as having ties with russia's internet research agency and thus likely state-sponsored trolls. we compare their behavior to the random set of twitter users, finding interesting differences inside terms of a content they disseminate, a evolution of their account, as well as their general behavior and use of a twitter platform. then, with the help of the statistical model known as hawkes processes, we quantify a influence that these accounts had on a dissemination of news on social platforms such as twitter, reddit, and 4chan. overall, our findings indicate that russian troll accounts managed to stay active considering long periods of time and to reach the substantial number of twitter users with their messages. when looking at their ability of spreading news content and making it viral, however, we find that their effect on social platforms is minor, with a significant exception of news published by a russian state-sponsored news outlet rt (russia today).",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
19509,"we study sparse high-dimensional negative binomial regression problem considering count data regression by showing non-asymptotic merits of a elastic-net regularized estimator. with a kkt conditions, we derive two types of non-asymptotic oracle inequalities considering a elastic net estimates of negative binomial regression by utilizing compatibility factor and stabil condition, respectively. based on oracle inequalities we proposed, we firstly show a sign consistency property of a elastic-net estimators provided that a non-zero components inside sparse true vector are large than the proper choice of a weakest signal detection threshold, and a second application was that we give an oracle inequality considering bounding a grouping effect with high probability, thirdly, under some assumptions of design matrix, we should recover a true variable set with high probability if a weakest signal detection threshold was large than 3 times a value of turning parameter, at last, we briefly discuss a de-biased elastic-net estimator.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0
9692,"we study the distributed learning process observed inside human groups and other social animals. this learning process appears inside settings inside which each individual inside the group was trying to decide over time, inside the distributed manner, which option to select among the shared set of options. specifically, we consider the stochastic dynamics inside the group inside which every individual selects an option inside a following two-step process: (1) select the random individual and observe a option that individual chose inside a previous time step, and (2) adopt that option if its stochastic quality is good at that time step. various instantiations of such distributed learning appear inside nature, and have also been studied inside a social science literature. from a perspective of an individual, an attractive feature of this learning process was that it was the simple heuristic that requires extremely limited computational capacities. but what does it mean considering a group -- could such the simple, distributed and essentially memoryless process lead a group as the whole to perform optimally? we show that a answer to this question was yes -- this distributed learning was highly effective at identifying a best option and was close to optimal considering a group overall. our analysis also gives quantitative bounds that show fast convergence of these stochastic dynamics. prior to our work a only theoretical work related to such learning dynamics has been either inside deterministic special cases or inside a asymptotic setting. finally, we observe that our infinite population dynamics was the stochastic variant of a classic multiplicative weights update (mwu) method. consequently, we arrive at a following interesting converse: a learning dynamics on the finite population considered here should be viewed as the novel distributed and low-memory implementation of a classic mwu method.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14759,"we derive a markov process equivalent to she-leveque scaling inside homogeneous and isotropic turbulence. a markov process was the jump process considering velocity increments $u(r)$ inside scale $r$ inside which a jumps occur randomly but with deterministic width inside $u$. from its master equation we establish the prescription to simulate a she-leveque process and compare it with kolmogorov scaling. to put a she-leveque process into a context of other established turbulence models on a markov level, we derive the diffusion process considering $u(r)$ from two properties of a navier-stokes equation. this diffusion process already includes kolmogorov scaling, extended self-similarity and the class of random cascade models. a fluctuation theorem of this markov process implies the ""second law"" that puts the loose bound on a multipliers of a random cascade models. this bound explicitly allows considering inverse cascades, which are necessary to satisfy a fluctuation theorem. by adding the jump process to a diffusion process, we go beyond kolmogorov scaling and formulate a most general scaling law considering a class of markov processes having both diffusion and jump parts. this markov scaling law includes she-leveque scaling and the scaling law derived by yakhot.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19710,"inside this paper, we reconsider the circular cylinder horizontally floating on an unbounded reservoir inside the gravitational field directed downwards, which is studied by bhatnargar and finn inside 2006. we follow their idea behind the method but with some modifications. we establish a relation between a total energy relative to a undisturbed state and a total force. there was the monotone relation between a height of a centre and a wetting angle. we study a number of equilibria, a floating configurations and their stability considering all parameter values. we find that a system admits at most two equilibrium points considering arbitrary contact angle, a one with smaller wetting angle was stable and a one with larger wetting angle was unstable. a initial model has the limitation that a fluid interfaces may intersect. we show that a stable equilibrium point never lies inside a intersection region, while a unstable equilibrium point may lie inside a intersection region.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
547,"we study a realization of acyclic cluster algebras as coordinate rings of coxeter double bruhat cells inside kac-moody groups. we prove that all cluster monomials with g-vector lying inside a doubled cambrian fan are restrictions of principal generalized minors. as the corollary, cluster algebras of finite and affine type admit the complete and non-recursive description using (ind-)algebraic group representations, inside the way similar inside spirit to a caldero-chapoton description using quiver representations. inside type a_1^{(1)}, we further show that elements of several canonical bases (generic, triangular, and theta) which complete a partial basis of cluster monomials are composed entirely of restrictions of minors. a discrepancy among these bases was accounted considering by continuous parameters appearing inside a classification of irreducible level-zero representations of affine lie groups. we discuss how our results illuminate certain parallels between a classification of representations of finite-dimensional algebras and of integrable weight representations of kac-moody algebras.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
334,"inside this paper, we study a problem of secret key generation from both correlated sources and the secure channel. we obtain a optimal secret key rate inside this problem and show that a optimal scheme was to conduct secret key generation and key distribution jointly, where every bit inside a secret channel will yield more than one bit of secret key rate. this joint scheme was better than a separation-based scheme, where a secure channel was used considering key distribution, and as the result, every bit inside a secure channel should only provide one bit of secret key rate.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
8096,"inside this article we study considering $p\in (1,\infty)$ a $l^p$-realization of a vector-valued schr√∂dinger operator $\mathcal{l}u := \mathrm{div} (q\nabla u) + v u$. with the help of the noncommutative version of a dore-venni theorem due to monniaux and pr√ºss, we prove that a $l^p$-realization of $\mathcal{l}$, defined on a intersection of a natural domains of a differential and multiplication operators which form $\mathcal{l}$, generates the strongly continuous contraction semigroup on $l^p(\mathbb{r}^d; \mathbb{r}^m)$. we also study additional properties of a semigroup such as extension to $l^1$, positivity, ultracontractivity and prove that a generator has compact resolvent.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11169,"inside this paper, we study stochastic coupon probing problem inside social networks. assume there was the social network and the set of coupons. we should offer coupons to some users adaptively and those users who accept a offer will act as seeds and influence their friends inside a social network. there are two constraints which are called a inner and outer constraints, respectively. a set of coupons redeemed by users must satisfy inner constraints, and a set of all probed users must satisfy outer constraints. one seeks to develop the coupon probing policy that achieves a maximum influence while satisfying both inner and outer constraints. our main result was the constant approximation policy considering a stochastic coupon probing problem considering any monotone submodular utility function.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
16654,"many tourist applications provide the personalized tourist agenda with a list of recommended activities to a user. these applications must undoubtedly deal with a constraints and preferences that define a user interests. among these preferences, we should find those that define a travel style of a user, such as a rhythm of a trip, a number of visits to include inside a tour or a priority to visits of special interest considering a user. inside this paper, we deal with a task of creating the customized tourist agenda as the planning and scheduling application capable of conveniently scheduling a most appropriate goals (visits) so as to maximize a user satisfaction with a tourist route. this paper makes an analysis of a meaning of a travel style preferences and compares a quality of a solutions obtained by two different solvers, the pddl-based planner and the constraint satisfaction problem solver. we also define several quality metrics and perform extensive experiments inside order to evaluate a results obtained with both solvers.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4854,"mexico city tracks ground-level ozone levels to assess compliance with national ambient air quality standards and to prevent environmental health emergencies. ozone levels show distinct daily patterns, within a city, and over a course of a year. to model these data, we use covariance models over space, circular time, and linear time. we review existing models and develop new classes of nonseparable covariance models of this type, models appropriate considering quasi-periodic data collected at many locations. with these covariance models, we use nearest-neighbor gaussian processes to predict hourly ozone levels at unobserved locations inside april and may, a peak ozone season, to infer compliance to mexican air quality standards and to approximate respiratory health risk associated with ozone. predicted compliance with air quality standards and estimated respiratory health risk vary greatly over space and time. inside some regions, we predict exceedance of national standards considering more than the third of a hours inside april and may. on many days, we predict that nearly all of mexico city exceeds nationally legislated ozone thresholds at least once. inside peak regions, we approximate respiratory risk considering ozone to be 55% higher on average than a annual average risk and as much at 170% higher on some days.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10194,"inside this paper we address a problem of electing the committee among the set of $m$ candidates and on a basis of a preferences of the set of $n$ voters. we consider a approval voting method inside which each voter should approve as many candidates as she/he likes by expressing the preference profile (boolean $m$-vector). inside order to elect the committee, the voting rule must be established to `transform' a $n$ voters' profiles into the winning committee. a problem was widely studied inside voting theory; considering the variety of voting rules a problem is shown to be computationally difficult and approximation algorithms and heuristic techniques were proposed inside a literature. inside this paper we follow an ordered weighted averaging idea behind the method and study a $k$-sum approval voting (optimization) problem inside a general case $1 \leq k <n$. considering this problem we provide different mathematical programming formulations that allow us to solve it inside an exact solution framework. we provide computational results showing that our idea behind the method was efficient considering medium-size test problems ($n$ up to 200, $m$ up to 60) since inside all tested cases it is able to find a exact optimal solution inside very short computational times.",1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
11891,"mote_2, with a orthorhombic t_d phase, was the new type (type-ii) of weyl semimetal, where a weyl fermions emerge at a boundary between electron and hole pockets. non-saturating magnetoresistance (mr), and superconductivity were also observed inside t_d-mote_2. understanding a superconductivity inside t_d-mote_2, which is proposed to be topologically non-trivial, was of eminent interest. here, we report high-pressure (p_max = 1.3 gpa) muon spin rotation experiments on a temperature-dependent magnetic penetration depth inside t_d-mote_2. the substantial increase of a superfluid density n_s/m^* and the linear scaling with t_c was observed under pressure. moreover, a superconducting order parameter inside t_d-mote_2 was determined to be two gap (s+s)-wave symmetric. we also excluded time reversal symmetry breaking inside a sc state with sensitive zero-field ${\mu}$sr experiments. considering a previous report \cite{balicas1} on a strong suppression of t_c inside t_d-mote_2 by disorder, we suggest that s^{+-} (topological order parameter) state was more likely to be realized inside mote_2 than a s^{++} (trivial) state. should s^{+-} be a sc gap symmetry, a t_d-mote_2 is, to our knowledge, a first known example of the time reversal invariant topological (weyl) superconductor.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0
8077,"we formulate the general criterion considering a exact preservation of a ""lake at rest"" solution inside general mesh-based and meshless numerical schemes considering a strong form of a shallow-water equations with bottom topography. a main idea was the careful mimetic design considering a spatial derivative operators inside a momentum flux equation that was paired with the compatible averaging rule considering a water column height arising inside a bottom topography source term. we prove consistency of a mimetic difference operators analytically and demonstrate a well-balanced property numerically with the help of finite difference and rbf-fd schemes inside a one- and two-dimensional cases.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16520,"the fast full-wave simulation technique was presented considering a analysis of large irregular planar arrays of identical 3-d metallic antennas. a solution method relies on a macro basis functions (mbf) idea behind the method and an interpolatory technique to compute a interactions between mbfs. a harmonic-polynomial (harp) model was established considering a near-field interactions inside the modified system of coordinates. considering extremely large arrays made of complex antennas, two approaches assuming the limited radius of influence considering mutual coupling are considered: one was based on the sparse-matrix lu decomposition and a other one on the tessellation of a array inside a form of overlapping sub-arrays. a computation of all embedded element patterns was sped up with a aid of a non-uniform fft algorithm. extensive validations are shown considering arrays of log-periodic antennas envisaged considering a low-frequency ska (square kilometer array) radio-telescope. a analysis of ska stations with such the large number of elements has not been treated yet inside a literature. validations include comparison with results obtained with commercial software and with experiments. a proposed method was particularly well suited to array synthesis, inside which several orders of magnitude should be saved inside terms of computation time.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
19003,"a promise of learning to learn considering robotics rests on a hope that by extracting some information about a learning process itself we should speed up subsequent similar learning tasks. here, we introduce the computationally efficient online meta-learning algorithm that builds and optimizes the memory model of a optimal learning rate landscape from previously observed gradient behaviors. while performing task specific optimization, this memory of learning rates predicts how to scale currently observed gradients. after applying a gradient scaling our meta-learner updates its internal memory based on a observed effect its prediction had. our meta-learner should be combined with any gradient-based optimizer, learns on a fly and should be transferred to new optimization tasks. inside our evaluations we show that our meta-learning algorithm speeds up learning of mnist classification and the variety of learning control tasks, either inside batch or online learning settings.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19907,"mammogram classification was directly related to computer-aided diagnosis of breast cancer. traditional methods rely on regions of interest (rois) which require great efforts to annotate. inspired by a success of with the help of deep convolutional features considering natural image analysis and multi-instance learning (mil) considering labeling the set of instances/patches, we propose end-to-end trained deep multi-instance networks considering mass classification based on whole mammogram without a aforementioned rois. we explore three different schemes to construct deep multi-instance networks considering whole mammogram classification. experimental results on a inbreast dataset demonstrate a robustness of proposed networks compared to previous work with the help of segmentation and detection annotations.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
289,"we study a following problem: considering given integers $d$, $k$ and graph $g$, should we reduce some fixed graph parameter $\pi$ of $g$ by at least $d$ using at most $k$ graph operations from some fixed set $s$? as parameters we take a chromatic number $\chi$, clique number $\omega$ and independence number $\alpha$, and as operations we choose a edge contraction ec and vertex deletion vd. we determine a complexity of this problem considering $s=\{\mbox{ec}\}$ and $s=\{\mbox{vd}\}$ and $\pi\in \{\chi,\omega,\alpha\}$ considering the number of subclasses of perfect graphs. we use these results to determine a complexity of a problem considering $s=\{\mbox{ec}\}$ and $s=\{\mbox{vd}\}$ and $\pi\in \{\chi,\omega,\alpha\}$ restricted to $h$-free graphs.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4033,"mid-ir colour selection techniques have proved to be very efficient inside finding agn. this was because a agn heats a surrounding dust producing warm mid-ir colours. with the help of a wise 3.6, 4.5 and 12 $\mu m$ colours, a largest sample of ir selected agn has already been produced containing 1.4 million agn over a whole sky. here, we explore a x-ray properties of this agn sample by cross-correlating it with a subsample of a 3xmm x-ray catalogue that has available x-ray spectra and at a same time optical spectroscopy from sdss. our goal was to find rare luminous obscured agn. our final sample contains 65 qsos with $\rm{log}\,\nu l_\nu \ge 46.2$\,erg\,s$^{-1}$. this ir luminosity cut corresponds to $\rm{log}\,l_x \approx 45$\,erg\,s$^{-1}$, at a median redshift of our sample ($z=2.3$), that lies at a bright end of a x-ray luminosity function at $z>2$. a x-ray spectroscopic analysis reveals seven obscured agn having the column density $\rm n_h>10^{22} cm^{-2}$. six of them show evidence considering broad [civ] absorption lines and five are classified as balqsos. we fit a optical spectra of our x-ray absorbed sources to approximate a optical reddening. we find that none of these show any obscuration according to a optical continuum. these sources add to a growing evidence considering populations of luminous qsos with evidence considering substantial absorption by outflowing ionised material, similar to those expected to be emerging from their absorbing cocoons inside a framework of agn/galaxy co-evolution.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9427,"we investigate a partitioning of partial orders into the minimal number of heapable subsets. we prove the characterization result reminiscent of a proof of dilworth's theorem, which yields as the byproduct the flow-based algorithm considering computing such the minimal decomposition. on a other hand, inside a particular case of sets and sequences of intervals we prove that this minimal decomposition should be computed by the simple greedy-type algorithm. a paper ends with the couple of open problems related to a analog of a ulam-hammersley problem considering decompositions of sets and sequences of random intervals into heapable sets.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11725,we give the construction of a second chern number of the vector bundle over the smooth projective surface by means of adelic transition matrices considering a vector bundle. a construction does not use an algebraic $k$-theory and depends on a canonical $\mathbb{z}$-torsor of the locally linearly compact $k$-vector space. analogs of certain auxiliary results considering a case of an arithmetic surface are also discussed.,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
17592,"we analyze a intracluster medium (icm) and circumgalactic medium (cgm) inside 7 x-ray detected galaxy clusters with the help of spectra of background qsos (hst-cos/stis), optical spectroscopy of a cluster galaxies (mmt/hectospec and sdss), and x-ray imaging/spectroscopy (xmm-newton and chandra). first, we report the very low covering fraction of h i absorption inside a cgm of these cluster galaxies, f_c = 25% +25%/-15%, to stringent detection limits (log n(h i) < 13 cm^-2). as field galaxies have an h i covering fraction of ~100% at similar radii, a dearth of cgm h i inside our data indicates that a cluster environment has effectively stripped or overionized a gaseous halos of these cluster galaxies. second, we assess a contribution of warm-hot (10^5 - 10^6 k) gas to a icm as traced by o vi and broad ly-alpha (bla) absorption. despite a high signal-to-noise of our data, we do not detect o vi inside any cluster, and we only detect bla features inside a qso spectrum probing one cluster. we approximate that a total column density of warm-hot gas along this line of sight totals to ~3% of that contained inside a hot t > 10^7 k x-ray emitting phase. residing at high relative velocities, these features may trace pre-shocked material outside a cluster. comparing gaseous galaxy halos from a low-density 'field' to galaxy groups and high-density clusters, we find that a cgm was progressively depleted of h i with increasing environmental density, and a cgm was most severely transformed inside galaxy clusters. this cgm transformation may play the key role inside environmental galaxy quenching.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14177,"landau damping was a tendency of solutions to a vlasov equation towards spatially homogeneous distribution functions. a distribution functions however idea behind the method a spatially homogeneous manifold only weakly, and boltzmann entropy was not changed by vlasov equation. on a other hand, density and kinetic energy density, which are integrals of a distribution function, idea behind the method spatially homogeneous states strongly, which was accompanied by growth of a hydrodynamic entropy. such the behavior should be seen when vlasov equation was reduced to a evolution equations considering density and kinetic energy density by means of a ehrenfest reduction.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4851,"we provide the new bound on a amplitude of primordial magnetic fields (pmfs) by with the help of the novel mechanism, named {\it magnetic reheating}. before a epoch of recombination, pmfs induce a fluid motions inside the photon-baryon plasma through a lorentz force. due to a viscosity inside a plasma, such induced fluid motions would be damped and this means a dissipation of pmfs. inside a early universe with $z \gtrsim 2 \times 10^6$, cosmic microwave background (cmb) photons are quickly thermalized with a dissipated energy and shift to the different planck distribution with the new temperature. inside other words, a energy injection due to a dissipation of pmfs changes a baryon-photon number ratio during this era and we name such the process {\it magnetic reheating}. by with the help of a current results of a baryon-photon number ratio obtained from a big bang nucleosynthesis and cmb observations, we put the strongest constraint on a amplitude of pmfs on small scales which we should not access through cmb anisotropy and cmb distortions, $b_{0} \lesssim 1.0 \; \mu{\rm g}$ at a scales $10^{4} \; h{\rm mpc}^{-1} < k < 10^{8} \; h{\rm mpc}^{-1}$. moreover, when a spectrum of pmfs was given by a power-law, a magnetic reheating puts the quite strong constraint inside a case of a blue-tilted spectrum, considering example, $b_0 \lesssim 10^{-17} \;{\rm ng}$, $10^{-23} \;{\rm ng}$, and $10^{-29} \;{\rm ng}$ at 1~comoving mpc considering $n_{b}=1.0$, $2.0$, and $3.0$, respectively. this constraint would give an impact on generation mechanisms of pmfs inside a early universe.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14399,"we describe inferactive data analysis, so-named to denote an interactive idea behind the method to data analysis with an emphasis on inference after data analysis. our idea behind the method was the compromise between tukey's exploratory (roughly speaking ""model free"") and confirmatory data analysis (roughly speaking classical and ""model based""), also allowing considering bayesian data analysis. we view this idea behind the method as close inside spirit to current practice of applied statisticians and data scientists while allowing frequentist guarantees considering results to be reported inside a scientific literature, or bayesian results where a data scientist may choose a statistical model (and thus a prior) after some initial exploratory analysis. while this idea behind the method to data analysis does not cover every scenario, and every possible algorithm data scientists may use, we see this as the useful step inside concrete providing tools (with frequentist statistical guarantees) considering current data scientists. a basis of inference we use was selective inference [lee et al., 2016, fithian et al., 2014], inside particular its randomized form [tian and taylor, 2015a]. a randomized framework, besides providing additional power and shorter confidence intervals, also provides explicit forms considering relevant reference distributions (up to normalization) through a {\em selective sampler} of tian et al. [2016]. a reference distributions are constructed from the particular conditional distribution formed from what we call the dag-dag -- the data analysis generative dag. as sampling conditional distributions inside dags was generally complex, a selective sampler was crucial to any practical implementation of inferactive data analysis. our principal goal was inside reviewing a recent developments inside selective inference as well as describing a general philosophy of selective inference.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
2680,"we consider a problem of detecting the deformation from the symmetric gaussian random $p$-tensor $(p\geq 3)$ with the rank-one spike sampled from a rademacher prior. recently inside lesieur et al. (2017), it is proved that there exists the critical threshold $\beta_p$ so that when a signal-to-noise ratio exceeds $\beta_p$, one should distinguish a spiked and unspiked tensors and weakly recover a prior using a minimal mean-square-error method. on a other side, perry, wein, and bandeira (2017) proved that there exists the $\beta_p'<\beta_p$ such that any statistical hypothesis test should not distinguish these two tensors, inside a sense that their total variation distance asymptotically vanishes, when a signa-to-noise ratio was less than $\beta_p'$. inside this work, we show that $\beta_p$ was indeed a critical threshold that strictly separates a distinguishability and indistinguishability between a two tensors under a total variation distance. our idea behind the method was based on the subtle analysis of a high temperature behavior of a pure $p$-spin model with ising spin, arising initially from a field of spin glasses. inside particular, we identify a signal-to-noise criticality $\beta_p$ as a critical temperature, distinguishing a high and low temperature behavior, of a ising pure $p$-spin mean-field spin glass model.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
10690,"$^{121,123}sb$ nuclear quadrupole resonance (nqr) is applied to $fe(sb_{1-x}te_x)_2$ inside a low doping regime (\emph{x = 0, 0.01} and \emph{0.05}) as the microscopic zero field probe to study a evolution of \emph{3d} magnetism and a emergence of metallic behavior. whereas a nqr spectra itself reflects a degree of local disorder using a width of a individual nqr lines, a spin lattice relaxation rate (slrr) $1/t_1(t)$ probes a fluctuations at a $sb$ - site. a fluctuations originate either from conduction electrons or from magnetic moments. inside contrast to a semi metal $fesb_2$ with the clear signature of a charge and spin gap formation inside $1/t_1(t)t ( \sim exp/ (\delta k_bt) ) $, a 1\% $te$ doped system exhibits almost metallic conductivity and the almost filled gap. the weak divergence of a slrr coefficient $1/t_1(t)t \sim t^{-n} \sim t^{-0.2}$ points towards a presence of electronic correlations towards low temperatures wheras a \textit{5\%} $te$ doped sample exhibits the much larger divergence inside a slrr coefficient showing $1/t_1(t)t \sim t^{-0.72} $. according to a specific heat divergence the power law with $n\ =\ 2\ m\ =\ 0.56$ was expected considering a slrr. furthermore $te$-doped $fesb_2$ as the disordered paramagnetic metal might be the platform considering a electronic griffith phase scenario. nqr evidences the substantial asymmetric broadening of a $^{121,123}sb$ nqr spectrum considering a \emph{5\%} sample. this has purely electronic origin inside agreement with a electronic griffith phase and stems probably from an enhanced $sb$-$te$ bond polarization and electronic density shift towards a $te$ atom in $sb$-$te$ dumbbell.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
14897,"inside many settings, it was important that the model be capable of providing reasons considering its predictions (i.e., a model must be interpretable). however, a model's reasoning may not conform with well-established knowledge. inside such cases, while interpretable, a model lacks \textit{credibility}. inside this work, we formally define credibility inside a linear setting and focus on techniques considering learning models that are both accurate and credible. inside particular, we propose the regularization penalty, expert yielded estimates (eye), that incorporates expert knowledge about well-known relationships among covariates and a outcome of interest. we give both theoretical and empirical results comparing our proposed method to several other regularization techniques. across the range of settings, experiments on both synthetic and real data show that models learned with the help of a eye penalty are significantly more credible than those learned with the help of other penalties. applied to the large-scale patient risk stratification task, our proposed technique results inside the model whose top features overlap significantly with known clinical risk factors, while still achieving good predictive performance.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6070,"image denoising techniques are essential to reducing noise levels and enhancing diagnosis reliability inside low-dose computed tomography (ct). machine learning based denoising methods have shown great potential inside removing a complex and spatial-variant noises inside ct images. however, some residue artifacts would appear inside a denoised image due to complexity of noises. the cascaded training network is proposed inside this work, where a trained cnn is applied on a training dataset to initiate new trainings and remove artifacts induced by denoising. the cascades of convolutional neural networks (cnn) were built iteratively to achieve better performance with simple cnn structures. experiments were carried out on 2016 low-dose ct grand challenge datasets to evaluate a method's performance.",1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5741,"polar was space-borne detector designed considering the precise measurement of gamma-ray polarization of a prompt emissions of gamma-ray bursts inside a energy range 50 kev - 500 kev. polar was the compact compton polarimeter consisting of 40$\times$ 40 plastic scintillator bars read out by 25 multi-anode pmts. inside may 2015, we performed the series of tests of a polar flight model with 100\% polarized x-rays beams at a european synchrotron radiation facility beam-line id11 aming to study thresholds, crosstalk between channels and responses of polar flight model to polarized x-ray beams. inside this paper we present a data analysis method and some analysis results. according a results, polar fm has good polarimetric capabilities.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
10571,"randomized trials, also known as a/b tests, are used to select between two policies: the control and the treatment. given the corresponding set of features, we should ideally learn an optimized policy p that maps a a/b test data features to action space and optimizes reward. however, although a/b testing provides an unbiased estimator considering a value of deploying b (i.e., switching from policy the to b), direct application of those samples to learn a a optimized policy p generally does not provide an unbiased estimator of a value of p as a samples were observed when constructing p. inside situations where a cost and risks associated of deploying the policy are high, such an unbiased estimator was highly desirable. we present the procedure considering learning optimized policies and getting unbiased estimates considering a value of deploying them. we wrap any policy learning procedure with the bagging process and obtain out-of-bag policy inclusion decisions considering each sample. we then prove that inverse-propensity-weighting effect estimator was unbiased when applied to a optimized subset. likewise, we apply a same idea to obtain out-of-bag unbiased per-sample value approximate of a measurement that was independent of a randomized treatment, and use these estimates to build an unbiased doubly-robust effect estimator. lastly, we empirically shown that even when a average treatment effect was negative we should find the positive optimized policy.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5954,"we describe the general framework --compressive statistical learning-- considering resource-efficient large-scale learning: a training collection was compressed inside one pass into the low-dimensional sketch (a vector of random empirical generalized moments) that captures a information relevant to a considered learning task. the near-minimizer of a risk was computed from a sketch through a solution of the nonlinear least squares problem. we investigate sufficient sketch sizes to control a generalization error of this procedure. a framework was illustrated on compressive clustering, compressive gaussian mixture modeling with fixed known variance, and compressive pca.",1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0
10313,"inside this paper we study a problem of learning the shallow artificial neural network that best fits the training data set. we study this problem inside a over-parameterized regime where a number of observations are fewer than a number of parameters inside a model. we show that with quadratic activations a optimization landscape of training such shallow neural networks has certain favorable characteristics that allow globally optimal models to be found efficiently with the help of the variety of local search heuristics. this result holds considering an arbitrary training data of input/output pairs. considering differentiable activation functions we also show that gradient descent, when suitably initialized, converges at the linear rate to the globally optimal model. this result focuses on the realizable model where a inputs are chosen i.i.d. from the gaussian distribution and a labels are generated according to planted weight coefficients.",1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0
18592,"a objective assessment of a prestige of an academic institution was the difficult and hotly debated task. inside a last few years, different types of university rankings have been proposed to quantify a excellence of different research institutions inside a world. albeit met with criticism inside some cases, a relevance of university rankings was being increasingly acknowledged: indeed, rankings are having the major impact on a design of research policies, both at a institutional and governmental level. yet, a debate on what rankings are {\em exactly} measuring was enduring. here, we address a issue by measuring the quantitive and reliable proxy of a academic reputation of the given institution and by evaluating its correlation with different university rankings. specifically, we study citation patterns among universities inside five different web of science subject categories and use a \pr~algorithm on a five resulting citation networks. a rationale behind our work was that scientific citations are driven by a reputation of a reference so that a pagerank algorithm was expected to yield the rank which reflects a reputation of an academic institution inside the specific field. our results allow to quantifying a prestige of the set of institutions inside the certain research field based only on hard bibliometric data. given a volume of a data analysed, our findings are statistically robust and less prone to bias, at odds with ad--hoc surveys often employed by ranking bodies inside order to attain similar results. because our findings are found to correlate extremely well with a arwu subject rankings, a idea behind the method we propose inside our paper may open a door to new, academic ranking methodologies that go beyond current methods by reconciling a qualitative evaluation of academic prestige with its quantitative measurements using publication impact.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
13947,"motivated by a stellar fossil record of local group (lg) dwarf galaxies, we show that a star-forming ancestors of a faintest ultra-faint dwarf galaxies (ufds; ${\rm m}_{\rm v}$ $\sim -2$ or ${\rm m}_{\star}$ $\sim 10^{2}$ at $z=0$) had ultra-violet (uv) luminosities of ${\rm m}_{\rm uv}$ $\sim -3$ to $-6$ during reionization ($z\sim6-10$). a existence of such faint galaxies has substantial implications considering early epochs of galaxy formation and reionization. if a faint-end slopes of a uv luminosity functions (uvlfs) during reionization are steep ($\alpha\lesssim-2$) to ${\rm m}_{\rm uv}$ $\sim -3$, then: (i) a ancestors of ufds produced $>50$% of uv flux from galaxies; (ii) galaxies should maintain reionization with escape fractions that are $>$2 times lower than currently-adopted values; (iii) direct hst and jwst observations may detect only $\sim10-50$% of a uv light from galaxies; (iv) a cosmic star formation history increases by $\gtrsim4-6$ at $z\gtrsim6$. significant flux from ufds, and resultant tensions with lg dwarf galaxy counts, are reduced if a high-redshift uvlf turns over. independent of a uvlf shape, a existence of the large population of ufds requires the non-zero luminosity function to ${\rm m}_{\rm uv}$ $\sim -3$ during reionization.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19054,"a resemblance between a methods used inside quantum-many body physics and inside machine learning has drawn considerable attention. inside particular, tensor networks (tns) and deep learning architectures bear striking similarities to a extent that tns should be used considering machine learning. previous results used one-dimensional tns inside image recognition, showing limited scalability and flexibilities. inside this work, we train two-dimensional hierarchical tns to solve image recognition problems, with the help of the training algorithm derived from a multipartite entanglement renormalization ansatz. this idea behind the method introduces novel mathematical connections among quantum many-body physics, quantum information theory, and machine learning. while keeping a tn unitary inside a training phase, tn states are defined, which optimally encode classes of images into quantum many-body states. we study a quantum features of a tn states, including quantum entanglement and fidelity. we find these quantities could be novel properties that characterize a image classes, as well as a machine learning tasks. our work could contribute to a research on identifying/modeling quantum artificial intelligences.",0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0
4307,"inside this paper, we consider the three-node cooperative wireless powered communication system consisting of the multi-antenna hybrid access point (h-ap) and the single-antenna relay and the single-antenna user. a energy constrained relay and user first harvest energy inside a downlink and then a relay assists a user with the help of a harvested power considering information transmission inside a uplink. a optimal energy beamforming vector and a time split between harvest and cooperation are investigated. to reduce a computational complexity, suboptimal designs are also studied, where closed-form expressions are derived considering a energy beamforming vector and a time split. considering comparison purposes, we also present the detailed performance analysis inside terms of a achievable outage probability and a average throughput of an intuitive energy beamforming scheme, where a h-ap directs all a energy towards a user. a findings of a paper suggest that implementing multiple antennas at a h-ap should significantly improve a system performance, and a closed-form suboptimal energy beamforming vector and time split yields near optimal performance. also, considering a intuitive beamforming scheme, the diversity order of (n+1)/2 should be achieved, where n was a number of antennas at a h-ap.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
2317,"we present a pristine survey, the new narrow-band photometric survey focused on a metallicity-sensitive ca h & k lines and conducted inside a northern hemisphere with a wide-field imager megacam on a canada-france-hawaii telescope (cfht). this paper reviews our overall survey strategy and discusses a data processing and metallicity calibration. additionally we review a application of these data to a main aims of a survey, which are to gather the large sample of a most metal-poor stars inside a galaxy, to further characterise a faintest milky way satellites, and to map a (metal-poor) substructure inside a galactic halo. a current pristine footprint comprises over 1,000 deg2 inside a galactic halo ranging from b~30 to 78 and covers many known stellar substructures. we demonstrate that, considering sdss stellar objects, we should calibrate a photometry at a 0.02-magnitude level. a comparison with existing spectroscopic metallicities from sdss/segue and lamost shows that, when combined with sdss broad-band g and i photometry, we should use a cahk photometry to infer photometric metallicities with an accuracy of ~0.2 dex from [fe/h]=-0.5 down to a extremely metal-poor regime ([fe/h]<-3.0). after a removal of various contaminants, we should efficiently select metal-poor stars and build the very complete sample with high purity. a success rate of uncovering [fe/h]segue<-3.0 stars among [fe/h]pristine<-3.0 selected stars was 24% and 85% of a remaining candidates are still very metal poor ([fe/h]<-2.0). we further demonstrate that pristine was well suited to identify a very rare and pristine galactic stars with [fe/h]<-4.0, which should teach us valuable lessons about a early universe.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
13201,"precession and relaxation predominantly characterize a real-time dynamics of the spin driven by the magnetic field and coupled to the large fermi sea of conduction electrons. we demonstrate an anomalous precession with frequency higher than a larmor frequency or with inverted orientation inside a limit where a electronic motion adiabatically follows a spin dynamics. considering the classical spin, a effect was traced back to the geometrical torque resulting from the finite spin berry curvature.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
14886,"as intelligent systems gain autonomy and capability, it becomes vital to ensure that their objectives match those of their human users; this was known as a value-alignment problem. inside robotics, value alignment was key to a design of collaborative robots that should integrate into human workflows, successfully inferring and adapting to their users' objectives as they go. we argue that the meaningful solution to value alignment must combine multi-agent decision theory with rich mathematical models of human cognition, enabling robots to tap into people's natural collaborative capabilities. we present the solution to a cooperative inverse reinforcement learning (cirl) dynamic game based on well-established cognitive models of decision making and theory of mind. a solution captures the key reciprocity relation: a human will not plan her actions inside isolation, but rather reason pedagogically about how a robot might learn from them; a robot, inside turn, should anticipate this and interpret a human's actions pragmatically. to our knowledge, this work constitutes a first formal analysis of value alignment grounded inside empirically validated cognitive models.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
6847,"we investigate a use of attentional neural network layers inside order to learn the `behavior characterization' which should be used to drive novelty search and curiosity-based policies. a space was structured towards answering the particular distribution of questions, which are used inside the supervised way to train a attentional neural network. we find that inside the 2d exploration task, a structure of a space successfully encodes local sensory-motor contingencies such that even the greedy local `do a most novel action' policy with no reinforcement learning or evolution should explore a space quickly. we also apply this to the high/low number guessing game task, and find that guessing according to a learned attention profile performs active inference and should discover a correct number more quickly than an exact but passive approach.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1567,"with a increasing abundance of 'digital footprints' left by human interactions inside online environments, e.g., social media and app use, a ability to model complex human behavior has become increasingly possible. many approaches have been proposed, however, most previous model frameworks are fairly restrictive. we introduce the new social modeling idea behind the method that enables a creation of models directly from data with minimal the priori restrictions on a model class. inside particular, we infer a minimally complex, maximally predictive representation of an individual's behavior when viewed inside isolation and as driven by the social input. we then apply this framework to the heterogeneous catalog of human behavior collected from fifteen thousand users on a microblogging platform twitter. a models allow us to describe how the user processes their past behavior and their social inputs. despite a diversity of observed user behavior, most models inferred fall into the small subclass of all possible finite-state processes. thus, our work demonstrates that user behavior, while quite complex, belies simple underlying computational structures.",1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
628,"inside this work, motivated by recent manufacturing trends, we investigate autonomous robotic assembly. industrial assembly tasks require contact-rich manipulation skills, which are challenging to acquire with the help of classical control and motion planning approaches. consequently, robot controllers considering assembly domains are presently engineered to solve the particular task, and cannot easily handle variations inside a product or environment. reinforcement learning (rl) was the promising idea behind the method considering autonomously acquiring robot skills that involve contact-rich dynamics. however, rl relies on random exploration considering learning the control policy, which requires many robot executions, and often gets trapped inside locally suboptimal solutions. instead, we posit that prior knowledge, when available, should improve rl performance. we exploit a fact that inside modern assembly domains, geometric information about a task was readily available using a cad design files. we propose to leverage this prior knowledge by guiding rl along the geometric motion plan, calculated with the help of a cad data. we show that our idea behind the method effectively improves over traditional control approaches considering tracking a motion plan, and should solve assembly tasks that require high precision, even without accurate state estimation. inside addition, we propose the neural network architecture that should learn to track a motion plan, and generalize a assembly controller to changes inside a object positions.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
16009,"share valuations are known to adjust to new information entering a market, such as regulatory disclosures. we study whether a language of such news items should improve short-term and especially long-term (24 months) forecasts of stock indices. considering this purpose, this work utilizes predictive models suited to high-dimensional data and specifically compares techniques considering data-driven and knowledge-driven dimensionality reduction inside order to avoid overfitting. our experiments, based on 75,927 ad hoc announcements from 1996-2016, reveal a following results: inside a long run, text-based models succeed inside reducing forecast errors below baseline predictions from historic lags at the statistically significant level. our research provides implications to business applications of decision-support inside financial markets, especially given a growing prevalence of index etfs (exchange traded funds).",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16006,"whilst there are many approaches to detecting changes inside mean considering the univariate time-series, a problem of detecting multiple changes inside slope has comparatively been ignored. part of a reason considering this was that detecting changes inside slope was much more challenging. considering example, simple binary segmentation procedures do not work considering this problem, whilst efficient dynamic programming methods that work well considering a change inside mean problem cannot be directly used considering detecting changes inside slope. we present the novel dynamic programming approach, cpop, considering finding a ""best"" continuous piecewise-linear fit to data. we define best based on the criterion that measures fit to data with the help of a residual sum of squares, but penalises complexity based on an $l_0$ penalty on changes inside slope. we show that with the help of such the criterion was more reliable at estimating changepoint locations than approaches that penalise complexity with the help of an $l_1$ penalty. empirically cpop has good computational properties, and should analyse the time-series with over 10,000 observations and over 100 changes inside the few minutes. our method was used to analyse data on a motion of bacteria, and provides fits to a data that both have substantially smaller residual sum of squares and are more parsimonious than two competing approaches.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0
2699,"we present the coherent multi-band modelling of a co spectral energy distribution of a local seyfert galaxy ngc7130 to assess a impact of a agn activity on a molecular gas. we take advantage of all a available data from x-ray to a sub-mm, including alma data. a high-resolution (~0.2"") alma co(6-5) data constrain a spatial extension of a co emission down to ~70 pc scale. from a analysis of a archival chandra and nustar data, we infer a presence of the buried, compton-thick agn of moderate luminosity, l_2-10kev ~ 1.6x10^{43} ergs-1. we explore photodissociation and x-ray-dominated regions (pdrs and xdrs) models to reproduce a co emission. we find that pdrs should reproduce a co lines up to j~6, however, a higher rotational ladder requires a presence of the separate source of excitation. we consider x-ray heating by a agn as the source of excitation, and find that it should reproduce a observed co spectral energy distribution. by adopting the composite pdr+xdr model, we derive molecular cloud properties. our study clearly indicates a capabilities offered by current-generation of instruments to shed light on a properties of nearby galaxies adopting state-of-the art physical modelling.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
599,"abstractive summarization aims to generate the shorter version of a document covering all a salient points inside the compact and coherent fashion. on a other hand, query-based summarization highlights those points that are relevant inside a context of the given query. a encode-attend-decode paradigm has achieved notable success inside machine translation, extractive summarization, dialog systems, etc. but it suffers from a drawback of generation of repeated phrases. inside this work we propose the model considering a query-based summarization task based on a encode-attend-decode paradigm with two key additions (i) the query attention model (in addition to document attention model) which learns to focus on different portions of a query at different time steps (instead of with the help of the static representation considering a query) and (ii) the new diversity based attention model which aims to alleviate a problem of repeating phrases inside a summary. inside order to enable a testing of this model we introduce the new query-based summarization dataset building on debatepedia. our experiments show that with these two additions a proposed model clearly outperforms vanilla encode-attend-decode models with the gain of 28% (absolute) inside rouge-l scores.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1431,"inside this article we present pictorially a foundation of differential geometry which was the crucial tool considering multiple areas of physics, notably general and special relativity, but also mechanics, thermodynamics and solving differential equations. as all a concepts are presented as pictures, there are no equations inside this article. as such this article may be read by pre-university students who enjoy physics, mathematics and geometry. however it will also greatly aid a intuition of an undergraduate and masters students, learning general relativity and similar courses. it concentrates on a tools needed to understand maxwell's equations thus leading to a goal of presenting maxwell's equations as 3 pictures.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4959,"inside this paper, we consider a problem of estimating a lead-lag parameter between two stochastic processes driven by fractional brownian motions (fbms) of a hurst parameter greater than 1/2. first we propose the lead-lag model between two stochastic processes involving fbms, and then construct the consistent estimator of a lead-lag parameter with possible convergence rate. our estimator has a following two features. firstly, we should construct a lead-lag estimator without with the help of a hurst parameters of a underlying fbms. secondly, our estimator should deal with some non-synchronous and irregular observations. we explicitly calculate possible convergence rate when a observation times are (1) synchronous and equidistant, and (2) given by a poisson sampling scheme. we also present numerical simulations of our results with the help of a r package yuima.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
17842,"given the constant vector field $z$ inside minkowski space, the timelike surface was said to have the canonical null direction with respect to $z$ if a projection of $z$ on a tangent space of a surface gives the lightlike vector field. inside this paper we describe these surfaces inside a ruled case. considering example when a minkowski space has three dimensions then the surface with the canonical null direction was minimal and flat. on a other hand, we describe several properties inside a non ruled case and we partially describe these surfaces inside four-dimensional minkowski space. we give different ways considering building these surfaces inside four-dimensional minkowski space and we finally use a gauss map considering describe another properties of these surfaces.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18625,"we present the novel object detection pipeline considering localization and recognition inside three dimensional environments. our idea behind the method makes use of an rgb-d sensor and combines state-of-the-art techniques from a robotics and computer vision communities to create the robust, real-time detection system. we focus specifically on solving a object detection problem considering tabletop scenes, the common environment considering assistive manipulators. our detection pipeline locates objects inside the point cloud representation of a scene. these clusters are subsequently used to compute the bounding box around each object inside a rgb space. each defined patch was then fed into the convolutional neural network (cnn) considering object recognition. we also demonstrate that our region proposal method should be used to develop novel datasets that are both large and diverse enough to train deep learning models, and easy enough to collect that end-users should develop their own datasets. lastly, we validate a resulting system through an extensive analysis of a accuracy and run-time of a full pipeline.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
8388,"planets interact with their host stars through gravity, radiation and magnetic fields, and considering those giant planets that orbit their stars within $\sim$10 stellar radii ($\sim$0.1 au considering the sun-like star), star-planet interactions (spi) are observable with the wide variety of photometric, spectroscopic and spectropolarimetric studies. at such close distances, a planet orbits within a sub-alfv√©nic radius of a star inside which a transfer of energy and angular momentum between a two bodies was particularly efficient. a magnetic interactions appear as enhanced stellar activity modulated by a planet as it orbits a star rather than only by stellar rotation. these spi effects are informative considering a study of a internal dynamics and atmospheric evolution of exoplanets. a nature of magnetic spi was modeled to be strongly affected by both a stellar and planetary magnetic fields, possibly influencing a magnetic activity of both, as well as affecting a irradiation and even a migration of a planet and rotational evolution of a star. as phase-resolved observational techniques are applied to the large statistical sample of hot jupiter systems, extensions to other tightly orbiting stellar systems, such as smaller planets close to m dwarfs become possible. inside these systems, star-planet separations of tens of stellar radii begin to coincide with a radiative habitable zone where planetary magnetic fields are likely the necessary condition considering surface habitability.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8640,"this paper introduces dex, the reinforcement learning environment toolkit specialized considering training and evaluation of continual learning methods as well as general reinforcement learning problems. we also present a novel continual learning method of incremental learning, where the challenging environment was solved with the help of optimal weight initialization learned from first solving the similar easier environment. we show that incremental learning should produce vastly superior results than standard methods by providing the strong baseline method across ten dex environments. we finally develop the saliency method considering qualitative analysis of reinforcement learning, which shows a impact incremental learning has on network attention.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6494,"optical coherence tomography (oct) enables high-resolution and non-invasive 3d imaging of a human retina but was inherently impaired by speckle noise. this paper introduces the spatio-temporal denoising algorithm considering oct data on the b-scan level with the help of the novel quantile sparse image (quasi) prior. to remove speckle noise while preserving image structures of diagnostic relevance, we implement our quasi prior using median filter regularization coupled with the huber data fidelity model inside the variational approach. considering efficient energy minimization, we develop an alternating direction method of multipliers (admm) scheme with the help of the linearization of median filtering. our spatio-temporal method should handle both, denoising of single b-scans and temporally consecutive b-scans, to gain volumetric oct data with enhanced signal-to-noise ratio. our algorithm based on 4 b-scans only achieved comparable performance to averaging 13 b-scans and outperformed other current denoising methods.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13938,"recently, a visual question answering (vqa) task has gained increasing attention inside artificial intelligence. existing vqa methods mainly adopt a visual attention mechanism to associate a input question with corresponding image regions considering effective question answering. a free-form region based and a detection-based visual attention mechanisms are mostly investigated, with a former ones attending free-form image regions and a latter ones attending pre-specified detection-box regions. we argue that a two attention mechanisms are able to provide complementary information and should be effectively integrated to better solve a vqa problem. inside this paper, we propose the novel deep neural network considering vqa that integrates both attention mechanisms. our proposed framework effectively fuses features from free-form image regions, detection boxes, and question representations using the multi-modal multiplicative feature embedding scheme to jointly attend question-related free-form image regions and detection boxes considering more accurate question answering. a proposed method was extensively evaluated on two publicly available datasets, coco-qa and vqa, and outperforms state-of-the-art approaches. source code was available at this https url.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5442,atomic metallic hydrogen with the lattice with fddd symmetry was shown to have the stable phase under hydrostatic compression inside a range of pressure 350 - 500 gpa.,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0
7137,"we show that $t^{s/2} \vert (\mbox{\boldmath $u$},\mbox{\boldmath $b$})(.,t)\vert_{\dot{h}(\mathbb{r}^{n})} \rightarrow 0,$ as $t\rightarrow \infty$ considering leray solutions $(\mbox{\boldmath $u$}, \mbox{\boldmath $b$})(.,t)$ of a incompressible mhd equations, where $2 \leq n \leq 4$ and $s \geq 0.$ as the corollary of main result described previously we have also that $\lim_{t\rightarrow\infty} t^{\frac{n}{2} - \frac{n}{2q}} \vert(\mbox{\boldmath $u$},\mbox{\boldmath $b$})(.,t)\vert_{l^{q}(\mathbb{r}^{n})} = 0, 2\leq q\leq \infty.$",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13435,"the novel distributed energy allocation mechanism considering distribution system operator (dso) market through the bi-level iterative auction was proposed. with a locational marginal price at a substation node known, a dso runs an upper level auction with aggregators as intermediate agents competing considering energy. this dso level auction takes into account physical grid constraints such as line flows, transformer capacities and node voltage limits. this auction mechanism was the straightforward implementation of projected gradient descent on a social welfare (sw) of all home level agents. aggregators, which serve home level agents - both buyers and sellers, implement lower level auctions inside parallel, through proportional allocation and without asking considering utility functions and generation capacities that are considered private information. a overall bi-level auction was shown to be efficient and weakly budget balanced.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
10758,"community detection inside networks was the very actual and important field of research with applications inside many areas. but, given that a amount of processed data increases more and more, existing algorithms need to be adapted considering very large graphs. a objective of this project is to parallelise a synchronised louvain method, the community detection algorithm developed by arnaud browet, inside order to improve its performances inside terms of computation time and thus be able to faster detect communities inside very large graphs. to reach this goal, we used a api openmp to parallelise a algorithm and then carried out performance tests. we studied a computation time and speedup of a parallelised algorithm and were able to bring out some qualitative trends. we obtained the great speedup, compared with a theoretical prediction of amdahl law. to conclude, with the help of a parallel implementation of a algorithm of browet on large graphs seems to give good results, both inside terms of computation time and speedup. further tests should be carried out inside order to obtain more quantitative results.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
19666,"millimeter-wave (mmwave) communication opens up tens of giga-hertz (ghz) spectrum inside a mmwave band considering use by next-generation wireless systems, thereby solving a problem of spectrum scarcity. maintaining connectivity stands out to be the key design challenge considering mmwave networks deployed inside urban regions due to a blockage effect characterizing mmwave propagation. specifically, mmwave signals should be blocked by buildings and other large urban objects. inside this paper, we set out to investigate a blockage effect on a connectivity of mmwave networks inside the manhattan-type urban region modeled with the help of the random regular lattice while base stations (bss) are poisson distributed inside a plane. inside particular, we analyze a connectivity probability that the typical user was within a transmission range of the bs and connected by the line-of-sight. with the help of random lattice and stochastic geometry theories, different lower bounds on a connectivity probability are derived as functions of a buildings' size and probability of the lattice cell being occupied by the building as well as bs density and transmission range. a asymptotic connectivity probability was also derived considering cases of dense buildings. last, a results are extended to heterogeneous networks. our study yields closed-form relations between a parameters of a building process and a bs process, providing useful guidelines considering practical mmwave network deployment.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
4816,"we prove that there does not exist any real hypersurface inside complex grassmannians of rank two with semi-parallel structure jacobi operator. with this result, a nonexistence of real hypersurface inside complex grassmannians of rank two with recurrent structure jacobi operator was proved.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19157,"a emergence of structure through aggregation was the fascinating topic and of both fundamental and practical interest. here we demonstrate that self-generated solvent flow should be used to generate long-range attractions on a colloidal scale, with sub-pico newton forces extending into a millimeter-range. we observe the rich dynamic behavior with a formation and fusion of small clusters resembling molecules, a dynamics of which was governed by an effective conservative energy that decays as $1/r$. breaking a flow symmetry, these clusters should be made active.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
4751,"we consider a problem of finding critical points of functions that are non-convex and non-smooth. studying the fairly broad class of such problems, we analyze a behavior of three gradient-based methods (gradient descent, proximal update, and frank-wolfe update). considering each of these methods, we establish rates of convergence considering general problems, and also prove faster rates considering continuous sub-analytic functions. we also show that our algorithms should escape strict saddle points considering the class of non-smooth functions, thereby generalizing known results considering smooth functions. our analysis leads to the simplification of a popular cccp algorithm, used considering optimizing functions that should be written as the difference of two convex functions. our simplified algorithm retains all a convergence properties of cccp, along with the significantly lower cost per iteration. we illustrate our methods and theory using applications to a problems of best subset selection, robust estimation, mixture density estimation, and shape-from-shading reconstruction.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18308,"we prove upper bounds considering geodesic periods of automorphic forms over general rank one locally symmetric spaces. such periods are integrals of automorphic forms restricted to special totally geodesic cycles of a ambient manifold and twisted with automorphic forms on a cycles. a upper bounds are inside terms of a laplace eigenvalues of a two automorphic forms, and they generalize previous results considering real hyperbolic manifolds to a context of all rank one locally symmetric spaces.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0
11250,"e-functions are entire functions with algebraic taylor coefficients satisfying certain arithmetic conditions, and which are also solutions of linear differential equations with rational functions coefficients. they were introduced by siegel inside 1929 to generalize diophantine properties of a exponential function, and studied further by shidlovskii inside 1956. a celebrated siegel-shidlovskii theorem deals with a algebraic (in)dependence of values at algebraic points of e-functions solutions of the differential system. however, somewhat paradoxically, this deep result may fail to decide whether the given e-fuction assumes an algebraic or the transcendental value at some given algebraic point. building upon andr√©'s theory of e-operators, beukers refined inside 2006 a siegel-shidlovskii theorem inside an optimal way. inside this paper, we use beukers' work to prove a following result: there exists an algorithm which, given the transcendental e-function $f(z)$ as input, outputs a finite list of all exceptional algebraic points $\alpha$ such that $f(\alpha)$ was also algebraic, together with a corresponding list of values $f(\alpha)$. this result solves a problem of deciding whether values of e-functions at algebraic points are transcendental.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
8512,"generalization of a euler polynomials ${{a}_{n}}\left( x \right)={{\left( 1-x \right)}^{n+1}}\sum\nolimits_{m=0}^{\infty }{{{m}^{n}}{{x}^{m}}}$ are a polynomials ${{\alpha }_{n}}\left( x \right)={{\left( 1-x \right)}^{n+1}}\sum\nolimits_{m=0}^{\infty }{{{u}_{n}}}\left( m \right){{x}^{m}}$, where ${{u}_{n}}\left( x \right)$ was a polynomial of degree $n$. these polynomials appear inside various fields of mathematics, which causes the variety of methods considering their study. inside present paper we will consider generalized euler polynomials as an attribute of a theory of riordan arrays. from this point of view, we will consider a transformations associated with them, with the participation of such objects as binomial sequences, stirling numbers, multinomial coefficients, shift operator, and demonstrate the constructiveness of a chosen point of view.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
16319,"a globalization of trade and a organization of work are currently causing the large migratory flow towards a cities. this growth of cities requires new urban planning where digital tools take the preponderant place to capture data and understand and decide inside face of changes. these tools however hardly resist to natural disasters, terrorism, accidents, etc. based on a expertise of a citi laboratory of insa lyon and sc3 of a industrial university of santander, we propose to create a alert project - autonomous liable emergency service inside real time - with decentralized, reliable and efficient services, physically close to a citizens, taking decisions locally, inside the relevant manner without risk of disconnection with the central authority. these information gathering and decision-making will involve a population with participatory and social approaches.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
2668,"low mass galaxies are thought to provide a bulk of a ionizing radiation necessary to reionize a universe. a amount of photons escaping a galaxies was poorly constrained theoretically, and difficult to measure observationally. yet it was an essential parameter of reionization models. we study inside detail how ionizing radiation should leak from high redshift galaxies. considering this purpose, we use the series of high resolution radiation hydrodynamics simulations, zooming on three dwarf galaxies inside the cosmological context. we find that a energy and momentum input from a supernova explosions has the pivotal role inside regulating a escape fraction, by disrupting dense star forming clumps, and clearing sight lines inside a halo. inside a absence of supernovae, photons are absorbed very locally, within a birth clouds of massive stars. we follow a time evolution of a escape fraction, and find that it should vary by more than six orders of magnitude. this explains a large scatter inside a value of a escape fraction found by previous studies. this fast variability also impacts a observability of a sources of reionization: the survey even as deep as $m_{\rm uv} = -14$ would miss about half of a underlying population of lyman-continuum emitters.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16918,"we consider the bayesian framework considering estimating the high-dimensional sparse precision matrix, inside which adaptive shrinkage and sparsity are induced by the mixture of laplace priors. besides discussing our formulation from a bayesian standpoint, we investigate a map (maximum the posteriori) estimator from the penalized likelihood perspective that gives rise to the new non-convex penalty approximating a $\ell_0$ penalty. optimal error rates considering approximation consistency inside terms of various matrix norms along with selection consistency considering sparse structure recovery are shown considering a unique map estimator under mild conditions. considering fast and efficient computation, an em algorithm was proposed to compute a map estimator of a precision matrix and (approximate) posterior probabilities on a edges of a underlying sparse structure. through extensive simulation studies and the real application to the call center data, we have demonstrated a fine performance of our method compared with existing alternatives.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
10525,"both bottom-up and top-down strategies have been used considering neural transition-based constituent parsing. a parsing strategies differ inside terms of a order inside which they recognize productions inside a derivation tree, where bottom-up strategies and top-down strategies take post-order and pre-order traversal over trees, respectively. bottom-up parsers benefit from rich features from readily built partial parses, but lack lookahead guidance inside a parsing process; top-down parsers benefit from non-local guidance considering local decisions, but rely on the strong encoder over a input to predict the constituent hierarchy before its construction.to mitigate both issues, we propose the novel parsing system based on in-order traversal over syntactic trees, designing the set of transition actions to find the compromise between bottom-up constituent information and top-down lookahead information. based on stack-lstm, our psycholinguistically motivated constituent parsing system achieves 91.8 f1 on wsj benchmark. furthermore, a system achieves 93.6 f1 with supervised reranking and 94.2 f1 with semi-supervised reranking, which are a best results on a wsj benchmark.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13854,"considering a first time, spectral signs of subtle coma activity were observed simultaneously on four main-belt primitive asteroids (145) adeona, (704) interamnia, (779) nina, and (1474) beira around their perihelion distances inside september 2012, which were interpreted as manifestations of a sublimation of h2o ice in/under a surface matter (busarev et al., 2015a, 2015b). we confirm a phenomenon considering nina when it approached perihelion inside september 2016. at a same time, based on results of spectral observations of near-earth asteroid (162173) ryugu (vilas, 2008) being the target of japan's hayabusa 2 space mission, we suspected the periodic similar transient activity on a cg-type asteroid. however, unlike a main-belt primitive asteroids demonstrating sublimation of ices close to their perihelion distances, a effect on ryugu is apparently registered near aphelion. to explain a difference, we calculated a subsolar temperature depending on heliocentric distance of a asteroids, considered qualitative models of internal structure of main-belt and near-earth primitive asteroids including ice and performed some analytical estimations. presumed temporal sublimation/degassing activity of ryugu points to a existence of the residual frozen core inside its interior. it could be an indication of the relatively recent transition of a asteroid from a main asteroid belt to a near-earth area.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12676,"a structure of a network has great impact on its traffic dynamics. most of a real world networks follow a heterogeneous structure and exhibit scale-free feature. inside scale-free network, the new node prefers to connect with hub nodes and a network capacity was curtailed by smaller degree nodes. therefore, we propose rewiring the fraction of links inside a network, to improve a network transport efficiency. inside this paper, we discuss some efficient link rewiring strategies and perform simulations on scale-free networks, confirming a effectiveness of these strategies. a rewiring strategies actually reduce a centrality of a nodes having higher betweenness centrality. after a link rewiring process, a degree distribution of a network remains a same. this work will be beneficial considering a enhancement of network performance.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
2610,"gravitational wave astronomy was the rapidly growing field of modern astrophysics, with observations being made frequently by a ligo detectors. gravitational wave signals are often extremely weak and a data from a detectors, such as ligo, was contaminated with non-gaussian and non-stationary noise, often containing transient disturbances which should obscure real signals. traditional denoising methods, such as principal component analysis and dictionary learning, are not optimal considering dealing with this non-gaussian noise, especially considering low signal-to-noise ratio gravitational wave signals. furthermore, these methods are computationally expensive on large datasets. to overcome these issues, we apply state-of-the-art signal processing techniques, based on recent groundbreaking advancements inside deep learning, to denoise gravitational wave signals embedded either inside gaussian noise or inside real ligo noise. we introduce smtdae, the staired multi-timestep denoising autoencoder, based on sequence-to-sequence bi-directional long-short-term-memory recurrent neural networks. we demonstrate a advantages of with the help of our unsupervised deep learning idea behind the method and show that, after training only with the help of simulated gaussian noise, smtdae achieves superior recovery performance considering gravitational wave signals embedded inside real non-gaussian ligo noise.",1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0
8814,"we consider the thin normal metal sandwiched between two ferromagnetic insulators. at a interfaces, a exchange coupling causes electrons within a metal to interact with magnons inside a insulators. this electron-magnon interaction induces electron-electron interactions, which, inside turn, should result inside p-wave superconductivity. inside a weak-coupling limit, we solve a gap equation numerically and approximate a critical temperature. inside yig-au-yig trilayers, superconductivity sets inside at temperatures somewhere inside a interval between 1 and 10 k. euo-au-euo trilayers require the lower temperature, inside a range from 0.01 to 1 k.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
5388,"we study a curve diffusion flow considering closed curves immersed inside a minkowski plane $\mathcal{m}$, which was equivalent to a euclidean plane endowed with the closed, symmetric, convex curve called an indicatrix that scales a length of the vector inside $\mathcal{m}$ depending on its length. a indiactrix $\partial\mathcal{u}$ (where $\mathcal{u}\subset\mathbb{r}^{2}$ was the convex, centrally symmetric domain) induces the second convex body, a isoperimetrix $\tilde{\mathcal{i}}$. this set was a unique convex set that miniminises a isoperimetric ratio (modulo homothetic rescaling) inside a minkowski plane. we prove that under a flow, closed curves that are initially close to the homothetic rescaling of a isoperimetrix inside an averaged $l^{2}$ sense exists considering all time and converge exponentially fast to the homothetic rescaling of a isoperimetrix that has enclosed area equal to a enclosed area of a initial immersion.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2453,"a issue of the recurrence of a modulationally unstable water wave trains within a framework of a fully nonlinear potential euler equations was addressed. it was examined, inside particular, if the modulation which appears from nowhere (i.e., was infinitesimal initially) and generates the rogue wave which then disappears with no trace. if so, this wave solution would be the breather solution of a primitive hydrodynamic equations. it was shown with a aid of a fully nonlinear numerical simulation that when the rogue wave occurs from the uniform stokes wave train, it excites other waves which have different lengths, what prevents a complete recurrence and, eventually, results inside the quasi-periodic breathing of a wave envelope. meanwhile a discovered effects are rather small inside magnitude, and a period of a modulation breathing may be thousands of a dominant wave periods. thus, a obtained solution may be called the quasi-breather of a euler equations.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2057,"helical magnetic structures and its responses to external magnetic fields inside yb(ni$_x$cu$_{1-x}$)$_3$al$_9$, with the chiral crystal structure of a space group $r32$, have been investigated by resonant x-ray diffraction. it was shown that a crystal chirality was reflected to a helicity of a magnetic structure by the one to one relationship, indicating that there exists an antisymmetric exchange interaction mediated using a conduction electrons. when the magnetic field was applied perpendicular to a helical axis ($c$ axis), a second harmonic peak of $(0, 0, 2q)$ develops with increasing a field. a third harmonic peak of $(0, 0, 3q)$ has also been observed considering a $x$=0.06 sample. this result provides the strong evidence considering a formation of the chiral magnetic soliton lattice state, the periodic array of a chiral twist of spins, which has been suggested by a characteristic magnetization curve. a helical ordering of magnetic octupole moments, accompanying with a magnetic dipole order, has also been detected.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
17143,"we study a relationship between a regularity and dirichlet boundary value problems considering parabolic equations of a form $lu=\text{div}(a \nabla u)-u_t=0$ inside lip$(1,1/2)$ time-varying cylinders, where a coefficient matrix $a = \left[ a_{ij}(x,t)\right] $ was uniformly elliptic and bounded. we show that if a regularity problem $(r)_p$ considering a equation $lu=0$ was solvable considering some $1<p<\infty$ then a dirichlet problem $(d^*)_{p'}$ considering a adjoint equation $l^*v=0$ was also solvable, where $p'=p/(p-1)$. this result was an analogue of a result established inside a elliptic case by kenig and pipher. inside a parabolic settings inside a special case of a heat equation inside slightly smoother domains this has been established by hofmann and lewis and nystr√∂m considering scalar parabolic systems. inside comparison, our result was abstract with no assumption on a coefficients beyond a ellipticity condition and was valid inside more general class of domains.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10280,"given the distribution of defects on the structured surface, such as those represented by 2-dimensional crystalline materials, liquid crystalline surfaces, and thin sandwiched shells, what was a resulting stress field and a deformed shape? motivated by this concern, we first classify, and quantify, a translational, rotational, and metrical defects allowable over the broad class of structured surfaces. with an appropriate notion of strain, a defect densities are then shown to appear as sources of strain incompatibility. a strain incompatibility relations, with appropriate kinematical assumptions on a decomposition of strain into elastic and plastic parts, and a stress equilibrium relations, with the suitable choice of material response, provide a necessary equations considering determining both a internal stress field and a deformed shape. we demonstrate this by applying our theory to kirchhoff-love shells with the kinematics which allows considering small in-surface strains but moderately large rotations.",0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
4955,"a growth of data, a need considering scalability and a complexity of models used inside modern machine learning calls considering distributed implementations. yet, as of today, distributed machine learning frameworks have largely ignored a possibility of arbitrary (i.e., byzantine) failures. inside this paper, we study a robustness to byzantine failures at a fundamental level of stochastic gradient descent (sgd), a heart of most machine learning algorithms. assuming the set of $n$ workers, up to $f$ of them being byzantine, we ask how robust should sgd be, without limiting a dimension, nor a size of a parameter space. we first show that no gradient descent update rule based on the linear combination of a vectors proposed by a workers (i.e, current approaches) tolerates the single byzantine failure. we then formulate the resilience property of a update rule capturing a basic requirements to guarantee convergence despite $f$ byzantine workers. we finally propose krum, an update rule that satisfies a resilience property aforementioned. considering the $d$-dimensional learning problem, a time complexity of krum was $o(n^2 \cdot (d + \log n))$.",1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0
15351,"we introduce synchronization strings as the novel way of efficiently dealing with synchronization errors, i.e., insertions and deletions. synchronization errors are strictly more general and much harder to deal with than commonly considered half-errors, i.e., symbol corruptions and erasures. considering every $\epsilon >0$, synchronization strings allow to index the sequence with an $\epsilon^{-o(1)}$ size alphabet such that one should efficiently transform $k$ synchronization errors into $(1+\epsilon)k$ half-errors. this powerful new technique has many applications. inside this paper, we focus on designing insdel codes, i.e., error correcting block codes (eccs) considering insertion deletion channels. while eccs considering both half-errors and synchronization errors have been intensely studied, a later has largely resisted progress. indeed, it took until 1999 considering a first insdel codes with constant rate, constant distance, and constant alphabet size to be constructed by schulman and zuckerman. insdel codes considering asymptotically large or small noise rates were given inside 2016 by guruswami et al. but these codes are still polynomially far from a optimal rate-distance tradeoff. this makes a understanding of insdel codes up to this work equivalent to what is known considering regular eccs after forney introduced concatenated codes inside his doctoral thesis 50 years ago. the direct application of our synchronization strings based indexing method gives the simple black-box construction which transforms any ecc into an equally efficient insdel code with the slightly larger alphabet size. this instantly transfers much of a highly developed understanding considering regular eccs over large constant alphabets into a realm of insdel codes. most notably, we obtain efficient insdel codes which get arbitrarily close to a optimal rate-distance tradeoff given by a singleton bound considering a complete noise spectrum.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
19288,"with the help of recently introduced debord-skandalis blup's groupoids we study index theory considering the compact foliated manifold with boundary inducing the foliation inside its boundary. considering this we consider first the blup groupoid whose lie algebroid has sections consisting of vector fields tangent to a leaves inside a interior and tangent to a leaves of a foliation inside a boundary. inside particular a holonomy $b$-groupoid allows us to consider a appropriate pseudodifferential calculus and a appropriate index problems. we further use a blup groupoids as a one above, and inside particular its functoriality properties, to actually get index theorems. inside this situtation there are two index morphisms, one related to ellipticity and the second one related to fully ellipticity. considering a first one, we are able to extend to this setting a longitudinal connes-skandalis index theorem and to use it to get that the $b$-longitudinal elliptic operator should be perturbed (up stable homotopy within elliptic operators) with the regularizing operator inside a calculus to get the fully elliptic operator if and only if the certain boundary topological index vanishes. considering example inside a case of the fibration (family case) this topological obstruction was always zero. considering a second index morphism, a one related to fully elliptic operators (families of fredholm operators), we restrict ourselves to a case of families of manifolds with boundary and we prove the new k-theoretical index theorem, i.e. construct the topological index and prove a equality with a analytic-fredholm index, and use it to get the cohomological index formula considering every fully elliptic operator. inside particular, considering the perturbed family of generalized dirac operators we should compare our formula with a one by melrose-piazza to get the new geometric expression considering a eta form of a family.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7064,"inside recent years, a learned local descriptors have outperformed handcrafted ones by the large margin, due to a powerful deep convolutional neural network architectures such as l2-net [1] and triplet based metric learning [2]. however, there are two problems inside a current methods, which hinders a overall performance. firstly, a widely-used margin loss was sensitive to incorrect correspondences, which are prevalent inside a existing local descriptor learning datasets. second, a l2 distance ignores a fact that a feature vectors have been normalized to unit norm. to tackle these two problems and further boost a performance, we propose the robust angular loss which 1) uses cosine similarity instead of l2 distance to compare descriptors and 2) relies on the robust loss function that gives smaller penalty to triplets with negative relative similarity. a resulting descriptor shows robustness on different datasets, reaching a state-of-the-art result on brown dataset , as well as demonstrating excellent generalization ability on a hpatches dataset and the wide baseline stereo dataset.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3722,"we show that, within the linear approximation of bcs theory, the weak homogeneous magnetic field lowers a critical temperature by an explicit constant times a field strength, up to higher order terms. this provides the rigorous derivation and generalization of results obtained inside a physics literature from whh theory of a upper critical magnetic field. the new ingredient inside our proof was the rigorous phase approximation to control a effects of a magnetic field.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
86,"we have studied ferroelectricity and photovoltaic effects inside atomic layer deposited (ald) 40-nm thick sntio$_{x}$ films deposited directly onto p-type (001)si substrate. these films showed well-saturated, square and repeatable hysteresis loops with remnant polarization of 1.5 $\mu$c/cm$^{2}$ at room temperature, as detected by out-of-plane polarization versus electric field (p-e) and field cycling measurements. the photo-induced enhancement inside ferroelectricity is also observed as a spontaneous polarization increased under white-light illumination. a ferroelectricity exhibits relaxor characteristics with dielectric peak shifting from ca. t = 600 k at f = 1 mhz to ca. 500 k at 100 hz. moreover, our films showed ferroelectric photovoltaic behavior under a illumination of the wide spectrum of light, from visible to ultraviolet regions. the combination of experiment and theoretical calculation provided optical band gap of sntio$_{x}$ films which lies inside a visible range of white light spectra. our study leads the way to develop green ferroelectric sntio$_{x}$ thin films, which are compatible to semiconducting processes, and should be used considering various ferroelectric and dielectric applications.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
1204,"the conformal map from the riemann surface to the euclidean space of dimension greater than or equal to three was explained by with the help of a clifford algebra, inside the similar fashion to quaternionic holomorphic geometry of surfaces inside a euclidean three- or four-space. a weierstrass representation, a spin transform, a darboux transforms, surfaces of parallel mean curvature vector, families of flat connections associated with the harmonic map from the riemann surface to the sphere are explained. a degree of a spinor bundle associated with the conformal immersion was calculated. analogues of the polar surface and the bipolar surface of the minimal immersion into the three-sphere are defined. they are shown to be minimal surfaces inside the sphere.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7734,"inside this letter, we apply previous array receiver architecture which employs time-domain sub-nyquist sampling techniques to jointly approximate frequency and direction-of-arrival(doa) of narrowband far-field signals. herein, the more general situation was taken into consideration, where there may be more than one signal inside the subband. we build time-space union model, analyze a identification of a model, and give a maximum signal number which should be classified. we also proof that a cramer-rao bound (crb) was lower than that of which employs nyquist sampling. simulation results verify a capacity to approximate a number of sources. meanwhile, simulations show that our approximation performance closely matches a crb and was superior considering more sources than sensors, especially when a minimum redundancy array (mra) was employed.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
8337,"on a torus, it was possible to assign the global symbol to the pseudodifferential operator with the help of fourier series. inside this paper we investigate a relations between a local and global symbols considering a operators inside a classical h√∂rmander calculus and describe a principal symbols, a non-commutative residue and a canonical trace of an operator inside terms of its global symbol. we also generalise these results to any compact lie group.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
8552,"we have grown and characterized 30 nm thick yba$_2$cu$_3$o$_{7-\delta}$ (ybco) films, deposited by pulsed laser deposition on both mgo (110) and srtio$_3$ (001) substrates, which induce opposite strain to a superconducting layer. by carefully tuning a in-situ post-annealing oxygen pressure, we achieved, inside the reproducible way, films at different oxygen doping, spanning from a slightly overdoped down to a strongly underdoped region of a phase diagram. a transport properties of a films, investigated through resistance versus temperature measurements, are inside perfect qualitative agreement with single crystals. starting from these films, we have also successfully fabricated nanowires with widths down to 65 nm, at different oxygen doping. a nanostructures exhibit characteristic temperatures (as a critical temperature $t_{\mathrm{c}}$ and a pseudogap temperature $t^*$) similar to those of a as-grown films and carry critical current densities $j_{\mathrm{c}}$ close to a critical depairing value, limited by vortex entry. this implies that a superconducting and a normal state properties of underdoped ybco are preserved inside our films, and they should be studied as the function of a dimensionality of a system, down to a nanoscale.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
14414,"opinion mining and demographic attribute inference have many applications inside social science. inside this paper, we propose models to infer daily joint probabilities of multiple latent attributes from twitter data, such as political sentiment and demographic attributes. since it was costly and time-consuming to annotate data considering traditional supervised classification, we instead propose scalable learning from label proportions (llp) models considering demographic and opinion inference with the help of u.s. census, national and state political polls, and cook partisan voting index as population level data. inside llp classification settings, a training data was divided into the set of unlabeled bags, where only a label distribution inside of each bag was known, removing a requirement of instance-level annotations. our proposed llp model, weighted label regularization (wlr), provides the scalable generalization of prior work on label regularization to support weights considering samples in bags, which was applicable inside this setting where bags are arranged hierarchically (e.g., county-level bags are nested in of state-level bags). we apply our model to twitter data collected inside a year leading up to a 2016 u.s. presidential election, producing estimates of a relationships among political sentiment and demographics over time and place. we find that our idea behind the method closely tracks traditional polling data stratified by demographic category, resulting inside error reductions of 28-44% over baseline approaches. we also provide descriptive evaluations showing how a model may be used to approximate interactions among many variables and to identify linguistic temporal variation, capabilities which are typically not feasible with the help of traditional polling methods.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
18041,"the new synthesis scheme was proposed to effectively generate the random vector with prescribed joint density that induces the (latent) gaussian tree structure. a quality of synthesis was measured by total variation distance between a synthesized and desired statistics. a proposed layered and successive encoding scheme relies on a learned structure of tree to use minimal number of common random variables to synthesize a desired density. we characterize a achievable rate region considering a rate tuples of multi-layer latent gaussian tree, through which a number of bits needed to simulate such gaussian joint density are determined. a random sources used inside our algorithm are a latent variables at a top layer of tree, a additive independent gaussian noises, and a bernoulli sign inputs that capture a ambiguity of correlation signs between a variables.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
15831,"simultaneous localization and mapping (slam) was considered to be the fundamental capability considering intelligent mobile robots. over a past decades, many impressed slam systems have been developed and achieved good performance under certain circumstances. however, some problems are still not well solved, considering example, how to tackle a moving objects inside a dynamic environments, how to make a robots truly understand a surroundings and accomplish advanced tasks. inside this paper, the robust semantic visual slam towards dynamic environments named ds-slam was proposed. five threads run inside parallel inside ds-slam: tracking, semantic segmentation, local mapping, loop closing, and dense semantic map creation. ds-slam combines semantic segmentation network with moving consistency check method to reduce a impact of dynamic objects, and thus a localization accuracy was highly improved inside dynamic environments. meanwhile, the dense semantic octo-tree map was produced, which could be employed considering high-level tasks. we conduct experiments both on tum rgb-d dataset and inside a real-world environment. a results demonstrate a absolute trajectory accuracy inside ds-slam should be improved by one order of magnitude compared with orb-slam2. it was one of a state-of-the-art slam systems inside high-dynamic environments. now a code was available at our github: this https url",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
13750,"inside biological research machine learning algorithms are part of nearly every analytical process. they are used to identify new insights into biological phenomena, interpret data, provide molecular diagnosis considering diseases and develop personalized medicine that will enable future treatments of diseases. inside this paper we (1) illustrate a importance of machine learning inside a analysis of large scale sequencing data, (2) present an illustrative standardized workflow of a analysis process, (3) perform the differential expression (de) analysis of the publicly available rna sequencing (rnaseq) data set to demonstrate a capabilities of various algorithms at each step of a workflow, and (4) show the machine learning solution inside improving a computing time, storage requirements, and minimize utilization of computer memory inside analyses of rna-seq datasets. a source code of a analysis pipeline and associated scripts are presented inside a paper appendix to allow replication of experiments.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11073,"to select a best algorithm considering the new problem was an expensive and difficult task. however, there are automatic solutions to address this problem: with the help of metalearning, which takes advantage of problem characteristics (i.e. metafeatures), one was able to predict a relative performance of algorithms. inside a collaborative filtering scope, recent works have proposed diverse metafeatures describing several dimensions of this problem. despite interesting and effective findings, it was still unknown whether these are a most effective metafeatures. hence, this work proposes the new set of graph metafeatures, which idea behind the method a collaborative filtering problem from the graph theory perspective. furthermore, inside order to understand whether metafeatures from multiple dimensions are the better fit, we investigate a effects of comprehensive metafeatures. these metafeatures are the selection of a best metafeatures from all existing collaborative filtering metafeatures. a impact of a most representative metafeatures was investigated inside the controlled experimental setup. another contribution we present was a use of the pareto-efficient ranking procedure to create multicriteria metatargets. these new rankings of algorithms, which take into account multiple evaluation measures, allow to explore a algorithm selection problem inside the fairer and more detailed way. according to a experimental results, a graph metafeatures are the good alternative to related work metafeatures. however, a results have shown that a feature selection procedure used to create a comprehensive metafeatures was was not effective, since there was no gain inside predictive performance. finally, an extensive metaknowledge analysis is conducted to identify a most influential metafeatures.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
10268,"interfaces between complex oxides constitute the unique playground considering 2d electron systems (2des), where superconductivity and magnetism should arise from combinations of bulk insulators. a 2des at a laalo3/srtio3 interface was one of a most studied inside this regard, and its origin was determined by both a presence of the polar field inside laalo3 and a insurgence of point defects, such as oxygen vacancies and intermixed cations. these defects usually reside inside a conduction channel and are responsible considering the decreased electronic mobility. inside this work we use an amorphous wo3 overlayer to control a defect formation and obtain an increased electron mobility and effective mass inside wo3/laalo3/srtio3 heterostructures. a studied system shows the sharp insulator-to-metal transition as the function of both laalo3 and wo3 layer thickness. low-temperature magnetotransport reveals the strong magnetoresistance reaching 900% at 10 t and 1.5 k, a presence of multiple conduction channels with carrier mobility up to 80 000 cm2/vs and an unusually high effective mass of 5.6 me. a amorphous character of a wo3 overlayer makes this the versatile idea behind the method considering defect control at oxide interfaces, which could be applied to other heterestrostures disregarding a constraints imposed by crystal symmetry.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
5167,"inside this paper, we investigate distributed generalized nash equilibrium (gne) computation of monotone games with affine coupling constraints. each player should only utilize its local objective function, local feasible set and the local block of a coupling constraint, and should only communicate with its neighbours. we assume a game has monotone pseudo-subdifferential without lipschitz continuity restrictions. we design novel center-free distributed gne seeking algorithms considering equality and inequality affine coupling constraints, respectively. the proximal alternating direction method of multipliers(admm) was proposed considering a equality case, while considering a inequality case, the parallel splitting type algorithm was proposed. inside both algorithms, a gne seeking task was decomposed into the sequential ne computation of regularized subgames and distributed update of multipliers and auxiliary variables, based on local data and local communication. our two double-layer gne algorithms need not specify a inner-loop ne seeking algorithm and moreover, only require that a strongly monotone subgames are inexactly solved. we prove their convergence by showing that a two algorithms should be seen as specific instances of preconditioned proximal point algorithms} (pppa) considering finding zeros of monotone operators. applications and numerical simulations are given considering illustration.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
4019,"grip control during robotic in-hand manipulation was usually modeled as part of the monolithic task, relying on complex controllers specialized considering specific situations. such approaches do not generalize well and are difficult to apply to novel manipulation tasks. here, we propose the modular object stabilization method based on the proposition that explains how humans achieve grasp stability. inside this bio-mimetic approach, independent tactile grip stabilization controllers ensure that slip does not occur locally at a engaged robot fingers. such local slip was predicted from a tactile signals of each fingertip sensor i.e., biotac and biotac sp by syntouch. we show that stable grasps emerge without any form of central communication when such independent controllers are engaged inside a control of multi-digit robotic hands. these grasps are resistant to external perturbations while being capable of stabilizing the large variety of objects.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
7215,"we propose a learned primal-dual algorithm considering tomographic reconstruction. a algorithm accounts considering the (possibly non-linear) forward operator inside the deep neural network by unrolling the proximal primal-dual optimization method, but where a proximal operators have been replaced with convolutional neural networks. a algorithm was trained end-to-end, working directly from raw measured data and it does not depend on any initial reconstruction such as fbp. we compare performance of a proposed method on low dose ct reconstruction against fbp, tv, and deep learning based post-processing of fbp. considering a shepp-logan phantom we obtain >6db psnr improvement against all compared methods. considering human phantoms a corresponding improvement was 6.6db over tv and 2.2db over learned post-processing along with the substantial improvement inside a ssim. finally, our algorithm involves only ten forward-back-projection computations, making a method feasible considering time critical clinical applications.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
11717,"a analogy between mechanical and electromagnetic resonators has been the celebrated paradigm of science and engineering. exploration of this analogy inside recent years has resulted inside several exciting research directions, including cavity optomechanics[1], phononic bandgap materials[2] and phononic metamaterials[3-5]. inside these examples, progress inside electromagnetic research has usually led a way considering their mechanical counterparts. here, we contribute to this analogy from the different perspective by adapting the sensing technique originally developed considering mechanical devices to increase a capabilities of sensors based on electromagnetic fields. more specifically, multimode resonance techniques, which enable spatial resolution inside inertial mass sensing experiments with nanoelectromechanical systems (nems), are tailored considering use inside microwave resonant sensing, which was commonly employed inside microfluidics. we show that a use of higher-order modes of such sensors should provide electrical volume, position and geometric size data. a combination of such spatial features implies a potential considering image reconstruction when the large number of modes are used. with a analytical and experimental framework presented here, we should move beyond simple counting and achieve a sizing and imaging of analytes with impedance spectroscopy.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0
4093,"we prove that finite morse index solutions to a allen-cahn equation inside $\r^2$ have {\bf finitely many ends} and {\bf linear energy growth}. a main tool was the {\bf curvature decay estimate} on level sets of these finite morse index solutions, which inside turn was reduced to the problem on a uniform second order regularity of clustering interfaces considering a singularly perturbed allen-cahn equation inside $\r^n$. with the help of an indirect blow-up technique, inside a spirit of a classical colding-minicozzi theory inside minimal surfaces, we show that a {\bf obstruction} to a uniform second order regularity of clustering interfaces inside $\r^n$ was associated to a existence of nontrivial entire solutions to the (finite or infinite) {\bf toda system} inside $\r^{n-1}$. considering finite morse index solutions inside $\r^2$, we show that this obstruction does not exist by with the help of information on stable solutions of a toda system.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18831,"this paper provides efficient solutions to maximize profit considering commercial ridesharing services, under the pricing model with detour-based discounts considering passengers. we propose greedy heuristics considering real-time ride matching that offer different trade-offs between optimality and speed. simulations on new york city (nyc) taxi trip data show that our heuristics are up to 90% optimal and 10^5 times faster than a (necessarily) exponential-time optimal algorithm. commercial ridesharing service providers generate significant savings by matching multiple ride requests with the help of heuristic methods. a resulting savings are typically shared between a service provider (in a form of increased profit) and a ridesharing passengers (in a form of discounts). it was not clear the priori how this split should be effected, since higher discounts would encourage more ridesharing, thereby increasing total savings, but a fraction of savings taken as profit was reduced. we simulate the scenario where a decisions of a passengers to opt considering ridesharing depend on a discount offered by a service provider. we provide an adaptive learning algorithm idfla that learns a optimal profit-maximizing discount factor considering a provider. an evaluation over nyc data shows that idfla, on average, learns a optimal discount factor inside under 16 iterations. finally, we investigate a impact of imposing the detour-aware routing policy based on sequential individual rationality, the recently proposed concept. such restricted policies offer the better ride experience, increasing a provider's market share, but at a cost of decreased average per-ride profit due to a reduced number of matched rides. we construct the model that captures these opposing effects, wherein simulations based on nyc data show that the 7% increase inside market share would suffice to offset a decreased average per-ride profit.",1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
4581,"a zone of avoidance (zoa), whose emptiness was an artifact of our galaxy dust, has been challenging observers as well as theorists considering many years. multiple attempts have been made on a observational side to map this region inside order to better understand a local flows. on a theoretical side, however, this region was often simply statistically populated with structures but no real attempt has been made to confront theoretical and observed matter distributions. this paper takes the step forward with the help of constrained realizations of a local universe shown to be perfect substitutes of local universe-like simulations considering smoothed high density peak studies. far from generating completely `random' structures inside a zoa, a reconstruction technique arranges matter according to a surrounding environment of this region. more precisely, a mean distributions of structures inside the series of constrained and random realizations differ: while densities annihilate each other when averaging over 200 random realizations, structures persist when summing 200 constrained realizations. a probability distribution function of zoa grid cells to be highly overdense was the gaussian with the 15% mean inside a random case, while that of a constrained case exhibits large tails. this implies that areas with a largest probabilities host most likely the structure. comparisons between these predictions and observations, like those of a puppis 3 cluster, show the remarkable agreement and allow us to assert a presence of the, recently highlighted by observations, vela supercluster at about 180 mpc/h, right behind a thickest dust layers of our galaxy.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13485,"skin cancer was one of a major types of cancers and its incidence has been increasing over a past decades. skin lesions should arise from various dermatologic disorders and should be classified to various types according to their texture, structure, color and other morphological features. a accuracy of diagnosis of skin lesions, specifically a discrimination of benign and malignant lesions, was paramount to ensure appropriate patient treatment. machine learning-based classification approaches are among popular automatic methods considering skin lesion classification. while there are many existing methods, convolutional neural networks (cnn) have shown to be superior over other classical machine learning methods considering object detection and classification tasks. inside this work, the fully automatic computerized method was proposed, which employs well established pre-trained convolutional neural networks and ensembles learning to classify skin lesions. we trained a networks with the help of 2000 skin lesion images available from a isic 2017 challenge, which has three main categories and includes 374 melanoma, 254 seborrheic keratosis and 1372 benign nevi images. a trained classifier is then tested on 150 unlabeled images. a results, evaluated by a challenge organizer and based on a area under a receiver operating characteristic curve (auc), were 84.8% and 93.6% considering melanoma and seborrheic keratosis binary classification problem, respectively. a proposed method achieved competitive results to experienced dermatologist. further improvement and optimization of a proposed method with the larger training dataset could lead to the more precise, reliable and robust method considering skin lesion classification.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17609,"autonomous unmanned aerial manipulators (uams) have shown promising potentials to transform passive sensing missions into active 3-dimension interactive missions, but they still suffer from some difficulties impeding their wide applications, such as target detection and stabilization. this letter presents the vision-based autonomous uam with the 3dof robotic arm considering rotational grasping, with the compensation on displacement considering center of gravity. first, a hardware, software architecture and state approximation methods are detailed. all a mechanical designs are fully provided as open-source hardware considering a reuse by a community. then, we analyze a flow distribution generated by rotors and plan a robotic arm's motion based on this analysis. next, the novel detection idea behind the method called rotation-squeezedet was proposed to enable rotation-aware grasping, which should give a target position and rotation angle inside near real-time on jetson tx2. finally, a effectiveness of a proposed scheme was validated inside multiple experimental trials, highlighting it's applicability of autonomous aerial grasping inside gps-denied environments.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
5172,"a present contribution offers the simple methodology considering a obtainment of data-driven interval forecasting models by combining pairs of quantile regressions. those regressions are created without a usage of a non-differentiable pinball-loss function, but through the k-nearest-neighbors based training set transformation and traditional regression approaches. by leaving a underlying training algorithms of a data mining techniques unchanged, a presented idea behind the method simplifies a creation of quantile regressions with more complex techniques (e.g. artificial neural networks). a quality of a presented methodology was tested on a usecase of photovoltaic power forecasting, considering which quantile regressions with the help of polynomial models as well as artificial neural networks and support vector regressions are created. from a resulting evaluation values it should be concluded that acceptable interval forecasting models are created.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
11000,"policy gradient methods have achieved remarkable successes inside solving challenging reinforcement learning problems. however, it still often suffers from a large variance issue on policy gradient estimation, which leads to poor sample efficiency during training. inside this work, we propose the control variate method to effectively reduce variance considering policy gradient methods. motivated by a stein's identity, our method extends a previous control variate methods used inside reinforce and advantage actor-critic by introducing more general action-dependent baseline functions. empirical studies show that our method significantly improves a sample efficiency of a state-of-the-art policy gradient approaches.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7846,"inside recent years, a rapidly increasing amounts of data created and processed through a internet resulted inside distributed storage systems employing erasure coding based schemes. aiming to balance a tradeoff between data recovery considering correlated failures and efficient encoding and decoding, distributed storage systems employing maximally recoverable codes came up. unifying the number of topologies considered both inside theory and practice, gopalan \cite{gopalan2017} initiated a study of maximally recoverable codes considering grid-like topologies. inside this paper, we focus on a maximally recoverable codes that instantiate grid-like topologies $t_{m\times n}(1,b,0)$. to characterize a property of codes considering these topologies, we introduce a notion of \emph{pseudo-parity check matrix}. then, with the help of a hypergraph independent set approach, we establish a first polynomial upper bound on a field size needed considering achieving a maximal recoverability inside topologies $t_{m\times n}(1,b,0)$, when $n$ was large enough. and we further improve this general upper bound considering topologies $t_{4\times n}(1,2,0)$ and $t_{3\times n}(1,3,0)$. by relating a problem to generalized \emph{sidon sets} inside $\mathbb{f}_q$, we also obtain non-trivial lower bounds on a field size considering maximally recoverable codes that instantiate topologies $t_{4\times n}(1,2,0)$ and $t_{3\times n}(1,3,0)$.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
6728,"malware authors have always been at an advantage of being able to adversarially test and augment their malicious code, before deploying a payload, with the help of anti-malware products at their disposal. a anti-malware developers and threat experts, on a other hand, do not have such the privilege of tuning anti-malware products against zero-day attacks pro-actively. this allows a malware authors to being the step ahead of a anti-malware products, fundamentally biasing a cat and mouse game played by a two parties. inside this paper, we propose the way that would enable machine learning based threat prevention models to bridge that gap by being able to tune against the deep generative adversarial network (gan), which takes up a role of the malware author and generates new types of malware. a gan was trained over the reversible distributed rgb image representation of known malware behaviors, encoding a sequence of api call ngrams and a corresponding term frequencies. a generated images represent synthetic malware that should be decoded back to a underlying api call sequence information. a image representation was not only demonstrated as the general technique of incorporating necessary priors considering exploiting convolutional neural network architectures considering generative or discriminative modeling, but also as the visualization method considering easy manual software or malware categorization, by having individual api ngram information distributed across a image space. inside addition, we also propose with the help of smart-definitions considering detecting malwares based on perceptual hashing of these images. such hashes are potentially more effective than cryptographic hashes that do not carry any meaningful similarity metric, and hence, do not generalize well.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3119,"approximation of shannon and r√©nyi entropies of unknown discrete distributions was the fundamental problem inside statistical property testing and an active research topic inside both theoretical computer science and information theory. tight bounds on a number of samples to approximate these entropies have been established inside a classical setting, while little was known about their quantum counterparts. inside this paper, we give a first quantum algorithms considering estimating $\alpha$-r√©nyi entropies (shannon entropy being 1-renyi entropy). inside particular, we demonstrate the quadratic quantum speedup considering shannon entropy approximation and the generic quantum speedup considering $\alpha$-r√©nyi entropy approximation considering all $\alpha\geq 0$, including the tight bound considering a collision-entropy (2-r√©nyi entropy). we also provide quantum upper bounds considering extreme cases such as a hartley entropy (i.e., a logarithm of a support size of the distribution, corresponding to $\alpha=0$) and a min-entropy case (i.e., $\alpha=+\infty$), as well as a kullback-leibler divergence between two distributions. moreover, we complement our results with quantum lower bounds on $\alpha$-r√©nyi entropy approximation considering all $\alpha\geq 0$.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
619,"we show experimentally that a spin direction of a spin current generated by spin-orbit interactions within the ferromagnetic layer should be reoriented by turning a magnetization direction of this layer. we do this by measuring a field-like component of spin-orbit torque generated by an exchange-biased fegd thin film and acting on the nearby cofeb layer. a relative angle of a cofeb and fegd magnetic moments was varied by applying an external magnetic field. we find that a resulting torque was inside good agreement with predictions that a spin current generated by a anomalous hall effect from a fegd layer depends on a fegd magnetization direction $\hat{m}_{fegd}$ according to $\vec{\sigma}\propto\left ( \hat{y}\cdot \hat{m}_{fegd} \right )\hat{m}_{fegd}$, where $\hat{y}$ was a in-plane direction perpendicular to a applied charge current. because of this angular dependence, a spin-orbit torque arising from a anomalous hall effect should be non-zero inside the sample geometry considering which a spin hall torque generated by non-magnetic materials was identically zero.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
10228,"we introduce carla, an open-source simulator considering autonomous driving research. carla has been developed from a ground up to support development, training, and validation of autonomous urban driving systems. inside addition to open-source code and protocols, carla provides open digital assets (urban layouts, buildings, vehicles) that were created considering this purpose and should be used freely. a simulation platform supports flexible specification of sensor suites and environmental conditions. we use carla to study a performance of three approaches to autonomous driving: the classic modular pipeline, an end-to-end model trained using imitation learning, and an end-to-end model trained using reinforcement learning. a approaches are evaluated inside controlled scenarios of increasing difficulty, and their performance was examined using metrics provided by carla, illustrating a platform's utility considering autonomous driving research. a supplementary video should be viewed at this https url",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
7884,"deep reinforcement learning was becoming increasingly popular considering robot control algorithms, with a aim considering the robot to self-learn useful feature representations from unstructured sensory input leading to a optimal actuation policy. inside addition to sensors mounted on a robot, sensors might also be deployed inside a environment, although these might need to be accessed using an unreliable wireless connection. inside this paper, we demonstrate deep neural network architectures that are able to fuse information coming from multiple sensors and are robust to sensor failures at runtime. we evaluate our method on the search and pick task considering the robot both inside simulation and a real world.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1
11869,"as part of the complete software stack considering autonomous driving, nvidia has created the neural-network-based system, known as pilotnet, which outputs steering angles given images of a road ahead. pilotnet was trained with the help of road images paired with a steering angles generated by the human driving the data-collection car. it derives a necessary domain knowledge by observing human drivers. this eliminates a need considering human engineers to anticipate what was important inside an image and foresee all a necessary rules considering safe driving. road tests demonstrated that pilotnet should successfully perform lane keeping inside the wide variety of driving conditions, regardless of whether lane markings are present or not. a goal of a work described here was to explain what pilotnet learns and how it makes its decisions. to this end we developed the method considering determining which elements inside a road image most influence pilotnet's steering decision. results show that pilotnet indeed learns to recognize relevant objects on a road. inside addition to learning a obvious features such as lane markings, edges of roads, and other cars, pilotnet learns more subtle features that would be hard to anticipate and program by engineers, considering example, bushes lining a edge of a road and atypical vehicle classes.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
18415,"convolutional neural networks have recently achieved significant breakthroughs inside various image classification tasks. however, they are computationally expensive,which should make their feasible mplementation on embedded and low-power devices difficult. inside this paper convolutional neural network binarization was implemented on gpu-based platforms considering real-time inference on resource constrained devices. inside binarized networks, all weights and intermediate computations between layers are quantized to +1 and -1, allowing multiplications and additions to be replaced with bit-wise operations between 32-bit words. this representation completely eliminates a need considering floating point multiplications and additions and decreases both a computational load and a memory footprint compared to the full-precision network implemented inside floating point, making it well-suited considering resource-constrained environments. we compare a performance of our implementation with an equivalent floating point implementation on one desktop and two embedded gpu platforms. our implementation achieves the maximum speed up of 7. 4x with only 4.4% loss inside accuracy compared to the reference implementation.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8637,"a most recent experimental advances could provide ways considering a fabrication of several atomic thick and planar forms of boron atoms. considering a first time, we explore a mechanical properties of five types of boron films with various vacancy ratios ranging from 0.1 to 0.15, with the help of molecular dynamics simulations with reaxff force field. it was found that a young's modulus and tensile strength decrease with increasing a temperature. we found that boron sheets exhibit an anisotropic mechanical response due to a different arrangement of atoms along a armchair and zigzag directions. at room temperature, 2d young's modulus and fracture stress of these five sheets appear inside a range 63 n/m and 12 n/m, respectively. inside addition, a strains at tensile strength are inside a ranges of 9, 11, and 10 percent at 1, 300, and 600 k, respectively. this investigation not only reveals a remarkable stiffness of 2d boron, but establishes relations between a mechanical properties of a boron sheets to a loading direction, temperature and atomic structures.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
11946,"we introduce a threshold $q$-voter opinion dynamics where an agent, facing the binary choice, should change its mind when at least $q_0$ amongst $q$ neighbors share a opposite opinion. otherwise, a agent should still change its mind with the certain probability $\varepsilon$. this threshold dynamics contemplates a possibility of persuasion by an influence group even when there was not full agreement among its members. inside fact, individuals should follow their peers not only when there was unanimity ($q_0=q$) inside a lobby group, as assumed inside a $q$-voter model, but, depending on a circumstances, also when there was simple majority ($q_0>q/2$), byzantine consensus ($q_0>2q/3$), or any minimal number $q_0$ amongst $q$. this realistic threshold gives place to emerging collective states and phase transitions which are not observed inside a standard $q$-voter. a threshold $q_0$, together with a stochasticity introduced by $\varepsilon$, yields the phenomenology that mimics as particular cases a $q$-voter with stochastic drivings such as nonconformity and independence. inside particular, nonconsensus majority states are possible, as well as mixed phases. continuous and discontinuous phase transitions should occur, but also transitions from fluctuating phases into absorbing states.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
2947,"we present the new method of identifying the specific module inside the dynamic network, possibly with feedback loops. assuming known topology, we express a dynamics by an acyclic network composed of two blocks where a first block accounts considering a relation between a known reference signals and a input to a target module, while a second block contains a target module. with the help of an empirical bayes approach, we model a first block as the gaussian vector with covariance matrix (kernel) given by a recently introduced stable spline kernel. a parameters of a target module are estimated by solving the marginal likelihood problem with the novel iterative scheme based on a expectation-maximization algorithm. additionally, we extend a method to include additional measurements downstream of a target module. with the help of markov chain monte carlo techniques, it was shown that a same iterative scheme should solve also this formulation. numerical experiments illustrate a effectiveness of a proposed methods.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
18140,"a deep reinforcement learning community has made several independent improvements to a dqn algorithm. however, it was unclear which of these extensions are complementary and should be fruitfully combined. this paper examines six extensions to a dqn algorithm and empirically studies their combination. our experiments show that a combination provides state-of-the-art performance on a atari 2600 benchmark, both inside terms of data efficiency and final performance. we also provide results from the detailed ablation study that shows a contribution of each component to overall performance.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
10520,"this paper deals with two related problems, namely distance-preserving binary embeddings and quantization considering compressed sensing . first, we propose fast methods to replace points from the subset $\mathcal{x} \subset \mathbb{r}^n$, associated with a euclidean metric, with points inside a cube $\{\pm 1\}^m$ and we associate a cube with the pseudo-metric that approximates euclidean distance among points inside $\mathcal{x}$. our methods rely on quantizing fast johnson-lindenstrauss embeddings based on bounded orthonormal systems and partial circulant ensembles, both of which admit fast transforms. our quantization methods utilize noise-shaping, and include sigma-delta schemes and distributed noise-shaping schemes. a resulting approximation errors decay polynomially and exponentially fast inside $m$, depending on a embedding method. this dramatically outperforms a current decay rates associated with binary embeddings and hamming distances. additionally, it was a first such binary embedding result that applies to fast johnson-lindenstrauss maps while preserving $\ell_2$ norms. second, we again consider noise-shaping schemes, albeit this time to quantize compressed sensing measurements arising from bounded orthonormal ensembles and partial circulant matrices. we show that these methods yield the reconstruction error that again decays with a number of measurements (and bits), when with the help of convex optimization considering reconstruction. specifically, considering sigma-delta schemes, a error decays polynomially inside a number of measurements, and it decays exponentially considering distributed noise-shaping schemes based on beta encoding. these results are near optimal and a first of their kind dealing with bounded orthonormal systems.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14859,"graph models have long been used inside lieu of real data which should be expensive and hard to come by. the common class of models constructs the matrix of probabilities, and samples an adjacency matrix by flipping the weighted coin considering each entry. examples include a erd≈ës-r√©nyi model, chung-lu model, and a kronecker model. here we present a hyperkron graph model: an extension of a kronecker model, but with the distribution over hyperedges. we prove that we should efficiently generate graphs from this model inside order proportional to a number of edges times the small log-factor, and find that inside practice a runtime was linear with respect to a number of edges. we illustrate the number of useful features of a hyperkron model including non-trivial clustering and highly skewed degree distributions. finally, we fit a hyperkron model to real-world networks, and demonstrate a model's flexibility with the complex application of a hyperkron model to networks with coherent feed-forward loops.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
9473,"let $f$ be the cuspidal newform (holomorphic or maass) of arbitrary level and nebentypus and denote by $\lambda_f(n)$ its $n$-th hecke eigenvalue. let $$ r(n)=\#\left\{(n_1,n_2)\in \mathbb{z}^2:n_1^2+n_2^2=n\right\}. $$ inside this paper, we study a shifted convolution sum $$ \mathcal{s}_h(x)=\sum_{n\leq x}\lambda_f(n+h)r(n), \qquad 1\leq h\leq x, $$ and establish uniform bounds with respect to a shift $h$ considering $\mathcal{s}_h(x)$.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
10310,"we introduce the new definition of exponential family of markov chains, and show that many characteristic properties of a usual exponential family of probability distributions are properly extended to markov chains. a method of information geometry was effectively applied to our framework, which enables us to characterize a divergence rate of markov chain from the differential geometric viewpoint.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
15234,"a crossover from bardeen-cooper-schrieffer (bcs) superconductivity to bose-einstein condensation (bec) was difficult to realize inside quantum materials because, unlike inside ultracold atoms, one cannot tune a pairing interaction. we realize a bcs-bec crossover inside the nearly compensated semimetal fe$_{1+y}$se$_x$te$_{1-x}$ by tuning a fermi energy, $\epsilon_f$, using chemical doping, which permits us to systematically change $\delta / \epsilon_f$ from 0.16 to 0.5 were $\delta$ was a superconducting (sc) gap. we use angle-resolved photoemission spectroscopy to measure a fermi energy, a sc gap and characteristic changes inside a sc state electronic dispersion as a system evolves from the bcs to the bec regime. our results raise important questions about a crossover inside multiband superconductors which go beyond those addressed inside a context of cold atoms.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
11771,"interactions between vehicles and pedestrians have always been the major problem inside traffic safety. experienced human drivers are able to analyze a environment and choose driving strategies that will aid them avoid crashes. what was not yet clear, however, was how automated vehicles will interact with pedestrians. this paper proposes the new method considering evaluating a safety and feasibility of a driving strategy of automated vehicles when encountering unsignalized crossings. mobileye sensors installed on buses inside ann arbor, michigan, collected data on 2,973 valid crossing events. the stochastic interaction model is then created with the help of the multivariate gaussian mixture model. this model allowed us to simulate a movements of pedestrians reacting to an oncoming vehicle when approaching unsignalized crossings, and to evaluate a passing strategies of automated vehicles. the simulation is then conducted to demonstrate a evaluation procedure.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1
1213,"inside recent years, car makers and tech companies have been racing towards self driving cars. it seems that a main parameter inside this race was who will have a first car on a road. a goal of this paper was to add to a equation two additional crucial parameters. a first was standardization of safety assurance --- what are a minimal requirements that every self-driving car must satisfy, and how should we verify these requirements. a second parameter was scalability --- engineering solutions that lead to unleashed costs will not scale to millions of cars, which will push interest inside this field into the niche academic corner, and drive a entire field into the ""winter of autonomous driving"". inside a first part of a paper we propose the white-box, interpretable, mathematical model considering safety assurance, which we call responsibility-sensitive safety (rss). inside a second part we describe the design of the system that adheres to our safety assurance requirements and was scalable to millions of cars.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
13147,"we report a experimental demonstration of the class of ultrasonic metasurfaces made of patterned silicon thin wafers partially covered by si3n4 film that exhibit over 24 db of sound transmission loss around 0.7 mhz, which was caused by a cancelation of sound waves emitted by a resonant si3n4 membrane and a ones through a silicon backbone inside each unit cell. these metasurfaces are expected to have high reflection with little total loss even at ultrasonic frequency. they could be good candidates as a building blocks considering low-loss cavities, phase zone plates, and other underwater acoustic metamaterials. as a working principle was scalable, it provides guidance considering a designs of audible underwater sound barriers as well.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
17778,"our understanding of a world depends highly on our capacity to produce intuitive and simplified representations which should be easily used to solve problems. we reproduce this simplification process with the help of the neural network to build the low dimensional state representation of a world from images acquired by the robot. as inside jonschkowski et al. 2015, we learn inside an unsupervised way with the help of prior knowledge about a world as loss functions called robotic priors and extend this idea behind the method to high dimension richer images to learn the 3d representation of a hand position of the robot from rgb images. we propose the quantitative evaluation of a learned representation with the help of nearest neighbors inside a state space that allows to assess its quality and show both a potential and limitations of robotic priors inside realistic environments. we augment image size, add distractors and domain randomization, all crucial components to achieve transfer learning to real robots. finally, we also contribute the new prior to improve a robustness of a representation. a applications of such low dimensional state representation range from easing reinforcement learning (rl) and knowledge transfer across tasks, to facilitating learning from raw data with more efficient and compact high level representations. a results show that a robotic prior idea behind the method was able to extract high level representation as a 3d position of an arm and organize it into the compact and coherent space of states inside the challenging dataset.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
2755,"we address a multi-focus image fusion problem, where multiple images captured with different focal settings are to be fused into an all-in-focus image of higher quality. algorithms considering this problem necessarily admit a source image characteristics along with focused and blurred feature. however, most sparsity-based approaches use the single dictionary inside focused feature space to describe multi-focus images, and ignore a representations inside blurred feature space. here, we propose the multi-focus image fusion idea behind the method based on coupled sparse representation. a idea behind the method exploits a facts that (i) a patches inside given training set should be sparsely represented by the couple of overcomplete dictionaries related to a focused and blurred categories of images; and (ii) merging such representations leads to the more flexible and therefore better fusion strategy than a one based on just selecting a sparsest representation inside a original image estimate. by jointly learning a coupled dictionary, we enforce a similarity of sparse representations inside a focused and blurred feature spaces, and then introduce the fusion idea behind the method to combine these representations considering generating an all-in-focus image. we also discuss a advantages of a fusion idea behind the method based on coupled sparse representation and present an efficient algorithm considering learning a coupled dictionary. extensive experimental comparisons with state-of-the-art multi-focus image fusion algorithms validate a effectiveness of a proposed approach.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3538,"we calculate a spectrum of a andreev bound states inside the one-dimensional superconductor with the strong rashba spin-orbit coupling. we focus on a fate of a zero-energy andreev modes inside a presence of time reversal symmetry-breaking perturbations, both at a boundary and inside a bulk. it was shown that a zero modes are destroyed by time reversal symmetry-breaking fluctuations, even if a mean-field state of a system was time-reversal invariant and topologically nontrivial.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
9289,"it was shown that under certain conditions it was possible to model the complex system inside the way that leads to results that do not depend on system size. as an example of complex system an innovation diffusion model was considered. inside that model the set of individuals (the agents), which are interconnected, must decide if adopt or not an innovation. a agents are connected inside the member of a networks family known as small worlds networks (swn). it was found that considering the subfamily of a swn a saturation time and a form of a adoption curve are invariants respect to a change inside a size of a system.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
14873,"stochastic composition optimization draws much attention recently and has been successful inside many emerging applications of machine learning, statistical analysis, and reinforcement learning. inside this paper, we focus on a composition problem with nonsmooth regularization penalty. previous works either have slow convergence rate or do not provide complete convergence analysis considering a general problem. inside this paper, we tackle these two issues by proposing the new stochastic composition optimization method considering composition problem with nonsmooth regularization penalty. inside our method, we apply variance reduction technique to accelerate a speed of convergence. to a best of our knowledge, our method admits a fastest convergence rate considering stochastic composition optimization: considering strongly convex composition problem, our algorithm was proved to admit linear convergence; considering general composition problem, our algorithm significantly improves a state-of-the-art convergence rate from $o(t^{-1/2})$ to $o((n_1+n_2)^{{2}/{3}}t^{-1})$. finally, we apply our proposed algorithm to portfolio management and policy evaluation inside reinforcement learning. experimental results verify our theoretical analysis.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11476,"bots have been playing the crucial role inside online platform ecosystems, as efficient and automatic tools to generate content and diffuse information to a social media human population. inside this chapter, we will discuss a role of social bots inside content spreading dynamics inside social media. inside particular, we will first investigate some differences between diffusion dynamics of content generated by bots, as opposed to humans, inside a context of political communication, then study a characteristics of bots behind a diffusion dynamics of social media spam campaigns.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
3287,"networks provide an informative, yet non-redundant description of complex systems only if links represent truly dyadic relationships that cannot be directly traced back to node-specific properties such as size, importance, or coordinates inside some embedding space. inside any real-world network, some links may be reducible, and others irreducible, to such local properties. this dichotomy persists despite a steady increase inside data availability and resolution, which actually determines an even stronger need considering filtering techniques aimed at discerning essential links from non-essential ones. here we introduce the rigorous method that, considering any desired level of statistical significance, outputs a network backbone that was irreducible to a local properties of nodes, i.e. their degrees and strengths. unlike previous approaches, our method employs an exact maximum-entropy formulation guaranteeing that a filtered network encodes only a links that cannot be inferred from local information. extensive empirical analysis confirms that this idea behind the method uncovers essential backbones that are otherwise hidden amidst many redundant relationships and inaccessible to other methods. considering instance, we retrieve a hub-and-spoke skeleton of a us airport network and many specialised patterns of international trade. being irreducible to local transportation and economic constraints of supply and demand, these backbones single out genuinely higher-order wiring principles.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
732,"given a tension between a values of a hubble parameter $h_0$ inferred from a cosmic microwave background (cmb) and from supernovae, attention was turning to time delays of strongly lensed quasars. current time-delay measurements indicate the value of $h_0$ closer to that from supernovae, with errors on a order of the few percent, and future measurements aim to bring a errors down to a subpercent level. here we consider a uncertainties inside a mass distribution inside a outskirts of a lens. we show that these should lead to errors inside a inferred $h_0$ on a order of the percent and, once accounted for, would correct $h_0$ upward (thus increasing slightly a tension with a cmb). weak gravitational lensing and simulations may aid to reduce these uncertainties.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11512,"we develop the local cohomology theory considering fi$^m$-modules, and show that it inside many ways mimics a classical theory considering multi-graded modules over the polynomial ring. inside particular, we define an invariant of fi$^m$-modules with the help of this local cohomology theory which closely resembles an invariant of multi-graded modules over cox rings defined by maclagan and smith. it was then shown that this invariant behaves almost identically to a invariant of maclagan and smith.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
7722,"inside this paper, we propose an offline counterfactual policy approximation framework called genie to optimize sponsored search marketplace. genie employs an open box simulation engine with click calibration model to compute a kpi impact of any modification to a system. from a experimental results on bing traffic, we showed that genie performs better than existing observational approaches that employs randomized experiments considering traffic slices that have frequent policy updates. we also show that genie should be used to tune completely new policies efficiently without creating risky randomized experiments due to cold start problem. as time of today, genie hosts more than 10000 optimization jobs yearly which runs more than 30 million processing node hours of big data jobs considering bing ads. considering a last 3 years, genie has been proven to be a one of a major platforms to optimize bing ads marketplace due to its reliability under frequent policy changes and its efficiency to minimize risks inside real experiments.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
9067,"a high-temperature normal state of a unconventional cuprate superconductors has resistivity linear inside temperature $t$, which persists to values well beyond a mott-ioffe-regel upper bound. at low-temperature, within a pseudogap phase, a resistivity was instead quadratic inside $t$, as would be expected from fermi liquid theory. developing an understanding of these normal phases of a cuprates was crucial to explain a unconventional superconductivity. we present the simple explanation considering this behavior, inside terms of umklapp scattering of electrons. this fits within a general picture emerging from functional renormalization group calculations that spurred a yang-rice-zhang ansatz: umklapp scattering was at a heart of a behavior inside a normal phase.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
17954,"we describe and experimentally evaluate the stabilized radio-frequency transfer technique that employs optical phase sensing and optical phase actuation. this technique should be achieved by modifying existing stabilized optical frequency equipment and also exhibits advantages over previous stabilized radio-frequency transfer techniques inside terms of size and complexity. we demonstrate a stabilized transfer of the 160 mhz signal over an 166 km fiber optical link, achieving an allan deviation of 9.7x10^-12 hz/hz at 1 s of integration, and 3.9x10^-1414 hz/hz at 1000 s. this technique was being considered considering application to a square kilometre array ska1-low radio telescope.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
15882,"bounded weak solutions of burgers' equation $\partial_tu+\partial_x(u^2/2)=0$ that are not entropy solutions need inside general not be $bv$. nevertheless it was known that solutions with finite entropy productions have the $bv$-like structure: the rectifiable jump set of dimension one should be identified, outside which $u$ has vanishing mean oscillation at all points. but it was not known whether all points outside this jump set are lebesgue points, as they would be considering $bv$ solutions. inside a present article we show that a set of non-lebesgue points of $u$ has hausdorff dimension at most one. inside contrast with a aforementioned structure result, we need only one particular entropy production to be the finite radon measure, namely $\mu=\partial_t (u^2/2)+\partial_x(u^3/3)$. we prove h√∂lder regularity at points where $\mu$ has finite $(1+\alpha)$-dimensional upper density considering some $\alpha>0$. a proof was inspired by the result of de lellis, westdickenberg and a second author : if $\mu_+$ has vanishing 1-dimensional upper density, then $u$ was an entropy solution. we obtain the quantitative version of this statement: if $\mu_+$ was small then $u$ was close inside $l^1$ to an entropy solution.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15220,"this paper describes a details of sighthound's fully automated age, gender and emotion recognition system. a backbone of our system consists of several deep convolutional neural networks that are not only computationally inexpensive, but also provide state-of-the-art results on several competitive benchmarks. to power our novel deep networks, we collected large labeled datasets through the semi-supervised pipeline to reduce a annotation effort/time. we tested our system on several public benchmarks and report outstanding results. our age, gender and emotion recognition models are available to developers through a sighthound cloud api at this https url",1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16315,"a control of field robots inside varying and uncertain terrain conditions presents the challenge considering autonomous navigation. online approximation of a wheel-terrain slip characteristics was essential considering generating a accurate control predictions necessary considering tracking trajectories inside off-road environments. receding horizon approximation (rhe) provides the powerful framework considering constrained estimation, and when combined with receding horizon control (rhc), yields an adaptive optimisation-based control method. presently, such methods assume slip to be constant over a approximation horizon, while our proposed structured blocking idea behind the method relaxes this assumption, resulting inside improved state and parameter estimation. we demonstrate and compare a performance of this method inside simulation, and propose an overlapping-block strategy to ameliorate some of a limitations encountered inside applying noise-blocking inside the receding horizon approximation and control (rhec) context.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
16380,"inside this paper, we propose deep learning techniques considering econometrics, specifically considering causal inference and considering estimating individual as well as average treatment effects. a contribution of this paper was twofold: 1. considering generalized neighbor matching to approximate individual and average treatment effects, we analyze a use of autoencoders considering dimensionality reduction while maintaining a local neighborhood structure among a data points inside a embedding space. this deep learning based technique was shown to perform better than simple k nearest neighbor matching considering estimating treatment effects, especially when a data points have several features/covariates but reside inside the low dimensional manifold inside high dimensional space. we also observe better performance than manifold learning methods considering neighbor matching. 2. propensity score matching was one specific and popular way to perform matching inside order to approximate average and individual treatment effects. we propose a use of deep neural networks (dnns) considering propensity score matching, and present the network called propensitynet considering this. this was the generalization of a logistic regression technique traditionally used to approximate propensity scores and we show empirically that dnns perform better than logistic regression at propensity score matching. code considering both methods will be made available shortly on github at: this https url",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
13009,"we have developed the method to improve a doping computation efficiency, this method was based on first principles calculations and cluster expansion. first principles codes produce highly accurate total energies and optimized geometries considering any given structure. cluster expansion method constructs the cluster expansion with the help of partial first principles results and computes a energies considering other structures derived from the parent lattice. with the help of this method, energies considering multiple doping structures should be predicted quickly without series of first principles calculations. this method has been packaged into the tool named as ce screen and integrated into matcloud (a high-throughput first principles calculation platform). this makes a tool simple and easy considering all a users.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
12903,"with typical periods of order 10 minutes, a pulsation signatures of zz ceti variables (pulsating hydrogen-atmosphere white dwarf stars) are severely undersampled by long-cadence (29.42 minutes per exposure) k2 observations. nyquist aliasing renders a intrinsic frequencies ambiguous, stifling precision asteroseismology. we report a discovery of two new zz cetis inside long-cadence k2 data: epic 210377280 and epic 220274129. guided by 3-4 nights of follow-up, high-speed (<=30 s) photometry from mcdonald observatory, we recover accurate pulsation frequencies considering k2 signals that reflected 4-5 times off a nyquist with a full precision of over 70 days of monitoring (~0.01 muhz). inside turn, a k2 observations enable us to select a correct peaks from a alias structure of a ground-based signals caused by gaps inside a observations. we identify at least seven independent pulsation modes inside a light curves of each of these stars. considering epic 220274129, we detect three complete sets of rotationally split ell=1 (dipole mode) triplets, which we use to asteroseismically infer a stellar rotation period of 12.7+/-1.3 hr. we also detect two sub-nyquist k2 signals that are likely combination (difference) frequencies. we attribute our inability to match some of a k2 signals to a ground-based data to changes inside pulsation amplitudes between epochs of observation. model fits to soar spectroscopy place both epic 210377280 and epic 220274129 near a middle of a zz ceti instability strip, with teff = 11590+/-200 k and 11810+/-210 k, and masses 0.57+/-0.03 msun and 0.62+/-0.03 msun, respectively.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
6341,"excitonic states inside monolayer transition metal dichalcogenides (tmdcs) have been a subject of extensive recent interest. their intrinsic properties can, however, be obscured due to a influence of inhomogeneity inside a external environment. here we report methods considering fabricating high quality tmdc monolayers with narrow photoluminescence (pl) linewidth approaching a intrinsic limit. we find that encapsulation inside hexagonal boron nitride (h-bn) sharply reduces a pl linewidth, and that passivation of a oxide substrate by an alkyl monolayer further decreases a linewidth and also minimizes a charged exciton (trion) peak. a combination of these sample preparation methods results inside much reduced spatial variation inside a pl emission, with the full-width-at-half-maximum as low as 1.7 mev. analysis of a pl line shape yields the homogeneous width of 1.43$\pm$0.08 mev and inhomogeneous broadening of 1.1$\pm$0.3 mev.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
9762,"we present the new method of blackbox optimization using gradient approximation with a use of structured random orthogonal matrices, providing more accurate estimators than baselines and with provable theoretical guarantees. we show that this algorithm should be successfully applied to learn better quality compact policies than those with the help of standard gradient approximation techniques. a compact policies we learn have several advantages over unstructured ones, including faster training algorithms and faster inference. these benefits are important when a policy was deployed on real hardware with limited resources. further, compact policies provide more scalable architectures considering derivative-free optimization (dfo) inside high-dimensional spaces. we show that most robotics tasks from a openai gym should be solved with the help of neural networks with less than 300 parameters, with almost linear time complexity of a inference phase, with up to 13x fewer parameters relative to a evolution strategies (es) algorithm introduced by salimans et al. (2017). we do not need heuristics such as fitness shaping to learn good quality policies, resulting inside the simple and theoretically motivated training mechanism.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
15292,"inside this study, we present an analysis regarding a performance of a state-of-art phrase-based statistical machine translation (smt) on multiple indian languages. we report baseline systems on several language pairs. a motivation of this study was to promote a development of smt and linguistic resources considering these language pairs, as a current state-of-the-art was quite bleak due to sparse data resources. a success of an smt system was contingent on a availability of the large parallel corpus. such data was necessary to reliably approximate translation probabilities. we report a performance of baseline systems translating from indian languages (bengali, guajarati, hindi, malayalam, punjabi, tamil, telugu and urdu) into english with average 10% accurate results considering all a language pairs.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7739,"bourgain and chang recently showed that any subset of $\mathbb{f}_p$ of density $\gg p^{-1/15}$ contains the nontrivial progression $x,x+y,x+y^2$. we answer the question of theirs by proving that if $p_1,p_2\in\mathbb{z}[y]$ are linearly independent and satisfy $p_1(0)=p_2(0)=0$, then any subset of $\mathbb{f}_p$ of density $\gg_{p_1,p_2}p^{-1/24}$ contains the nontrivial polynomial progression $x,x+p_1(y),x+p_2(y)$.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
4525,"inside this paper, the subgroup least squares and the convex clustering are introduced considering inferring the partially heterogenous linear regression that has potential application inside a areas of precision marketing and precision medicine. a homogenous parameter and a subgroup-average of a heterogenous parameters should be consistently estimated by a subgroup least squares, without need of a sparsity assumption on a heterogenous parameters. a heterogenous parameters should be consistently clustered using a convex clustering. unlike a existing methods considering regression clustering, our clustering procedure was the standard mean clustering, although a model under study was the type of regression, and a corresponding algorithm only involves low dimensional parameters. thus, it was simple and stable even if a sample size was large. a advantage of a method was further illustrated using simulation studies and a analysis of car sales data.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
7569,"we describe the new method to automatically discriminate between patients with alzheimer's disease (ad) or mild cognitive impairment (mci) and elderly controls, based on multidimensional classification of hippocampal shape features. this idea behind the method uses spherical harmonics (spharm) coefficients to model a shape of a hippocampi, which are segmented from magnetic resonance images (mri) with the help of the fully automatic method that we previously developed. spharm coefficients are used as features inside the classification procedure based on support vector machines (svm). a most relevant features considering classification are selected with the help of the bagging strategy. we evaluate a accuracy of our method inside the group of 23 patients with ad (10 males, 13 females, age $\pm$ standard-deviation (sd) = 73 $\pm$ 6 years, mini-mental score (mms) = 24.4 $\pm$ 2.8), 23 patients with amnestic mci (10 males, 13 females, age $\pm$ sd = 74 $\pm$ 8 years, mms = 27.3 $\pm$ 1.4) and 25 elderly healthy controls (13 males, 12 females, age $\pm$ sd = 64 $\pm$ 8 years), with the help of leave-one-out cross-validation. considering ad vs controls, we obtain the correct classification rate of 94%, the sensitivity of 96%, and the specificity of 92%. considering mci vs controls, we obtain the classification rate of 83%, the sensitivity of 83%, and the specificity of 84%. this accuracy was superior to that of hippocampal volumetry and was comparable to recently published svm-based whole-brain classification methods, which relied on the different strategy. this new method may become the useful tool to assist inside a diagnosis of alzheimer's disease.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19650,"we investigate a distribution of relative velocities between small heavy particles of different sizes inside turbulence by analysing the statistical model considering bidisperse turbulent suspensions, containing particles with two different stokes numbers. this number, ${\rm st}$, was the measure of particle inertia which inside turn depends on particle size. when a stokes numbers are similar, a distribution exhibits power-law tails, just as inside a case of equal ${\rm st}$. a power-law exponent was the non-analytic function of a mean stokes number $\overline{\rm st}$, so that a exponent cannot be calculated inside perturbation theory around a advective limit. when a stokes-number difference was larger, a power law disappears, but a tails of a distribution still dominate a relative-velocity moments, if $\overline{\rm st}$ was large enough.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5371,"a aim of this work was to establish that two recently published projection theorems, one dealing with the parametric generalization of relative entropy and another dealing with r√©nyi divergence, are equivalent under the correspondence on a space of probability measures. further, we demonstrate that a associated ""pythagorean"" theorems are equivalent under this correspondence. finally, we apply eguchi's method of obtaining riemannian metrics from general divergence functions to show that a geometry arising from a above divergences are equivalent under a aforementioned correspondence.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
10351,"we present the novel extension of thompson sampling considering stochastic sequential decision problems with graph feedback, even when a graph structure itself was unknown and/or changing. we provide theoretical guarantees on a bayesian regret of a algorithm, linking its performance to a underlying properties of a graph. thompson sampling has a advantage of being applicable without a need to construct complicated upper confidence bounds considering different problems. we illustrate its performance through extensive experimental results on real and simulated networks with graph feedback. more specifically, we tested our algorithms on power law, planted partitions and erdo's-renyi graphs, as well as on graphs derived from facebook and flixster data. these all show that our algorithms clearly outperform related methods that employ upper confidence bounds, even if a latter use more information about a graph.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16028,"we study a sparse entropy-regularized reinforcement learning (erl) problem inside which a entropy term was the special form of a tsallis entropy. a optimal policy of this formulation was sparse, i.e.,~at each state, it has non-zero probability considering only the small number of actions. this addresses a main drawback of a standard shannon entropy-regularized rl (soft erl) formulation, inside which a optimal policy was softmax, and thus, may assign the non-negligible probability mass to non-optimal actions. this problem was aggravated as a number of actions was increased. inside this paper, we follow a work of nachum et al. (2017) inside a soft erl setting, and propose the class of novel path consistency learning (pcl) algorithms, called {\em sparse pcl}, considering a sparse erl problem that should work with both on-policy and off-policy data. we first derive the {\em sparse consistency} equation that specifies the relationship between a optimal value function and policy of a sparse erl along any system trajectory. crucially, the weak form of a converse was also true, and we quantify a sub-optimality of the policy which satisfies sparse consistency, and show that as we increase a number of actions, this sub-optimality was better than that of a soft erl optimal policy. we then use this result to derive a sparse pcl algorithms. we empirically compare sparse pcl with its soft counterpart, and show its advantage, especially inside problems with the large number of actions.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3506,"we introduce a fastron, the configuration space (c-space) model to be used as the proxy to kinematic-based collision detection. a fastron allows iterative updates to account considering the changing environment through the combination of the novel formulation of a kernel perceptron learning algorithm and an active learning strategy. our simulations on the 7 degree-of-freedom arm indicate that proxy collision checks may be performed at least 2 times faster than an efficient polyhedral collision checker and at least 8 times faster than an efficient high-precision collision checker. a fastron model provides conservative collision status predictions by padding c-space obstacles, and proxy collision checking time does not scale poorly as a number of workspace obstacles increases. all results were achieved without gpu acceleration or parallel computing.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
9028,"we describe the mechanism by which artificial neural networks should learn rapid adaptation - a ability to adapt on a fly, with little data, to new tasks - that we call conditionally shifted neurons. we apply this mechanism inside a framework of metalearning, where a aim was to replicate some of a flexibility of human learning inside machines. conditionally shifted neurons modify their activation values with task-specific shifts retrieved from the memory module, which was populated rapidly based on limited task experience. on metalearning benchmarks from a vision and language domains, models augmented with conditionally shifted neurons achieve state-of-the-art results.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2232,"this paper derives new formulations considering designing dominant pole placement based proportional-integral-derivative (pid) controllers to handle second order processes with time delays (soptd). previously, similar attempts have been made considering pole placement inside delay-free systems. a presence of a time delay term manifests itself as the higher order system with variable number of interlaced poles and zeros upon pade approximation, which makes it difficult to achieve precise pole placement control. we here report a analytical expressions to constrain a closed loop dominant and non-dominant poles at a desired locations inside a complex s-plane, with the help of the third order pade approximation considering a delay term. however, invariance of a closed loop performance with different time delay approximation has also been verified with the help of increasing order of pade, representing the closed to reality higher order delay dynamics. a choice of a nature of non-dominant poles e.g. all being complex, real or the combination of them modifies a characteristic equation and influences a achievable stability regions. a effect of different types of non-dominant poles and a corresponding stability regions are obtained considering nine test-bench processes indicating different levels of open-loop damping and lag to delay ratio. next, we investigate which expression yields the wider stability region inside a design parameter space by with the help of monte carlo simulations while uniformly sampling the chosen design parameter space. various time and frequency domain control performance parameters are investigated next, as well as their deviations with uncertain process parameters, with the help of thousands of monte carlo simulations, around a robust stable solution considering each of a nine test-bench processes.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13627,"inside this article we show a duality between tensor networks and undirected graphical models with discrete variables. we study tensor networks on hypergraphs, which we call tensor hypernetworks. we show that a tensor hypernetwork on the hypergraph exactly corresponds to a graphical model given by a dual hypergraph. we translate various notions under duality. considering example, marginalization inside the graphical model was dual to contraction inside a tensor network. algorithms also translate under duality. we show that belief propagation corresponds to the known algorithm considering tensor network contraction. this article was the reminder that a research areas of graphical models and tensor networks should benefit from interaction.",1,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0
11275,"we study a coupling between conventional (maxwell) and emergent electrodynamics inside quantum spin ice, the 3+1-dimensional $u(1)$ quantum spin liquid. we find that the uniform electric field should be used to tune a properties of both a ground state and excitations of a spin liquid. inside particular, it induces emergent birefringence, rendering a speed of a emergent light anisotropic and polarization-dependent. the sufficiently strong electric field triggers the quantum phase transition into new $u(1)$ quantum spin liquid phases which trap emergent electric $\pi$-fluxes. a flux patterns of these new phases depend on a direction of a electric field. strikingly, some of a canonical pinch points inside a spin structure factor, characteristic of classical spin ice, emerge near a phase transition, while they are absent inside a quantum spin liquid phases. estimating a electric field strength required, we find that this transition was potentially accessible experimentally. finally, we propose the minimal mechanism by which an oscillating electric field should generate emergent radiation in the quantum spin ice material with non-kramers spin doublets.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
17059,"let $p\ge5$ be the prime and $t$ the kodaira type of a special fiber of an elliptic curve. we approximate a number of elliptic curves over $\mathbb q$ up to height $x$ with kodaira type $t$ at $p$. this enables us find a proportion of elliptic curves over $\mathbb q$, when ordered by height, with kodaira type $t$ at the prime $p\ge5$ in a set of all elliptic curves. this proportion was the rational function inside $p$. considering instance, we show that $\displaystyle\frac{p^8(p-1)}{p^9-1}$ of all elliptic curves with bad reduction at $p$ are of multiplicative reduction. furthermore, we prove that a prime-to-$6$ part of a conductors of the majority ($=\zeta(10)/\zeta(2)\approx 0.6$) of elliptic curves are squarefree, where $\zeta$ was a riemann-zeta function.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
10716,"a prime graph question considering integral group rings asks if it was true that if a normalized unit group of a integral group ring of the finite group $g$ contains an element of order $pq$, considering some primes $p$ and $q$, also $g$ contains an element of that order. we answer this question considering a three conway sporadic simple groups after reducing it to the combinatorial question about young tableaus and littlewood-richardson coefficients. this finishes work of v. bovdi, a. konovalov and s. linton.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
3483,"dictionary learning and component analysis are part of one of a most well-studied and active research fields, at a intersection of signal and image processing, computer vision, and statistical machine learning. inside dictionary learning, a current methods of choice are arguably k-svd and its variants, which learn the dictionary (i.e., the decomposition) considering sparse coding using singular value decomposition. inside robust component analysis, leading methods derive from principal component pursuit (pcp), which recovers the low-rank matrix from sparse corruptions of unknown magnitude and support. however, k-svd was sensitive to a presence of noise and outliers inside a training set. additionally, pcp does not provide the dictionary that respects a structure of a data (e.g., images), and requires expensive svd computations when solved by convex relaxation. inside this paper, we introduce the new robust decomposition of images by combining ideas from sparse dictionary learning and pcp. we propose the novel kronecker-decomposable component analysis which was robust to gross corruption, should be used considering low-rank modeling, and leverages separability to solve significantly smaller problems. we design an efficient learning algorithm by drawing links with the restricted form of tensor factorization. a effectiveness of a proposed idea behind the method was demonstrated on real-world applications, namely background subtraction and image denoising, by performing the thorough comparison with a current state of a art.",1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6460,"global recruitment into radical islamic movements has spurred renewed interest inside a appeal of political extremism. was a appeal the rational response to material conditions or was it a expression of psychological and personality disorders associated with aggressive behavior, intolerance, conspiratorial imagination, and paranoia? empirical answers with the help of surveys have been limited by lack of access to extremist groups, while field studies have lacked psychological measures and failed to compare extremists with contrast groups. we revisit a debate over a appeal of extremism inside a u.s. context by comparing publicly available twitter messages written by over 355,000 political extremist followers with messages written by non-extremist u.s. users. analysis of text-based psychological indicators supports a moral foundation theory which identifies emotion as the critical factor inside determining political orientation of individuals. extremist followers also differ from others inside four of a big five personality traits.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
17083,"obtaining reliable uncertainty estimates of neural network predictions was the long standing challenge. bayesian neural networks have been proposed as the solution, but it remains open how to specify their prior. inside particular, a common practice of the standard normal prior inside weight space imposes only weak regularities, causing a function posterior to possibly generalize inside unforeseen ways on inputs outside of a training distribution. we propose noise contrastive priors (ncps) to obtain reliable uncertainty estimates. a key idea was to train a model to output high uncertainty considering data points outside of a training distribution. ncps do so with the help of an input prior, which adds noise to a inputs of a current mini batch, and an output prior, which was the wide distribution given these inputs. ncps are compatible with any model that should output uncertainty estimates, are easy to scale, and yield reliable uncertainty estimates throughout training. empirically, we show that ncps prevent overfitting outside of a training distribution and result inside uncertainty estimates that are useful considering active learning. we demonstrate a scalability of our method on a flight delays data set, where we significantly improve upon previously published results.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
10517,"we exhibit the method to use continued fractions inside function fields to find new families of hyperelliptic curves over a rationals with given torsion order inside their jacobians. to show a utility of a method, we exhibit the new infinite family of curves over $\mathbb q$ with genus two whose jacobians have torsion order eleven.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
16110,"experiment and theory indicate that upt3 was the topological superconductor inside an odd-parity state, based inside part from temperature independence of a nmr knight shift. however, quasiparticle spin-flip scattering near the surface, where a knight shift was measured, might be responsible. we use polarized neutron scattering to measure a bulk susceptibility with h||c, finding consistency with a knight shift but inconsistent with theory considering this field orientation. we infer that neither spin susceptibility nor knight shift are the reliable indication of odd-parity.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
16661,"inside 2000, dergachev and kirillov introduced subalgebras of ""seaweed type"" inside $\mathfrak{gl}_n$ and computed their index with the help of certain graphs, which we call type-${\sf a}$ meander graphs. then a subalgebras of seaweed type, or just ""seaweeds"", have been defined by panyushev (2001) considering arbitrary reductive lie algebras. recently, the meander graph idea behind the method to computing a index inside types ${\sf b}$ and ${\sf c}$ has been developed by a authors. inside this article, we consider a most difficult and interesting case of type ${\sf d}$. some new phenomena occurring here are related to a fact that a dynkin diagram has the branching node.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
3260,"knowledge of a noise distribution inside magnitude diffusion mri images was a centerpiece to quantify uncertainties arising from a acquisition process. a use of parallel imaging methods, a number of receiver coils and imaging filters applied by a scanner, amongst other factors, dictate a resulting signal distribution. accurate approximation beyond textbook rician or noncentral chi distributions often requires information about a acquisition process (e.g. coils sensitivity maps or reconstruction coefficients), which was not usually available. we introduce the new method where the change of variable naturally gives rise to the particular form of a gamma distribution considering background signals. a first moments and maximum likelihood estimators of this gamma distribution explicitly depend on a number of coils, making it possible to approximate all unknown parameters with the help of only a magnitude data. the rejection step was used to make a method automatic and robust to artifacts. experiments on synthetic datasets show that a proposed method should reliably approximate both a degrees of freedom and a standard deviation. a worst case errors range from below 2% (spatially uniform noise) to approximately 10% (spatially variable noise). repeated acquisitions of inside vivo datasets show that a estimated parameters are stable and have lower variances than compared methods.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8530,"generative adversarial networks (gans) are the class of generative algorithms that have been shown to produce state-of-the art samples, especially inside a domain of image creation. a fundamental principle of gans was to approximate a unknown distribution of the given data set by optimizing an objective function through an adversarial game between the family of generators and the family of discriminators. inside this paper, we offer the better theoretical understanding of gans by analyzing some of their mathematical and statistical properties. we study a deep connection between a adversarial principle underlying gans and a jensen-shannon divergence, together with some optimality characteristics of a problem. an analysis of a role of a discriminator family using approximation arguments was also provided. inside addition, taking the statistical point of view, we study a large sample properties of a estimated distribution and prove inside particular the central limit theorem. some of our results are illustrated with simulated examples.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18485,"many real-world phenomena are best represented as interaction networks with dynamic structures (e.g., transaction networks, social networks, traffic networks). interaction networks capture flow of data which was transferred between their vertices along the timeline. analyzing such networks was crucial toward comprehend- ing processes inside them. the typical analysis task was a finding of motifs, which are small subgraph patterns that repeat themselves inside a network. inside this paper, we introduce network flow motifs, the novel type of motifs that model significant flow transfer among the set of vertices within the constrained time window. we design an algorithm considering identifying flow motif instances inside the large graph. our algorithm should be easily adapted to find a top-k instances of maximal flow. inside addition, we design the dynamic programming module that finds a instance with a maximum flow. we evaluate a performance of a algorithm on three real datasets and identify flow motifs which are significant considering these graphs. our results show that our algorithm was scalable and that a real networks indeed include interesting motifs, which appear much more frequently than inside randomly generated networks having similar characteristics.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
17012,a space of embedded submanifolds plays an important role inside applications such as computational anatomy and shape analysis. we should define two different classes on riemannian metrics on this space: so-called outer metrics are metrics that measure shape changes with the help of deformations of a ambient space and they find applications mostly inside computational anatomy; a second class that are defined directly on a space of embeddings with the help of intrinsic differential operations and they are used inside shape analysis. inside this paper we compare considering a first time a topologies and a geodesic distance functions induced by these a two classes of metrics.,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4194,"while convergence of a alternating direction method of multipliers (admm) on convex problems was well studied, convergence on nonconvex problems was only partially understood. inside this paper, we consider a gaussian phase retrieval problem, formulated as the linear constrained optimization problem with the biconvex objective. a particular structure allows considering the novel application of a admm. it should be shown that a dual variable was zero at a global minimizer. this motivates a analysis of the block coordinate descent algorithm, which was equivalent to a admm with a dual variable fixed to be zero. we show that a block coordinate descent algorithm converges to a global minimizer at the linear rate, when starting from the deterministically achievable initialization point.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1675,"we show how to compute considering $n$-vertex planar graphs inside $o(n^{11/6}{\rm polylog}(n))$ expected time a diameter and a sum of a pairwise distances. a algorithms work considering directed graphs with real weights and no negative cycles. inside $o(n^{15/8}{\rm polylog}(n))$ expected time we should also compute a number of pairs of vertices at distance smaller than the given threshold. these are a first algorithms considering these problems with the help of time $o(n^c)$ considering some constant $c<2$, even when restricted to undirected, unweighted planar graphs.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
264,"let k be the base field of positive characteristic. making use of topological periodic cyclic homology, we start by proving that a category of noncommutative numerical motives over k was abelian semi-simple, as conjectured by kontsevich. then, we establish the far-reaching noncommutative generalization of a weil conjectures, originally proved by dwork and grothendieck. inside a same vein, we establish the far-reaching noncommutative generalization of a cohomological interpretations of a hasse-weil zeta function inside terms of topological periodic cyclic homology, originally proven by hesselholt. as the third main result, we prove that a numerical grothendieck group of every smooth proper dg category was the finitely generated free abelian group. then, we introduce a noncommutative motivic galois (super-)groups and, following an insight of kontsevich, relate them to their classical commutative counterparts. finally, we explain how a motivic measure induced by berthelot's rigid cohomology theory should be recovered from noncommutative motives.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
12946,"we present the coupled 3-d atmospheric dynamics and radiative transfer model to predict a disk-integrated thermal emission spectra of transiting exoplanets inside edge-on orbits. we calculate spectra at high resolution to examine a extent to which high-resolution emission spectra are influenced by 3-d atmospheric dynamics and planetary rotation, and to determine whether and how we should constrain thermal structures and atmospheric dynamics through high-resolution spectroscopy. this study represents a first time that a line-of-sight geometry and resulting doppler shifts from winds and rotation have been treated self-consistently inside an emission spectrum radiative transfer model, which allow us to assess a impact of a velocity field on thermal emission spectra. we apply our model to predict emission spectra as the function of orbital phase considering three hot jupiters, hd 209458b, wasp-43b and hd 189733b. we find net doppler shifts inside modeled spectra due to the combination of winds and rotation at the level of 1-3 km/s. these doppler signatures vary inside the quasi-sinusoidal pattern over a course of a planets' orbits as a hot spots idea behind the method and recede from a observer's viewpoint. we predict that wasp-43b produces a largest doppler shift due to its fast rotation rate. we find that a net doppler shift inside an exoplanet's disk-integrated thermal emission spectrum results from the complex combination of winds, rotation, and thermal structure. however, we offer the simple method that estimates a magnitude of equatorial wind speeds inside hot jupiters through measurements of net doppler shifts and lower resolution thermal phase curves.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5392,"we consider a heat equation associated with the class of hypoelliptic operators of kolmogorov-fokker-planck type inside dimension two. we explicitly compute a first meaningful coefficient of a small time asymptotic expansion of a heat kernel on a diagonal, and we interpret it inside terms of curvature-like invariants of a optimal control problem associated with a diffusion. this gives the first example of geometric interpretation of a small-time heat kernel asymptotics of non-homogeneous h√∂rmander operators which are not associated with the sub-riemannian structure, i.e., whose second-order part does not satisfy a h√∂rmander condition.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15080,"inside order to generate a valid numerical simulation model, a sufficient amount of gathered data from a oil field was required. however, it was not always possible to acquire such data at a initial stage of project development. buckley and leverett (1942) developed a analytical solution allowing to assess a oil displacement efficiency. one of a main assumptions of this model was incompressibility of oil and injected fluid. considering slightly compressible water and oil such assumption was rational. however, that was not always a case when a gas was injected. this research aims to identify a conditions at which a usage of a incompressible gas model was appropriate. likewise, a cases when a model of compressible gas was required are also evaluated. to accomplish a goals of this research, a comparative analysis between a injection of compressible and incompressible gases is undertaken with the help of a numerical solution of a correspondent reservoir engineering problem. a validation of a numerical model is undertaken showing that it matches a analytical buckley-leverett solution. a findings of this research indicate that a relative and absolute density change with a pressure of a injected gas has a profound impact on a convergence between two models under consideration. with a increase inside a injection pressure, a discrepancy between a models of compressible and incompressible gas raises considering all a considered injection fluids (co2, ch4 and n2). due to the steep slope of 'density-pressure' curve considering co2 at low initial reservoir pressure, a incompressible model cannot accurately predict a oil displacement efficiency by this gas at any reasonable injection pressure. all 1d results are also representative considering 2d simulation. however, a mismatch between two models increases considerably considering 2d simulation scenarios.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1756,"this paper describes the collaborative modelling idea behind the method to automated and robotic agricultural vehicle design. a cresendo technology allows engineers from different disciplines to collaborate and produce system models. a combined models are called co-models and their execution co-simulation. to support future development efforts the template library of different vehicle and controllers types are provided. this paper describes the methodology to developing co-models from initial problem definition to deployment of a actual system. we illustrate a development methodology with an example development case from a agricultural domain. a case relates to an encountered speed controller problem on the differential driven vehicle, where we iterate through different candidate solutions and end up with an adaptive controller solution based on the combination of classical control and learning feedforward. a second case was an example of combining human control interface and co-simulation of agricultural robotic operation to illustrate collaborative development",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
5634,"we consider an intermediate category between a category of finite quivers and the certain category of pseudocompact associative algebras whose objects include all pointed finite dimensional algebras. we define a completed path algebra and a gabriel quiver as functors. we give an explicit quotient of a category of algebras on which these functors form an adjoint pair. we show that these functors respect ideals, obtaining inside this way an equivalence between related categories.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
15919,"ensemble weather predictions require statistical post-processing of systematic errors to obtain reliable and accurate probabilistic forecasts. traditionally, this was accomplished with distributional regression models inside which a parameters of the predictive distribution are estimated from the training period. we propose the flexible alternative based on neural networks that should incorporate nonlinear relationships between arbitrary predictor variables and forecast distribution parameters that are automatically learned inside the data-driven way rather than requiring pre-specified link functions. inside the case study of 2-meter temperature forecasts at surface stations inside germany, a neural network idea behind the method significantly outperforms benchmark post-processing methods while being computationally more affordable. key components to this improvement are a use of auxiliary predictor variables and station-specific information with a aid of embeddings. furthermore, a trained neural network should be used to gain insight into a importance of meteorological variables thereby challenging a notion of neural networks as uninterpretable black boxes. our idea behind the method should easily be extended to other statistical post-processing and forecasting problems. we anticipate that recent advances inside deep learning combined with a ever-increasing amounts of model and observation data will transform a post-processing of numerical weather forecasts inside a coming decade.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18468,"we introduce torchbearer, the model fitting library considering pytorch aimed at researchers working on deep learning or differentiable programming. a torchbearer library provides the high level metric and callback api that should be used considering the wide range of applications. we also include the series of built inside callbacks that should be used for: model persistence, learning rate decay, logging, data visualization and more. a extensive documentation includes an example library considering deep learning and dynamic programming problems and should be found at this http url. a code was licensed under a mit license and available at this https url.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
9291,"the strong mode of the probability measure on the normed space $x$ should be defined as the point $u$ such that a mass of a ball centred at $u$ uniformly dominates a mass of all other balls inside a small-radius limit. helin and burger weakened this definition by considering only pairwise comparisons with balls whose centres differ by vectors inside the dense, proper linear subspace $e$ of $x$, and posed a question of when these two types of modes coincide. we show that, inside the more general setting of metrisable vector spaces equipped with measures that are finite on bounded sets, a density of $e$ and the uniformity condition suffice considering a equivalence of these two types of modes. we accomplish this by introducing the new, intermediate type of mode. we also show that these modes should be inequivalent if a uniformity condition fails. our results shed light on a relationships between among various notions of maximum the posteriori estimator inside non-parametric bayesian inference.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
4351,"consider the vector bundle over the k√§hler manifold which admits the hermitian yang-mills connection. we show that a pullback bundle on a blowup of a k√§hler manifold at the collection of points also admits the hermitian yang-mills connection, considering k√§hler classes on a blowup which make a exceptional divisors small. our proof uses gluing techniques, and was thus asymptotically explict. this recovers, through a hitchin-kobayashi correspondence, algebro-geometric results due to buchdahl and sibley.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1198,"lower bound on a rate of decrease inside time of a uniform radius of spatial analyticity of solutions to a quartic generalized kdv equation was derived, which improves an earlier result by bona, grujiƒá and kalisch.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11849,"after introducing a concept of commutativity considering continuous-time linear time-varying systems, a related literature and a results obtained so far are presented. considering the simple introduction of a commutativity of discrete-time linear time-varying systems, a problem was formulated considering first-order systems. finally, explicit necessary and sufficient conditions considering a commutativity of first-order discrete-time linear time-varying systems are derived, and their advantageous use inside digital system design was illustrated; which are a main objectives of a paper. a results are verified by examples which include an application inside amplitude modulation considering digital telecommunication.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
1349,"knowing the biomolecule's structure was inherently linked to and the prerequisite considering any detailed understanding of its function. significant effort has gone into developing technologies considering structural characterization. these technologies do not directly provide 3d structures; instead they typically yield noisy and erroneous distance information between specific entities such as atoms or residues, which have to be translated into consistent 3d models. here we present an idea behind the method considering this translation process based on maxent-stress optimization. our new idea behind the method extends a original graph drawing method considering a new application's specifics by introducing additional constraints and confidence values as well as algorithmic components. extensive experiments demonstrate that our idea behind the method infers structural models (i. e., sensible 3d coordinates considering a molecule's atoms) that correspond well to a distance information, should handle noisy and error-prone data, and was considerably faster than established tools. our results promise to allow domain scientists nearly-interactive structural modeling based on distance constraints.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8657,"we study a regularity of a solutions of second order boundary value problems on manifolds with boundary and bounded geometry. we first show that a regularity property of the given boundary value problem $(p, c)$ was equivalent to a uniform regularity of a natural family $(p_x, c_x)$ of associated boundary value problems inside local coordinates. we verify that this property was satisfied considering a dirichlet boundary conditions and strongly elliptic operators using the compactness argument. we then introduce the uniform shapiro-lopatinski regularity condition, which was the modification of a classical one, and we prove that it characterizes a boundary value problems that satisfy a usual regularity property. we also show that a natural robin boundary conditions always satisfy a uniform shapiro-lopatinski regularity condition, provided that our operator satisfies a strong legendre condition. this was achieved by proving that ""well-posedness implies regularity"" using the modification of a classical ""nirenberg trick"". when combining our regularity results with a poincar√© inequality of (ammann-grosse-nistor, preprint 2015), one obtains a usual well-posedness results considering a classical boundary value problems inside a usual scale of sobolev spaces, thus extending these important, well-known theorems from smooth, bounded domains, to manifolds with boundary and bounded geometry. as we show inside several examples, these results do not hold true anymore if one drops a bounded geometry assumption. we also introduce the uniform agmon condition and show that it was equivalent to a coerciveness. consequently, we prove the well-posedness result considering parabolic equations whose elliptic generator satisfies a uniform agmon condition.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15229,"significant parts of a recent learning literature on stochastic optimization algorithms focused on a theoretical and practical behaviour of stochastic first order schemes under different convexity properties. due to its simplicity, a traditional method of choice considering most supervised machine learning problems was a stochastic gradient descent (sgd) method. many iteration improvements and accelerations have been added to a pure sgd inside order to boost its convergence inside various (strong) convexity setting. however, a lipschitz gradient continuity or bounded gradients assumptions are an essential requirement considering most existing stochastic first-order schemes. inside this paper novel convergence results are presented considering a stochastic proximal point algorithm inside different settings. inside particular, without any strong convexity, smoothness or bounded gradients assumptions, we show that the slightly modified quadratic growth assumption was sufficient to guarantee considering a stochastic proximal point $\mathcal{o}\left(\frac{1}{k}\right)$ convergence rate, inside terms of a distance to a optimal set. furthermore, linear convergence was obtained considering interpolation setting, when a optimal set of expected cost was included inside a optimal sets of each functional component.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4483,"a structural polymorphism inside transition metal dichalcogenides (tmds) provides exciting opportunities considering developing advanced electronics. considering example, mote$_2$ crystallizes inside a 2h semiconducting phase at ambient temperature and pressure, but transitions into a 1t$^\prime$ semimetallic phase at high temperatures. alloying mote$_2$ with wte$_2$ reduces a energy barrier between these two phases, while also allowing access to a t$_d$ weyl semimetal phase. a mowte$_2$ alloy system was therefore promising considering developing phase change memory technology. however, achieving this goal necessitates the detailed understanding of a phase composition inside a mote$_2$-wte$_2$ system. we combine polarization-resolved raman spectroscopy with x-ray diffraction (xrd) and scanning transmission electron microscopy (stem) to study mowte$_2$ alloys over a full compositional range x from 0 to 1. we identify raman and xrd signatures characteristic of a 2h, 1t$^\prime$, and t$_d$ structural phases that agree with density-functional theory (dft) calculations, and use them to identify phase fields inside a mote$_2$-wte$_2$ system, including single-phase 2h, 1t$^\prime$, and t$_d$ regions, as well as the two-phase 1t$^\prime$ + t$_d$ region. disorder arising from compositional fluctuations inside mowte$_2$ alloys breaks inversion and translational symmetry, leading to a activation of an infrared 1t$^\prime$-mote$_2$ mode and a enhancement of the double-resonance raman process inside 2h-mowte$_2$ alloys. compositional fluctuations limit a phonon correlation length, which we approximate by fitting a observed asymmetric raman lineshapes with the phonon confinement model. these observations reveal a important role of disorder inside mowte$_2$ alloys, clarify a structural phase boundaries, and provide the foundation considering future explorations of phase transitions and electronic phenomena inside this system.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
2268,"we study a problem of semantic code repair, which should be broadly defined as automatically fixing non-syntactic bugs inside source code. a majority of past work inside semantic code repair assumed access to unit tests against which candidate repairs could be validated. inside contrast, a goal here was to develop the strong statistical model to accurately predict both bug locations and exact fixes without access to information about a intended correct behavior of a program. achieving such the goal requires the robust contextual repair model, which we train on the large corpus of real-world source code that has been augmented with synthetically injected bugs. our framework adopts the two-stage idea behind the method where first the large set of repair candidates are generated by rule-based processors, and then these candidates are scored by the statistical model with the help of the novel neural network architecture which we refer to as share, specialize, and compete. specifically, a architecture (1) generates the shared encoding of a source code with the help of an rnn over a abstract syntax tree, (2) scores each candidate repair with the help of specialized network modules, and (3) then normalizes these scores together so they should compete against one another inside comparable probability space. we evaluate our model on the real-world test set gathered from github containing four common categories of bugs. our model was able to predict a exact correct repair 41\% of a time with the single guess, compared to 13\% accuracy considering an attentional sequence-to-sequence model.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7663,"a hybrid organic-inorganic lead halide perovskite materials have emerged as remarkable materials considering photovoltaic applications. their strengths include good electric transport properties inside spite of a disorder inherent inside them. motivated by this observation, we analyze a effects of disorder on a energy eigenstates of the tight-binding model of these materials. inside particular, we analyze a spatial extension of a energy eigenstates, which was quantified by a inverse participation ratio. this parameter exhibits the tendency, and possibly the phase transition, to localization as a on-site energy disorder strength was increased. however, we argue that a disorder inside a lead halide perovskites corresponds to the point inside a regime of highly delocalized states. our results also suggest that a electronic states of mixed-halide materials tend to be more localized than those of pure materials, which suggests the weaker tendency to form extended bonding states inside a mixed-halide materials and was therefore not favourable considering halide mixing.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
17351,"umap (uniform manifold approximation and projection) was the novel manifold learning technique considering dimension reduction. umap was constructed from the theoretical framework based inside riemannian geometry and algebraic topology. a result was the practical scalable algorithm that applies to real world data. a umap algorithm was competitive with t-sne considering visualization quality, and arguably preserves more of a global structure with superior run time performance. furthermore, umap has no computational restrictions on embedding dimension, making it viable as the general purpose dimension reduction technique considering machine learning.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1351,"we demonstrate the path to hitherto unachievable differential photometric precisions from a ground, both inside a optical and near-infrared (nir), with the help of custom-fabricated beam-shaping diffusers produced with the help of specialized nanofabrication techniques. such diffusers mold a focal plane image of the star into the broad and stable top-hat shape, minimizing photometric errors due to non-uniform pixel response, atmospheric seeing effects, imperfect guiding, and telescope-induced variable aberrations seen inside defocusing. this psf reshaping significantly increases a achievable dynamic range of our observations, increasing our observing efficiency and thus better averages over scintillation. diffusers work inside both collimated and converging beams. we present diffuser-assisted optical observations demonstrating $62^{+26}_{-16}$ppm precision inside 30 minute bins on the nearby bright star 16-cygni the (v=5.95) with the help of a arc 3.5m telescope---within the factor of $\sim$2 of kepler's photometric precision on a same star. we also show the transit of wasp-85-ab (v=11.2) and tres-3b (v=12.4), where a residuals bin down to $180^{+66}_{-41}$ppm inside 30 minute bins considering wasp-85-ab---a factor of $\sim$4 of a precision achieved by a k2 mission on this target---and to 101ppm considering tres-3b. inside a nir, where diffusers may provide even more significant improvements over a current state of a art, our preliminary tests have demonstrated $137^{+64}_{-36}$ppm precision considering the $k_s =10.8$ star on a 200"" hale telescope. these photometric precisions match or surpass a expected photometric precisions of tess considering a same magnitude range. this technology was inexpensive, scalable, easily adaptable, and should have an important and immediate impact on a observations of transits and secondary eclipses of exoplanets.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
10019,"we consider a initial value problem associated to a neural field equation of amari type with plasticity \[ u_t(x,t)=-u(x,t)+\int_{\omega}w(x,y)[1+\gamma g( u(x,t) - u(y,t) )] f(u(y,t))\; dy, \;(x,t) \in \omega \times (0, \infty), \] where $\omega\subset\mathbb{r}^m$, $f$ and $g$ are bounded and continuously differentiable functions with bounded derivative, and $\gamma\ge0$ was a plasticity synaptic coefficient. we show that a problem was well posed inside $c_b(\mathbb{r}^m)$ and $l^1(\omega)$ with $\omega$ compact. a proof follows from the classical fixed point argument when we consider a equation's flow. strong convergence of solutions inside a no plasticity limit ($\gamma\to0$) to solutions of amari's equation was analysed. finally, we prove existence of stationary solutions inside the general way. as the particular case, we show that a amari's model, after learning, leads to a stationary schr√∂dinger equation considering the type of gain modulation.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6155,"much of statistics relies upon four key elements: the law of large numbers, the calculus to operationalize stochastic convergence, the central limit theorem, and the framework considering constructing local approximations. these elements are well-understood considering objects inside the vector space (e.g., points or functions); however, much statistical theory does not directly translate to sets because they do not form the vector space. building on probability theory considering random sets, this paper uses variational analysis to develop operational tools considering statistics with set-valued functions. these tools are first applied to nonparametric approximation (kernel regression of set-valued functions). a second application was to a problem of inverse approximate optimization, inside which approximate solutions (corrupted by noise) to an optimization problem are observed and then used to approximate a amount of suboptimality of a solutions and a parameters of a optimization problem that generated a solutions. we show that previous approaches to this problem are statistically inconsistent when a data was corrupted by noise, whereas our idea behind the method was consistent under mild conditions.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0
17523,"transition metal dipnictides (tmds) have recently been identified as possible candidates to host topology protected electronic band structure. these materials belong to an isostructural family and show several exotic transport properties. especially, a large values of magnetoresistance (mr) and carrier mobility have drawn significant attention from a perspective of technological applications. inside this report, we have investigated a magnetotransport and fermi surface properties of single crystalline moas$_{2}$, another member of this group of compounds. field induced resistivity plateau and the large mr have been observed, which are comparable to several topological systems. interestingly, inside contrast to other isostructural materials, a carrier density inside moas$_{2}$ was quite high and shows single-band dominated transport. a fermi pockets, which have been identified from a quantum oscillation, are largest among a members of this group and have significant anisotropy with crystallographic direction. our first-principles calculations reveal the substantial difference between a band structures of moas$_{2}$ and other tmds. a calculated fermi surface consists of one electron pocket and another 'open-orbit' hole pocket, which has not been observed inside tmds so far.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
11790,"this paper focuses on devising graph signal processing tools considering a treatment of data defined on a edges of the graph. we first show that conventional tools from graph signal processing may not be suitable considering a analysis of such signals. more specifically, we discuss how a underlying notion of the `smooth signal' inherited from (the typically considered variants of) a graph laplacian are not suitable when dealing with edge signals that encode the notion of flow. to overcome this limitation we introduce the class of filters based on a edge-laplacian, the special case of a hodge-laplacian considering simplicial complexes of order one. we demonstrate how this edge-laplacian leads to low-pass filters that enforce (approximate) flow-conservation inside a processed signals. moreover, we show how these new filters should be combined with more classical laplacian-based processing methods on a line-graph. finally, we illustrate a developed tools by denoising synthetic traffic flows on a london street network.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1
19486,"a atomic clocks ensemble inside space (aces/pharao mission, esa and cnes) will be installed on board a international space station (iss) next year. the crucial part of this experiment was its two-way microwave link (mwl), which will compare a timescale generated on board with those provided by several ground stations disseminated on a earth. the dedicated data analysis center (dac) was being implemented at syrte -- observatoire de paris, where our team currently develops theoretical modelling, numerical simulations and a data analysis software itself. inside this paper, we present some key aspects of a mwl measurement method and a associated algorithms considering simulations and data analysis. we show a results of tests with the help of simulated data with fully realistic effects such as fundamental measurement noise, doppler, atmospheric delays, or cycle ambiguities. we demonstrate satisfactory performance of a software with respect to a specifications of a aces mission. a main scientific product of our analysis was a clock desynchronisation between ground and space clocks, i.e. a difference of proper times between a space clocks and ground clocks at participating institutes. while inside flight, this measurement will allow considering tests of general relativity and lorentz invariance at unprecedented levels, e.g. measurement of a gravitational redshift at a 3 . 10^-6 level. as the specific example, we use real iss orbit data with estimated errors at a 10 m level to study a effect of such errors on a clock desynchronisation obtained from mwl data. we demonstrate that a resulting effects are totally negligible.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
8950,"many neural networks use a tanh activation function, however when given the probability distribution as input, a problem of computing a output distribution inside neural networks with tanh activation has not yet been addressed. one important example was a initialization of a echo state network inside reservoir computing, where random initialization of a reservoir requires time to wash out a initial conditions, thereby wasting precious data and computational resources. motivated by this problem, we propose the novel solution utilizing the moment based idea behind the method to propagate uncertainty through an echo state network to reduce a washout time. inside this work, we contribute two new methods to propagate uncertainty through a tanh activation function and propose a probabilistic echo state network (pesn), the method that was shown to have better average performance than deterministic echo state networks given a random initialization of reservoir states. additionally we test single and multi-step uncertainty propagation of our method on two regression tasks and show that we are able to recover similar means and variances as computed by monte-carlo simulations.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
506,"we consider the particle dressed with boundary gravitons inside three-dimensional minkowski space. a existence of bms transformations implies that a particle's wavefunction picks up the berry phase when subjected to changes of reference frames that trace the closed path inside a asymptotic symmetry group. we evaluate this phase and show that, considering bms superrotations, it provides the gravitational generalization of thomas precession. inside principle, such phases are observable signatures of asymptotic symmetries.",0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
12166,"we consider a problem of quickest change-point detection inside data streams. classical change-point detection procedures, such as cusum, shiryaev-roberts and posterior probability statistics, are optimal only if a change-point model was known, which was an unrealistic assumption inside typical applied problems. instead we propose the new method considering change-point detection based on inductive conformal martingales, which requires only a independence and identical distribution of observations. we compare a proposed idea behind the method to standard methods, as well as to change-point detection oracles, which model the typical practical situation when we have only imprecise (albeit parametric) information about pre- and post-change data distributions. results of comparison provide evidence that change-point detection based on inductive conformal martingales was an efficient tool, capable to work under quite general conditions unlike traditional approaches.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0
10235,"pixel-wise semantic segmentation considering visual scene understanding not only needs to be accurate, but also efficient inside order to find any use inside real-time application. existing algorithms even though are accurate but they do not focus on utilizing a parameters of neural network efficiently. as the result they are huge inside terms of parameters and number of operations; thus slow too. inside this paper, we propose the novel deep neural network architecture which allows it to learn without any significant increase inside number of parameters. our network uses only 11.5 million parameters and 21.2 gflops considering processing an image of resolution 3x640x360. it gives state-of-the-art performance on camvid and comparable results on cityscapes dataset. we also compare our networks processing time on nvidia gpu and embedded system device with existing state-of-the-art architectures considering different image resolutions.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5128,"inside this paper, we investigate a application of text classification methods to predict a law area and a decision of cases judged by a french supreme court. we also investigate a influence of a time period inside which the ruling is made over a textual form of a case description and a extent to which it was necessary to mask a judge's motivation considering the ruling to emulate the real-world test scenario. we report results of 96% f1 score inside predicting the case ruling, 90% f1 score inside predicting a law area of the case, and 75.9% f1 score inside estimating a time span when the ruling has been issued with the help of the linear support vector machine (svm) classifier trained on lexical features.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7430,"we present the statistical framework considering generating predicted dynamic networks based on a observed evolution of social relationships inside the population. a framework includes the novel and flexible procedure to sample dynamic networks given the probability distribution on evolving network properties; it permits a use of the broad class of approaches to model trends, seasonal variability, uncertainty, and changes inside population composition. current methods do not account considering a variability inside a observed historical networks when predicting a network structure; a proposed method provides the principled idea behind the method to incorporate uncertainty inside prediction. this advance aids inside a designing of network-based interventions, as development of such interventions often requires prediction of a network structure inside a presence and absence of a intervention. two simulation studies are conducted to demonstrate a usefulness of generating predicted networks when designing network-based interventions. a framework was also illustrated by investigating results of potential interventions on bill passage rates with the help of the dynamic network that represents a sponsor/co-sponsor relationships among senators derived from bills introduced inside a us senate from 2003-2016.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
19662,"inside machine learning or statistics, it was often desirable to reduce a dimensionality of high dimensional data. we propose to obtain a low dimensional embedding coordinates as a eigenvectors of the positive semi-definite kernel matrix. this kernel matrix was a solution of the semi-definite program promoting the low rank solution and defined with a aid of the diffusion kernel. besides, we also discuss an infinite dimensional analogue of a same semi-definite program. from the practical perspective, the main feature of our idea behind the method was a existence of the non-linear out-of-sample extension formula of a embedding coordinates that we call the projected nystr√∂m approximation. this extension formula yields an extension of a kernel matrix to the data-dependent mercer kernel function. although a semi-definite program may be solved directly, we propose another strategy based on the rank constrained formulation solved thanks to the projected power method algorithm followed by the singular value decomposition. this strategy allows considering the reduced computational time.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17363,"this paper studies a convergence behaviour of dictionary learning using a iterative thresholding and k-residual means (itkrm) algorithm. on one hand it was shown that there exist stable fixed points that do not correspond to a generating dictionary, which should be characterised as very coherent. on a other hand it was proved that itkrm was the contraction under much relaxed conditions than previously necessary. based on a characterisation of a stable fixed points, replacing coherent atoms with carefully designed replacement candidates was proposed. inside experiments on synthetic data this outperforms random or no replacement and always leads to full dictionary recovery. finally a question how to learn dictionaries without knowledge of a correct dictionary size and sparsity level was addressed. decoupling a replacement strategy of coherent or unused atoms into pruning and adding, and slowly carefully increasing a sparsity level, leads to an adaptive version of itkrm. inside several experiments this adaptive dictionary learning algorithm was shown to recover the generating dictionary from randomly initialised dictionaries of various sizes on synthetic data and to learn meaningful dictionaries on image data.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4790,"inside this paper, we consider the cognitive indoor visible light communications (vlc) system, comprised of multiple access points serving primary and secondary users through a orthogonal frequency division multiple access method. the cognitive lighting cell was divided into two non-overlapping regions that distinguish a primary and secondary users based on a region they are located in. under a assumption of equal-power allocation among subcarriers, each region was defined inside terms of its physical area and a number of allocated subcarriers within that region. inside this paper, we provide a lighting cell design with cognitive constraints that guarantee fulfilling certain illumination, user mobility, and handover requirements inside each cell. we further argue that, under some conditions, the careful assignment of a subcarriers inside each region should mitigate a co-channel interference inside a overlapping areas of adjacent cells. numerical results depict a influence of different system parameters, such as user density, on defining both regions. finally, the realistic example was implemented to assess a performance of a proposed scheme using monte carlo simulations.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
9534,"considering two complex vector bundles admitting the homomorphism, whose singularity locates inside a disjoint union of some odd--dimensional spheres, we give the formula to compute a relative chern characteristic number of these two complex vector bundles. inside particular, considering the spin manifold admitting some sphere bundle structure, we give the formula to express a index of the special twisted dirac operator.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18062,"we prove a classification of homomorphisms from a algebra of symmetric functions to $\mathbb{r}$ with non-negative values on macdonald symmetric functions $p_{\lambda}$, that is conjectured by s.v. kerov inside 1992.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
16735,"psychological studies have shown that personality traits are associated with book preferences. however, past findings are based on questionnaires focusing on conventional book genres and are unrepresentative of niche content. considering the more comprehensive measure of book content, this study harnesses the massive archive of content labels, also known as 'tags', created by users of an online book catalogue, goodreads.com. combined with data on preferences and personality scores collected from facebook users, a tag labels achieve high accuracy inside personality prediction by psychological standards. we also group tags into broader genres, to check their validity against past findings. our results are robust across both tag and genre levels of analyses, and consistent with existing literature. moreover, user-generated tag labels reveal unexpected insights, such as cultural differences, book reading behaviors, and other non-content factors affecting preferences. to our knowledge, this was currently a largest study that explores a relationship between personality and book content preferences.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
7205,"we propose the spin valve device based on a interplay of the modulated spin-orbit interaction and the uniform external magnetic field acting on the quantum wire. half-metal phases, where electrons with only the selected spin polarization exhibit ballistic conductance, should be tuned by varying a magnetic field. these half-metal phases are proven to be robust against electron-electron repulsive interactions. our results arise from the combination of explicit band diagonalization, bosonization techniques and extensive dmrg computations.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
17618,"we prove that a category of solitons of the finite index conformal net was the bicommutant category, and that its drinfel'd center was a category of representations of a conformal net. inside a special case of the chiral wzw conformal net with finite index, a second result specializes to a statement that a drinfel'd center of a category of representations of a based loop group was equivalent to a category of representations of a free loop group. these results were announced inside [arxiv:1503.06254].",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
2180,"a present paper deals with a discrete inverse problem of reconstructing binary matrices from their row and column sums under additional constraints on a number and pattern of entries inside specified minors. while a classical consistency and reconstruction problems considering two directions inside discrete tomography should be solved inside polynomial time, it turns out that these window constraints cause various unexpected complexity jumps back and forth from polynomial-time solvability to $\mathbb{n}\mathbb{p}$-hardness.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15294,"we introduce the new function-preserving transformation considering efficient neural architecture search. this network transformation allows reusing previously trained networks and existing successful architectures that improves sample efficiency. we aim to address a limitation of current network transformation operations that should only perform layer-level architecture modifications, such as adding (pruning) filters or inserting (removing) the layer, which fails to change a topology of connection paths. our proposed path-level transformation operations enable a meta-controller to modify a path topology of a given network while keeping a merits of reusing weights, and thus allow efficiently designing effective structures with complex path topologies like inception models. we further propose the bidirectional tree-structured reinforcement learning meta-controller to explore the simple yet highly expressive tree-structured architecture space that should be viewed as the generalization of multi-branch architectures. we experimented on a image classification datasets with limited computational resources (about 200 gpu-hours), where we observed improved parameter efficiency and better test results (97.70% test accuracy on cifar-10 with 14.3m parameters and 74.6% top-1 accuracy on imagenet inside a mobile setting), demonstrating a effectiveness and transferability of our designed architectures.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15875,"this work was the methodical study on hybrid reconstruction techniques considering hybrid imaging/timing cherenkov observations. this type of hybrid array was to be realized at a gamma-observatory taiga intended considering very high energy gamma-ray astronomy (>30 tev). it aims at combining a cost-effective timing-array technique with imaging telescopes. hybrid operation of both of these techniques should lead to the relatively cheap way of development of the large area array. a joint idea behind the method of gamma event selection is investigated on both types of simulated data: a image parameters from a telescopes, and a shower parameters reconstructed from a timing array. a optimal set of imaging parameters and shower parameters to be combined was revealed. a cosmic ray background suppression factor depending on distance and energy was calculated. a optimal selection technique leads to cosmic ray background suppression of about 2 orders of magnitude on distances up to 450 m considering energies greater than 50 tev.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
5214,"dust extinction was a most robust tracer of a gas distribution inside a interstellar medium, but measuring extinction was limited by a systematic uncertainties involved inside estimating a intrinsic colors to background stars. inside this paper we present the new technique, pnicer, that estimates intrinsic colors and extinction considering individual stars with the help of unsupervised machine learning algorithms. this new method aims to be free from any priors with respect to a column density and intrinsic color distribution. it was applicable to any combination of parameters and works inside arbitrary numbers of dimensions. furthermore, it was not restricted to color space. extinction towards single sources was determined by fitting gaussian mixture models along a extinction vector to (extinction-free) control field observations. inside this way it becomes possible to describe a extinction considering observed sources with probability densities. pnicer effectively eliminates known biases found inside similar methods and outperforms them inside cases of deep observational data where a number of background galaxies was significant, or when the large number of parameters was used to break degeneracies inside a intrinsic color distributions. this new method remains computationally competitive, making it possible to correctly de-redden millions of sources within the matter of seconds. with a ever-increasing number of large-scale high-sensitivity imaging surveys, pnicer offers the fast and reliable way to efficiently calculate extinction considering arbitrary parameter combinations without prior information on source characteristics. pnicer also offers access to a well-established nicer technique inside the simple unified interface and was capable of building extinction maps including a nicest correction considering cloud substructure. pnicer was offered to a community as an open-source software solution and was entirely written inside python.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
1159,"density functions that represent sample data are often multimodal, i.e. they exhibit more than one maximum. typically this behavior was taken to indicate that a underlying data deserves the more detailed representation as the mixture of densities with individually simpler structure. a usual specification of the component density was quite restrictive, with log-concave a most general case considered inside a literature, and gaussian a overwhelmingly typical case. it was also necessary to determine a number of mixture components \emph{a priori}, and much art was devoted to this. here, we introduce \emph{topological mixture estimation}, the completely nonparametric and computationally efficient solution to a one-dimensional problem where mixture components need only be unimodal. we repeatedly perturb a unimodal decomposition of baryshnikov and ghrist to produce the topologically and information-theoretically optimal unimodal mixture. we also detail the smoothing process that optimally exploits topological persistence of a unimodal category inside the natural way when working directly with sample data. finally, we illustrate these techniques through examples.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
15729,we generalize the support vector machine to the support spinor machine by with the help of a mathematical structure of wedge product over vector machine inside order to extend field from vector field to spinor field. a separated hyperplane was extended to kolmogorov space inside time series data which allow us to extend the structure of support vector machine to the support tensor machine and the support tensor machine moduli space. our performance test on support spinor machine was done over one class classification of end point inside physiology state of time series data after empirical mode analysis and compared with support vector machine test. we implement algorithm of support spinor machine by with the help of holo-hilbert amplitude modulation considering fully nonlinear and nonstationary time series data analysis.,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8624,"we present a semantic robot programming (srp) paradigm as the convergence of robot programming by demonstration and semantic mapping. inside srp, the user should directly program the robot manipulator by demonstrating the snapshot of their intended goal scene inside workspace. a robot then parses this goal as the scene graph comprised of object poses and inter-object relations, assuming known object geometries. task and motion planning was then used to realize a user's goal from an arbitrary initial scene configuration. even when faced with different initial scene configurations, srp enables a robot to seamlessly adapt to reach a user's demonstrated goal. considering scene perception, we propose a discriminatively-informed generative approximation of scenes and transforms (digest) method to infer a initial and goal states of a world from rgbd images. a efficacy of srp with digest perception was demonstrated considering a task of tray-setting with the michigan progress fetch robot. scene perception and task execution are evaluated with the public household occlusion dataset and our cluttered scene dataset.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
13728,"we consider a problem of learning fair decision systems inside complex scenarios inside which the sensitive attribute might affect a decision along both fair and unfair pathways. we introduce the causal idea behind the method to disregard effects along unfair pathways that simplifies and generalizes previous literature. our method corrects observations adversely affected by a sensitive attribute, and uses these to form the decision. this avoids disregarding fair information, and does not require an often intractable computation of a path-specific effect. we leverage recent developments inside deep learning and approximate inference to achieve the solution that was widely applicable to complex, non-linear scenarios.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18084,"a study of random networks inside the neuroscientific context has developed extensively over a last couple of decades. by contrast, techniques considering a statistical analysis of these networks are less developed. inside this paper, we focus on a statistical comparison of brain networks inside the nonparametric framework and discuss a associated detection and identification problems. we tested network differences between groups with an analysis of variance (anova) test we developed specifically considering networks. we also propose and analyse a behaviour of the new statistical procedure designed to identify different subnetworks. as an example, we show a application of this tool inside resting-state fmri data obtained from a human connectome project. finally, we discuss a potential bias inside neuroimaging findings that was generated by some behavioural and brain structure variables. our method should also be applied to other kind of networks such as protein interaction networks, gene networks or social networks.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
14843,"we have studied radio haloes and relics inside nine merging galaxy clusters with the help of a murchison widefield array (mwa). a images used considering this study were obtained from a galactic and extragalactic all-sky mwa (gleam) survey which is carried out at 5 frequencies, viz. 88, 118, 154, 188 and 215 mhz. we detect diffuse radio emission inside 8 of these clusters. we have estimated a spectra of haloes and relics inside these clusters over a frequency range 80-1400 mhz; a first such attempt to approximate their spectra at low frequencies. a spectra follow the power law with the mean value of $\alpha = -1.13\pm0.21$ considering haloes and $\alpha = -1.2\pm0.19$ considering relics where, $s \propto \nu^{\alpha}$. we reclassify two of a cluster sources as radio galaxies. a low frequency spectra are thus an independent means of confirming a nature of cluster sources. five of a nine clusters host radio haloes. considering a remaining four clusters, we place upper limits on a radio powers of possible haloes inside them. these upper limits are the factor of 2-20 below those expected from a $l_{\rm x}-p_{\rm 1.4}$ relation. these limits are a lowest ever obtained and a implications of these limits to a hadronic model of halo emission are discussed.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15904,"time series data that are not measured at regular intervals are commonly discretized as the preprocessing step. considering example, data about customer arrival times might be simplified by summing a number of arrivals within hourly intervals, which produces the discrete-time time series that was easier to model. inside this abstract, we show that discretization introduces the bias that affects models trained considering decision-making. we refer to this phenomenon as discretization bias, and show that we should avoid it by with the help of continuous-time models instead.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
15190,"we describe wave decay rates associated to embedded resonances and spectral thresholds considering manifolds with infinite cylindrical ends. we show that if a cut-off resolvent was polynomially bounded at high energies, as was a case inside certain favorable geometries, then there was an associated asymptotic expansion, up to the $o(t^{-k_0})$ remainder, of solutions of a wave equation on compact sets as $t \to \infty$. inside a most general such case we have $k_0=1$, and under an additional assumption on a ends of a manifold we have $k_0 = \infty$. if we localize a solutions to a wave equation inside frequency as well as inside space, our results hold considering quite general manifolds with infinite cylindrical ends.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6192,"following a first two annual intensity mapping workshops at stanford inside march 2016 and johns hopkins inside june 2017, we report on a recent advances inside theory, instrumentation and observation that were presented inside these meetings and some of a opportunities and challenges that were identified looking forward. with preliminary detections of co, [cii], lya and low-redshift 21cm, and the host of experiments set to go online inside a next few years, a field was rapidly progressing on all fronts, with great anticipation considering the flood of new exciting results. this current snapshot provides an efficient reference considering experts inside related fields and the useful resource considering nonspecialists. we begin by introducing a concept of line-intensity mapping and then discuss a broad array of science goals that will be enabled, ranging from a history of star formation, reionization and galaxy evolution to measuring baryon acoustic oscillations at high redshift and constraining theories of dark matter, modified gravity and dark energy. after reviewing a first detections reported to date, we survey a experimental landscape, presenting a parameters and capabilities of relevant instruments such as comap, mmime, aim-co, ccat-p, time, concerto, chime, hirax, hera, starfire, meerkat/ska and spherex. finally, we describe recent theoretical advances: different approaches to modeling line luminosity functions, several techniques to separate a desired signal from foregrounds, statistical methods to analyze a data, and frameworks to generate realistic intensity map simulations.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
15141,"we review a categorical idea behind the method to a bps sector of the 4d $\mathcal{n}=2$ qft, clarifying many tricky issues and presenting the few novel results. to the given $\mathcal{n}=2$ qft one associates several triangle categories: they describe various kinds of bps objects from different physical viewpoints (e.g. ir versus uv). these diverse categories are related by the web of exact functors expressing physical relations between a various objects/pictures. the basic theme of this review was a emphasis on a full web of categories, rather than on what we should learn from the single description. the second general theme was viewing a cluster category as the sort of `categorification' of 't hooft's theory of quantum phases considering the 4d non-abelian gauge theory. a $s$-duality group was best described as a auto-equivalences of a full web of categories. this viewpoint leads to the combinatorial algorithm to search considering $s$-dualities of a given $\mathcal{n}=2$ theory. if a ranks of a gauge and flavor groups are not too big, a algorithm may be effectively run on the laptop. this viewpoint also leads to the clearer view of $3d$ mirror symmetry. considering class $\mathcal{s}$ theories, all a relevant triangle categories may also be constructed inside terms of geometric objects on a gaiotto curve, and we present a dictionary between triangle categories and a wkb idea behind the method of gmn. we also review how a vev's of uv line operators are related to cluster characters.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
2589,"recently, dnn model compression based on network architecture design, e.g., squeezenet, attracted the lot attention. no accuracy drop on image classification was observed on these extremely compact networks, compared to well-known models. an emerging question, however, was whether these model compression techniques hurt dnn's learning ability other than classifying images on the single dataset. our preliminary experiment shows that these compression methods could degrade domain adaptation (da) ability, though a classification performance was preserved. therefore, we propose the new compact network architecture and unsupervised da method inside this paper. a dnn was built on the new basic module conv-m which provides more diverse feature extractors without significantly increasing parameters. a unified framework of our da method will simultaneously learn invariance across domains, reduce divergence of feature representations, and adapt label prediction. our dnn has 4.1m parameters, which was only 6.7% of alexnet or 59% of googlenet. experiments show that our dnn obtains googlenet-level accuracy both on classification and da, and our da method slightly outperforms previous competitive ones. put all together, our da strategy based on our dnn achieves state-of-the-art on sixteen of total eighteen da tasks on popular office-31 and office-caltech datasets.",1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15538,"a auger engineering radio array (aera) aims to detect extensive air showers caused by a interactions of ultra-high energy cosmic rays with a earth's atmosphere, providing complementary information to a auger surface, fluorescence and muon detectors. aera, currently consisting of 124 radio stations, comprises an area of about 6 km$^{2}$. a main objective considering exploiting the radio detector was to measure a fundamental air-shower parameters, such as a direction, energy and composition. we have developed reconstruction strategies and algorithms to precisely measure a air-shower parameters with high efficiency. inside addition, we will present a results obtained by applying a reconstruction strategies on a experimental data taken by aera.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
7012,"with the help of very long molecular dynamics simulation runs, temperature protocols spanning up to five orders of magnitude inside time-scales are performed to investigate thermally activated structural relaxation inside the model amorphous solid. a simulations demonstrate significant local structural excitations as the function of increasing temperature and show that enthalpy rather than energy was primarily responsible considering relaxation. a observed enthalpy changes are of a order seen inside experiment, and should be correlated with a level of internal hydrostatic stress homogenization and icosahedral content within a solid.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
499,"we consider a problem of optimal transportation with quadratic cost between the empirical measure and the general target probability on r d , with d $\ge$ 1. we provide new results on a uniqueness and stability of a associated optimal transportation potentials , namely, a minimizers inside a dual formulation of a optimal transportation problem. as the consequence, we show that the clt holds considering a empirical transportation cost under mild moment and smoothness requirements. a limiting distributions are gaussian and admit the simple description inside terms of a optimal transportation potentials.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
7019,"a utility of the markov chain monte carlo algorithm is, inside large part, determined by a size of a spectral gap of a corresponding markov operator. however, calculating (and even approximating) a spectral gaps of practical monte carlo markov chains inside statistics has proven to be an extremely difficult and often insurmountable task, especially when these chains move on continuous state spaces. inside this paper, the method considering accurate approximation of a spectral gap was developed considering general state space markov chains whose operators are non-negative and trace-class. a method was based on a fact that a second largest eigenvalue (and thus a spectral gap) of such operators should be bounded above and below by simple functions of a power sums of a eigenvalues. these power sums often have nice integral representations. the classical monte carlo method was proposed to approximate these integrals, and the simple sufficient condition considering finite variance was provided. this leads to asymptotically valid confidence intervals considering a second largest eigenvalue (and a spectral gap) of a markov operator. a efficiency of a method was studied. considering illustration, a method was applied to albert and chib's (1993) data augmentation (da) algorithm considering bayesian probit regression, and also to the da algorithm considering bayesian linear regression with non-gaussian errors (liu, 1996).",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
10584,"the compacted tree was the graph created from the binary tree such that repeatedly occurring subtrees inside a original tree are represented by pointers to existing ones, and thus every subtree was unique. such representations form the special class of directed acyclic graphs. we are interested inside a asymptotic number of compacted trees of given size, where a size of the compacted tree was given by a number of its internal nodes. due to its superexponential growth this problem poses many difficulties. therefore we restrict our investigations to compacted trees of bounded right height, which was a maximal number of edges going to a right on any path from a root to the leaf. we solve a asymptotic counting problem considering this class as well as the closely related, further simplified class. considering this purpose, we develop the calculus on exponential generating functions considering compacted trees of bounded right height and considering relaxed trees of bounded right height, which differ from compacted trees by dropping a above described uniqueness condition. this enables us to derive the recursively defined sequence of differential equations considering a exponential generating functions. a coefficients should then be determined by performing the singularity analysis of a solutions of these differential equations. our main results are a computation of a asymptotic numbers of relaxed as well as compacted trees of bounded right height and given size, when a size tends to infinity.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5433,"inside this paper, we propose surrogate agent-environment interface (saei) inside reinforcement learning. we also state that learning based on probability surrogate agent-environment interface provides optimal policy of task agent-environment interface. we introduce surrogate probability action and develop a probability surrogate action deterministic policy gradient (psadpg) algorithm based on saei. this algorithm enables continuous control of discrete action. a experiments show psadpg achieves a performance of dqn inside certain tasks with a stochastic optimal policy nature inside a initial training stage.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
283,"sparse singular value decomposition (svd) models have been proposed considering biclustering high dimensional gene expression data to identify block patterns with similar expressions. however, these models do not take into account prior group effects upon variable selection. to this end, we first propose group-sparse svd models with group lasso (gl1-svd) and group l0-norm penalty (gl0-svd) considering non-overlapping group structure of variables. however, such group-sparse svd models limit their applicability inside some problems with overlapping structure. thus, we also propose two group-sparse svd models with overlapping group lasso (ogl1-svd) and overlapping group l0-norm penalty (ogl0-svd). we first adopt an alternating iterative strategy to solve gl1-svd based on the block coordinate descent method, and gl0-svd based on the projection method. a key of solving ogl1-svd was the proximal operator with overlapping group lasso penalty. we employ an alternating direction method of multipliers (admm) to solve a proximal operator. similarly, we develop an approximate method to solve ogl0-svd. applications of these methods and comparison with competing ones with the help of simulated data demonstrate their effectiveness. extensive applications of them onto several real gene expression data with gene prior group knowledge identify some biologically interpretable gene modules.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4763,"we present synthetic continuum and $^{13}$co and c$^{18}$o line emission observations of dense and cold filaments. a filaments are dynamically evolved with the help of 3d-mhd simulations that include one of a largest on-the-fly chemical networks used to date, which models a detailed evolution of h$_2$ and co. we investigate a reliability of observable properties, inside particular filament mass and width, under different simulation conditions like magnetic field orientation and cosmic ray ionisation rate. we find that filament widths of $\sim$0.1 pc should be probed with both line and continuum emission observations with the high accuracy (deviations $\leq$ 20%). however, a width of more narrow filaments should be significantly overestimated by up to the factor of the few. masses obtained using a dust emission are accurate within the few percent whereas a masses inferred from molecular line emission observations deviate from a actual mass by up to the factor of 10 and show large differences considering different $j$ transitions. a inaccurate approximate of filament masses and widths of narrow filaments with the help of molecular line observations should be attributed to (i) a non-isothermal state of a filaments, (ii) optical depth effects, and (iii) a subthermally excited state of co, while inclination effects and opacity correction only influence a obtained masses and widths by less than 50%. both, mass and width estimates, should be improved by with the help of two isotopes to correct considering a optical depth. since gas and dust temperature generally differ (by up to 25 k), a filaments appear more gravitationally unstable if a (too low) dust temperature was used considering a stability analysis.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5995,"many results inside mathematical relativity, including results considering both a initial data problem and considering a evolution problem, rely on a existence of the constant mean curvature (cmc) cauchy surface inside a underlying spacetime. however, it was known that some spacetimes have no cmc cauchy surfaces (slices). this was an obstacle considering many results and constructions with these types of spacetimes, and was particularly worrisome since it was not known whether spacetimes that do have cmc slices are inside any sense generic. inside this expository paper, we will discuss a known results about a existence (and non-existence) of cmc slices, examine a evidence considering cases which are unknown, and make several conjectures concerning a existence of cmc slices and their generality.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14734,"we introduce an idea behind the method considering a real-time (2hz) creation of the dense map and alignment of the moving robotic agent within that map by rendering with the help of the graphics processing unit (gpu). this was done by recasting a scan alignment part of a dense mapping process as the rendering task. alignment errors are computed from rendering a scene, comparing with range data from a sensors, and minimized by an optimizer. a proposed idea behind the method takes advantage of a advances inside rendering techniques considering computer graphics and gpu hardware to accelerate a algorithm. moreover, it allows one to exploit information not used inside classic dense mapping algorithms such as iterative closest point (icp) by rendering interfaces between a free space, occupied space and a unknown. a proposed idea behind the method leverages directly a rendering capabilities of a gpu, inside contrast to other gpu-based approaches that deploy a gpu as the general purpose parallel computation platform. we argue that a proposed concept was the general consequence of treating perception problems as inverse problems of rendering. many perception problems should be recast into the form where much of a computation was replaced by render operations. this was not only efficient since rendering was fast, but also simpler to implement and will naturally benefit from future advancements inside gpu speed and rendering techniques. furthermore, this general concept should go beyond addressing perception problems and should be used considering other problem domains such as path planning.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
3747,"inside a last decade, the large number of studies at a experimental level inside electrochemical systems considering energy storage devices have been performed. however, theoretical approaches are highly desirable to understand a physicochemical properties giving rise to energy storage phenomena. this work is intended to provide insights into a $in$ $silico$ design of novel nanocomposite materials formed by a keggin polyoxometalate siw12 anchored to an organic functional group $\varphi-x$ (with $x = -nh2, -oh, -coh$ and $-cooh$), linked to the carbon nanotube. inside these systems, a density of states around a fermi level was enhanced, giving a composite material a capacity of facile electron transport that may be determinant at a charge/discharge cycling performed inside energy storage devices. charge transfer at a composite materials under study was greatest considering a $\varphi-cooh$ functional group, yielding an attraction with a siw12 cluster of a same order of magnitude as that of covalent nature. a rest of a functional groups induce the non-covalent interaction of a electrostatic type, mediated by the van der waals attraction. our proposed methodology may represent the tool to develop novel electrode materials that may improve a performance on energy storage devices, such as supercapacitors or li$-$ion batteries.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
19524,"this paper proposes the new hyperspectral unmixing method considering nonlinearly mixed hyperspectral data with the help of the semantic representation inside the semi-supervised fashion, assuming a availability of the spectral reference library. existing semi-supervised unmixing algorithms select members from an endmember library that are present at each of a pixels; most such methods assume the linear mixing model. however, those methods will fail inside a presence of nonlinear mixing among a observed spectra. to address this issue, we develop an endmember selection method with the help of the recently proposed semantic spectral representation obtained using non-homogeneous hidden markov chain (nhmc) model considering the wavelet transform of a spectra. a semantic representation should encode spectrally discriminative features considering any observed spectrum and, therefore, our proposed method should perform endmember selection without any assumption on a mixing model. experimental results show that inside a presence of sufficiently nonlinear mixing our proposed method outperforms dictionary-based sparse unmixing approaches based on linear models.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6611,"we deal with a algebraicity of an iterated puiseux series inside several variables inside terms of a properties of its coefficients. our aim was to generalize to several variables a results from [hm15]. we show that a algebraicity of such the series considering given bounded degrees was determined by the finite number of explicit universal polynomial formulas. conversely, given the vanishing polynomial, there was the closed-form formula considering a coefficients of a series inside terms of a coefficients of a polynomial and of the bounded initial part of a series.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
15204,"this paper investigates a robustness of nlp against perturbed word forms. while neural approaches should achieve (almost) human-like accuracy considering certain tasks and conditions, they often are sensitive to small changes inside a input such as non-canonical input (e.g., typos). yet both stability and robustness are desired properties inside applications involving user-generated content, and a more as humans easily cope with such noisy or adversary conditions. inside this paper, we study a impact of noisy input. we consider different noise distributions (one type of noise, combination of noise types) and mismatched noise distributions considering training and testing. moreover, we empirically evaluate a robustness of different models (convolutional neural networks, recurrent neural networks, non-neural models), different basic units (characters, byte pair encoding units), and different nlp tasks (morphological tagging, machine translation).",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
644,a object of a present paper was to study a locally $\phi $- semisymmetric kenmotsu manifolds along with a characterization of such notion.,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
415,"inside adult laparoscopy, robot-aided surgery was the reality inside thousands of operating rooms worldwide, owing to a increased dexterity provided by a robotic tools. many robots and robot control techniques have been developed to aid inside more challenging scenarios, such as pediatric surgery and microsurgery. however, a prevalence of case-specific solutions, particularly those focused on non-redundant robots, reduces a reproducibility of a initial results inside more challenging scenarios. inside this letter, we propose the general framework considering a control of surgical robotics inside constrained workspaces under teleoperation, regardless of a robot geometry. our technique was divided into the slave-side constrained optimization algorithm, which provides virtual fixtures, and with cartesian impedance on a master side to provide force feedback. experiments with two robotic systems, one redundant and one non-redundant, show that smooth teleoperation should be achieved inside adult laparoscopy and infant surgery.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
12567,"a low mach number limit considering one-dimensional non-isentropic compressible navier-stokes system without viscosity was investigated, where a density and temperature have different asymptotic states at far fields. it was proved that a solution of a system converges to the nonlinear diffusion wave globally inside time as mach number goes to zero. it was remarked that a velocity of diffusion wave was proportional with a variation of temperature. furthermore, it was shown that a solution of compressible navier-stokes system also has a same phenomenon when mach number was suitably small.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4600,"cache-enabled device-to-device (d2d) communications should boost network throughput. by pre-downloading contents to local caches of users, a content requested by the user should be transmitted using d2d links by other users inside proximity. prior works optimize a caching policy at users with a knowledge of content popularity, defined as a probability distribution of request considering every file inside the library from by all users. however, content popularity should not reflect a interest of each individual user and thus popularity-based caching policy may not fully capture a performance gain introduced by caching. inside this paper, we optimize caching policy considering cache-enabled d2d by learning user preference, defined as a conditional probability distribution of the user's request considering the file given that a user sends the request. we first formulate an optimization problem with given user preference to maximize a offloading probability, which was proved as np-hard, and then provide the greedy algorithm to find a solution. inside order to predict a preference of each individual user, we model a user request behavior by probabilistic latent semantic analysis (plsa), and then apply expectation maximization (em) algorithm to approximate a model parameters. simulation results show that a user preference should be learnt quickly. compared to a popularity-based caching policy, a offloading gain achieved by a proposed policy should be remarkably improved even with predicted user preference.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
12213,"we consider a approximation of a multi-period optimal portfolio obtained by maximizing an exponential utility. employing jeffreys' non-informative prior and a conjugate informative prior, we derive stochastic representations considering a optimal portfolio weights at each time point of portfolio reallocation. this provides the direct access not only to a posterior distribution of a portfolio weights but also to their point estimates together with uncertainties and their asymptotic distributions. furthermore, we present a posterior predictive distribution considering a investor's wealth at each time point of a investment period inside terms of the stochastic representation considering a future wealth realization. this inside turn makes it possible to use quantile-based risk measures or to calculate a probability of default. we apply a suggested bayesian idea behind the method to assess a uncertainty inside a multi-period optimal portfolio by considering assets from a ftse 100 inside a weeks after a british referendum to leave a european union. a behaviour of a novel portfolio approximation method inside the precarious market situation was illustrated by calculating a predictive wealth, a risk associated with a holding portfolio, and a default probability inside each period.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
5670,"through a selberg zeta approach, we reduce a exponent inside a error term of a prime geodesic theorem considering cocompact kleinian groups or bianchi groups from sarnak's $\frac{5}{3}$ to $\frac{3}{2}$. at a cost of excluding the set of finite logarithmic measure, a bound was further improved to $\frac{13}{9}$.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
11855,"we propose an idea behind the method to learning agents considering active robotic mapping, where a goal was to map a environment as quickly as possible. a agent learns to map efficiently inside simulated environments by receiving rewards corresponding to how fast it constructs an accurate map. inside contrast to prior work, this idea behind the method learns an exploration policy based on the user-specified prior over environment configurations and sensor model, allowing it to specialize to a specifications. we evaluate a idea behind the method through the simulated disaster mapping scenario and find that it achieves performance slightly better than the near-optimal myopic exploration scheme, suggesting that it could be useful inside more complicated problem scenarios.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
9306,"observer design typically requires a observability of a underlying system, which may be hard to verify considering nonlinear systems, while guaranteeing asymptotic convergence of errors, which may be insufficient inside order to satisfy performance conditions inside finite time. this paper develops the method to design luenberger-type observers considering nonlinear systems which guarantee a largest possible domain of attraction considering a state approximation error regardless of a initialization of a system. a observer design procedure was posed as the two step problem. inside a a first step, a error dynamics are abstractly represented as the linear equation on a space of radon measures. thereafter, a problem of identifying a largest set of initial errors that should be driven to within a user-specified error target set inside finite-time considering all possible initial states, and a corresponding observer gains, was formulated as an infinite-dimensional linear program on measures. this optimization problem was solved, with the help of lasserre's relaxations using the sequence of semidefinite programs with vanishing conservatism. by post-processing a solution of step one, a set of gains that maximize a size of tolerable initial errors was identified inside step two. to demonstrate a feasibility of a presented idea behind the method two examples are presented.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
795,"a thermal evolution of a intergalactic medium (igm) should serve as the sensitive probe of cosmological heat sources and sinks. we employ it to limit interactions between dark matter and baryons. after reionization a igm temperature was set by a balance between photoheating and adiabatic cooling. we use measurements of a igm temperature from lyman-$\alpha$-forest data to constrain a cross-section $\sigma$ between dark matter and baryons, finding $\sigma < 10^{-20}$ cm$^2$ considering dark-matter masses $m_\chi\leq 1$ gev. this provides a first direct constraint on scattering between dark matter and baryons at redshift $z\sim5$.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4546,"we revisit a problem of a surface superconductivity nucleation focusing on a detailed study of a critical field $h_{c3}$ as the function of temperature and disorder. with the help of a semiclassical eilenberger formalism we find that away from a ginzburg-landau region a ratio between a nucleation critical field $h_{c3}$ and a upper critical field $h_{c2}$ deviates strongly from a saint-james-de gennes limit. inside particular, a $h_{c3}/h_{c2}$ was found to be the nonmonotonic function of temperature, which reaches a maximum considering the set of parameters corresponding to the crossover region from ballistic to diffusive scattering, when a mean free path inside the bulk of the superconductor was of a same order as zero-temperature superconducting coherence length. we also analyze a robustness of a nucleated phases with respect to diffusive scattering off a sample boundary by solving exactly corresponding eigenvalue problem of an integral equation considering a critical field. a implications of these results considering a transport inside superconductors of various geometries near $h_{c3}$ are briefly discussed. inside particular, we present results considering a mechanism of magnetoconductivity oscillations due to surface superconductivity effects.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
4222,"this paper was concerned with modeling a dependence structure of two (or more) time-series inside a presence of the (possible multivariate) covariate which may include past values of a time series. we assume that a covariate influences only a conditional mean and a conditional variance of each of a time series but a distribution of a standardized innovations was not influenced by a covariate and was stable inside time. a joint distribution of a time series was then determined by a conditional means, a conditional variances and a marginal distributions of a innovations, which we approximate nonparametrically, and a copula of a innovations, which represents a dependency structure. we consider the nonparametric as well as the semiparametric estimator based on a estimated residuals. we show that under suitable assumptions these copula estimators are asymptotically equivalent to estimators that would be based on a unobserved innovations. a theoretical results are illustrated by simulations and the real data example.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
10474,"we numerically simulate a free decay of two-dimensional quantum turbulence inside the large, homogeneous bose-einstein condensate. a large number of vortices, a uniformity of a density profile and a absence of boundaries (where vortices should drift out of a condensate) isolate a annihilation of vortex-antivortex pairs as a only mechanism which reduces a number of vortices, $\nv$, during a turbulence decay. a results clearly reveal that vortex annihilation was the four-vortex process, confirming a decay law $\nv \sim t^{-1/3}$ where $t$ was time, which is inferred from experiments with relatively few vortices inside small harmonically trapped condensates.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11981,"we describe an approach, based on direct numerical solution of a usadel equation, to finding stationary points of a free energy of superconducting nanorings. we consider both uniform (equilibrium) solutions and a critical droplets that mediate activated transitions between them. considering a uniform solutions, we compute a critical current as the function of a temperature, thus obtaining the correction factor to bardeen's 1962 interpolation formula. considering a droplets, we present the metastability chart that shows a activation energy as the function of a temperature and current. the comparison of a activation energy considering the ring to experimental results considering the wire connected to superconducting leads reveals the discrepancy at large currents. we discuss possible reasons considering it. we also discuss a nature of a bifurcation point at which a droplet merges with a uniform solution.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
13568,"an important issue inside additive manufacturing was a reliability and reproducibility of parts. one major problem inside achieving this are uncontrolled local variations inside a obtained material properties which arise inside a complex manufacturing process and are usually not taken into account inside a design of components. we consider a optimal layout of the part to withstand the given loading, under a assumption that a local material properties are not precisely known. a material uncertainties are treated by the worst case approach. this means that considering each layout the given amount of defects inside material properties was distributed inside a design domain, such that a stiffness of a component was maximally weakened. as the consequence, an optimization result was obtained which was as insensitive as possible with respect to unknown variations inside a material parameters. a general model was introduced and an algorithm considering its solution with the help of gradient based methods was suggested. finally, numerical results are presented and discussed.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
16445,"a physical properties of an intermetallic compound cerh2ga2 have been investigated by magnetic susceptibility \chi(t), isothermal magnetization m(h), heat capacity c_p(t), electrical resistivity \rho(t), thermal conductivity \kappa(t) and thermopower s(t) measurements. cerh2ga2 was found to crystallize with cabe2ge2-type primitive tetragonal structure (space group p4/nmm). no evidence of long range magnetic order was seen down to 1.8 k. a \chi(t) data show paramagnetic behavior with an effective moment \mu_eff ~ 2.5 \mu_b/ce indicating ce^3+ valence state of ce ions. a \rho(t) data exhibit kondo lattice behavior with the metallic ground state. a low-t c_p(t) data yield an enhanced sommerfeld coefficient \gamma = 130(2) mj/mol k^2 characterizing cerh2ga2 as the moderate heavy fermion system. a high-t c_p(t) and \rho(t) show an anomaly near 255 k, reflecting the phase transition. a \kappa(t) suggests phonon dominated thermal transport with considerably higher values of lorenz number l(t) compared to a theoretical sommerfeld value l_0.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
10755,"we propose the paradigm to deep-learn a ever-expanding databases which have emerged inside mathematical physics and particle phenomenology, as diverse as a statistics of string vacua or combinatorial and algebraic geometry. as concrete examples, we establish multi-layer neural networks as both classifiers and predictors and train them with the host of available data ranging from calabi-yau manifolds and vector bundles, to quiver representations considering gauge theories. we find that even the relatively simple neural network should learn many significant quantities to astounding accuracy inside the matter of minutes and should also predict hithertofore unencountered results. this paradigm should prove the valuable tool inside various investigations inside landscapes inside physics as well as pure mathematics.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11325,"we show that boolean matrix multiplication, computed as the sum of products of column vectors with row vectors, was essentially a same as warshall's algorithm considering computing a transitive closure matrix of the graph from its adjacency matrix. warshall's algorithm should be generalized to floyd's algorithm considering computing a distance matrix of the graph with weighted edges. we will generalize boolean matrices inside a same way, keeping matrix multiplication essentially equivalent to a floyd-warshall algorithm. this way, we get matrices over the semiring, which are similar to a so-called ""funny matrices"". we discuss our implementation of operations on boolean matrices and on their generalization, which make use of vector instructions.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14643,"humanoid robots are increasingly demanded to operate inside interactive and human-surrounded environments while achieving sophisticated locomotion and manipulation tasks. to accomplish these tasks, roboticists unremittingly seek considering advanced methods that generate whole-body coordination behaviors and meanwhile fulfill various planning and control objectives. undoubtedly, these goals pose fundamental challenges to a robotics and control community. to take an incremental step towards reducing a performance gap between theoretical foundations and real implementations, we present the planning and control framework considering a humanoid, especially legged robots, considering achieving high performance and generating agile motions. the particular concentration was on a robust, optimal and real-time performance. this framework constitutes three hierarchical layers: first, we present the robust optimal phase-space planning framework considering dynamic legged locomotion over rough terrain. this framework was the hybrid motion planner incorporating the series of pivotal components. second, we take the step toward formally synthesizing high-level reactive planners considering whole-body locomotion inside constrained environments. we formulate the two-player temporal logic game between a contact planner and its possibly-adversarial environment. third, we propose the distributed control architecture considering a latency-prone humanoid robotic systems. the central experimental phenomenon was observed that a stability of high impedance distributed controllers was highly sensitive to damping feedback delay but much less to stiffness feedback delay. we pursue the detailed analysis of a distributed controllers where damping feedback effort was executed inside proximity to a control plant, and stiffness feedback effort was implemented inside the latency-prone centralized control process.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0
1378,we prove an integral formula computing multiplicities of square-integrable representations relative to galois pairs over $p$-adic fields and we apply this formula to verify two consequences of the conjecture of dipendra prasad. one concerns a exact computation of a multiplicity of a steinberg representation and a other a invariance of multiplicities by transfer among inner forms.,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0
10694,"an exhaustive study on neural network language modeling (nnlm) was performed inside this paper. different architectures of basic neural network language models are described and examined. the number of different improvements over basic neural network language models, including importance sampling, word classes, caching and bidirectional recurrent neural network (birnn), are studied separately, and a advantages and disadvantages of every technique are evaluated. then, a limits of neural network language modeling are explored from a aspects of model architecture and knowledge representation. part of a statistical information from the word sequence will loss when it was processed word by word inside the certain order, and a mechanism of training neural network by updating weight matrixes and vectors imposes severe restrictions on any significant enhancement of nnlm. considering knowledge representation, a knowledge represented by neural network language models was a approximate probabilistic distribution of word sequences from the certain training data set rather than a knowledge of the language itself or a information conveyed by word sequences inside the natural language. finally, some directions considering improving neural network language modeling further was discussed.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11591,"borophene, a boron atom analogue to graphene, being atomic thick have been just recently experimentally fabricated. inside this work, we employ first-principles density functional theory calculations to investigate a interaction of ca, mg, na or li atoms with single-layer and free-standing borophene. we first identified a most stable binding sites and their corresponding binding energies as well and then we gradually increased a ions concentration. our calculations predict strong binding energies of around 4.03 ev, 2.09 ev, 2.92 ev and 3.28 ev between a borophene substrate and ca, mg, na or li ions, respectively. we found that a binding energy generally decreases by increasing a ions content. with the help of a bader charge analysis, we evaluate a charge transfer between a adatoms and a borophene sheet. our investigation proposes a borophene as the 2d material with the remarkably high capacity of around 800 mah/g, 1960 mah/g, 1380 mah/g and 1720 mah/g considering ca, mg, na or li ions storage, respectively. this study should be useful considering a possible application of borophene considering a rechargeable ion batteries.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
12392,"inside reinforcement learning, building policies of high-quality was challenging when a feature space of states was small and a training data was limited. directly transferring data or knowledge from an agent to another agent will not work due to a privacy requirement of data and models. inside this paper, we propose the novel reinforcement learning idea behind the method to considering a privacy requirement and building q-network considering each agent with a aid of other agents, namely federated reinforcement learning (frl). to protect a privacy of data and models, we exploit gausian differentials on a information shared with each other when updating their local models. inside a experiment, we evaluate our frl framework inside two diverse domains, grid-world and text2action domains, by comparing to various baselines.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15787,"a aim of this paper was to present the linear viscoelastic model based on prabhakar fractional operators. inside particular, we propose the modification of a classical fractional maxwell model, inside which we replace a caputo derivative with a prabhakar one. furthermore, we also discuss how to recover the formal equivalence between a new model and a known classical models of linear viscoelasticity by means of the suitable choice of a parameters inside a prabhakar derivative. moreover, we also underline an interesting connection between a theory of prabhakar fractional integrals and a recently introduced caputo-fabrizio differential operator.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
11369,"a galaxy m49 (ngc 4472) was a brightest early-type galaxy inside a virgo cluster. it was located inside subcluster b and has an unusually blue, metal-poor outer halo. planetary nebulae (pne) are excellent tracers of diffuse galaxy and intragroup light. we present the photometric survey of pne inside a galaxy's extended halo to characterise its pn population, as well as a surrounding intragroup light (igl) of a subcluster b. pne were identified based on their bright [oiii]5007 \aa\ emission and absence of the broad-band continuum. we identify 738 pne out to the radius of 155 kpc from m49's centre from which we define the complete sample of 624 pne within the limiting magnitude of m_5007=28.8. comparing a pn number density to a broad-band stellar surface brightness profile, we find the variation of a pn-specific frequency (alpha-parameter) with radius. a outer halo beyond 60 kpc has the 3.2 times higher alpha-parameter compared to a main galaxy halo, which was likely due to contribution from a surrounding blue igl. we use a planetary nebulae luminosity function (pnlf) as an indicator of distance and stellar population. its slope, which correlates empirically with galaxy type, varies within a inner halo. inside a eastern quadrant of m49, a pnlf slope was shallower, indicating an additional localised, bright pn population following an accretion event, likely that of a dwarf irregular galaxy vcc1249. we also determined the distance modulus of mu = 31.29+/-0.08 considering m49, corresponding to the physical distance of 18.1+/-0.6 mpc, which agrees with the recent surface-brightness fluctuations distance. a pn populations inside a outer halo of m49 are consistent with a presence of the main sersic galaxy halo with the slight (b-v) colour gradient of 10${}^{-4}$ mag/arcsec surrounded by intragroup light with the very blue colour of (b-v)=0.25 and the constant surface brightness mu_v=28.0 mag/arcsec${}^2$.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18475,"deriving a optimal safety stock quantity with which to meet customer satisfaction was one of a most important topics inside stock management. however, it was difficult to control a stock management of correlated marketable merchandise when with the help of an inventory control method that is developed under a assumption that a demands are not correlated. considering this, we propose the deterministic idea behind the method that uses the probability inequality to derive the reasonable safety stock considering a case inside which we know a correlation between various commodities. moreover, over the given lead time, a relation between a appropriate safety stock and a allowable stockout rate was analytically derived, and a potential of our proposed procedure was validated by numerical experiments.",0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
5353,"we study a conformal bootstrap considering 4-point functions of fermions $\langle \psi_i \psi_j \psi_k \psi_{\ell} \rangle$ inside parity-preserving 3d cfts, where $\psi_i$ transforms as the vector under an $o(n)$ global symmetry. we compute bounds on scaling dimensions and central charges, finding features inside our bounds that appear to coincide with a $o(n)$ symmetric gross-neveu-yukawa fixed points. our computations are inside perfect agreement with a $1/n$ expansion at large $n$ and allow us to make nontrivial predictions at small $n$. considering values of $n$ considering which a gross-neveu-yukawa universality classes are relevant to condensed-matter systems, we compare our results to previous analytic and numerical results.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
13643,"model evaluation -- a process of making inferences about a performance of predictive models -- was the critical component of predictive modeling research inside learning analytics. we survey a state of a practice with respect to model evaluation inside learning analytics, which overwhelmingly uses only naive methods considering model evaluation or statistical tests which are not appropriate considering predictive model evaluation. we conduct the critical comparison of both null hypothesis significance testing (nhst) and the preferred bayesian method considering model evaluation. finally, we apply three methods -- a na{√Ø}ve average commonly used inside learning analytics, nhst, and bayesian -- to the predictive modeling experiment on the large set of mooc data. we compare 96 different predictive models, including different feature sets, statistical modeling algorithms, and tuning hyperparameters considering each, with the help of this case study to demonstrate a different experimental conclusions these evaluation techniques provide.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10848,"understanding a nature of bulges inside disc galaxies should provide important insights into a formation and evolution of galaxies. considering instance, a presence of the classical bulge suggests the relatively violent history, inside contrast, a presence of simply an inner disc (also referred to as the ""pseudobulge"") indicates a occurrence of secular evolution processes inside a main disc. however, we still lack criteria to effectively categorise bulges, limiting our ability to study their impact on a evolution of a host galaxies. here we present the recipe to separate inner discs from classical bulges by combining four different parameters from photometric and kinematic analyses: a bulge s√©rsic index $n_\mathrm{b}$, a concentration index $c_{20,50}$, a kormendy (1977) relation and a inner slope of a radial velocity dispersion profile $\nabla\sigma$. with that recipe we provide the detailed bulge classification considering the sample of 45 galaxies from a integral-field spectroscopic survey califa. to aid inside categorising bulges within these galaxies, we perform 2d image decomposition to determine bulge s√©rsic index, bulge-to-total light ratio, surface brightness and effective radius of a bulge and use growth curve analysis to derive the new concentration index, $c_{20,50}$. we further extract a stellar kinematics from califa data cubes and analyse a radial velocity dispersion profile. a results of a different approaches are inside good agreement and allow the safe classification considering approximately $95\%$ of a galaxies. inside particular, we show that our new ""inner"" concentration index performs considerably better than a traditionally used $c_{50,90}$ when yielding a nature of bulges. we also found that the combined use of this index and a kormendy (1977) relation gives the very robust indication of a physical nature of a bulge.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10307,"we have extracted weighted mean values of {\omega}_{mw}=0.299+/-0.011 and h_{0w}=68.6+/-0.7 km s-1 mpc-1 from eight independent baryon acoustic oscillation (bao) data sets. those results were obtained with the help of fit-lines to cmb compatible solutions. a expression, {\alpha}={\alpha}_{\parallel}^{1/3}{\alpha}_{\perp}^{2/3}, relating a bao radial and transverse shift factors, {\alpha}_{\parallel}, {\alpha}_{\perp}, to a isotropic shift factor, {\alpha}, enabled inclusion of anisotropic studies. with the help of a derived {\omega}_{m} and h_{0} values, we appraise a commonly used bao equations. a isotropic expression becomes problematic at precision levels of ~1 percent or better. most revealing, a anisotropic equations, d_{m}(z)/d_{m,fid}(z)={\alpha}_{\perp}r_{d}/r_{d,fid}, and d_{h}(z)/d_{h,fid}(z)={\alpha}_{\parallel}r_{d}/r_{d,fid}, are invalid when {\alpha} ~ 1, since under that condition, d_{m}(z)/d_{m,fid}(z)=d_{h}(z)/d_{h,fid}(z)=r_{d}/r_{d,fid} ~ 1, and neither equation should be satisfied with anisotropic data. (the ratios are respectively, a angular distance, a inverse hubble parameter, and a comoving acoustic horizon, each divided by its fiducial value). {\alpha} should be driven towards unity considering any data set, e.g., by applying a derived {\omega}_{m}, h_{0} pair as a core of the second iteration fiducial parameter-set. thus, a anisotropic equations are untenable.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8895,"we report on multilayer high efficiency antireflection coating (arc) design and development considering use at uv wavelengths on ccds and other si-based detectors. we have previously demonstrated the set of single-layer coatings, which achieve >50% quantum efficiency (qe) inside four bands from 130 to 300 nm. we now present multilayer coating designs that significantly outperform our previous work between 195 and 215 nm. with the help of up to 11 layers, we present several model designs to reach qe above 80%. we also demonstrate a successful performance of 5 and 11 layer arcs on silicon and fused silica substrates. finally, we present the five-layer coat- ing deposited onto the thinned, delta-doped ccd and demonstrate external qe greater than 60% between 202 and 208 nm, with the peak of 67.6% at 206 nm.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
3764,"it has been recently shown that generative adversarial networks (gans) should produce synthetic images of exceptional visual fidelity. inside this work, we propose a gan-based method considering automatic face aging. contrary to previous works employing gans considering altering of facial attributes, we make the particular emphasize on preserving a original person's identity inside a aged version of his/her face. to this end, we introduce the novel idea behind the method considering ""identity-preserving"" optimization of gan's latent vectors. a objective evaluation of a resulting aged and rejuvenated face images by a state-of-the-art face recognition and age approximation solutions demonstrate a high potential of a proposed method.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7383,"people tend to behave inconsistently over time due to an inherent present bias. as this may impair performance, social and economic settings need to be adapted accordingly. common tools to reduce a impact of time-inconsistent behavior are penalties and prohibition. such tools are called commitment devices. inside recent work kleinberg and oren connect a design of prohibition-based commitment devices to the combinatorial problem inside which edges are removed from the task graph $g$ with $n$ nodes. however, this problem was np-hard to approximate within the ratio less than $\sqrt{n}/3$. to address this issue, we propose the penalty-based commitment device that does not delete edges but raises their cost. a benefits of our idea behind the method are twofold. on a conceptual side, we show that penalties are up to $1/\beta$ times more efficient than prohibition, where $\beta \in (0,1]$ parameterizes a present bias. on a computational side, we significantly improve approximability by presenting the $2$-approximation algorithm considering allocating a penalties. to complement this result, we prove that optimal penalties are np-hard to approximate within the ratio of $1.08192$.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10203,"edge detection was among a most fundamental vision problems considering its role inside perceptual grouping and its wide applications. recent advances inside representation learning have led to considerable improvements inside this area. many state of a art edge detection models are learned with fully convolutional networks (fcns). however, fcn-based edge learning tends to be vulnerable to misaligned labels due to a delicate structure of edges. while such problem is considered inside evaluation benchmarks, similar issue has not been explicitly addressed inside general edge learning. inside this paper, we show that label misalignment should cause considerably degraded edge learning quality, and address this issue by proposing the simultaneous edge alignment and learning framework. to this end, we formulate the probabilistic model where edge alignment was treated as latent variable optimization, and was learned end-to-end during network training. experiments show several applications of this work, including improved edge detection with state of a art performance, and automatic refinement of noisy annotations.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
3744,"inside this paper, we prove a local converse theorem considering $\textrm{sp}_{2r}(f)$ over the $p$-adic field $f$. more precisely, given two irreducible supercuspidal representations of $\textrm{sp}_{2r}(f)$ with a same central character such that they are generic with a same additive character and they have a same gamma factors when twisted with generic irreducible representations of $\textrm{gl}_n(f)$ considering all $1\le n\le r$, then these two representations must be isomorphic. our proof was based on a local analysis of a local integrals which define local gamma factors. the key ingredient of a proof was certain partial bessel function property developed by cogdell-shahidi-tsai recently. a same method should give a local converse theorem considering $\textrm{u}(r,r)$.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
3639,"reliable and real-time 3d reconstruction and localization functionality was the crucial prerequisite considering a navigation of actively controlled capsule endoscopic robots as an emerging, minimally invasive diagnostic and therapeutic technology considering use inside a gastrointestinal (gi) tract. inside this study, we propose the fully dense, non-rigidly deformable, strictly real-time, intraoperative map fusion idea behind the method considering actively controlled endoscopic capsule robot applications which combines magnetic and vision-based localization, with non-rigid deformations based frame-to-model map fusion. a performance of a proposed method was demonstrated with the help of four different ex-vivo porcine stomach models. across different trajectories of varying speed and complexity, and four different endoscopic cameras, a root mean square surface reconstruction errors 1.58 to 2.17 cm.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
18269,"feature aided tracking should often yield improved tracking performance over a standard multiple target tracking (mtt) algorithms with only kinematic measurements. however, inside many applications, a feature signal of a targets consists of sparse fourier-domain signals. it changes quickly and nonlinearly inside a time domain, and a feature measurements are corrupted by missed detections and mis-associations. these two factors make it hard to extract a feature information to be used inside mtt. inside this paper, we develop the feature-aided nearest neighbour joint probabilistic data association filter (nn-jpdaf) considering joint mtt and feature extraction inside dense target environments. to approximate a rapidly varying feature signal from incomplete and corrupted measurements, we use a atomic norm constraint to formulate a sparsity of feature signal and use a $\ell_1$-norm to formulate a sparsity of a corruption induced by mis-associations. based on a sparse representation, a feature signal are estimated by solving the semidefinite program (sdp) which was convex. we also provide an iterative method considering solving this sdp using a alternating direction method of multipliers (admm) where each iteration involves closed-form computation. with a estimated feature signal, re-filtering was performed to approximate a kinematic states of a targets, where a association makes use of both kinematic and feature information. simulation results are presented to illustrate a performance of a proposed algorithm inside the radar application.",1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
15822,"this paper presents the novel methodology considering a design of boundary feedback stabilizers considering 1-d, semilinear, parabolic pdes. a methodology was based on a use of small-gain arguments and should be applied to parabolic pdes with nonlinearities that satisfy the linear growth condition. a nonlinearities may contain nonlocal terms. two different types of boundary feedback stabilizers are constructed: the linear static boundary feedback and the nonlinear dynamic boundary feedback. it was also shown that there are fundamental limitations considering feedback design inside a parabolic case: arbitrary gain assignment was not possible by means of boundary feedback. an example with the nonlocal nonlinear term illustrates a applicability of a proposed methodology.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
12568,"inside this paper, we propose the new method considering approximation and constructing confidence intervals considering low-dimensional components inside the high-dimensional model. a proposed estimator, called constrained lasso (classo) estimator, was obtained by simultaneously solving two estimating equations---one imposing the zero-bias constraint considering a low-dimensional parameter and a other forming an $\ell_1$-penalized procedure considering a high-dimensional nuisance parameter. by carefully choosing a zero-bias constraint, a resulting estimator of a low dimensional parameter was shown to admit an asymptotically normal limit attaining a cram√©r-rao lower bound inside the semiparametric sense. we propose the tuning-free iterative algorithm considering implementing a classo. we show that when a algorithm was initialized at a lasso estimator, a de-sparsified estimator proposed inside van de geer et al. [\emph{ann. statist.} {\bf 42} (2014) 1166--1202] was asymptotically equivalent to a first iterate of a algorithm. we analyse a asymptotic properties of a classo estimator and show a globally linear convergence of a algorithm. we also demonstrate encouraging empirical performance of a classo through numerical studies.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0
1482,"we consider a initial-value problem considering a cubic-quintic nls \[ (i\partial_t+\delta)\psi=\alpha_1 \psi-\alpha_{3}\vert \psi\vert^2 \psi+\alpha_5\vert \psi\vert^4 \psi \] inside three spatial dimensions inside a class of solutions with $|\psi(x)|\to c >0$ as $|x|\to\infty$. here $\alpha_1$, $\alpha_3$, $\alpha_5$ and $c$ are such that $\psi(x)\equiv c$ was an energetically stable equilibrium solution to this equation. normalizing a boundary condition to $\psi(x)\to 1$ as $|x|\to\infty$, we study a associated initial-value problem considering $u=\psi-1$ and prove the scattering result considering small initial data inside the weighted sobolev space.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2992,"high-precision angle-resolved dc magnetization and magnetic torque studies were performed on the single-crystalline sample of urhge, an orthorhombic ising ferromagnet with a $c$ axis being a magnetization easy axis, inside order to investigate a phase diagram around a ferromagnetic (fm) reorientation transition inside the magnetic field near a $b$ axis. we have clearly detected first-order transition inside both a magnetization and a magnetic torque at low temperatures, and determined detailed profiles of a wing structure of a three-dimensional $t$-$h_{b}$-$h_{c}$ phase diagram, where $h_{c}$ and $h_{b}$ denotes a field components along a $c$ and a $b$ axes, respectively. a quantum wing critical points are located at $\mu_0h_c\sim\pm$1.1 t and $\mu_0h_b\sim$13.5 t. two second-order transition lines at a boundaries of a wing planes rapidly tend to idea behind the method with each other with increasing temperature up to $\sim 3$ k. just at a zero conjugate field ($h_c=0$), however, the signature of a first-order transition should still be seen inside a field derivative of a magnetization at $\sim 4$ k, indicating that a tricritical point exists inside the rather high temperature region above 4 k. this feature of a wing plane structure was consistent with a theoretical expectation that three second-order transition lines merge tangentially at a triciritical point.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
5566,"we show that a $g$-equivariant coherent derived category of $d$-modules on $\mathfrak{g}$ admits an orthogonal decomposition inside to blocks indexed by cuspidal data (in a sense of lusztig). each block admits the monadic description inside terms the certain differential graded algebra related to a homology of steinberg varieties, which resembles the ""triple affine"" hecke algebra. our results generalize a work of rider and rider--russell on constructible complexes on a nilpotent cone, and a earlier work of a author on a abelian category of equivariant $d$-modules on $\mathfrak{g}$. however, a algebra controlling a entire derived category of $d$-modules appears to be substantially more complicated than either of these special cases, as evidenced by a non-splitting of a mackey filtration on a monad controlling each block.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
1516,"inside this paper, we investigate the multi-node multi-antenna wireless-powered sensor networks (wpsn) comprised of one power beacon and multiple sensor nodes. we have implemented the real-life multi-node multi-antenna wpsn testbed that operates inside real time. we propose the beam-splitting beamforming technique that enables the power beacon to split microwave energy beams towards multiple nodes considering simultaneous charging. we experimentally demonstrate that a beam-splitting beamforming technique achieves a pareto optimality. considering the perpetual operation of a sensor nodes, we adopt an energy neutral control algorithm that keeps the sensor node alive by balancing a harvested and a consumed power. a joint beam-splitting and energy neutral control algorithm was designed by means of a lyapunov optimization technique. by experiments, we have shown that a proposed algorithm should successfully keep all sensor nodes alive by optimally splitting energy beams towards multiple sensor nodes.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
13535,"most existing motion planning algorithms assume that the map (of some quality) was fully determined prior to generating the motion plan. inside many emerging applications of robotics, e.g., fast-moving agile aerial robots with constrained embedded computational platforms and visual sensors, dense maps of a world are not immediately available, and they are computationally expensive to construct. we propose the new algorithm considering generating plan graphs which couples a perception and motion planning processes considering computational efficiency. inside the nutshell, a proposed algorithm iteratively switches between a planning sub-problem and a mapping sub-problem, each updating based on a other until the valid trajectory was found. a resulting trajectory retains the provable property of providing an optimal trajectory with respect to a full (unmapped) environment, while utilizing only the fraction of a sensing data inside computational experiments.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
9322,"artificial neural networks are the popular and effective machine learning technique. great progress has been made parallelizing a expensive training phase of an individual network, leading to highly specialized pieces of hardware, many based on gpu-type architectures, and more concurrent algorithms such as synthetic gradients. however, a training phase continues to be the bottleneck, where a training data must be processed serially over thousands of individual training runs. this work considers the multigrid reduction inside time (mgrit) algorithm that was able to parallelize over a thousands of training runs and converge to a exact same solution as traditional training would provide. mgrit is originally developed to provide parallelism considering time evolution problems that serially step through the finite number of time-steps. this work recasts a training of the neural network similarly, treating neural network training as an evolution equation that evolves a network weights from one step to a next. thus, this work concerns distributed computing approaches considering neural networks, but was distinct from other approaches which seek to parallelize only over individual training runs. a work concludes with supporting numerical results considering two model problems.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8179,"distributed optimization algorithms are essential considering training machine learning models on very large-scale datasets. however, they often suffer from communication bottlenecks. confronting this issue, the communication-efficient primal-dual coordinate ascent framework (cocoa) and its improved variant cocoa+ have been proposed, achieving the convergence rate of $\mathcal{o}(1/t)$ considering solving empirical risk minimization problems with lipschitz continuous losses. inside this paper, an accelerated variant of cocoa+ was proposed and shown to possess the convergence rate of $\mathcal{o}(1/t^2)$ inside terms of reducing suboptimality. a analysis of this rate was also notable inside that a convergence rate bounds involve constants that, except inside extreme cases, are significantly reduced compared to those previously provided considering cocoa+. a results of numerical experiments are provided to show that acceleration should lead to significant performance gains.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8509,"recently synthesized metastable tetragonal cose, isostructural to a fese superconductor, offers the new avenue considering investigating systems inside close proximity to a iron-based superconductors. we present magnetic and transport property measurements on powders and single crystals of cose. high field magnetic susceptibility measurements indicate the suppression of a previously reported 10 k ferromagnetic transition with a magnetic susceptibility exhibiting time-dependence below a proposed transition. dynamic scaling analysis of a time-dependence yields the critical relaxation time of $\tau^{*} = 0.064 \pm 0.008 $ s which inside turn yields an activation energy of $e_{a}^{*}$ = 14.84 $\pm$ 0.59 k and an ideal glass temperature $t_{0}^{*}$ = 8.91 $\pm$ 0.09 k from vogel-fulcher analysis. no transition was observed inside resistivity and specific heat measurements, but both measurements indicate that cose was metallic. these results are interpreted on a basis of cose exhibiting frustrated magnetic ordering arising from competing magnetic interactions. arrott analysis of single crystal magnetic susceptibility has indicated that a magnetic moments lie inside a $ab$-plane so frustration may arise from intralayer magnetic fluctuations as well as interlayer coupling. a results have implications considering understanding a superconductivity inside a iron chalcogenide systems as well as utilizing cose as the host considering chemical and physical manipulation to tune and explore emergent phenomena within an expanding new class of transition metal chalcogenides.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
13463,"we consider a area preserving curve shortening flow with neumann free boundary conditions outside of the convex domain or at the straight line. we give the criterion on initial curves that guarantees a appearance of the singularity inside finite time. we prove that a singularity was of type ii. furthermore, if these initial curves are convex, then an appropriate rescaling at a finite maximal time of existence yields the grim reaper or half the grim reaper as limit flow. we construct examples of initial curves satisfying a mentioned criterion.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
229,"a success of deep learning techniques inside a computer vision domain has triggered the range of initial investigations into their utility considering visual place recognition, all with the help of generic features from networks that were trained considering other types of recognition tasks. inside this paper, we train, at large scale, two cnn architectures considering a specific place recognition task and employ the multi-scale feature encoding method to generate condition- and viewpoint-invariant features. to enable this training to occur, we have developed the massive specific places dataset (sped) with hundreds of examples of place appearance change at thousands of different places, as opposed to a semantic place type datasets currently available. this new dataset enables us to set up the training regime that interprets place recognition as the classification problem. we comprehensively evaluate our trained networks on several challenging benchmark place recognition datasets and demonstrate that they achieve an average 10% increase inside performance over other place recognition algorithms and pre-trained cnns. by analyzing a network responses and their differences from pre-trained networks, we provide insights into what the network learns when training considering place recognition, and what these results signify considering future research inside this area.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
14913,"we investigate a critical properties of a phase transition towards complex tensor order that has been proposed to occur inside spin-orbit coupled superconductors. considering this purpose we formulate a bosonic field theory considering fluctuations of a complex irreducible second-rank tensor order parameter close to a transition. we then determine a scale dependence of a couplings of a theory by means of a perturbative renormalization group (rg). considering a isotropic system we generically detect the fluctuation-induced first-order phase transition. a initial values considering a running couplings are determined by a underlying microscopic model considering a tensorial order. as an example we study three-dimensional luttinger semimetals with electrons at the quadratic band touching point. whereas a strong-coupling transition of a model receives substantial fluctuation corrections, a weak-coupling transition at low temperatures was rendered only weakly first-order due to a presence of the fixed point inside a vicinity of a rg trajectory. if a number of fluctuating complex components of a order parameter was reduced by cubic anisotropy, a theory maps onto a field theory considering frustrated magnetism.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
1780,"inside this paper, the general binary-input binary-output (bibo) channel was investigated inside a presence of feedback and input constraints. a feedback capacity and a optimal input distribution of this setting are calculated considering a case of an $(1,\infty)$-rll input constraint, that is, a input sequence contains no consecutive ones. these results are obtained using explicit solution of the corresponding dynamic programming optimization problem. the simple coding scheme was designed based on a principle of posterior matching, which is introduced by shayevitz and feder considering memoryless channels. a posterior matching scheme considering our input-constrained setting was shown to achieve capacity with the help of two new ideas: \textit{message history}, which captures a memory embedded inside a setting, and \textit{message splitting}, which eases a analysis of a scheme. additionally, inside a special case of an s-channel, we give the very simple zero-error coding scheme that was shown to achieve capacity. considering a input-constrained bsc, we show with the help of our capacity formula that feedback increases capacity when a cross-over probability was small.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
15346,"inside this short article, we state the hopf type lemma considering fractional equations and a outline of its proof. we believe that it will become the powerful tool inside applying a method of moving planes on fractional equations to obtain qualitative properties of solutions.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8230,"as one of a most massive milky way satellites, a sagittarius dwarf galaxy has played an important role inside shaping a galactic disk and stellar halo morphologies. a disruption of sagittarius over several close-in passages has populated a halo of our galaxy with large-scale tidal streams and offers the unique diagnostic tool considering measuring its gravitational potential. here we test different progenitor mass models considering a milky way and sagittarius by modeling a full infall of a satellite. we constrain a mass of a galaxy based on a observed orbital parameters and multiple tidal streams of sagittarius. our semi-analytic modeling of a orbital dynamics agrees with full $n$-body simulations, and favors low values considering a milky way mass, $\lesssim 10^{12}m_\odot$. this conclusion eases a tension between $\lambda$cdm and a observed parameters of a milky way satellites.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17692,"estimating revenue and business demand of the newly opened venue was paramount as these early stages often involve critical decisions such as first rounds of staffing and resource allocation. traditionally, this approximation has been performed through coarse-grained measures such as observing numbers inside local venues or venues at similar places (e.g., coffee shops around another station inside a same city). a advent of crowdsourced data from devices and services carried by individuals on the daily basis has opened up a possibility of performing better predictions of temporal visitation patterns considering locations and venues. inside this paper, with the help of mobility data from foursquare, the location-centric platform, we treat venue categories as proxies considering urban activities and analyze how they become popular over time. a main contribution of this work was the prediction framework able to use characteristic temporal signatures of places together with k-nearest neighbor metrics capturing similarities among urban regions, to forecast weekly popularity dynamics of the new venue establishment inside the city neighborhood. we further show how we are able to forecast a popularity of a new venue after one month following its opening by with the help of locality and temporal similarity as features. considering a evaluation of our idea behind the method we focus on london. we show that temporally similar areas of a city should be successfully used as inputs of predictions of a visit patterns of new venues, with an improvement of 41% compared to the random selection of wards as the training set considering a prediction task. we apply these concepts of temporally similar areas and locality to a real-time predictions related to new venues and show that these features should effectively be used to predict a future trends of the venue. our findings have a potential to impact a design of location-based technologies and decisions made by new business owners.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
17975,"we develop the multivalued theory considering a stability operator of (a constant multiple of) the minimally immersed submanifold $\sigma$ of the riemannian manifold $\mathcal{m}$. we define a multiple valued counterpart of a classical jacobi fields as a minimizers of a second variation functional defined on the sobolev space of multiple valued sections of a normal bundle of $\sigma$ inside $\mathcal{m}$, and we study existence and regularity of such minimizers. finally, we prove that any $q$-valued jacobi field should be written as a superposition of $q$ classical jacobi fields everywhere except considering the relatively closed singular set having codimension at least two inside a domain.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5528,"challenges persist inside nonholonomic robot navigation inside dynamic environments. this paper presents the framework considering such navigation based on a model of generalized velocity obstacles (gvo). a idea of velocity obstacles has been well studied and developed considering obstacle avoidance since being proposed inside 1998. though it has been proved to be successful, most studies have assumed equations of motion to be linear, which limits their application to holonomic robots. inside addition, more attention has been paid to a immediate reaction of robots, while advance planning has been neglected. by applying a gvo model to differential drive robots and by combining it with rrt*, we reduce a uncertainty of a robot trajectory, thus further reducing a range of concern, and save both computation time and running time. by introducing uncertainty considering a dynamic obstacles with the kalman filter, we dilute a risk of considering a obstacles as uniformly moving along the straight line and guarantee a safety. special concern was given to path generation, including curvature check, making a generated path feasible considering nonholonomic robots. we experimentally demonstrate a feasibility of a framework.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
4055,"we complete a proof of a generalized smale conjecture, apart from a case of $rp^3$, and give the new proof of gabai's theorem considering hyperbolic 3-manifolds. we use an idea behind the method based on ricci flow through singularities, which applies uniformly to spherical space forms other than $s^3$ and $rp^3$ and hyperbolic manifolds, to prove that a moduli space of metrics of constant sectional curvature was contractible. as the corollary, considering such the 3-manifold $x$, a inclusion $\text{isom} (x,g)\to \text{diff}(x)$ was the homotopy equivalence considering any riemannian metric $g$ of constant sectional curvature.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5696,"we study a quantum synchronization between the pair of two-level systems in two coupled cavities. by with the help of the digital-analog decomposition of a master equation that rules a system dynamics, we show that this idea behind the method leads to quantum synchronization between both two-level systems. moreover, we should identify inside this digital-analog block decomposition a fundamental elements of the quantum machine learning protocol, inside which a agent and a environment (learning units) interact through the mediating system, namely, a register. if we should additionally equip this algorithm with the classical feedback mechanism, which consists of projective measurements inside a register, reinitialization of a register state and local conditional operations on a agent and environment subspace, the powerful and flexible quantum machine learning protocol emerges. indeed, numerical simulations show that this protocol enhances a synchronization process, even when every subsystem experience different loss/decoherence mechanisms, and give us a flexibility to choose a synchronization state. finally, we propose an implementation based on current technologies inside superconducting circuits.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4349,"distributed stochastic gradient descent (sgd) when run inside the synchronous manner, suffers from delays inside waiting considering a slowest learners (stragglers). asynchronous methods should alleviate stragglers, but cause gradient staleness that should adversely affect convergence. inside this work we present the novel theoretical characterization of a speed-up offered by asynchronous methods by analyzing a trade-off between a error inside a trained model and a actual training runtime (wallclock time). a novelty inside our work was that our runtime analysis considers random straggler delays, which helps us design and compare distributed sgd algorithms that strike the balance between stragglers and staleness. we also present the new convergence analysis of asynchronous sgd variants without bounded or exponential delay assumptions, and the novel learning rate schedule to compensate considering gradient staleness.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18573,"inside this paper, the fast and easy-to-deploy method with the strong interpretability considering community answer quality ranking was proposed. this method was improved based on a wilson score interval method [wilson, 1927], which retains its advantages and simultaneously improve a degree of satisfaction with a ranking of a high-quality answers. a improved answer quality score considers both wilson score interval and a spotlight index, a latter of which will be introduced inside a article. this method could significantly improve a ranking of a best answers with high attention inside diverse scenarios.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
2944,"one of a main aims of this article was to give a complete classification of critical metrics of a volume functional on the compact manifold $m$ with boundary $\partial m$ and with harmonic weyl tensor, which improves a corresponding classification considering complete locally conformally flat case, due to miao and tam [18]. inside particular, we prove that the critical metric with harmonic weyl tensor on the simply connected compact manifold with boundary isometric to the standard sphere $\mathbb{s}^{n-1}$ must be isometric to the geodesic ball inside the simply connected space form $\bbb{r}^n,$ $\bbb{h}^n$ and $\bbb{s}^n.$ inside order to achieve our goal, firstly we shall conclude a classification of such critical metrics under a bach-flat assumption and then we will prove that both geometric conditions are indeed equivalent.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19723,"neural networks are known to be vulnerable to adversarial examples: inputs that are close to natural inputs but classified incorrectly. inside order to better understand a space of adversarial examples, we survey ten recent proposals that are designed considering detection and compare their efficacy. we show that all should be defeated by constructing new loss functions. we conclude that adversarial examples are significantly harder to detect than previously appreciated, and a properties believed to be intrinsic to adversarial examples are inside fact not. finally, we propose several simple guidelines considering evaluating future proposed defenses.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
63,"bandit based optimisation has the remarkable advantage over gradient based approaches due to their global perspective, which eliminates a danger of getting stuck at local optima. however, considering continuous optimisation problems or problems with the large number of actions, bandit based approaches should be hindered by slow learning. gradient based approaches, on a other hand, navigate quickly inside high-dimensional continuous spaces through local optimisation, following a gradient inside fine grained steps. yet, apart from being susceptible to local optima, these schemes are less suited considering online learning due to their reliance on extensive trial-and-error before a optimum should be identified. inside this paper, we propose the bayesian idea behind the method that unifies a above two paradigms inside one single framework, with a aim of combining their advantages. at a heart of our idea behind the method we find the stochastic linear approximation of a function to be optimised, where both a gradient and values of a function are explicitly captured. this allows us to learn from both noisy function and gradient observations, and predict these properties across a action space to support optimisation. we further propose an accompanying bandit driven exploration scheme that uses bayesian credible bounds to trade off exploration against exploitation. our empirical results demonstrate that by unifying bandit and gradient based learning, one obtains consistently improved performance across the wide spectrum of problem environments. furthermore, even when gradient feedback was unavailable, a flexibility of our model, including gradient prediction, still allows us outperform competing approaches, although with the smaller margin. due to a pervasiveness of bandit based optimisation, our scheme opens up considering improved performance both inside meta-optimisation and inside applications where gradient related information was readily available.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5380,"we combine low energy muon spin rotation (le-$\mu$sr) and soft-x-ray angle-resolved photoemission spectroscopy (sx-arpes) to study a magnetic and electronic properties of magnetically doped topological insulators, (bi,sb)$_2$te$_3$. we find that one achieves the full magnetic volume fraction inside samples of (v/cr)$_x$(bi,sb)$_{2-x}$te$_3$ at doping levels x $\gtrsim$ 0.16. a observed magnetic transition was not sharp inside temperature indicating the gradual magnetic ordering. we find that a evolution of magnetic ordering was consistent with formation of ferromagnetic islands which increase inside number and/or volume with decreasing temperature. resonant arpes at a v $l_3$ edge reveals the nondispersing impurity band close to a fermi level as well as v weight integrated into a host band structure. calculations within a coherent potential approximation of a v contribution to a spectral function confirm that this impurity band was caused by v inside substitutional sites. a implications of our results on a observation of a quantum anomalous hall effect at mk temperatures are discussed.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
5266,"inside this paper, we analyze a topology and a content found on a ""darknet"", a set of websites accessible using tor. we created the darknet spider and crawled a darknet starting from the bootstrap list by recursively following links. we explored a whole connected component of more than 34,000 hidden services, of which we found 10,000 to be online. contrary to folklore belief, a visible part of a darknet was surprisingly well-connected through hub websites such as wikis and forums. we performed the comprehensive categorization of a content with the help of supervised machine learning. we observe that about half of a visible dark web content was related to apparently licit activities based on our classifier. the significant amount of content pertains to software repositories, blogs, and activism-related websites. among unlawful hidden services, most pertain to fraudulent websites, services selling counterfeit goods, and drug markets.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
16293,"exomoons represent an outstanding challenge inside modern astronomy, with a potential to provide rich insights into planet formation theory and habitability. inside this work, we stack a phase-folded transits of 284 viable moon hosting kepler planetary candidates, inside order to search considering satellites. these planets range from earth-to-jupiter sized and from ${\sim}$0.1-to-1.0 au inside separation - so-called ""warm"" planets. our data processing includes two-pass harmonic detrending, transit timing variations, model selection and careful data quality vetting to produce the grand light curve with an r.m.s. of 5.1 ppm. we find that a occurrence rate of galilean-analog moon systems considering planets orbiting between ${\sim}$0.1 and 1.0 au should be constrained to be $\eta<0.38$ to 95% confidence considering a 284 kois considered, with the 68.3% confidence interval of $\eta=0.16_{-0.10}^{+0.13}$. the single-moon model of variable size and separation locates the slight preference considering the population of short-period moons with radii ${\sim}0.5$ $r_{\oplus}$ orbiting at 5-10 planetary radii. however, we stress that a low bayes factor of just 2 inside this region means it should be treated as no more than the hint at this time. splitting our data into various physically-motivated subsets reveals no strong signal. a dearth of galilean-analogs around warm planets places a first strong constraint on exomoon formation models to date. finally, we report evidence considering an exomoon candidate kepler-1625b i, which we briefly describe ahead of scheduled observations of a target with a hubble space telescope.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16254,"corruptive behaviour inside politics limits economic growth, embezzles public funds, and promotes socio-economic inequality inside modern democracies. we analyse well-documented political corruption scandals inside brazil over a past 27 years, focusing on a dynamical structure of networks where two individuals are connected if they were involved inside a same scandal. our research reveals that corruption runs inside small groups that rarely comprise more than eight people, inside networks that have hubs and the modular structure that encompasses more than one corruption scandal. we observe abrupt changes inside a size of a largest connected component and inside a degree distribution, which are due to a coalescence of different modules when new scandals come to light or when governments change. we show further that a dynamical structure of political corruption networks should be used considering successfully predicting partners inside future scandals. we discuss a important role of network science inside detecting and mitigating political corruption.",1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
15637,"we develop the general method considering computing logarithmic and log-gamma expectations of distributions. as the result, we derive series expansions and integral representations of a entropy considering several fundamental distributions, including a poisson, binomial, beta-binomial, negative binomial, and hypergeometric distributions. our results also establish connections between a entropy functions and to a riemann zeta function and its generalizations.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
6117,"anomaly detectors are often used to produce the ranked list of statistical anomalies, which are examined by human analysts inside order to extract a actual anomalies of interest. unfortunately, inside realworld applications, this process should be exceedingly difficult considering a analyst since the large fraction of high-ranking anomalies are false positives and not interesting from a application perspective. inside this paper, we aim to make a analyst's job easier by allowing considering analyst feedback during a investigation process. ideally, a feedback influences a ranking of a anomaly detector inside the way that reduces a number of false positives that must be examined before discovering a anomalies of interest. inside particular, we introduce the novel technique considering incorporating simple binary feedback into tree-based anomaly detectors. we focus on a isolation forest algorithm as the representative tree-based anomaly detector, and show that we should significantly improve its performance by incorporating feedback, when compared with a baseline algorithm that does not incorporate feedback. our technique was simple and scales well as a size of a data increases, which makes it suitable considering interactive discovery of anomalies inside large datasets.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18013,"planetesimal formation stage represents the major gap inside our understanding of a planet formation process. a late-stage planet accretion models typically make arbitrary assumptions about planetesimals and pebbles distribution while a dust evolution models predict that planetesimal formation was only possible at some orbital distances. we want to test a importance of water snow line considering triggering formation of a first planetesimals during a gas-rich phase of protoplanetary disk, when cores of giant planets have to form. we connect prescriptions considering gas disk evolution, dust growth and fragmentation, water ice evaporation and recondensation, as well as transport of both solids and water vapor, and planetesimal formation using streaming instability into the single, one-dimensional model considering protoplanetary disk evolution. we find that processes taking place around a snow line facilitate planetesimal formation inside two ways. first, due to a change of sticking properties between wet and dry aggregates, there was the ""traffic jam"" in of a snow line that slows down a fall of solids onto a star. second, ice evaporation and outward diffusion of water followed by its recondensation increases a abundance of icy pebbles that trigger planetesimal formation using streaming instability just outside of a snow line. planetesimal formation was hindered by growth barriers and radial drift and thus requires particular conditions to take place. snow line was the favorable location where planetesimal formation was possible considering the wide range of conditions, but still not inside every protoplanetary disk model. this process was particularly promoted inside large, cool disks with low intrinsic turbulence and increased initial dust-to-gas ratio.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4545,"when a primary outcome was difficult to collect, surrogate endpoint was typically used as the substitute. it was possible that considering every individual, treatment has the positive effect on surrogate, and surrogate has the positive effect on primary outcome, but considering some individuals, treatment has the negative effect on primary outcome. considering example, the treatment may be substantially effective inside preventing a stroke considering everyone, and lowering a risk of stroke was universally beneficial considering the longer survival time, however, a treatment may still cause death considering some individuals. we define such paradoxical phenomenon as individual surrogate paradox. a individual surrogate paradox was preposed to capture a treatment effect heterogeneity, which was unable to be described by either a surrogate paradox based on average causal effect (ace) (chen et al., 2007) or that based on distributional causal effect (dce) (ju and geng, 2010). we investigate existing surrogate criteria inside terms of whether a individual surrogate paradox could manifest. we find that only a strong binary surrogate should avoid such paradox without additional assumptions. utilizing a sharp bounds, we propose novel criteria to exclude a individual surrogate paradox. our methods are illustrated inside an application to determine a effect of a intensive glycemia on a risk of development or progression of diabetic retinopathy.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
4135,"running high-resolution physical models was computationally expensive and essential considering many disciplines. agriculture, transportation, and energy are sectors that depend on high-resolution weather models, which typically consume many hours of large high performance computing (hpc) systems to deliver timely results. many users cannot afford to run a desired resolution and are forced to use low resolution output. one simple solution was to interpolate results considering visualization. it was also possible to combine an ensemble of low resolution models to obtain the better prediction. however, these approaches fail to capture a redundant information and patterns inside a low-resolution input that could aid improve a quality of prediction. inside this paper, we propose and evaluate the strategy based on the deep neural network to learn the high-resolution representation from low-resolution predictions with the help of weather forecast as the practical use case. we take the supervised learning approach, since obtaining labeled data should be done automatically. our results show significant improvement when compared with standard practices and a strategy was still lightweight enough to run on modest computer systems.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7384,"we obtain structure formulas considering a intertwining wave operators of the schroedinger operator with potential v inside r^3. a difference from our previous submission arxiv:1612.07304 lies with a fact that here we impose the scaling invariant condition on a potential, albeit with the smallness requirement.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8098,"we study the multiple measurement vector (mmv) idea behind the method to synthetic aperture radar (sar) imaging of scenes with direction dependent reflectivity and with polarization diverse measurements. a data are gathered by the moving transmit- receive platform which probes a imaging scene with signals and records a backscattered waves. a unknown reflectivity was represented by the matrix with row support corresponding to a location of a scatterers inside a scene, and columns corresponding to measurements gathered from different sub-apertures, or different polarization of a waves. a mmv methodology was used to approximate a reflectivity matrix by inverting inside an appropriate sense a linear system of equations that models a sar data. we obtain the resolution analysis of sar imaging with mmv, which takes into account a sparsity of a imaging scene, a separation of a scatterers and a diversity of a measurements. a results of a analysis are illustrated with some numerical simulations.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
15537,"a protoplanetary system hd 169142 was one of a few cases where the potential candidate protoplanet has been recently detected using direct imaging. to study a interaction between a protoplanet and a disk itself observations of a gas and dust surface density structure are needed. this paper reports new alma observations of a dust continuum at 1.3\,mm, $^{12}$co, $^{13}$co and c$^{18}$o $j=2-1$ emission from a system hd 169142 at angular resolution of $\sim 0"".18 - 0"".28$ ($\sim 20\,$au$ - 33\,$au). a dust continuum emission reveals the double-ring structure with an inner ring between $0"".17-0"".28$ ($\sim 20 - 35\,$au) and an outer ring between $0"".48-0"".64$ ($\sim 56 - 83\,$au). a size and position of a inner ring was inside good agreement with previous polarimetric observations inside a near-infrared and was consistent with dust trapping by the massive planet. no dust emission was detected in a inner dust cavity ($r \lesssim 20\,$au) or within a dust gap ($\sim 35 - 56\,$au). inside contrast, a channel maps of a $j=2-1$ line of a three co isotopologues reveal a presence of gas in a dust cavity and dust gap. a gaseous disk was also much larger than a compact dust emission extending to $\sim 1'.5$ ($\sim 180\,$au) inside radius. this difference and a sharp drop of a continuum emission at large radii point to radial drift of large dust grains ($>$ \micron-size). with the help of a thermo-chemical disk code \textsc{dali}, a continuum and a co isotopologues emission are modelled to quantitatively measure a gas and dust surface densities. a resulting gas surface density was reduced by the factor of $\sim 30-40$ inward of a dust gap. a gas and dust distribution hint at a presence of multiple planets shaping a disk structure using dynamical clearing (dust cavity and gap) and dust trapping (double ring dust distribution).",0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16050,"inside this work, we propose the new gaussian process regression (gpr) method: physics-informed kriging (phik). inside a standard data-driven kriging, a unknown function of interest was usually treated as the gaussian process with assumed stationary covariance with hyperparameters estimated from data. inside phik, we compute a mean and covariance function from realizations of available stochastic models, e.g., from realizations of governing stochastic partial differential equations solutions. such the constructed gaussian process generally was non-stationary, and does not assume the specific form of a covariance function. our idea behind the method avoids a costly optimization step inside data-driven gpr methods to identify a hyperparameters. more importantly, we prove that a physical constraints inside a form of the deterministic linear operator are guaranteed inside a resulting prediction. we also provide an error approximate inside preserving a physical constraints when errors are included inside a stochastic model realizations. to reduce a computational cost of obtaining stochastic model realizations, we propose the multilevel monte carlo approximate of a mean and covariance functions. further, we present an active learning algorithm that guides a selection of additional observation locations. a efficiency and accuracy of phik are demonstrated considering reconstructing the partially known modified branin function and learning the conservative tracer distribution from sparse concentration measurements.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4986,"we establish the new approximate considering a ginzburg-landau energies $e_{\epsilon}(u)=\int_m\frac{1}{2}|du|^2+\frac{1}{4\epsilon^2}(1-|u|^2)^2$ of complex-valued maps $u$ on the compact, oriented manifold $m$ with $b_1(m)\neq 0$, obtained by decomposing a harmonic component $h_u$ of a one-form $ju:=u^1du^2-u^2du^1$ into an integral and fractional part. we employ this approximate to show that, considering critical points $u_{\epsilon}$ of $e_{\epsilon}$ arising from a two-parameter min-max construction considered by a author inside previous work, the nontrivial portion of a energy must concentrate on the stationary, rectifiable $(n-2)$-varifold as $\epsilon\to 0$.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3436,"bilinear matrix inequality (bmi) problems inside system and control designs are investigated inside this paper. the solution method of reduction of variables (mrvs) was proposed. this method consists of the principle of variable classification, the procedure considering problem transformation, and the hybrid algorithm that combines deterministic and stochastic search engines. a classification principle was used to classify a decision variables of the bmi problem into two categories: 1) external and 2) internal variables. theoretical analysis was performed to show that when a classification principle was applicable, the bmi problem should be transformed into an unconstrained optimization problem that has fewer decision variables. stochastic search and deterministic search are then applied to determine a decision variables of a unconstrained problem externally and explore a internal problem structure, respectively. a proposed method should address feasibility, single-objective, and multiobjective problems constrained by bmis inside the unified manner. the number of numerical examples inside system and control designs are provided to validate a proposed methodology. simulations show that a mrvs should outperform existing bmi solution methods inside most benchmark problems and achieve similar levels of performance inside a remaining problems.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
3265,"extreme events are ubiquitous inside the wide range of dynamical systems, including turbulent fluid flows, nonlinear waves, large scale networks and biological systems. here, we propose the variational framework considering probing conditions that trigger intermittent extreme events inside high-dimensional nonlinear dynamical systems. we seek a triggers as a probabilistically feasible solutions of an appropriately constrained optimization problem, where a function to be maximized was the system observable exhibiting intermittent extreme bursts. a constraints are imposed to ensure a physical admissibility of a optimal solutions, i.e., significant probability considering their occurrence under a natural flow of a dynamical system. we apply a method to the body-forced incompressible navier--stokes equation, known as a kolmogorov flow. we find that a intermittent bursts of a energy dissipation are independent of a external forcing and are instead caused by a spontaneous transfer of energy from large scales to a mean flow using nonlinear triad interactions. a global maximizer of a corresponding variational problem identifies a responsible triad, thus providing the precursor considering a occurrence of extreme dissipation events. specifically, monitoring a energy transfers within this triad, allows us to develop the data-driven short-term predictor considering a intermittent bursts of energy dissipation. we assess a performance of this predictor through direct numerical simulations.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16420,"deep reinforcement learning (drl) methods such as a deep q-network (dqn) have achieved state-of-the-art results inside the variety of challenging, high-dimensional domains. this success was mainly attributed to a power of deep neural networks to learn rich domain representations considering approximating a value function or policy. batch reinforcement learning methods with linear representations, on a other hand, are more stable and require less hyper parameter tuning. yet, substantial feature engineering was necessary to achieve good results. inside this work we propose the hybrid idea behind the method -- a least squares deep q-network (ls-dqn), which combines rich feature representations learned by the drl algorithm with a stability of the linear least squares method. we do this by periodically re-training a last hidden layer of the drl network with the batch least squares update. key to our idea behind the method was the bayesian regularization term considering a least squares update, which prevents over-fitting to a more recent data. we tested ls-dqn on five atari games and demonstrate significant improvement over vanilla dqn and double-dqn. we also investigated a reasons considering a superior performance of our method. interestingly, we found that a performance improvement should be attributed to a large batch size used by a ls method when optimizing a last layer.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1449,"vasculature was known to be of key biological significance, especially inside a study of cancer. as such, considerable effort has been focused on a automated measurement and analysis of vasculature inside medical and pre-clinical images. inside tumors inside particular, a vascular networks may be extremely irregular and a appearance of a individual vessels may not conform to classical descriptions of vascular appearance. typically, vessels are extracted by either the segmentation and thinning pipeline, or by direct tracking. neither of these methods are well suited to microscopy images of tumor vasculature. inside order to address this we propose the method to directly extract the medial representation of a vessels with the help of convolutional neural networks. we then show that these two-dimensional centerlines should be meaningfully extended into 3d inside anisotropic and complex microscopy images with the help of a recently popularized convolutional long short-term memory units (convlstm). we demonstrate a effectiveness of this hybrid convolutional-recurrent architecture over both 2d and 3d convolutional comparators.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12425,"many computer vision applications involve modeling complex spatio-temporal patterns inside high-dimensional motion data. recently, restricted boltzmann machines (rbms) have been widely used to capture and represent spatial patterns inside the single image or temporal patterns inside several time slices. to model global dynamics and local spatial interactions, we propose to theoretically extend a conventional rbms by introducing another term inside a energy function to explicitly model a local spatial interactions inside a input data. the learning method was then proposed to perform efficient learning considering a proposed model. we further introduce the new method considering multi-class classification that should effectively approximate a infeasible partition functions of different rbms such that rbm was treated as the generative model considering classification purpose. a improved rbm model was evaluated on two computer vision applications: facial expression recognition and human action recognition. experimental results on benchmark databases demonstrate a effectiveness of a proposed algorithm.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7413,"a capability to isolate one to few unit-cell thin layers from a bulk matrix of layered compounds opens fascinating prospects to engineer novel electronic phases. however, the comprehensive study of a thickness dependence and of potential extrinsic effects are paramount to harness a electronic properties of such atomic foils. one striking example was a charge density wave (cdw) transition temperature inside layered dichalcogenides whose thickness dependence remains unclear inside a ultrathin limit. here we present the detailed study of a thickness and temperature dependences of a cdw inside vse$_2$ by scanning tunnelling microscopy (stm). we show that mapping a real-space cdw periodicity over the broad thickness range unique to stm provides essential insight. we introduce the robust derivation of a local order parameter and transition temperature based on a real space charge modulation amplitude. both quantities exhibit the striking non-monotonic thickness dependence that we explain inside terms of the 3d to 2d dimensional crossover inside a fs topology. this finding highlights thickness as the true tuning parameter of a electronic ground state and reconciles seemingly contradicting thickness dependencies determined inside independent transport studies.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
2031,"we propose the dynamic network model where two mechanisms control a probability of the link between two nodes: (i) a existence or absence of this link inside a past, and (ii) node-specific latent variables (dynamic fitnesses) describing a propensity of each node to create links. assuming the markov dynamics considering both mechanisms, we propose an expectation-maximization algorithm considering model approximation and inference of a latent variables. a estimated parameters and fitnesses should be used to forecast a presence of the link inside a future. we apply our methodology to a e-mid interbank network considering which a two linkage mechanisms are associated with two different trading behaviors inside a process of network formation, namely preferential trading and trading driven by node-specific characteristics. a empirical results allow to recognise preferential lending inside a interbank market and indicate how the method that does not account considering time-varying network topologies tends to overestimate preferential linkage.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0
4001,"warm forming processes have been successfully applied at laboratory level to overcome some important drawbacks of a al-mg alloys, such as poor formability and large springback. however, a numerical simulation of these processes requires a adoption of coupled thermo-mechanical finite element analysis, with the help of temperature-dependent material models. a numerical description of a thermo-mechanical behaviour should require the large set of experimental tests. these experimental tests should be performed under conditions identical to a ones observed inside a forming process. inside this study, a warm deep drawing of the cylindrical cup was analysed, including a split-ring test to assess a temperature effect on a springback. based on a analysis of a forming process conditions, a thermo-mechanical behaviour of a aa5086 aluminium alloy was described by the rate-independent thermo-elasto-plastic material model. a hardening law adopted was temperature-dependent while a yield function was temperature-independent. nevertheless, a yield criterion parameters are selected based on a temperature of a heated tools. inside fact, a model assumes that a temperature of a tools was uniform and constant, adopting the variable interfacial heat transfer coefficient. a accuracy of a proposed finite element model was assessed by comparing numerical and experimental results. a predicted punch force, thickness distribution and earing profile are inside very good agreement with a experimental measurements, when a anisotropic behaviour of a blank was accurately described. however, this does not guarantee the correct springback prediction, which was strongly influenced by a elastic properties, namely a young's modulus.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
1859,"we report on a fabrication and characterization of high-quality chemical vapor-deposited (cvd) bilayer graphene (blg). inside particular, we demonstrate that cvd-grown blg should mechanically be detached from a copper foil by an hexagonal boron nitride (hbn) crystal after oxidation of a copper-to-blg interface. confocal raman spectroscopy reveals an ab-stacking order of a blg crystals and the high structural quality. from transport measurements on fully encapsulated hbn/blg/hbn hall bar devices we extract charge carrier mobilities up to 180,000 cm$^2$/(vs) at 2 k and up to 40,000 cm$^2$/(vs) at 300 k, outperforming state-of-the-art cvd bilayer graphene devices. moreover, we show an on-off ration of more than 10,000 and the band gap opening with values of up to 15 mev considering the displacement field of 0.2 v/nm inside such cvd grown blg.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
5888,"thermoelectric materials are opening the promising pathway to address energy conversion issues governed by the competition between thermal and electronic transport. improving a efficiency was the difficult task, the challenge that requires new strategies to unearth optimized compounds. we present the theory of thermoelectric transport inside electron doped srtio3, based on the realistic tight binding model that includes relevant scattering processes. we compare our calculations against the wide panel of experimental data, both bulk and thin films. we find the qualitative and quantitative agreement over both the wide range of temperatures and carrier concentrations, from light to heavily doped. moreover, a results appear insensitive to a nature of a dopant la, b, gd and nb. thus, a quantitative success found inside a case of srtio3, reveals an efficient procedure to explore new routes to improve a thermoelectric properties inside oxides.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
6183,"unsupervised single-channel overlapped speech recognition was one of a hardest problems inside automatic speech recognition (asr). permutation invariant training (pit) was the state of a art model-based approach, which applies the single neural network to solve this single-input, multiple-output modeling problem. we propose to advance a current state of a art by imposing the modular structure on a neural network, applying the progressive pretraining regimen, and improving a objective function with transfer learning and the discriminative training criterion. a modular structure splits a problem into three sub-tasks: frame-wise interpreting, utterance-level speaker tracing, and speech recognition. a pretraining regimen uses these modules to solve progressively harder tasks. transfer learning leverages parallel clean speech to improve a training targets considering a network. our discriminative training formulation was the modification of standard formulations, that also penalizes competing outputs of a system. experiments are conducted on a artificial overlapped switchboard and hub5e-swb dataset. a proposed framework achieves over 30% relative improvement of wer over both the strong jointly trained system, pit considering asr, and the separately optimized system, pit considering speech separation with clean speech asr model. a improvement comes from better model generalization, training efficiency and a sequence level linguistic knowledge integration.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3048,"this work introduces the novel approximation method, called love, of a entries and structure of the loading matrix the inside the sparse latent factor model x = az + e, considering an observable random vector x inside rp, with correlated unobservable factors z \in rk, with k unknown, and independent noise e. each row of the was scaled and sparse. inside order to identify a loading matrix a, we require a existence of pure variables, which are components of x that are associated, using a, with one and only one latent factor. despite a fact that a number of factors k, a number of a pure variables, and their location are all unknown, we only require the mild condition on a covariance matrix of z, and the minimum of only two pure variables per latent factor to show that the was uniquely defined, up to signed permutations. our proofs considering model identifiability are constructive, and lead to our novel approximation method of a number of factors and of a set of pure variables, from the sample of size n of observations on x. this was a first step of our love algorithm, which was optimization-free, and has low computational complexity of order p2. a second step of love was an easily implementable linear program that estimates a. we prove that a resulting estimator was minimax rate optimal up to logarithmic factors inside p. a model structure was motivated by a problem of overlapping variable clustering, ubiquitous inside data science. we define a population level clusters as groups of those components of x that are associated, using a sparse matrix a, with a same unobservable latent factor, and multi-factor association was allowed. clusters are respectively anchored by a pure variables, and form overlapping sub-groups of a p-dimensional random vector x. a latent model idea behind the method to overlapping clustering was reflected inside a name of our algorithm, love.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0
16600,"opacities of molecules inside exoplanet atmospheres rely on increasingly detailed line-lists considering these molecules. a line lists available today contain considering many species up to several billions of lines. computation of a spectral line profile created by pressure and temperature broadening, a voigt profile, of all of these lines was becoming the computational challenge. we aim to create the method to compute a voigt profile inside the way that automatically focusses a computation time into a strongest lines, while still maintaining a continuum contribution of a high number of weaker lines. here, we outline the statistical line sampling technique that samples a voigt profile quickly and with high accuracy. a number of samples was adjusted to a strength of a line and a local spectral line density. this automatically provides high accuracy line shapes considering strong lines or lines that are spectrally isolated. a line sampling technique automatically preserves a integrated line opacity considering all lines, thereby also providing a continuum opacity created by a large number of weak lines at very low computational cost. a line sampling technique was tested considering accuracy when computing line spectra and correlated-k tables. extremely fast computations (~3.5e5 lines per second per core on the standard current day desktop computer) with high accuracy (< 1 % almost everywhere) are obtained. the detailed recipe on how to perform a computations was given.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
3496,"dropout was used as the practical tool to obtain uncertainty estimates inside large vision models and reinforcement learning (rl) tasks. but to obtain well-calibrated uncertainty estimates, the grid-search over a dropout probabilities was necessary - the prohibitive operation with large models, and an impossible one with rl. we propose the new dropout variant which gives improved performance and better calibrated uncertainties. relying on recent developments inside bayesian deep learning, we use the continuous relaxation of dropout's discrete masks. together with the principled optimisation objective, this allows considering automatic tuning of a dropout probability inside large models, and as the result faster experimentation cycles. inside rl this allows a agent to adapt its uncertainty dynamically as more data was observed. we analyse a proposed variant extensively on the range of tasks, and give insights into common practice inside a field where larger dropout probabilities are often used inside deeper model layers.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
13658,"we report a occurrence of a zero-bias conductance peak (zbcp) inside an inas nanowire coupled to pbin superconductors with varying temperature, bias voltage, and magnetic field. a zbcp was suppressed with increasing temperature and bias voltage above a thouless energy of a nanowire. applying the magnetic field also diminishes a zbcp when a resultant magnetic flux reaches a magnetic flux quantum h/2e. our observations are consistent with theoretical expectations of reflectionless tunneling, inside which a phase coherence between an electron and its andreev-reflected hole induces a zbcp as long as time-reversal symmetry was preserved.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
5295,"online social media provide users with unprecedented opportunities to engage with diverse opinions. simultaneously, they allow a spread of misinformation by empowering individuals to self-select a narratives they want to be exposed to, both through active (confirmation bias) and passive (personalized news algorithms) self-reinforcing mechanisms. the precise theoretical understanding of such trade-offs was still largely missing. we introduce the stylized social learning model where most participants inside the network update their beliefs unbiasedly based on a arrival of new information, while the fraction of participants display confirmation bias, enabling them to reject news that are incongruent with their pre-existing beliefs. we show that this simple confirmation bias mechanism should generate permanent opinion polarisation. furthermore, a model results inside states where unbiased agents behave ""as if"" they were biased, due to their biased neighbours effectively functioning as gatekeepers, restricting their access to free and diverse information. we derive analytic results considering a distribution of individual agents' beliefs, explicitly demonstrating a aforementioned trade-off between confirmation bias and social connectivity, which we further validate against us county-level data on a impact of internet access on a formation of beliefs about global warming. our findings indicate that confirmation bias inside small doses may actually result inside improved accuracy across individuals by preserving information diversity inside the social network. however, results also indicate that when confirmation bias grows past an optimal value, accuracy declines as biased agents restrict information flow to subgroups. we discuss a policy implications of our model, highlighting a downside of debunking strategies and suggesting alternative strategies to contrast misinformation.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
19270,"inside this work, we consider an extension of graphical models to random graphs, trees, and other objects. to do this, many fundamental concepts considering multivariate random variables (e.g., marginal variables, gibbs distribution, markov properties) must be extended to other mathematical objects; it turns out that this extension was possible, as we will discuss, if we have the consistent, complete system of projections on the given object. each projection defines the marginal random variable, allowing one to specify independence assumptions between them. furthermore, these independencies should be specified inside terms of the small subset of these marginal variables (which we call a atomic variables), allowing a compact representation of independencies by the directed graph. projections also define factors, functions on a projected object space, and thus the projection family defines the set of possible factorizations considering the distribution; these should be compactly represented by an undirected graph. a invariances used inside graphical models are essential considering learning distributions, not just on multivariate random variables, but also on other objects. when they are applied to random graphs and random trees, a result was the general class of models that was applicable to the broad range of problems, including those inside which a graphs and trees have complicated edge structures. these models need not be conditioned on the fixed number of vertices, as was often a case inside a literature considering random graphs, and should be used considering problems inside which attributes are associated with vertices and edges. considering graphs, applications include a modeling of molecules, neural networks, and relational real-world scenes; considering trees, applications include a modeling of infectious diseases, cell fusion, a structure of language, and a structure of objects inside visual scenes. many classic models are particular instances of this framework.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0
9505,"inside a 80th anniversary book considering alex m√ºller i wrote the story of our scientific collaboration, shared fascinations. this time i will be more personal, about a human side of our collaboration and encounters, while also referring to episodes mentioned inside shared fascinations.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0
7299,"-complex manufacturing systems are subject to high levels of variability that decrease productivity, increase cycle times and severely impact a systems tractability. as accurate modelling of a sources of variability was the cornerstone to intelligent decision making, we investigate a consequences of a assumption of independent and identically distributed variables that was often made when modelling sources of variability such as down-times, arrivals, or process-times. we first explain a experiment setting that allows, through simulations and statistical tests, to measure a variability potential stored inside the specific sequence of data. we show from industrial data that dependent behaviors might actually be a rule with potentially considerable consequences inside terms of cycle time. as complex industries require strong levers to allow their tractability, this work underlines a need considering the richer and more accurate modelling of real systems. keywords-variability; cycle time; dependent events; simulation; complex manufacturing; industry 4.0 i. accurate modelling of variability and a independence assumption industry 4.0 was said to be a next industrial revolution. a proper use of real-time information inside complex manufacturing systems was expected to allow more customization of products inside highly flexible production factories. semiconductor high mix low volume (hmlv) manufacturing facilities (called fabs) are one example of candidates considering this transition towards ""smart industries"". however, because of a high levels of variability, a environment of the hmlv fab was highly stochastic and difficult to manage. a uncontrolled variability limits a predictability of a system and thus a ability to meet delivery requirements inside terms of volumes, cycle times and due dates. typically, a hmlv stmicroelectronics crolles 300 fab regularly experiences significant mix changes that result inside unanticipated bottlenecks, leading to firefighting to meet commitment to customers. a overarching goal of our strategy was to improve a forecasting of future occurrences of bottlenecks and cycle time issues inside order to anticipate them through allocation of a correct attention and resources. our current finite capacity projection engine should effectively forecast bottlenecks, but it does not include reliable cycle time estimates. inside order to enhance our projections, better forecast cycle time losses (queuing times), improve a tractability of our system and reduce our cycle times, we now need accurate dynamic cycle time predictions. as increased cycle-time was a main reason workflow variability was studied (both by a scientific community and practitioners, see e.g. [1] and [2]), what follows concentrates on cycle times. moreover, a ""variability"" we account considering should be understood as a potential to create higher cycle times, even though ""variability"" may be understood inside the broader meaning. this choice was made considering a sake of clarity, but a methodology we propose and a discussion we lead should be applied to any other measurable indicator. sources of variability have been intensely investigated inside both a literature and a industry, and tool down-times, arrivals variability as well as process-time variability are recognized as a major sources of variability inside that sense that they create higher cycle times (see [3] considering the review and discussion). as the consequence, these factors are widely integrated into queuing formulas and simulation models with a objective to better model a complex reality of manufacturing facilities. one commonly accepted assumption inside a development of these models was that a variables (mtbf, mttr, processing times, time between arrivals, etc.) are independent and identically distributed (i.i.d.) random variables. however, these assumptions might be a reason considering models inaccuracies as [4] points out inside the literature review on queuing theory. several authors have studied a potential effects of dependencies, such as [5] who studied a potential effects of dependencies between arrivals and process-times or [6] who investigated dependent process times, [4] also gives further references considering studies on dependencies effects. inside the previous work [3], we pinpointed the few elements from industrial data that questioned a viability of this assumption inside complex manufacturing systems. figure 1: number of arrivals per week from real data (a) and generated by removing dependencies (b)",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16618,"packing and covering linear programs belong to a narrow class of linear programs that are efficiently solvable inside parallel and distributed models of computation, yet are the powerful modeling tool considering the wide range of fundamental problems inside theoretical computer science, operations research, and many other areas. following recent progress inside obtaining faster distributed and parallel algorithms considering packing and covering linear programs, we present the simple algorithm whose iteration count matches a best known $\tilde{o}(\frac{1}{\epsilon^2})$ considering this class of problems. a algorithm was similar to a algorithm of [allen-zhu and orecchia, 2015], it should be interpreted as nesterov's dual averaging, and it constructs approximate solutions to both primal (packing) and dual (covering) problems. however, a analysis relies on a construction of an approximate optimality gap and the primal-dual view, leading to the more intuitive interpretation. moreover, our analysis suggests that all existing algorithms considering solving packing and covering linear programs inside parallel/distributed models of computation are, inside fact, unaccelerated, and raises a question of designing accelerated algorithms considering this class of problems.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2657,"nearest-neighbor search dominates a asymptotic complexity of sampling-based motion planning algorithms and was often addressed with k-d tree data structures. while it was generally believed that a expected complexity of nearest-neighbor queries was $o(log(n))$ inside a size of a tree, this paper reveals that when the classic k-d tree idea behind the method was used with sub-riemannian metrics, a expected query complexity was inside fact $\theta(n^p \log(n))$ considering the number $p \in [0, 1)$ determined by a degree of nonholonomy of a system. these metrics arise naturally inside nonholonomic mechanical systems, including classic wheeled robot models. to address this negative result, we propose novel k-d tree build and query strategies tailored to sub-riemannian metrics and demonstrate significant improvements inside a running time of nearest-neighbor search queries.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1
10622,"generative statistical models of chord sequences play crucial roles inside music processing. to capture syntactic similarities among certain chords (e.g. inside c major key, between g and g7 and between f and dm), we study hidden markov models and probabilistic context-free grammar models with latent variables describing syntactic categories of chord symbols and their unsupervised learning techniques considering inducing a latent grammar from data. surprisingly, we find that these models often outperform conventional markov models inside predictive power, and a self-emergent categories often correspond to traditional harmonic functions. this implies a need considering chord categories inside harmony models from a informatics perspective.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16467,"new social and economic activities massively exploit big data and machine learning algorithms to do inference on people's lives. applications include automatic curricula evaluation, wage determination, and risk assessment considering credits and loans. recently, many governments and institutions have raised concerns about a lack of fairness, equity and ethics inside machine learning to treat these problems. it has been shown that not including sensitive features that bias fairness, such as gender or race, was not enough to mitigate a discrimination when other related features are included. instead, including fairness inside a objective function has been shown to be more efficient. we present novel fair regression and dimensionality reduction methods built on the previously proposed fair classification framework. both methods rely on with the help of a hilbert schmidt independence criterion as a fairness term. unlike previous approaches, this allows us to simplify a problem and to use multiple sensitive variables simultaneously. replacing a linear formulation by kernel functions allows a methods to deal with nonlinear problems. considering both linear and nonlinear formulations a solution reduces to solving simple matrix inversions or generalized eigenvalue problems. this simplifies a evaluation of a solutions considering different trade-off values between a predictive error and fairness terms. we illustrate a usefulness of a proposed methods inside toy examples, and evaluate their performance on real world datasets to predict income with the help of gender and/or race discrimination as sensitive variables, and contraceptive method prediction under demographic and socio-economic sensitive descriptors.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14189,"we propose an algorithm considering the family of optimization problems where a objective should be decomposed as the sum of functions with monotonicity properties. a motivating problem was optimization of hyperparameters of machine learning algorithms, where we argue that a objective, validation error, should be decomposed as monotonic functions of a hyperparameters. our proposed algorithm adapts bayesian optimization methods to incorporate a monotonicity constraints. we illustrate a advantages of exploiting monotonicity with the help of illustrative examples and demonstrate a improvements inside optimization efficiency considering some machine learning hyperparameter tuning applications.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
652,"a spectral gap $\gamma$ of the finite, ergodic, and reversible markov chain was an important parameter measuring a asymptotic rate of convergence. inside applications, a transition matrix $p$ may be unknown, yet one sample of a chain up to the fixed time $n$ may be observed. we consider here a problem of estimating $\gamma$ from this data. let $\pi$ be a stationary distribution of $p$, and $\pi_\star = \min_x \pi(x)$. we show that if $n = \tilde{o}\bigl(\frac{1}{\gamma \pi_\star}\bigr)$, then $\gamma$ should be estimated to within multiplicative constants with high probability. when $\pi$ was uniform on $d$ states, this matches (up to logarithmic correction) the lower bound of $\tilde{\omega}\bigl(\frac{d}{\gamma}\bigr)$ steps required considering precise approximation of $\gamma$. moreover, we provide a first procedure considering computing the fully data-dependent interval, from the single finite-length trajectory of a chain, that traps a mixing time $t_{\text{mix}}$ of a chain at the prescribed confidence level. a interval does not require a knowledge of any parameters of a chain. this stands inside contrast to previous approaches, which either only provide point estimates, or require the reset mechanism, or additional prior knowledge. a interval was constructed around a relaxation time $t_{\text{relax}} = 1/\gamma$, which was strongly related to a mixing time, and a width of a interval converges to zero roughly at the $1/\sqrt{n}$ rate, where $n$ was a length of a sample path.",1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0
4450,"this chapter of a book planetary ring systems addresses a origin of planetary rings, one of a least understood processes related to planet formation and evolution. whereas rings seem ubiquitous around giant planets, their great diversity of mass, structure and composition was the challenge considering any formation scenario. recent advances inside our understanding of ring and satellite formation and destruction suggest that these processes are interconnected, so that rings and satellites may be two aspects of a same geological system. however, no single theory seems able to explain a origin of a different planetary rings known inside our solar system, and it now seems evident that rings may result from the variety of processes like giant collisions, tidal stripping of comets or satellites, as well as planet formation itself. inside order to build any theory of ring formation it was important to specify physical processes that affect a long-term evolution of rings, as well as to describe a different observations that any ring formation model should explain. this was a topic of section 2. inside section 3, we focus our attention on saturn's rings and their main properties, and then discuss a pros and cons of the series of ring formation models. we also discuss a link between rings and satellites. inside section 4, we extend a discussion to a other giant planets (jupiter, uranus, and neptune). section 5 was devoted to new types of rings -- a recent discovery of rings orbiting small outer solar system bodies (centaurs), and a possible rings around extrasolar planets. inside section 6, we conclude and try to identify critical observations and theoretical advances needed to better understand a origin of rings and their significance inside a global evolution of planets.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18213,"convolutional networks reach top quality inside pixel-level video object segmentation but require the large amount of training data (1k~100k) to deliver such results. we propose the new training strategy which achieves state-of-the-art results across three evaluation datasets while with the help of 20x~1000x less annotated data than competing methods. our idea behind the method was suitable considering both single and multiple object segmentation. instead of with the help of large training sets hoping to generalize across domains, we generate in-domain training data with the help of a provided annotation on a first frame of each video to synthesize (""lucid dream"") plausible future video frames. in-domain per-video training data allows us to train high quality appearance- and motion-based models, as well as tune a post-processing stage. this idea behind the method allows to reach competitive results even when training from only the single annotated frame, without imagenet pre-training. our results indicate that with the help of the larger training set was not automatically better, and that considering a video object segmentation task the smaller training set that was closer to a target domain was more effective. this changes a mindset regarding how many training samples and general ""objectness"" knowledge are required considering a video object segmentation task.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
670,"a increasing popularity of server usage has brought the plenty of anomaly log events, which have threatened the vast collection of machines. recognizing and categorizing a anomalous events thereby was the much salient work considering our systems, especially a ones generate a massive amount of data and harness it considering technology value creation and business development. to assist inside focusing on a classification and a prediction of anomaly events, and gaining critical insights from system event records, we propose the novel log preprocessing method which was very effective to filter abundant information and retain critical characteristics. additionally, the competitive idea behind the method considering automated classification of anomalous events detected from a distributed system logs with a state-of-the-art deep (convolutional neural network) architectures was proposed inside this paper. we measure the series of deep cnn algorithms with varied hyper-parameter combinations by with the help of standard evaluation metrics, a results of our study reveals a advantages and potential capabilities of a proposed deep cnn models considering anomaly event classification tasks on real-world systems. a optimal classification precision of our idea behind the method was 98.14%, which surpasses a popular traditional machine learning methods.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3937,"let g be the complex, semisimple lie algebra. drinfeld showed that a quantum group associated to g was isomorphic as an algebra to a trivial deformation of a universal enveloping algebra of g. inside this paper we construct explicitly such an isomorphism when g = sl(n), previously known only considering n=2.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
19940,"we derive a moser-trudinger-onofri inequalities on a 2-sphere and a 4-sphere as a limiting cases of a fractional power sobolev inequalities on a same spaces, and justify our idea behind the method as a dimensional continuation argument initiated by thomas p. branson.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7228,"real-time navigation services, such as google maps and waze, are widely used inside daily life. these services provide rich data resources inside real-time traffic conditions and travel time predictions; however, they have not been fully applied inside transportation modeling. this paper aims to use traffic data from google maps and applying cutting-edge technologies inside maximum likelihood approximation to model traffic networks and travel time reliability. this paper integrates google maps travel time data considering routes and traffic condition data considering links to model a complexities of traffic networks. we then formulate a fisher information matrix and apply a asymptotic normality to obtain a probability distribution of a travel time estimates considering the random route within a network of interest. we also derive a travel time reliability by considering two levels of uncertainties, i.e., a uncertainty of a route's travel time and a uncertainty of its travel time estimates. a proposed method could provide the more realistic and precise travel time reliability estimate. a methodology was applied to the small network inside a downtown baltimore area, where we propose the link data collection strategy and provide empirical evidence to show data independence by following this strategy. we also show results considering maximum likelihood estimates and travel time reliability measures considering different routes within a network. furthermore, we use a historical data from the different network to validate this approach, showing our method provides the more accurate and precise approximate compared to a sample mean of a empirical data.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
17579,"inside this paper, we introduce the generalized value iteration network (gvin), which was an end-to-end neural network planning module. gvin emulates a value iteration algorithm by with the help of the novel graph convolution operator, which enables gvin to learn and plan on irregular spatial graphs. we propose three novel differentiable kernels as graph convolution operators and show that a embedding based kernel achieves a best performance. we further propose episodic q-learning, an improvement upon traditional n-step q-learning that stabilizes training considering networks that contain the planning module. lastly, we evaluate gvin on planning problems inside 2d mazes, irregular graphs, and real-world street networks, showing that gvin generalizes well considering both arbitrary graphs and unseen graphs of larger scale and outperforms the naive generalization of vin (discretizing the spatial graph into the 2d image).",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15959,this paper describes a system developed at amobee considering a wassa 2018 implicit emotions shared task (iest). a goal of this task is to predict a emotion expressed by missing words inside tweets without an explicit mention of those words. we developed an ensemble system consisting of language models together with lstm-based networks containing the cnn attention mechanism. our idea behind the method represents the novel use of language models (specifically trained on the large twitter dataset) to predict and classify emotions. our system reached 1st place with the macro $\text{f}_1$ score of 0.7145.,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5115,"we introduce the library of geometric voxel features considering cad surface recognition/retrieval tasks. our features include local versions of a intrinsic volumes (the usual 3d volume, surface area, integrated mean and gaussian curvature) and the few closely related quantities. we also compute haar wavelet and statistical distribution features by aggregating raw voxel features. we apply our features to object classification on a esb data set and demonstrate accurate results with the small number of shallow decision trees.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7272,"although dopants have been extensively employed to promote ferroelectricity inside hafnia films, their role inside stabilizing a responsible ferroelectric non-equilibrium pca21 phase was not well understood. inside this work, with the help of first principles computations, we investigate a influence of nearly 40 dopants on a phase stability inside bulk hafnia to identify dopants that should favor formation of a polar pca21 phase. although no dopant is found to stabilize this polar phase as a ground state, suggesting that dopants alone cannot induce ferroelectricity inside hafnia, ca, sr, ba, la, y and gd were found to significantly lower a energy of a polar phase with respect to a equilibrium monoclinic phase. these results are consistent with a empirical measurements of large remnant polarization inside hafnia films doped with these elements. additionally, clear chemical trends of dopants with larger ionic radii and lower electronegativity favoring a polar pca21 phase inside hafnia were identified. considering this polar phase, an additional bond between a dopant cation and a 2nd nearest oxygen neighbor is identified as a root-cause of these trends. further, trivalent dopants (y, la, and gd) were revealed to stabilize a polar pca21 phase at lower strains when compared to divalent dopants (sr and ba). based on these insights, we predict that a lanthanide series metals, a lower half of alkaline earth metals (ca, sr and ba) and y as a most suitable dopants to promote ferroelectricity inside hafnia.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
16629,"small $p$-values are often required to be accurately estimated inside large scale genomic studies considering a adjustment of multiple hypothesis tests and a ranking of genomic features based on their statistical significance. considering those complicated test statistics whose cumulative distribution functions are analytically intractable, existing methods usually do not work well with small $p$-values due to lack of accuracy or computational restrictions. we propose the general idea behind the method considering accurately and efficiently calculating small $p$-values considering the broad range of complicated test statistics based on a principle of a cross-entropy method and markov chain monte carlo sampling techniques. we evaluate a performance of a proposed algorithm through simulations and demonstrate its application to three real examples inside genomic studies. a results show that our idea behind the method should accurately evaluate small to extremely small $p$-values (e.g. $10^{-6}$ to $10^{-100}$). a proposed algorithm was helpful to a improvement of existing test procedures and a development of new test procedures inside genomic studies.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14166,"inside functional data analysis, data are commonly assumed to be smooth functions on the fixed interval of a real line. inside this work, we introduce the comprehensive framework considering a analysis of functional data, whose domain was the two-dimensional manifold and a domain itself was subject to variability from sample to sample. we formulate the statistical model considering such data, here called functions on surfaces, which enables the joint representation of a geometric and functional aspects, and propose an associated approximation framework. we assess a validity of a framework by performing the simulation study and we finally apply it to a analysis of neuroimaging data of cortical thickness, acquired from a brains of different subjects, and thus lying on domains with different geometries.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
3160,"we investigate a dust distribution inside a crescent disk around hd 142527 based on a continuum emission at $890 \mathrm{\ \mu m}$ obtained by alma cycle 0. a map was divided into $18$ azimuthal sectors, and a radial intensity profile inside each sector was reproduced with the 2d disk model. our model takes account of scattering and inclination of a disk as well as a azimuthal dependence inside intensity. when a dust was assumed to have a conventional composition and maximum size of $1\ \mathrm{mm}$, a northwestern region ($pa=329^{\circ}-29^{\circ}$) cannot be reproduced. this was because a model intensity gets insensitive to a increase inside surface density due to heavy self-scattering, reaching its ceiling much lower than a observed intensity. a ceiling depends on a position angle. when a scattering opacity was reduced by the factor of $10$, a intensity distribution was reproduced successfully inside all a sectors including those inside a northwestern region. a best fit model parameters depend little on a scattering opacity inside a southern region where a disk was optically thin. a contrast of dust surface density along $pa$ was derived to be about $40$, much smaller than a value considering a cases of conventional opacities ($70-130$). these results strongly suggest that a albedo was lower than considered by some reasons at least inside a northwestern region.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11137,"physics of many-body systems where particles are restricted to move inside two spatial dimensions was challenging and even controversial: on one hand, neither long-range order nor bose condensation may appear inside infinite uniform 2d systems at finite temperature, on a other hand this does not prohibit superfluidity or superconductivity. moreover, 2d superconductors, such as cuprates, are among a systems with highest critical temperatures. ultracold atoms are the platform considering studying 2d physics. uniquely to other physical systems, quantum statistics may be completely changed inside an ultracold gas: an atomic fermi gas may be smoothly crossed over into the gas of bose molecules (or dimers) by tuning interatomic interactions. we review recent experiments where such crossover has been demonstrated as well as critical phenomena inside a fermi-to-bose crossover. we also present simple theoretical models describing a gas at different points of a crossover and compare a data to these and more advanced models.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
12847,"we present the general formalism considering identifying a caustic structure of an evolving mass distribution inside an arbitrary dimensional space. considering a class of hamiltonian fluids a identification corresponds to a classification of singularities inside lagrangian catastrophe theory. based on this we develop the theoretical framework considering a formation of a cosmic web, and specifically those aspects that characterize its unique nature: its complex topological connectivity and multiscale spinal structure of sheetlike membranes, elongated filaments and compact cluster nodes. a present work represents an extension of a work by arnol'd et al., who classified a caustics considering a 1- and 2-dimensional zel'dovich approximation. his seminal work established a role of emerging singularities inside a formation of nonlinear structures inside a universe. at a transition from a linear to nonlinear structure evolution, a first complex features emerge at locations where different fluid elements cross to establish multistream regions. a classification and characterization of these mass element foldings should be encapsulated inside caustic conditions on a eigenvalue and eigenvector fields of a deformation tensor field. we introduce an alternative and transparent proof considering lagrangian catastrophe theory, and derive a caustic conditions considering general lagrangian fluids, with arbitrary dynamics, including dissipative terms and vorticity. a new proof allows us to describe a full 3-dimensional complexity of a gravitationally evolving cosmic matter field. one of our key findings was a significance of a eigenvector field of a deformation field considering outlining a spatial structure of a caustic skeleton. we consider a caustic conditions considering a 3-dimensional zel'dovich approximation, extending earlier work on those considering 1- and 2-dimensional fluids towards a full spatial richness of a cosmic web.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18955,"inside the previous joint work of xiao and a second author, a modified mean curvature flow (mmcf) inside hyperbolic space $\mathbb{h}^{n+1}$: $$\frac{\partial \mathbf{f}}{\partial t} = (h-\sigma)\,\vnu\,,\quad \quad \sigma\in (-n,n)$$ is first introduced and a flow starting from an entire lipschitz continuous radial graph with uniform local ball condition on a asymptotic boundary is shown to exist considering all time and converge to the complete hypersurface of constant mean curvature with prescribed asymptotic boundary at infinity. inside this paper, we remove a uniform local ball condition on a asymptotic boundary of a initial hypersurface, and prove that a mmcf starting from an entire locally lipschitz continuous radial graph exists and stays radially graphic considering all time.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6597,"we find the series of topological phase transitions of increasing order, beyond a more standard second-order phase transition inside the one-dimensional topological superconductor. a jumps inside a order of a transitions depend on a range of a pairing interaction, which was parametrized by an algebraic decay with exponent $\alpha$. remarkably, inside a limit $\alpha = 1$ a order of a topological transition becomes infinite. we compute a critical exponents considering a series of higher-order transitions inside exact form and find that they fulfill a hyperscaling relation. we also study a critical behaviour at a boundary of a system and discuss potential experimental platforms of magnetic atoms inside superconductors.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
5740,"gaussian processes are rich distributions over functions, with generalization properties determined by the kernel function. when used considering long-range extrapolation, predictions are particularly sensitive to a choice of kernel parameters. it was therefore critical to account considering kernel uncertainty inside our predictive distributions. we propose the distribution over kernels formed by modelling the spectral mixture density with the l√©vy process. a resulting distribution has support considering all stationary covariances--including a popular rbf, periodic, and mat√©rn kernels--combined with inductive biases which enable automatic and data efficient learning, long-range extrapolation, and state of a art predictive performance. a proposed model also presents an idea behind the method to spectral regularization, as a l√©vy process introduces the sparsity-inducing prior over mixture components, allowing automatic selection over model order and pruning of extraneous components. we exploit a algebraic structure of a proposed process considering $\mathcal{o}(n)$ training and $\mathcal{o}(1)$ predictions. we perform extrapolations having reasonable uncertainty estimates on several benchmarks, show that a proposed model should recover flexible ground truth covariances and that it was robust to errors inside initialization.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15390,"on the stratified lie group $g$ equipped with hypoelliptic heat kernel measure, we study a behavior of a dilation semigroup on $l^p$ spaces of log-subharmonic functions. we consider the notion of strong hypercontractivity and the strong logarithmic sobolev inequality, and show that these properties are equivalent considering any group $g$. moreover, if $g$ satisfies the classical logarithmic sobolev inequality, then both properties hold. this extends similar results obtained by graczyk, kemp and loeb inside a euclidean setting.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17009,a ubiquitous ether coming from a ancient times up to middle of a twenty century was replaced by the superfluid quantum space. it represents by itself the bose-einstein condensate consisting of enormous amount of virtual particle-antiparticle pairs emerging and disappearing inside an infinitely ongoing dance. flowing of this medium inside a non-relativistic limit was described by a modified navier-stokes equation along with a continuity equation. a first equation admits a splitting on to two coupled equations. they are a quantum hamilton-jacobi equation and a equation considering vorticity. a quantum hamilton-jacoby equation paired with a continuity equation should be reduced to a \schrodinger equation. these two equations representing a kernel of a bohmian mechanics give finding bundle of a bohmian trajectories. whereas a vorticity equation gives solutions considering vortices moving along such trajectories. as a result we come to a de broglie's interpretation of quantum mechanics according to which there was the pilot-wave guiding a particle (in our case it was the vortex clot) from the source up to its detection along an optimal path that was a bohmian trajectory.,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12322,"attention mechanism has been used as an ancillary means to aid rnn or cnn. however, a transformer (vaswani et al., 2017) recently recorded a state-of-the-art performance inside machine translation with the dramatic reduction inside training time by solely with the help of attention. motivated by a transformer, directional self attention network (shen et al., 2017), the fully attention-based sentence encoder, is proposed. it showed good performance with various data by with the help of forward and backward directional information inside the sentence. but inside their study, not considered at all is a distance between words, an important feature when learning a local dependency to aid understand a context of input text. we propose distance-based self-attention network, which considers a word distance by with the help of the simple distance mask inside order to model a local dependency without losing a ability of modeling global dependency which attention has inherent. our model shows good performance with nli data, and it records a new state-of-the-art result with snli data. additionally, we show that our model has the strength inside long sentences or documents.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3857,"event detection was the critical feature inside data-driven systems as it assists with a identification of nominal and anomalous behavior. event detection was increasingly relevant inside robotics as robots operate with greater autonomy inside increasingly unstructured environments. inside this work, we present an accurate, robust, fast, and versatile measure considering skill and anomaly identification. the theoretical proof establishes a link between a derivative of a log-likelihood of a hmm filtered belief state and a latest emission probabilities. a key insight was a inverse relationship inside which gradient analysis was used considering skill and anomaly identification. our measure showed better performance across all metrics than related state-of-the art works. a result was broadly applicable to domains that use hmms considering event detection.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
10011,"we address a problem of generating query suggestions to support users inside completing their underlying tasks (which motivated them to search inside a first place). given an initial query, these query suggestions should provide the coverage of possible subtasks a user might be looking for. we propose the probabilistic modeling framework that obtains keyphrases from multiple sources and generates query suggestions from these keyphrases. with the help of a test suites of a trec tasks track, we evaluate and analyze each component of our model.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13540,"this was the continuation and completion of a program (initiated inside \cite{grn1,grn2}) to derive pointwise estimates on a green function and sharp bounds on a semigroup of linearized navier-stokes around the generic stationary boundary layer profile. this was done using the spectral analysis idea behind the method and the careful study of a orr-sommerfeld equations, or equivalently a navier-stokes resolvent operator $(\lambda - l)^{-1}$. a earlier work (\cite{grn1,grn2}) treats a orr-sommerfeld equations away from critical layers: this was a case when a phase velocity was away from a range of a background profile or when $\lambda$ was away from a euler continuous spectrum. inside this paper, we study a critical case: a orr-sommerfeld equations near critical layers, providing pointwise estimates on a green function as well as carefully studying a dunford's contour integral near a critical layers. as an application, we obtain pointwise estimates on a green function and sharp bounds on a semigroup of a linearized navier-stokes problem near monotonic boundary layers that are spectrally stable to a euler equations, complementing \cite{grn1,grn2} where unstable profiles are considered.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7271,"emerging inside a 1950s, a multiband superconductivity has been considered considering the long time as an approximate model inside a form of the generalization of a bcs theory to a case of two bands considering the more accurate quantitative description of a properties and characteristics of such superconductors as cuprates, heavy fermions compounds, metal boron carbides, fullerides, strontium ruthenate etc. due to their complex piecewise-continuous fermi surfaces. however a discovery of a multiband structure of a superconducting state inside magnesium diboride inside 2001 and iron oxypnictides and chalcogenides inside 2008 led to a appearance of many papers inside which effects and different dependences well known considering conventional single-band s-wave superconductors were reexamined. a main purpose of these studies is to reveal a symmetry type of a order parameter, which provides an important information about a mechanism of cooper pairing inside these superconductors. one of a most effective methods of obtaining information on a symmetry properties of a order parameter inside a multiband superconductors was phase-sensitive techniques. this review summarizes a results of theoretical and experimental studies of a proximity and josephson effects inside systems based on multiband superconductors inside contact with normal metals, insulators and other superconductors.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
19212,"inside this paper we study a problem of photoacoustic inversion inside the weakly attenuating medium. we present explicit reconstruction formulas inside such media and show that a inversion based on such formulas was moderately ill--posed. moreover, we present the numerical algorithm considering imaging and demonstrate inside numerical experiments a feasibility of this approach.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5640,"a pressure-induced reemergence of a second high-tc superconducting phase (sc-ii) inside a alkali-metal intercalated axfe2-yse2 (a = k, rb, cs, tl) remains an enigma and proper characterizations on a superconducting- and normal-state properties of a sc-ii phase were hampered by a intrinsic inhomogeneity and phase separation. to elucidate this intriguing problem, we performed the detailed high-pressure magnetotransport study on a recently discovered (li1-xfex)ohfe1-yse single crystals, which have high tc~40 k and share similar fermi surface topology as axfe2-yse2, but are free from a sample complications. we found that a ambient-pressure tc~41 k was suppressed gradually to below 2 k upon increasing pressure to pc ~5 gpa, above which the sc-ii phase with higher tc emerges and a tc increases progressively to above 50 k up to 12.5 gpa. interestingly, our high-precision resistivity data enable us to uncover a sharp transition of a normal state from the fermi liquid considering sc-i phase (0 < p < 5 gpa) to the non-fermi-liquid considering sc-ii phase (p > 5gpa). inside addition, a reemergence of high-tc sc-ii phase was found to accompany with the concurrent enhancement of electron carrier density. since high-pressure structural study based on a synchrotron x-ray diffraction rules out a structural transition below 10 gpa, a observed sc-ii phase with enhanced carrier density should be ascribed to an electronic origin associated with the pressure-induced fermi surface reconstruction.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
6683,"should textual data be compressed intelligently without losing accuracy inside evaluating sentiment? inside this study, we propose the novel evolutionary compression algorithm, parsec (parts-of-speech considering sentiment compression), which makes use of parts-of-speech tags to compress text inside the way that sacrifices minimal classification accuracy when used inside conjunction with sentiment analysis algorithms. an analysis of parsec with eight commercial and non-commercial sentiment analysis algorithms on twelve english sentiment data sets reveals that accurate compression was possible with (0%, 1.3%, 3.3%) loss inside sentiment classification accuracy considering (20%, 50%, 75%) data compression with parsec with the help of lingpipe, a most accurate of a sentiment algorithms. other sentiment analysis algorithms are more severely affected by compression. we conclude that significant compression of text data was possible considering sentiment analysis depending on a accuracy demands of a specific application and a specific sentiment analysis algorithm used.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14617,a dft calculations were performed of densities of states of semiconductor gaas and magnetic semiconductor ga0.9375mn0.0625as. it was obtained that the part of mn3d- states was hybridized with valence band at fermi level. a exchange integrals of anderson impurity model were calculated making use of atomic hartree-fock package and angular momentum coupling technique. theoretical tc of ga0.9375mn0.0625as obtained inside a multiscale ab initio method was inside reasonable agreement with experiment. a application of hubbard parameters inside dft calculations with the help of ultrasoft pseudopotentials was discussed.,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
3559,"community discovery inside a social network was one of a tremendously expanding areas which earn interest among researchers considering a past one decade. there are many already existing algorithms. however, new seed-based algorithms establish an emerging drift inside this area. a basic idea behind these strategies was to identify exceptional nodes inside a given network, called seeds, around which communities should be located. this paper proposes the blended strategy considering locating suitable superior seed set by applying various centrality measures and with the help of them to find overlapping communities. a examination of a algorithm has been performed regarding a goodness of a identified communities with a aid of intra-cluster density and inter-cluster density. finally, a runtime of a proposed algorithm has been compared with a existing community detection algorithms showing remarkable improvement.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
8822,"we demonstrate that spin supercurrents are conserved upon transmission through the conventional superconductor, even inside a presence of spin-dependent scattering by impurities with magnetic moments or spin-orbit coupling. this was fundamentally different from conventional spin currents, which decay inside a presence of such scattering, and this has important implications considering a usage of superconducting materials inside spintronic hybrid structures.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
812,"understanding a singular value spectrum of the matrix $a \in \mathbb{r}^{n \times n}$ was the fundamental task inside countless applications. inside matrix multiplication time, it was possible to perform the full svd and directly compute a singular values $\sigma_1,...,\sigma_n$. however, little was known about algorithms that break this runtime barrier. with the help of tools from stochastic trace estimation, polynomial approximation, and fast system solvers, we show how to efficiently isolate different ranges of $a$'s spectrum and approximate a number of singular values inside these ranges. we thus effectively compute the histogram of a spectrum, which should stand inside considering a true singular values inside many applications. we use this primitive to give a first algorithms considering approximating the wide class of symmetric matrix norms inside faster than matrix multiplication time. considering example, we give the $(1 + \epsilon)$ approximation algorithm considering a schatten-$1$ norm (the nuclear norm) running inside just $\tilde o((nnz(a)n^{1/3} + n^2)\epsilon^{-3})$ time considering $a$ with uniform row sparsity or $\tilde o(n^{2.18} \epsilon^{-3})$ time considering dense matrices. a runtime scales smoothly considering general schatten-$p$ norms, notably becoming $\tilde o (p \cdot nnz(a) \epsilon^{-3})$ considering any $p \ge 2$. at a same time, we show that a complexity of spectrum approximation was inherently tied to fast matrix multiplication inside a small $\epsilon$ regime. we prove that achieving milder $\epsilon$ dependencies inside our algorithms would imply faster than matrix multiplication time triangle detection considering general graphs. this further implies that highly accurate algorithms running inside subcubic time yield subcubic time matrix multiplication. as an application of our bounds, we show that precisely computing all effective resistances inside the graph inside less than matrix multiplication time was likely difficult, barring the major algorithmic breakthrough.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4663,"the simple and efficient algorithm to numerically compute a genus of surfaces of three-dimensional objects with the help of a euler characteristic formula was presented. a algorithm applies to objects obtained by thresholding the scalar field inside the structured-collocated grid, and does not require any triangulation of a data. this makes a algorithm fast, memory-efficient and suitable considering large datasets. applications to a characterization of complex surfaces inside turbulent flows are presented to illustrate a method.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10787,"shan-chen model was the numerical scheme to simulate multiphase fluid flows with the help of lattice boltzmann approach. a original shan-chen model suffers from inability to accurately predict behavior of air bubbles interacting inside the non-aqueous fluid. inside a present study, we extended a shan-chen model to take a effect of a attraction-repulsion barriers among bubbles inside to account. a proposed model corrects a interaction and coalescence criterion of a original shan-chen scheme inside order to have the more accurate simulation of bubbles morphology inside the metal foam. a model was based on forming the thin film (narrow channel) between merging bubbles during growth. rupturing of a film occurs when an oscillation inside velocity and pressure arises in a channel followed by merging of a bubbles. comparing numerical results obtained from proposed model with mettallorgraphy images considering aluminum a356 demonstrated the good consistency inside mean bubble size and bubbles distribution",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10990,"inside this article, we consider a sparse tensor singular value decomposition, which aims considering dimension reduction on high-dimensional high-order data with certain sparsity structure. the method named sparse tensor alternating thresholding considering singular value decomposition (stat-svd) was proposed. a proposed procedure features the novel double projection \& thresholding scheme, which provides the sharp criterion considering thresholding inside each iteration. compared with regular tensor svd model, stat-svd permits more robust approximation under weaker assumptions. both a upper and lower bounds considering approximation accuracy are developed. a proposed procedure was shown to be minimax rate-optimal inside the general class of situations. simulation studies show that stat-svd performs well under the variety of configurations. we also illustrate a merits of a proposed procedure on the longitudinal tensor dataset on european country mortality rates.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8975,"we present linear-time algorithms considering partitioning the path or the tree with weights on a vertices by removing $k$ edges to maximize a minimum-weight component. we also use a same framework to partition the path with weight on a vertices, removing $k$ edges to minimize a maximum-weight component. a algorithms use a parametric search paradigm, testing candidate values until an optimum was found while simultaneously reducing a running time needed considering each test. considering path-partitioning, a algorithm employs the synthetic weighting scheme that results inside the constant fraction reduction inside running time after each test. considering tree-partitioning, our dual-pronged strategy makes progress no matter what a specific structure of our tree is.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2442,"inside a past years several authors studied a abundance of satellites around galaxies inside order to better approximate a halo masses of host galaxies. to investigate this connection, we analyze galaxies with $m_\mathrm{star}\geq\,10^{10}\,m_{\odot}$ from a hydrodynamical cosmological simulation magneticum. we find that a satellite fraction of centrals was independent of their morphology. with a exception of very massive galaxies at low redshift, our results do not support a assumption that a dark matter (dm) haloes of spheroidal galaxies are significantly more massive than those of disc galaxies at fixed $m_\mathrm{star}$. we show that a density-morphology-relation starts to build up at $z\sim2$ and was independent of a star-formation properties of central galaxies. we conclude that environmental quenching was more important considering satellites than considering centrals. our simulations indicate that conformity was already inside place at $z=2$, where formation redshift and current star-formation rate (sfr) of central and satellite galaxies correlate. centrals with low sfrs have formed earlier (at fixed $m_\mathrm{star}$) while centrals with high sfr formed later, with typical formation redshifts well inside agreement with observations. however, we confirm a recent observations that a apparent number of satellites of spheroidal galaxies was significantly larger than considering disc galaxies. this difference completely originates from a inclusion of companion galaxies, i.e. galaxies that do not sit inside a potential minimum of the dm halo. thus, due to a density-morphological-relation a number of satellites was not the good tracer considering a halo mass, unless samples are restricted to a central galaxies of dm haloes.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11470,"a monitoring of large dynamic networks was the major chal- lenge considering the wide range of application. a complexity stems from properties of a underlying graphs, inside which slight local changes should lead to sizable variations of global prop- erties, e.g., under certain conditions, the single link cut that may be overlooked during monitoring should result inside splitting a graph into two disconnected components. moreover, it was often difficult to determine whether the change will propagate globally or remain local. traditional graph theory measure such as a centrality or a assortativity of a graph are not satisfying to characterize global properties of a graph. inside this paper, we tackle a problem of real-time monitoring of dynamic large scale graphs by developing the geometric idea behind the method that leverages notions of geometric curvature and recent development inside graph embeddings with the help of ollivier-ricci curvature [47]. we illustrate a use of our method by consid- ering a practical case of monitoring dynamic variations of global internet with the help of topology changes information provided by combining several bgp feeds. inside particular, we use our method to detect major events and changes using a geometry of a embedding of a graph.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0
6394,"a problem presented inside this paper was the generalization of a usual coupled-tasks scheduling problem inside presence of compatibility constraints. a reason behind this study was a data acquisition problem considering the submarine torpedo. we investigate the particular configuration considering coupled tasks (any task was divided into two sub-tasks separated by an idle time), inside which a idle time of the coupled task was equal to a sum of durations of its two sub-tasks. we prove -completeness of a minimization of a schedule length, we show that finding the solution to our problem amounts to solving the graph problem, which inside itself was close to a minimum-disjoint-path cover (min-dcp) problem. we design the (3a+2b)/(2a+2b)-approximation, where the and b (the processing time of a two sub-tasks) are two input data such as a>b>0, and that leads to the ratio between 3/2 and 5/4. with the help of the polynomial-time algorithm developed considering some class of graph of min-dcp, we show that a ratio decreases to 1.37 .",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13500,"modern social media platforms facilitate a rapid spread of information online. modelling phenomena such as social contagion and information diffusion are contingent upon the detailed understanding of a information-sharing processes. inside twitter, an important aspect of this occurs with retweets, where users rebroadcast a tweets of other users. to improve our understanding of how these distributions arise, we analyse a distribution of retweet times. we show that the power law with exponential cutoff provides the better fit than a power laws previously suggested. we explain this fit through a burstiness of human behaviour and a priorities individuals place on different tasks.",1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
2210,"suppose that considering each n >= 0 we have the representation $m_n$ of a symmetric group s_n. such sequences arise inside the wide variety of contexts, and often exhibit uniformity inside some way. we prove the number of general results along these lines inside this paper: our prototypical theorem states that if $m_n$ should be given the suitable module structure over the twisted commutative algebra then a sequence $m_n$ follows the predictable pattern. we phrase these results precisely inside a language of hilbert series (or poincar√© series, or formal characters) of modules over tca's.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
4956,"this study compares various superlearner and deep learning architectures (machine-learning-based and neural-network-based) considering classification problems across several simulated and industrial datasets to assess performance and computational efficiency, as both methods have nice theoretical convergence properties. superlearner formulations outperform other methods at small to moderate sample sizes (500-2500) on nonlinear and mixed linear/nonlinear predictor relationship datasets, while deep neural networks perform well on linear predictor relationship datasets of all sizes. this suggests faster convergence of a superlearner compared to deep neural network architectures on many messy classification problems considering real-world data. superlearners also yield interpretable models, allowing users to examine important signals inside a data; inside addition, they offer flexible formulation, where users should retain good performance with low-computational-cost base algorithms. k-nearest-neighbor (knn) regression demonstrates improvements with the help of a superlearner framework, as well; knn superlearners consistently outperform deep architectures and knn regression, suggesting that superlearners may be better able to capture local and global geometric features through utilizing the variety of algorithms to probe a data space.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18418,"we prove a global stability of a minkowski space viewed as a trivial solution of a einstein-vlasov system. to approximate a vlasov field, we use a vector field and modified vector field techniques developed inside [fjs15; fjs17]. inside particular, a initial support inside a velocity variable does not need to be compact. to control a effect of a large velocities, we identify and exploit several structural properties of a vlasov equation to prove that a worst non-linear terms inside a vlasov equation either enjoy the form of a null condition or should be controlled with the help of a wave coordinate gauge. a basic propagation estimates considering a vlasov field are then obtained with the help of only weak interior decay considering a metric components. since some of a error terms are not time-integrable, several hierarchies inside a commuted equations are exploited to close a top order estimates. considering a einstein equations, we use wave coordinates and a main new difficulty arises from a commutation of a energy-momentum tensor, which needs to be rewritten with the help of a modified vector fields.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13651,"monotonicity formulae play the crucial role considering many geometric pdes, especially considering their regularity theories. considering minimal submanifolds inside the euclidean ball, a classical monotonicity formula implies that if such the submanifold passes through a centre of a ball, then its area was at least that of a equatorial disk. recently brendle and hung proved the sharp area bound considering minimal submanifolds when a prescribed point was not a centre of a ball, which resolved the conjecture of alexander, hoffman and osserman. their proof involves asymptotic analysis of an ingeniously chosen vector field, and a divergence theorem. inside this article we prove the sharp `moving-centre' monotonicity formula considering minimal submanifolds, which implies a aforementioned area bound. we also describe similar moving-centre monotonicity formulae considering stationary $p$-harmonic maps, mean curvature flow and a harmonic map heat flow.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3194,"we consider a problem of minimizing the convex function over the convex set given access only to an evaluation oracle considering a function and the membership oracle considering a set. we give the simple algorithm which solves this problem with $\tilde{o}(n^2)$ oracle calls and $\tilde{o}(n^3)$ additional arithmetic operations. with the help of this result, we obtain more efficient reductions among a five basic oracles considering convex sets and functions defined by gr√∂tschel, lovasz and schrijver.",1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
8831,this short paper reports a method and a evaluation results of casio and shinshu university joint team considering a isbi challenge 2017 - skin lesion analysis towards melanoma detection - part 3: lesion classification hosted by isic. our online validation score is 0.958 with melanoma classifier auc 0.924 and seborrheic keratosis classifier auc 0.993.,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
950,"we obtain the probabilistic proof of a local lipschitz continuity considering a optimal stopping boundary of the class of problems with state space $[0,t]\times\mathbb{r}^d$, $d\ge 1$. to a best of our knowledge this was a only existing proof that relies exclusively upon stochastic calculus, all a other proofs making use of pde techniques and integral equations. thanks to our idea behind the method we obtain our result considering the class of diffusions whose associated second order differential operator was not necessarily uniformly elliptic. a latter condition was normally assumed inside a related pde literature.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
17385,"holes and clumps inside a interstellar gas of dwarf irregular galaxies are gravitational scattering centers that heat field stars and change their radial and vertical distributions. because a gas structures are extended and each stellar scattering was relatively weak, a stellar orbits remain nearly circular and a net effect accumulates slowly over time. we calculate a radial profile of scattered stars with an idealized model and find that it approaches an equilibrium shape that was exponential, similar to a observed shapes of galaxy discs. our models treat only scattering and have no bars or spiral arms, so a results apply mostly to dwarf irregular galaxies where there are no other obvious scattering processes. stellar scattering by gaseous perturbations slows down when a stellar population gets thicker than a gas layer. an accreting galaxy with the growing thin gas layer should form multiple stellar exponential profiles from a inside-out, preserving a remnants of each gyr interval inside the sequence of ever-lengthening and thinning stellar subdiscs.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6534,"we present the pipeline that allows recovering reliable information considering all four stokes parameters with high accuracy. its novelty relies on a treatment of a instrumental effects already prior to a computation of a stokes parameters contrary to conventional methods, such as a m√ºller matrix one. a instrumental linear polarization was corrected across a whole telescope beam and significant stokes $q$ and $u$ should be recovered even when a recorded signals are severely corrupted. a accuracy we reach inside terms of polarization degree was of a order of 0.1-0.2 %. a polarization angles are determined with an accuracy of almost 1$^{\circ}$. a presented methodology is applied to recover a linear and circular polarization of around 150 active galactic nuclei. a sources were monitored from july 2010 to april 2016 with a effelsberg 100-m telescope at 4.85 ghz and 8.35 ghz with the cadence of around 1.2 months. a polarized emission of a moon is used to calibrate a polarization angle. our analysis showed the small system-induced rotation of about 1$^{\circ}$ at both observing frequencies. finally, we identify five sources with significant and stable linear polarization; three sources remain constantly linearly unpolarized over a period we examined; the total of 11 sources have stable circular polarization degree $m_\mathrm{c}$ and four of them with non-zero $m_\mathrm{c}$. we also identify eight sources that maintain the stable polarization angle over a examined period. all this was provided to a community considering polarization observations reference. we finally show that our analysis method was conceptually different from a traditionally used ones and performs better than a m√ºller matrix method. although it is developed considering the system equipped with circularly polarized feeds it should easily be modified considering systems with linearly polarized feeds as well.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
6551,"stochastic gradient methods are a workhorse (algorithms) of large-scale optimization problems inside machine learning, signal processing, and other computational sciences and engineering. this paper studies markov chain gradient descent, the variant of stochastic gradient descent where a random samples are taken on a trajectory of the markov chain. existing results of this method assume convex objectives and the reversible markov chain and thus have their limitations. we establish new non-ergodic convergence under wider step sizes, considering nonconvex problems, and considering non-reversible finite-state markov chains. nonconvexity makes our method applicable to broader problem classes. non-reversible finite-state markov chains, on a other hand, should mix substatially faster. to obtain these results, we introduce the new technique that varies a mixing levels of a markov chains. a reported numerical results validate our contributions.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16652,"deep learning methods, and inside particular convolutional neural networks (cnns), have led to an enormous breakthrough inside the wide range of computer vision tasks, primarily by with the help of large-scale annotated datasets. however, obtaining such datasets inside a medical domain remains the challenge. inside this paper, we present methods considering generating synthetic medical images with the help of recently presented deep learning generative adversarial networks (gans). furthermore, we show that generated medical images should be used considering synthetic data augmentation, and improve a performance of cnn considering medical image classification. our novel method was demonstrated on the limited dataset of computed tomography (ct) images of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). we first exploit gan architectures considering synthesizing high quality liver lesion rois. then we present the novel scheme considering liver lesion classification with the help of cnn. finally, we train a cnn with the help of classic data augmentation and our synthetic data augmentation and compare performance. inside addition, we explore a quality of our synthesized examples with the help of visualization and expert assessment. a classification performance with the help of only classic data augmentation yielded 78.6% sensitivity and 88.4% specificity. by adding a synthetic data augmentation a results increased to 85.7% sensitivity and 92.4% specificity. we believe that this idea behind the method to synthetic data augmentation should generalize to other medical classification applications and thus support radiologists' efforts to improve diagnosis.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
10922,"inside a gastrointestinal (gi) tract endoscopy field, ingestible wireless capsule endoscopy was considered as the minimally invasive novel diagnostic technology to inspect a entire gi tract and to diagnose various diseases and pathologies. since a development of this technology, medical device companies and many groups have made significant progress to turn such passive capsule endoscopes into robotic active capsule endoscopes to achieve almost all functions of current active flexible endoscopes. however, a use of robotic capsule endoscopy still has some challenges. one such challenge was a precise localization of such active devices inside 3d world, which was essential considering the precise three-dimensional (3d) mapping of a inner organ. the reliable 3d map of a explored inner organ could assist a doctors to make more intuitive and correct diagnosis. inside this paper, we propose to our knowledge considering a first time inside literature the visual simultaneous localization and mapping (slam) method specifically developed considering endoscopic capsule robots. a proposed rgb-depth slam method was capable of capturing comprehensive dense globally consistent surfel-based maps of a inner organs explored by an endoscopic capsule robot inside real time. this was achieved by with the help of dense frame-to-model camera tracking and windowed surfelbased fusion coupled with frequent model refinement through non-rigid surface deformations.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15736,"inside this paper, we describe the phenomenon, which we named ""super-convergence"", where neural networks should be trained an order of magnitude faster than with standard training methods. a existence of super-convergence was relevant to understanding why deep networks generalize well. one of a key elements of super-convergence was training with one learning rate cycle and the large maximum learning rate. the primary insight that allows super-convergence training was that large learning rates regularize a training, thus requiring the reduction of all other forms of regularization inside order to preserve an optimal regularization balance. we also derive the simplification of a hessian free optimization method to compute an approximate of a optimal learning rate. experiments demonstrate super-convergence considering cifar-10/100, mnist and imagenet datasets, and resnet, wide-resnet, densenet, and inception architectures. inside addition, we show that super-convergence provides the greater boost inside performance relative to standard training when a amount of labeled training data was limited. a architectures and code to replicate a figures inside this paper are available at github.com/lnsmith54/super-convergence. see this http url considering an application of super-convergence to win a dawnbench challenge (see this https url).",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8959,"we consider scheduling on identical and unrelated parallel machines with job assignment restrictions. these problems are np-hard and they do not admit polynomial time approximation algorithms with approximation ratios smaller than $1.5$ unless p$=$np. however, if we impose limitations on a set of machines that should process the job, a problem sometimes becomes easier inside a sense that algorithms with approximation ratios better than $1.5$ exist. we introduce three graphs, based on a assignment restrictions and study a computational complexity of a scheduling problem with respect to structural properties of these graphs, inside particular their tree- and rankwidth. we identify cases that admit polynomial time approximation schemes or fpt algorithms, generalizing and extending previous results inside this area.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7755,"ashtiani et al. (nips 2016) introduced the semi-supervised framework considering clustering (ssac) where the learner was allowed to make same-cluster queries. more specifically, inside their model, there was the query oracle that answers queries of a form given any two vertices, do they belong to a same optimal cluster?. ashtiani et al. showed a usefulness of such the query framework by giving the polynomial time algorithm considering a k-means clustering problem where a input dataset satisfies some separation condition. ailon et al. extended a above work to a approximation setting by giving an efficient (1+\eps)-approximation algorithm considering k-means considering any small \eps > 0 and any dataset within a ssac framework. inside this work, we extend this line of study to a correlation clustering problem. correlation clustering was the graph clustering problem where pairwise similarity (or dissimilarity) information was given considering every pair of vertices and a objective was to partition a vertices into clusters that minimise a disagreement (or maximises agreement) with a pairwise information given as input. these problems are popularly known as mindisagree and maxagree problems, and mindisagree[k] and maxagree[k] are versions of these problems where a number of optimal clusters was at most k. there exist polynomial time approximation schemes (ptas) considering mindisagree[k] and maxagree[k] where a approximation guarantee was (1+\eps) considering any small \eps and a running time was polynomial inside a input parameters but exponential inside k and 1/\eps. we obtain an (1+\eps)-approximation algorithm considering any small \eps with running time that was polynomial inside a input parameters and also inside k and 1/\eps. we also give non-trivial upper and lower bounds on a number of same-cluster queries, a lower bound being based on a exponential time hypothesis (eth).",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8901,"we propose the novel procedure considering outlier detection inside functional data, inside the semi-supervised framework. as a data was functional, we consider a coefficients obtained after projecting a observations onto orthonormal bases (wavelet, pca). the multiple testing procedure based on a two-sample test was defined inside order to highlight a levels of a coefficients on which a outliers appear as significantly different to a normal data. a selected coefficients are then called features considering a outlier detection, on which we compute a local outlier factor to highlight a outliers. this procedure to select a features was applied on simulated data that mimic a behaviour of space telemetries, and compared with existing dimension reduction techniques.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0
1804,"as the result of a rapid changes inside information and communication technology (ict), a world has become the small village where people from all over a world connect with each other inside dialogue and communication using a internet. also, communications have become the daily routine activity due to a new globalization where companies and even universities become global residing cross countries borders. as the result, translation becomes the needed activity inside this connected world. ict made it possible to have the student inside one country take the course or even the degree from the different country anytime anywhere easily. a resulted communication still needs the language as the means that helps a receiver understands a contents of a sent message. people need an automated translation application because human translators are hard to find all a times, and a human translations are very expensive comparing to a translations automated process. several types of research describe a electronic process of a machine-translation. inside this paper, a authors are going to study some of these previous researches, and they will explore some of a needed tools considering a machine-translation. this research was going to contribute to a machine-translation area by helping future researchers to have the summary considering a machine-translation groups of research and to let lights on a importance of a translation mechanism.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18988,"this paper considers a channel approximation (ce) and multi-user detection (mud) problems inside cloud radio access network (c-ran). assuming that active users are sparse inside a network, we solve ce and mud problems with compressed sensing (cs) technology to greatly reduce a long identification pilot overhead. the mixed l{2,1}-regularization functional considering extended sparse group-sparsity recovery was proposed to exploit a inherently sparse property existing both inside user activities and remote radio heads (rrhs) that active users are attached to. empirical and theoretical guidelines are provided to aid choosing tuning parameters which have critical effect on a performance of a penalty functional. to speed up a processing procedure, based on alternating direction method of multipliers and variable splitting strategy, an efficient algorithm was formulated which was guaranteed to be convergent. numerical results are provided to illustrate a effectiveness of a proposed functional and efficient algorithm.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
12870,"camera viewpoint selection was an important aspect of visual grasp detection, especially inside clutter where many occlusions are present. where other approaches use the static camera position or fixed data collection routines, our multi-view picking (mvp) controller uses an active perception idea behind the method to choose informative viewpoints based directly on the distribution of grasp pose estimates inside real time, reducing uncertainty inside a grasp poses caused by clutter and occlusions. inside trials of grasping 20 objects from clutter, our mvp controller achieves 80% grasp success, outperforming the single-viewpoint grasp detector by 12%. we also show that our idea behind the method was both more accurate and more efficient than approaches which consider multiple fixed viewpoints.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
15424,"deep neural networks represent a state of a art inside machine learning inside the growing number of fields, including vision, speech and natural language processing. however, recent work raises important questions about a robustness of such architectures, by showing that it was possible to induce classification errors through tiny, almost imperceptible, perturbations. vulnerability to such ""adversarial attacks"", or ""adversarial examples"", has been conjectured to be due to a excessive linearity of deep networks. inside this paper, we study this phenomenon inside a setting of the linear classifier, and show that it was possible to exploit sparsity inside natural data to combat $\ell_{\infty}$-bounded adversarial perturbations. specifically, we demonstrate a efficacy of the sparsifying front end using an ensemble averaged analysis, and experimental results considering a mnist handwritten digit database. to a best of our knowledge, this was a first work to show that sparsity provides the theoretically rigorous framework considering defense against adversarial attacks.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15744,"this paper presents the new way to increase interconnectivity inside small wikipedias (fewer than the 100,000 articles), by automatically linking articles based on interlanguage links. many small wikipedias have many articles with very few links, this was mainly due to a short article length. this makes it difficult to navigate between a articles. inside many cases a article does exist considering the small wikipedia, however a article was just missing the link. due to a fact that wikipedias are translated inside to many languages, it allows us to generate new links considering small wikipedias with the help of a links from the large wikipedia (more than the 100,000 articles).",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
4128,"we present a first combination of thermal sunyaev-zel'dovich (tsz) map with the multi-frequency quality assessment of a sky pixels based on artificial neural networks (ann) aiming at detecting tsz sources from sub-millimeter observations of a sky by planck. we construct an adapted full-sky ann assessment on a fullsky and we present a construction of a resulting filtered and cleaned tsz map, milcann. we show that this combination allows to significantly reduce a noise fluctuations and foreground residuals compared to standard tsz maps. from a milcann map, we constructed a had tsz source catalog that consists of 3969 sources with the purity of 90\%. finally, we compare this catalog with ancillary catalogs and show that a galaxy-cluster candidates inside a had catalog are essentially low-mass (down to $m_{500} = 10^{14}$ m$_\odot$) high-redshift (up to $z \leq 1$) galaxy cluster candidates.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12066,"a increase inside network connectivity has also resulted inside several high-profile attacks on cyber-physical systems. an attacker that manages to access the local network could remotely affect control performance by tampering with sensor measurements delivered to a controller. recent results have shown that with network-based attacks, such as man-in-the-middle attacks, a attacker should introduce an unbounded state approximation error if measurements from the suitable subset of sensors contain false data when delivered to a controller. while these attacks should be addressed with a standard cryptographic tools that ensure data integrity, their continuous use would introduce significant communication and computation overhead. consequently, we study effects of intermittent data integrity guarantees on system performance under stealthy attacks. we consider linear estimators equipped with the general type of residual-based intrusion detectors (including $\chi^2$ and sprt detectors), and show that even when integrity of sensor measurements was enforced only intermittently, a attack impact was significantly limited; specifically, a state approximation error was bounded or a attacker cannot remain stealthy. furthermore, we present methods to: (1) evaluate a effects of any given integrity enforcement policy inside terms of reachable state-estimation errors considering any type of stealthy attacks, and (2) design an enforcement policy that provides a desired approximation error guarantees under attack. finally, on three automotive case studies we show that even with less than 10% of authenticated messages we should ensure satisfiable control performance inside a presence of attacks.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
3331,"the schur decomposition of a velocity gradient tensor (vgt) considering homogeneous, isotropic turbulence (hit) was undertaken and its physical consequences examined. this decomposition permits a normal parts of a tensor (represented by a eigenvalues) to be separated explicitly from a non-normal effects. given a restricted {e}uler approximation to a vgt dynamics was written inside terms of a isotropic part of a pressure hessian and a invariants of a characteristic equation of a vgt (in turn expressed inside terms of a eigenvalues), a non-normal terms are related to a non-local aspects of a dynamics and a anisotropic part of a pressure hessian. with the help of the direct numerical simulation of hit, we show that a norm of a non-normal part of a tensor was of the similar order to a normal part, highlighting a importance of non-local effects. inside fact, beneath a discriminant function inside the q-r plot, all enstrophy arises from a non-normal term, meaning that vorticity and intermediate strain eigenvector alignment inside this region was an immediate consequence of non-normality. the non-normal term appears inside a expressions considering both enstrophy and total strain and cancels when calculating a second invariant of a vgt, while a self-amplification of non-normality and a normal straining of non-normality appear inside a strain production and enstrophy production equations and cancel when calculating a third invariant. however, these terms are significant considering understanding a full vgt dynamics, explaining how flow structures evolve to the disc-like state despite a strain eigenvalues sometimes indicating opposite (rod-like) behaviour, as well as explaining vorticity and strain alignments inside hit.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8066,"we investigate a optical properties inside the minimal model of spin-orbit coupled systems on honeycomb lattice at half-filling. a absorption of a circularly polarized light inside a charge and antiferromagnetic ordered states was particularly studied, and a spin-valley selective excitation depending on a handedness of a light was found. it was shown that a optical selection rules plainly differ with a type of broken symmetries and topological properties of a electronic states. consequently, a abrupt change of spin-valley selectivity inside a optical absorption occurs at a topological transition caused by changing a magnitude of a order parameters.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
14589,"inside this paper, we address a basic problem of recognizing moving objects inside video images with the help of sp theory of intelligence. a concept of sp theory of intelligence which was the framework of artificial intelligence, is first introduced by gerard j wolff, where s stands considering simplicity and p stands considering power. with the help of a concept of multiple alignment, we detect and recognize object of our interest inside video frames with multilevel hierarchical parts and subparts, based on polythetic categories. we track a recognized objects with the help of a species based particle swarm optimization (pso). first, we extract a multiple alignment of our object of interest from training images. inside order to recognize accurately and handle occlusion, we use a polythetic concepts on raw data line to omit a redundant noise using searching considering best alignment representing a features from a extracted alignments. we recognize a domain of interest from a video scenes inside form of wide variety of multiple alignments to handle scene variability. unsupervised learning was done inside a sp model following a donsvic principle and natural structures are discovered using information compression and pattern analysis. after successful recognition of objects, we use species based pso algorithm as a alignments of our object of interest was analogues to observation likelihood and fitness ability of species. subsequently, we analyze a competition and repulsion among species with annealed gaussian based pso. we have tested our algorithms on david, walking2, faceocc1, jogging and dudek, obtaining very satisfactory and competitive results.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5654,"a next leap on a internet has already started as semantic web. at its core, semantic web transforms a document oriented web to the data oriented web enriched with semantics embedded as metadata. this change inside perspective towards a web offers numerous benefits considering vast amount of data intensive industries that are bound to a web and its related applications. a industries are diverse as they range from oil & gas exploration to a investigative journalism, and everything inside between. this paper discusses eight different industries which currently reap a benefits of semantic web. a paper also offers the future outlook into semantic web applications and discusses a areas inside which semantic web would play the key role inside a future.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16787,inside this paper we prove bernstein type theorems considering the class of stationary points of a alt-caffarelli functional inside $\mathbb r^2$ and $\mathbb r^3$.,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15651,"this paper investigates a impact of link formation between the pair of agents on resource availability of other agents inside the social cloud network, which was the special case of socially-based resource sharing systems. specifically, we study a correlation between externalities, network size, and network density. we first conjecture and experimentally support that if an agent experiences positive externalities, then its closeness (harmonic centrality measure) should increase. next, we show a following considering ring networks: inside less populated networks no agent experiences positive externalities; inside more populated networks the set of agents experience positive externalities, and larger a distance between agents forming the link, more a number of beneficiaries; and a number of beneficiaries was always less than a number of non-beneficiaries. finally, we show that network density was inversely proportional to positive externalities, and further, it plays the crucial role inside determining a kind of externalities.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
18850,"a influence of long-range spin and charge fluctuations on spectra of a two-dimensional fermionic hubbard model was considered with the help of a strong coupling diagram technique. infinite sequences of diagrams containing ladder inserts, which describe a interaction of electrons with these fluctuations, are summed, and obtained equations are self-consistently solved considering a range of hubbard repulsions $4t\leq u\leq 8t$ and temperatures $0.3t\lesssim t\lesssim t$ with $t$ a intersite hopping constant. it is found that the metal-insulator transition curve goes from larger $u$ and $t$ to smaller values of these parameters. a temperature decrease causes a transition to a long-range antiferromagnetic order. it was responsible considering a splitting out of the narrow band from the hubbard subband with doping considering $u=8t$ and low $t$. this segregated band was located near a fermi level and forms the pseudogap here.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
1070,"we show that, considering any cluster-tilted algebra of finite representation type over an algebraically closed field, a following three definitions of the maximal green sequence are equivalent: (1) a usual definition inside terms of fomin-zelevinsky mutation of a extended exchange matrix, (2) the forward hom-orthogonal sequence of schurian modules, (3) a sequence of wall crossings of the generic green path. together with [24], this completes a foundational work needed to support a author's work with p.j. apruzzese [1], namely, to determine all lengths of all maximal green sequences considering all quivers whose underlying graph was an oriented or unoriented cycle and to determine which are ""linear"". inside an appendix, written jointly with g. todorov, we give the conjectural description of maximal green sequences of maximum length considering any cluster-tilted algebra of finite representation type.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
125,"the chain of cofacial molecules with cn or cnh symmetry supports excitonic states with the screw-like structure. these should be quantified with a combination of an axial wavenumber and an azimuthal winding number. combinations of these states should be used to construct excitonic wave packets that spiral down a chain with well-determined linear and angular momenta. these twisted exciton wave packets should be created and annihilated with the help of laser pulses, and their angular momentum should be optically modified during transit. this allows considering a creation of opto-excitonic circuits inside which information, encoded inside a angular momentum of light, was converted into excitonic wave packets that should be manipulated, transported, and then re-emitted. the tight-binding paradigm was used to demonstrate a key ideas. a idea behind the method was then extended to quantify a evolution of twisted exciton wave packets inside the many-body, multi-level time-domain density functional theory setting. inside both settings, numerical methods are developed that allow a site-to-site transfer of angular momentum to be quantified.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
18293,this was an informal paper presenting historical results around a recent paper of a author about lang's conjecture and torsion of elliptic curves. this paper also discusses the few aspects of a proof.,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
563,"this paper presents the new safety specification method that was robust against errors inside a probability distribution of disturbances. our proposed distributionally robust safe policy maximizes a probability of the system remaining inside the desired set considering all times, subject to a worst possible disturbance distribution inside an ambiguity set. we propose the dynamic game formulation of constructing such policies and identify conditions under which the non-randomized markov policy was optimal. based on this existence result, we develop the practical design idea behind the method to safety-oriented stochastic controllers with limited information about disturbance distributions. this control method should be used to minimize another cost function while ensuring safety inside the probabilistic way. however, an associated bellman equation involves infinite-dimensional minimax optimization problems since a disturbance distribution may have the continuous density. to resolve computational issues, we propose the duality-based reformulation method that converts a infinite-dimensional minimax problem into the semi-infinite program that should be solved with the help of existing convergent algorithms. we prove that there was no duality gap, and that this idea behind the method thus preserves optimality. a results of numerical tests confirm that a proposed method was robust against distributional errors inside disturbances, while the standard stochastic safety specification tool was not.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
12657,"the spin fractionalizes into matter and gauge fermions inside kitaev's spin liquid on a honeycomb lattice. this follows from the jordan-wigner mapping to fermions, allowing considering a construction of minimal entropy ground state wavefunction on a cylinder. we use this to calculate a entanglement entropy by choosing several distinct partitionings. first, by partitioning an infinite cylinder into two, a $-\ln 2$ topological entanglement entropy was reconfirmed. second, a reduced density matrix of a gauge sector on a full cylinder was obtained after tracing out a matter degrees of freedom. this allows considering evaluating a gauge entanglement hamiltonian, which contains infinitely long range correlations along a symmetry axis of a cylinder. a matter-gauge entanglement entropy was $(n_y-1)\ln 2$ with $n_y$ a circumference of a cylinder. third, a rules considering calculating a gauge sector entanglement of any partition are determined. rather small correctly chosen gauge partitions should still account considering a topological entanglement entropy inside spite of long-range correlations inside a gauge entanglement hamiltonian.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
17966,"we propose stochastic optimization algorithms that should find local minima faster than existing algorithms considering nonconvex optimization problems, by exploiting a third-order smoothness to escape non-degenerate saddle points more efficiently. more specifically, a proposed algorithm only needs $\tilde{o}(\epsilon^{-10/3})$ stochastic gradient evaluations to converge to an approximate local minimum $\mathbf{x}$, which satisfies $\|\nabla f(\mathbf{x})\|_2\leq\epsilon$ and $\lambda_{\min}(\nabla^2 f(\mathbf{x}))\geq -\sqrt{\epsilon}$ inside a general stochastic optimization setting, where $\tilde{o}(\cdot)$ hides logarithm polynomial terms and constants. this improves upon a $\tilde{o}(\epsilon^{-7/2})$ gradient complexity achieved by a state-of-the-art stochastic local minima finding algorithms by the factor of $\tilde{o}(\epsilon^{-1/6})$. considering nonconvex finite-sum optimization, our algorithm also outperforms a best known algorithms inside the certain regime.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7310,"this paper addresses automatic extraction of abbreviations (encompassing acronyms and initialisms) and corresponding long-form expansions from plain unstructured text. we create and are going to release the multilingual resource considering abbreviations and their corresponding expansions, built automatically by exploiting wikipedia redirect and disambiguation pages, that should be used as the benchmark considering evaluation. we address the shortcoming of previous work where only a redirect pages were used, and so every abbreviation had only the single expansion, even though multiple different expansions are possible considering many of a abbreviations. we also develop the principled machine learning based idea behind the method to scoring expansion candidates with the help of different techniques such as indicators of near synonymy, topical relatedness, and surface similarity. we show improved performance over seven languages, including two with the non-latin alphabet, relative to strong baselines.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15062,"we generalize a log gaussian cox process (lgcp) framework to model multiple correlated point data jointly. a observations are treated as realizations of multiple lgcps, whose log intensities are given by linear combinations of latent functions drawn from gaussian process priors. a combination coefficients are also drawn from gaussian processes and should incorporate additional dependencies. we derive closed-form expressions considering a moments of a intensity functions and develop an efficient variational inference algorithm that was orders of magnitude faster than competing deterministic and stochastic approximations of multivariate lgcp, coregionalization models, and multi-task permanental processes. our idea behind the method outperforms these benchmarks inside multiple problems, offering a current state of a art inside modeling multivariate point processes.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15795,"a nasa exoplanet exploration program's sag15 group has solicited, collected, and organized community input on high-level science questions that could be addressed with future direct imaging exoplanet missions and a type and quality of data answering these questions will require. input is solicited through the variety of forums and a report draft is shared with a exoplanet community continuously during a period of a report development (nov 2015 -- may 2017). a report benefitted from a input of over 50 exoplanet scientists and from multiple open-forum discussions at exoplanet and astrobiology meetings. a sag15 team has identified three group of high-level questions, those that focus on a properties of planetary systems (questions a1--a2), those that focus on a properties of individual planets (questions b1--b4), and questions that relate to planetary processes (questions c1--c4). a questions inside categories a, b, and c require different target samples and often different observational approaches. considering each questions we summarize a current body of knowledge, a available and future observational approaches that should directly or indirectly contribute to answering a question, and provide examples and general considerations considering a target sample required.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12722,"understanding patterns of demand was fundamental considering fleet management of bike sharing systems. inside this paper we analyze data from a divvy system of a city of chicago. we show that a demand of bicycles should be modeled as the multivariate temporal point process, with each dimension corresponding to the bike station inside a network. a availability of daily replications of a process allows nonparametric approximation of a intensity functions, even considering stations with low daily counts, and straightforward approximation of pairwise correlations between stations. these correlations are then used considering clustering, revealing different patterns of bike usage.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3182,"with millimeter wave wireless communications, a resulting radiation reflects on most visible objects, creating rich multipath environments, namely inside urban scenarios. a radiation captured by the listening device was thus shaped by a obstacles encountered, which carry latent information regarding their relative positions. inside this paper, the system to convert a received millimeter wave radiation into a device's position was proposed, making use of a aforementioned hidden information. with the help of deep learning techniques and the pre-established codebook of beamforming patterns transmitted by the base station, a simulations show that average approximation errors below 10 meters are achievable inside realistic outdoors scenarios that contain mostly non-line-of-sight positions, paving a way considering new positioning systems.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14828,"networks provide the powerful formalism considering modeling complex systems by with the help of the model of pairwise interactions. but much of a structure within these systems involves interactions that take place among more than two nodes at once; considering example, communication within the group rather than person-to person, collaboration among the team rather than the pair of coauthors, or biological interaction between the set of molecules rather than just two. such higher-order interactions are ubiquitous, but their empirical study has received limited attention, and little was known about possible organizational principles of such structures. here we study a temporal evolution of 19 datasets with explicit accounting considering higher-order interactions. we show that there was the rich variety of structure inside our datasets but datasets from a same system types have consistent patterns of higher-order structure. furthermore, we find that tie strength and edge density are competing positive indicators of higher-order organization, and these trends are consistent across interactions involving differing numbers of nodes. to systematically further a study of theories considering such higher-order structures, we propose higher-order link prediction as the benchmark problem to assess models and algorithms that predict higher-order structure. we find the fundamental differences from traditional pairwise link prediction, with the greater role considering local rather than long-range information inside predicting a appearance of new interactions.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0
1598,"this paper introduces the packet-based dual-rate control strategy to face time-varying network-induced delays, packet dropouts and packet disorder inside the networked control system. slow-rate sensing enables to achieve energy saving by reducing network load. inside addition, choosing the slower sensing period than a longest round-trip time delay should avoid packet disorder. on a other hand, the slow-rate sensing usually degrades control performance inside the conventional control framework. therefore, including dual-rate control techniques should be useful to maintain a desired performance, since a controller was able to generate the fast-rate control signal from the slow-rate sensing signal. the dual-rate pid controller was used, which should be split into two parts: the slow-rate pi controller was located at a remote side (with no permanent communication to a plant) and the fast-rate pd controller, at a local side (close to a plant, sensor, and in a actuator, which should offer the low computation power). inside addition, at a remote side, where the powerful computation device was located, the prediction stage was included inside order to generate a packet of future, estimated slow-rate control actions to be sent to a local side. at this side, these actions are converted to fast-rate ones and used when the packet does not arrive due to a network-induced delay or due to occurring dropouts. a control proposal was able to reach a nominal (no-dropout, no-delay) performance despite a existence of time-varying delays and packet dropouts. using real-time control considering the cartesian robot, results clearly reveal a superiority of a control idea behind the method compared to the previous authors\' proposal, where a time-varying delays are faced by means of the gain scheduling control strategy.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
5724,"analysis of an organization's computer network activity was the key component of early detection and mitigation of insider threat, the growing concern considering many organizations. raw system logs are the prototypical example of streaming data that should quickly scale beyond a cognitive power of the human analyst. as the prospective filter considering a human analyst, we present an online unsupervised deep learning idea behind the method to detect anomalous network activity from system logs inside real time. our models decompose anomaly scores into a contributions of individual user behavior features considering increased interpretability to aid analysts reviewing potential cases of insider threat. with the help of a cert insider threat dataset v6.2 and threat detection recall as our performance metric, our novel deep and recurrent neural network models outperform principal component analysis, support vector machine and isolation forest based anomaly detection baselines. considering our best model, a events labeled as insider threat activity inside our dataset had an average anomaly score inside a 95.53 percentile, demonstrating our approach's potential to greatly reduce analyst workloads.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18007,"weyl semimetals (wsms) have recently attracted the great deal of attention as they provide condensed matter realization of chiral anomaly, feature topologically protected fermi arc surface states and sustain sharp chiral weyl quasiparticles up to the critical disorder at which the continuous quantum phase transition (qpt) drives a system into the metallic phase. we here numerically demonstrate that with increasing strength of disorder a fermi arc gradually looses its sharpness, and close to a wsm-metal qpt it completely dissolves into a metallic bath of a bulk. predicted topological nature of a wsm-metal qpt and a resulting bulk-boundary correspondence across this transition should directly be observed inside angle-resolved-photo-emmision-spectroscopy (arpes) and fourier transformed scanning-tunneling-microscopy (stm) measurements by following a continuous deformation of a fermi arcs with increasing disorder inside recently discovered weyl materials.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
6849,"feature selection procedures considering spatial point processes parametric intensity approximation have been recently developed since more and more applications involve the large number of covariates. inside this paper, we investigate a setting where a number of covariates diverges as a domain of observation increases. inside particular, we consider estimating equations based on campbell theorems derived from poisson and logistic regression likelihoods regularized by the general penalty function. we prove that, under some conditions, a consistency, a sparsity, and a asymptotic normality are valid considering such the setting. we support a theoretical results by numerical ones obtained from simulation experiments and an application to forestry datasets.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
6698,"once a terrestrial planets had mostly completed their assembly, bombardment continued by planetesimals left-over from accretion. highly siderophile element (hse) abundances inside mars' mantle imply its late accretion supplement is 0.8 wt.%; earth and a moon obtained an additional 0.7 wt.% and 0.02 wt.%, respectively. a disproportionately high earth/moon accretion ratio was explicable by stochastic addition of the few remaining ceres-sized bodies that preferentially targeted earth. here we show that mars' late accretion budget also requires the colossal impact, the plausible visible remnant of which was a hemispheric dichotomy. a addition of sufficient hses to a martian mantle entails an impactor of at least 1200 km inside diameter to have struck mars before ca. 4430 ma, by which time crust formation is well underway. thus, a dichotomy could be one of a oldest geophysical features of a martian crust. ejected debris could be a source material considering its satellites.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13656,"simulation optimization (so) refers to a optimization of an objective function subject to constraints, both of which should be evaluated through the stochastic simulation. to address specific features of the particular simulation---discrete or continuous decisions, expensive or cheap simulations, single or multiple outputs, homogeneous or heterogeneous noise---various algorithms have been proposed inside a literature. as one should imagine, there exist several competing algorithms considering each of these classes of problems. this document emphasizes a difficulties inside simulation optimization as compared to mathematical programming, makes reference to state-of-the-art algorithms inside a field, examines and contrasts a different approaches used, reviews some of a diverse applications that have been tackled by these methods, and speculates on future directions inside a field.",1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
18601,"a stability of planets inside a alpha-centauri ab stellar system has been studied extensively. however, most studies either focus on a orbital plane of a binary or consider inclined circular orbits. here, we numerically investigate a stability of the possible planet inside a alpha-centauri ab binary system considering s-type orbits inside an arbitrary spatial configuration. inside particular, we focus on inclined orbits and explore a stability considering different eccentricities and orientation angles. we show that large stable and regular regions are present considering very eccentric and inclined orbits, corresponding to libration inside a lidov-kozai resonance. we additionally show that these extreme orbits should survive over a age of a system, despite a effect of tides. our results remain qualitatively a same considering any compact binary system.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12089,"recently, topological semimetals become hot topic inside condensed matter physics, including dirac semimetal, weyl semimetal, and nodal line semimetal (nlsm). inside this paper, the new type of node- line semimetal - type-ii nlsm was proposed based on the two-band cubic lattice model. considering type-ii nlsm, a zero energy bulk states have the closed loop inside momentum space but a (local) weyl cones on nodal line become tilted. a effect of magnetic field and that of correlation on type-ii nlsm are studied. inside particular, after considering repulsive interaction and additional spin degrees of freedom, different types of long range magnetic orders appear inside bulk states. inside addition, a interaction-induced ferromagnetic order of surface states may exist. at critical point between type-i nlsm and type-ii nlsm, arbitrary tiny interaction induces ferromagnetic order due to the flat band at fermi surface.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
16938,"wisdom of a crowd, a collective intelligence derived from responses of multiple human or machine individuals to a same questions, should be more accurate than each individual, and improve social decision-making and prediction accuracy. this should also integrate multiple programs or datasets, each as an individual, considering a same predictive questions. crowd wisdom estimates each individual's independent error level arising from their limited knowledge, and finds a crowd consensus that minimizes a overall error. however, previous studies have merely built isolated, problem-specific models with limited generalizability, and mainly considering binary (yes/no) responses. here we show with simulation and real-world data that a crowd wisdom problem was analogous to one-dimensional unsupervised dimension reduction inside machine learning. this provides the natural class of crowd wisdom solutions, such as principal component analysis and isomap, which should handle binary and also continuous responses, like confidence levels, and consequently should be more accurate than existing solutions. they should even outperform supervised-learning-based collective intelligence that was calibrated on historical performance of individuals, e.g. penalized linear regression and random forest. this study unifies crowd wisdom and unsupervised dimension reduction, and thereupon introduces the broad range of highly-performing and widely-applicable crowd wisdom methods. as a costs considering data acquisition and processing rapidly decrease, this study will promote and guide crowd wisdom applications inside a social and natural sciences, including data fusion, meta-analysis, crowd-sourcing, and committee decision making.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0
19426,"we give uniform upper bounds considering a number of rational points of height at most $b$ on non-singular complete intersections of two quadrics inside $\mathbb{p}^3$ defined over $\mathbb{q}$. to do this, we combine determinant methods with descent arguments.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
7132,"higher levels of renewable electricity generation increase uncertainty inside power system operation. to ensure secure system operation, new tools that account considering this uncertainty are required. inside this paper, we formulate the chance-constrained ac optimal power flow problem, which guarantees that generation, power flows and voltages remain within their bounds with the pre-defined probability. we then propose an accurate, yet tractable analytical reformulation of a chance constraints. a reformulation maintains a full, non-linear ac power flow equations considering a forecasted operating point, and models a impact of uncertainty through the linearization around this point. we discuss different solution algorithms, including one-shot optimization with and without recourse, and an iterative algorithm which enables scalable implementations. we further discuss how more general chance constraint reformulations should be incorporated within a iterative solution algorithm. inside the case study based on four different ieee systems, we compare a performance of a solution algorithms, and demonstrate scalability of a iterative scheme. we further show that a analytical reformulation accurately and efficiently enforces chance constraints inside both in- and out-of-sample tests, and that a analytical idea behind the method outperforms two alternative, sample based chance constraint reformulations.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
2182,"we perform the set of general relativistic, radiative, magneto-hydrodynamical simulations (gr-rmhd) to study a transition from radiatively inefficient to efficient state of accretion on the non-rotating black hole. we study ion to electron temperature ratios ranging from $t_{\rm i}/t_{\rm e}=10$ to $100$, and simulate flows corresponding to accretion rates as low as $10^{-6}\dot m_{\rm edd}$, and as high as $10^{-2}\dot m_{\rm edd}$. we have found that a radiative output of accretion flows increases with accretion rate, and that a transition occurs earlier considering hotter electrons (lower $t_{\rm i}/t_{\rm e}$ ratio). at a same time, a mechanical efficiency hardly changes and accounts to ${\approx}\,3\%$ of a accreted rest mass energy flux, even at a highest simulated accretion rates. this was particularly important considering a mechanical agn feedback regulating massive galaxies, groups, and clusters. comparison with recent observations of radiative and mechanical agn luminosities suggests that a ion to electron temperature ratio inside a inner, collisionless accretion flow should fall within $10<t_{\rm i}/t_{\rm e}<30$, i.e., a electron temperature should be several percent of a ion temperature.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16571,"motivated by a recent experimental realization of a haldane model by ultracold fermions inside an optical lattice, we investigate phase diagrams of a hard-core bose-hubbard model on the honeycomb lattice. this model was closely related with the spin-1/2 antiferromagnetic (af) quantum spin model. nearest-neighbor (nn) hopping amplitude was positive and it prefers an af configurations of phases of bose-einstein condensates. on a other hand, an amplitude of a next-nn hopping depends on an angle variable as inside a haldane model. phase diagrams are obtained by means of an extended path-integral monte-carlo simulations. besides a af state, the 120$^o$-order state, there appear other phases including the bose metal inside which no long-range orders exist.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
15166,"cratering on small bodies was crucial considering a collision cascade and also contributes to a ejection of dust particles into interplanetary space. the crater cavity forms against a mechanical strength of a surface, gravitational acceleration, or both. a formation of moderately sized craters that are sufficiently larger than a thickness of a regolith on small bodies, inside which mechanical strength plays a dominant role rather than gravitational acceleration, was inside a strength regime. a formation of microcraters on blocks on a surface was also within a strength regime. on a other hand, a formation of the crater of the size comparable to a thickness of a regolith was affected by both gravitational acceleration and cohesion between regolith particles. inside this short review, we compile data from a literature pertaining to impact cratering experiments on porous targets, and summarize a ratio of spall diameter to pit diameter, a depth, diameter, and volume of a crater cavity, and a ratio of depth to diameter. among targets with various porosities studied inside a laboratory to date, based on conventional scaling laws (holsapple and schmidt, j. geophys. res., 87, 1849-1870, 1982) a cratering efficiency obtained considering porous sedimentary rocks (suzuki et al., j. geophys. res. 117, e08012, 2012) was intermediate. the comparison with microcraters formed on the glass target with impact velocities up to 14 $km s^{-1}$ indicates the different dependence of cratering efficiency and depth-to-diameter ratio on impact velocity.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16158,"we consider elections where a voters come one at the time, inside the streaming fashion, and devise space-efficient algorithms which identify an approximate winning committee with respect to common multiwinner proportional representation voting rules; specifically, we consider a approval-based and a borda-based variants of both a chamberlin-- ourant rule and a monroe rule. we complement our algorithms with lower bounds. somewhat surprisingly, our results imply that, with the help of space which does not depend on a number of voters it was possible to efficiently identify an approximate representative committee of fixed size over vote streams with huge number of voters.",1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17268,"a modulation instability (mi) was the universal mechanism that was responsible considering a disintegration of weakly nonlinear narrow-banded wave fields and a emergence of localized extreme events inside dispersive media. a instability dynamics was naturally triggered, when unstable energy side-bands located around a main energy peak are excited and then follow an exponential growth law. as the consequence of four wave mixing effect, these primary side-bands generate an infinite number of additional side-bands, forming the triangular side-band cascade. after saturation, it was expected that a system experiences the return to initial conditions followed by the spectral recurrence dynamics. much complex nonlinear wave field motion was expected, when a secondary or successive side-band pair that are created are also located inside a finite instability gain range around a main carrier frequency peak. this latter process was referred to as higher-order mi. we report the numerical and experimental study that confirm observation of higher-order mi dynamics inside water waves. furthermore, we show that a presence of weak dissipation may counter-intuitively enhance wave focusing inside a second recurrent cycle of wave amplification. a interdisciplinary weakly nonlinear idea behind the method inside addressing a evolution of unstable nonlinear waves dynamics may find significant resonance inside other nonlinear dispersive media inside physics, such as optics, solids, superfluids and plasma.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17293,"we give the new upper bound on a quantum query complexity of deciding $st$-connectivity on certain classes of planar graphs, and show a bound was sometimes exponentially better than previous results. we then show boolean formula evaluation reduces to deciding connectivity on just such the class of graphs. applying a algorithm considering $st$-connectivity to boolean formula evaluation problems, we match a $o(\sqrt{n})$ bound on a quantum query complexity of evaluating formulas on $n$ variables, give the quadratic speed-up over a classical query complexity of the certain class of promise boolean formulas, and show this idea behind the method should yield superpolynomial quantum/classical separations. these results indicate that this $st$-connectivity-based idea behind the method may be a ""right"" way of looking at quantum algorithms considering formula evaluation.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17691,motion planning considering the general 2-trailer system poses the hard problem considering any motion planning algorithm and previous methods have lacked any completeness or optimality guarantees. inside this work we present the lattice-based motion planning framework considering the general 2-trailer system that was resolution complete and resolution optimal. a solution will satisfy both differential and obstacle imposed constraints and was intended as the driver support system to automatically plan complicated maneuvers inside backward and forward motion. a proposed framework relies on the precomputing step that was performed offline to generate the finite set of kinematically feasible motion primitives. these motion primitives are then used to create the regular state lattice that should be searched considering the solution with the help of standard graph-search algorithms. to make this graph-search problem tractable considering real-time applications the novel parametrization of a reachable state space was proposed where each motion primitive moves a system from and to the selected set of circular equilibrium configurations. a idea behind the method was evaluated over three different scenarios and impressive real-time performance was achieved.,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
7480,"a fast iterative soft thresholding algorithm (fista) was used to solve convex regularized optimization problems inside machine learning. distributed implementations of a algorithm have become popular since they enable a analysis of large datasets. however, existing formulations of fista communicate data at every iteration which reduces its performance on modern distributed architectures. a communication costs of fista, including bandwidth and latency costs, was closely tied to a mathematical formulation of a algorithm. this work reformulates fista to communicate data at every k iterations and reduce data communication when operating on large data sets. we formulate a algorithm considering two different optimization methods on a lasso problem and show that a latency cost was reduced by the factor of k while bandwidth and floating-point operation costs remain a same. a convergence rates and stability properties of a reformulated algorithms are similar to a standard formulations. a performance of communication-avoiding fista and proximal newton methods was evaluated on 1 to 1024 nodes considering multiple benchmarks and demonstrate average speedups of 3-10x with scaling properties that outperform a classical algorithms.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
12885,"recent work has proposed various adversarial losses considering training generative adversarial networks. yet, it remains unclear what certain types of functions are valid adversarial loss functions, and how these loss functions perform against one another. inside this paper, we aim to gain the deeper understanding of adversarial losses by decoupling a effects of their component functions and regularization terms. we first derive some necessary and sufficient conditions of a component functions such that a adversarial loss was the divergence-like measure between a data and a model distributions. inside order to systematically compare different adversarial losses, we then propose dantest, the new, simple framework based on discriminative adversarial networks. with this framework, we evaluate an extensive set of adversarial losses by combining different component functions and regularization approaches. this study leads to some new insights into a adversarial losses. considering reproducibility, all source code was available at this https url .",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19881,"a cheeger inequality considering undirected graphs, which relates a conductance of an undirected graph and a second smallest eigenvalue of its normalized laplacian, was the cornerstone of spectral graph theory. a cheeger inequality has been extended to directed graphs and hypergraphs with the help of normalized laplacians considering those, that are no longer linear but piecewise linear transformations. inside this paper, we introduce a notion of the submodular transformation $f:\{0,1\}^n \to \mathbb{r}^m$, which applies $m$ submodular functions to a $n$-dimensional input vector, and then introduce a notions of its laplacian and normalized laplacian. with these notions, we unify and generalize a existing cheeger inequalities by showing the cheeger inequality considering submodular transformations, which relates a conductance of the submodular transformation and a smallest non-trivial eigenvalue of its normalized laplacian. this result recovers a cheeger inequalities considering undirected graphs, directed graphs, and hypergraphs, and derives novel cheeger inequalities considering mutual information and directed information. computing a smallest non-trivial eigenvalue of the normalized laplacian of the submodular transformation was np-hard under a small set expansion hypothesis. inside this paper, we present the polynomial-time $o(\log n)$-approximation algorithm considering a symmetric case, which was tight, and the polynomial-time $o(\log^2n+\log n \cdot \log m)$-approximation algorithm considering a general case. we expect a algebra concerned with submodular transformations, or \emph{submodular algebra}, to be useful inside a future not only considering generalizing spectral graph theory but also considering analyzing other problems that involve piecewise linear transformations, e.g., deep learning.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14600,"we present the technique with the help of data depth functions and resampling to perform best subset variable selection considering the wide range of statistical models. we do this by assigning the score, called an $e$-value, to the candidate model, and use the fast bootstrap method to approximate sample versions of these $e$-values. under general conditions, $e$-values should separate statistical models that adequately explain properties of a data from those that do not. this results inside the fast algorithm that fits only the single model and evaluates $p +1$ models, $p$ being a number of predictors under consideration, as opposed to a traditional requirement of fitting and evaluating $2^{p}$ models. we illustrate inside simulation experiments that our proposed method typically performs better than an array of currently used methods considering variable selection inside linear models and fixed effect selection inside linear mixed models. as the real data application, we use our procedure to elicit climatic drivers of indian summer monsoon precipitation.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
4559,"let $\mathfrak{g}$ be the hyperbolic kac-moody algebra of rank 2, and set $\lambda=\lambda_{1} - \lambda_{2}$, where $\lambda_{1}$, $\lambda_{2}$ are a fundamental weights. denote by $v(\lambda)$ a extremal weight module of extremal weight $\lambda$ with $v_\lambda$ a extremal weight vector, and by $\mathcal{b}(\lambda)$ a crystal basis of $v(\lambda)$ with $u_\lambda$ a element corresponding to $v_\lambda$. we prove that (i) $\mathcal{b}(\lambda)$ was connected, (ii) a subset $\mathcal{b}(\lambda)_{\mu}$ of elements of weight $\mu$ inside $\mathcal{b}(\lambda)$ was the finite set considering every integral weight $\mu$, and $\mathcal{b}(\lambda)_{\lambda} = \{u_\lambda\}$, (iii) every extremal element inside $\mathcal{b}(\lambda)$ was contained inside a weyl group orbit of $u_\lambda$, (iv) $v(\lambda)$ was irreducible. finally, we prove that a crystal basis $\mathcal{b}(\lambda)$ was isomorphic, as the crystal, to a crystal $\mathbb{b}(\lambda)$ of lakshmibai-seshadri paths of shape $\lambda$.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
8022,"variational message passing (vmp), belief propagation (bp) and expectation propagation (ep) have found their wide uses inside complex statistical signal processing problems. inside addition to view them as the class of algorithms operating on graphical models, this paper unifies them under an optimization framework, namely, bethe free energy minimization with differently and appropriately imposed constraints. this new perspective inside terms of constraint manipulation should offer additional insights on a connection between message passing algorithms and it was valid considering the generic statistical model. it also founds the theoretical framework to systematically derive message passing variants. taking a sparse bayesian learning (sbl) problem as an example, the low-complexity ep variant should be obtained by simple constraint reformulation, delivering better approximation performance with lower complexity than a standard ep algorithm. furthermore, we should resort to a framework considering systematic derivation of hybrid message passing considering complex inference tasks. the hybrid message passing algorithm was exemplarily derived considering joint sparse signal reconstruction and statistical model learning. it achieves near-ideal inference performance with manageable complexity.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
17316,"we address a problem of designing the conversational avatar capable of the sequence of casual conversations with older adults. users at risk of loneliness, social anxiety or the sense of ennui may benefit from practicing such conversations inside private, at their convenience. we describe an automatic spoken dialogue manager considering lissa, an on-screen virtual agent that should keep older users involved inside conversations over several sessions, each lasting 10-20 minutes. a idea behind lissa was to improve users' communication skills by providing feedback on their non-verbal behavior at certain points inside a course of a conversations. inside this paper, we analyze a dialogues collected from a first session between lissa and each of 8 participants. we examine a quality of a conversations by comparing a transcripts with those collected inside the woz setting. lissa's contributions to a conversations were judged by research assistants who rated a extent to which a contributions were ""natural"", ""on track"", ""encouraging"", ""understanding"", ""relevant"", and ""polite"". a results show that a automatic dialogue manager is able to handle conversation with a users smoothly and naturally.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11654,"conventional text classification models make the bag-of-words assumption reducing text into word occurrence counts per document. recent algorithms such as word2vec are capable of learning semantic meaning and similarity between words inside an entirely unsupervised manner with the help of the contextual window and doing so much faster than previous methods. each word was projected into vector space such that similar meaning words such as ""strong"" and ""powerful"" are projected into a same general euclidean space. open questions about these embeddings include their utility across classification tasks and a optimal properties and source of documents to construct broadly functional embeddings. inside this work, we demonstrate a usefulness of pre-trained embeddings considering classification inside our task and demonstrate that custom word embeddings, built inside a domain and considering a tasks, should improve performance over word embeddings learnt on more general data including news articles or wikipedia.",1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4096,"an inverted flag has its trailing edge clamped and exhibits dynamics distinct from that of the conventional flag, whose leading edge was restrained. we perform nonlinear simulations and the global stability analysis of a inverted-flag system considering the range of reynolds numbers, flag masses and stiffnesses. our global stability analysis was based on the linearisation of a fully-coupled fluid-structure system of equations. a calculated equilibria are steady-state solutions of a fully-coupled nonlinear equations. by implementing this approach, we (i) explore a mechanisms that initiate flapping, (ii) study a role of vortex shedding and vortex-induced vibration (viv) inside large-amplitude flapping, and (iii) characterise a chaotic flapping regime. considering point (i), we identify the deformed-equilibrium state and show through the global stability analysis that a onset of flapping was due to the supercritical hopf bifurcation. considering large-amplitude flapping, point (ii), we confirm a arguments of sader et al. (2016) that considering the range of parameters this regime was the viv. we also show that there are other flow regimes considering which large-amplitude flapping persists and was not the viv. specifically, flapping should occur at low reynolds numbers ($<50$), albeit using the previously unexplored mechanism. finally, with respect to point (iii), chaotic flapping has been observed experimentally considering reynolds numbers of $o(10^4)$, and here we show that chaos also persists at the moderate reynolds number of 200. we characterise this chaotic regime and calculate its strange attractor, whose structure was controlled by a above-mentioned deformed equilibria and was similar to the lorenz attractor. these results are contextualised with bifurcation diagrams that depict a different equilibria and various flapping regimes.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18088,"inside a fluid transport of particles, it was generally expected that heavy particles carried by the laminar fluid flow moving downward will also move downward. we establish the theory to show, however, that particles should be dynamically levitated and lifted by interacting vortices inside such flows, thereby moving against gravity and a asymptotic direction of a flow, even when they are orders of magnitude denser than a fluid. a particle levitation was rigorously demonstrated considering potential flows and supported by simulations considering viscous flows. we suggest that this counterintuitive effect has potential implications considering a air-transport of water droplets and a lifting of sediments inside water.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12947,"we consider the free boundary problem on cones depending on the parameter c and study when a free boundary was allowed to pass through a vertex of a cone. we show that when a cone was three-dimensional and c was large enough, a free boundary avoids a vertex. we also show that when c was small enough but still positive, a free boundary was allowed to pass through a vertex. this establishes 3 as a critical dimension considering which a free boundary may pass through a vertex of the right circular cone. inside view of a well-known connection between area-minimizing surfaces and a free boundary problem under consideration, our result was analogous to the result of morgan that classifies when an area-minimizing surface on the cone passes through a vertex.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9245,"understanding a feasible power flow region was of central importance to power system analysis. inside this paper, we propose the geometric view of a power system loadability problem. by with the help of rectangular coordinates considering complex voltages, we provide an integrated geometric understanding of active and reactive power flow equations on loadability boundaries. based on such an understanding, we develop the linear programming framework to 1) verify if an operating point was on a loadability boundary, 2) compute a margin of an operating point to a loadability boundary, and 3) calculate the loadability boundary point of any direction. a proposed method was computationally more efficient than existing methods since it does not require solving nonlinear optimization problems or calculating a eigenvalues of a power flow jacobian. standard ieee test cases demonstrate a capability of a new method compared to a current state-of-the-art methods.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
19716,"motivated by a recent experiments indicating superconductivity inside metal-decorated graphene sheets, we investigate their quasi-particle structure within a framework of an effective tight-binding hamiltonian augmented by appropriate bcs-like pairing terms considering p-type order parameter. a normal state band structure of graphene was modified not only through interaction with adsorbed metal atoms, but also due to a folding of bands at brillouin zone boundaries resulting from the $\sqrt{3}\times\sqrt{3}r30^{\circ}$ reconstruction. several different types of pairing symmetries are analyzed utilizing nambu-gorkov green's function techniques to show that $p+ip$-symmetric nearest-neighbor pairing yields a most enhanced superconducting gap. a character of a order parameter depends on a nature of a atomic orbitals involved inside a pairing process and exhibits interesting angular and radial asymmetries. finally, we suggest the method to distinguish between singlet and triplet type superconductivity inside a presence of magnetic substitutional impurities with the help of scanning tunneling spectroscopy.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
16276,"we publicly release the new large-scale dataset, called searchqa, considering machine comprehension, or question-answering. unlike recently released datasets, such as deepmind cnn/dailymail and squad, a proposed searchqa is constructed to reflect the full pipeline of general question-answering. that is, we start not from an existing article and generate the question-answer pair, but start from an existing question-answer pair, crawled from j! archive, and augment it with text snippets retrieved by google. following this approach, we built searchqa, which consists of more than 140k question-answer pairs with each pair having 49.6 snippets on average. each question-answer-context tuple of a searchqa comes with additional meta-data such as a snippet's url, which we believe will be valuable resources considering future research. we conduct human evaluation as well as test two baseline methods, one simple word selection and a other deep learning based, on a searchqa. we show that there was the meaningful gap between a human and machine performances. this suggests that a proposed dataset could well serve as the benchmark considering question-answering.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10260,"a vector-valued mock modular forms of umbral moonshine may be repackaged into meromorphic jacobi forms of weight one. inside this work we constructively solve two cases of a meromorphic module problem considering umbral moonshine. specifically, considering a type the niemeier root systems with coxeter numbers seven and thirteen, we construct corresponding bigraded super vertex operator algebras, equip them with actions of a corresponding umbral groups, and verify that a resulting trace functions on canonically twisted modules recover a meromorphic jacobi forms that are specified by umbral moonshine. we also obtain partial solutions to a meromorphic module problem considering a type the niemeier root systems with coxeter numbers four and five, by constructing super vertex operator algebras that recover a meromorphic jacobi forms attached to maximal subgroups of a corresponding umbral groups.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
18680,"space systems such as communication satellites, earth observation satellites and space telescopes require precise pointing to observe fixed targets over prolonged time. these systems typically use reaction-wheels to slew a spacecraft and gimballing systems containing motors to achieve precise pointing. motor based actuators have limited life as they contain moving parts that require lubrication inside space. alternate methods have utilized piezoelectric actuators. this paper presents shape memory alloys (sma) actuators considering control of the deployable antenna placed on the satellite. a smas are operated as the series of distributed linear actuators. these distributed linear actuators are not prone to single point failures and although each individual actuator was imprecise due to hysteresis and temperature variation. a system as the whole achieves reliable results. a smas should be programmed to perform the series of periodic motion and operate as the mechanical guidance system that was not prone to damage from radiation or space weather. efforts are focused on developing the system that should achieve one degree pointing accuracy at first, with an ultimate goal of achieving the few arc seconds accuracy. bench top models of a actuator system has been developed and working towards testing a system under vacuum. the demonstration flight of a technology was planned aboard the cubesat.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
18763,"turbulent state of spectrally stable shear flows may be developed and sustained according to a bypass scenario of transition. if it works inside non-magnetised boundless and homogeneous quasi-keplerian flow, transiently growing shearing vortices should supply turbulence with energy. employing a large shearing box approximation, as well as the set of global disc models, we study a optimal growth of a shearing vortices inside such the flow inside a whole range of azimuthal length-scales, $\lambda_y$, as compared to a flow scale-height, $h$. it was shown that with a account of a viscosity a highest possible amplification of shearing vortices, $g_{max}$, attains maximum at $\lambda_y\lesssim h$ and declines towards both a large scales $\lambda_y\gg h$ and a small scales $\lambda_y\ll h$ inside the good agreement with analytical estimations based on balanced solutions. we pay main attention to a large-scale vortices $\lambda_y\gg h$, which produce $g_{max}\propto (\omega/\kappa)^4$, where $\omega$ and $\kappa$ denote local rotational and epicyclic frequencies, respectively. it was demonstrated that a large-scale vortices acquire high density perturbation as they idea behind the method a instant of swing. at a same time, their growth was not affected by bulk viscosity. we check that $g_{max}$ obtained globally was comparable to its local counterpart and a shape and localisation of global optimal vortices should be explained inside terms of a local approach. a obtained results allow us to suggest that a critical reynolds number of subcritical transition to turbulence inside quasi-keplerian flow, as well as a corresponding turbulent effective azimuthal stress should substantially depend on shear rate.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4368,"we investigate a performance of a standard greedy algorithm considering cardinality constrained maximization of non-submodular nondecreasing set functions. while there are strong theoretical guarantees on a performance of greedy considering maximizing submodular functions, there are few guarantees considering non-submodular ones. however, greedy enjoys strong empirical performance considering many important non-submodular functions, e.g., a bayesian a-optimality objective inside experimental design. we prove theoretical guarantees supporting a empirical performance. our guarantees are characterized by the combination of a (generalized) curvature $\alpha$ and a submodularity ratio $\gamma$. inside particular, we prove that greedy enjoys the tight approximation guarantee of $\frac{1}{\alpha}(1- e^{-\gamma\alpha})$ considering cardinality constrained maximization. inside addition, we bound a submodularity ratio and curvature considering several important real-world objectives, including a bayesian a-optimality objective, a determinantal function of the square submatrix and certain linear programs with combinatorial constraints. we experimentally validate our theoretical findings considering both synthetic and real-world applications.",1,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0
1384,"we study impurity scattering inside a normal and d-wave superconducting states of line nodal semimetals and show that, due to additional scattering phase space available considering impurities on a surface, a quasiparticle interference pattern acquires an extended character instead of the discrete collection of delta function peaks. moreover, with the help of a t-matrix formalism, we demonstrate that a conventional behavior of the scalar impurity inside the d-wave superconductor breaks down on a surface of the line nodal semimetal inside a quasi flat band limit.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0
1817,"standard optimality criteria (e.g. a-, d-optimality criterion, etc.) have been commonly used considering obtaining optimal designs. considering the given statistical model, standard criteria assume a error variance was known at a design stage. however, inside practice a error variance was estimated to make inference about a model parameters. modified criteria are defined as the function of a standard criteria and a corresponding error degrees of freedom, which may lead to extreme optimal design. compound criteria are defined as a function of different modified criteria and corresponding user specified weights. standard, modified, and compound criteria based optimal designs are obtained considering $3^3$ factorial design. robustness properties of a optimal designs are also compared.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
18297,"this paper addresses a problem of robust and optimal control considering a class of nonlinear quadratic systems subject to norm-bounded parametric uncertainties and disturbances, and inside presence of some amplitude constraints on a control input. by with the help of an idea behind the method based on a guaranteed cost control theory, the technique was proposed to design the state feedback controller ensuring considering a closed-loop system: i) a local exponential stability of a zero equilibrium point; ii) a inclusion of the given region into a domain of exponential stability of a equilibrium point; iii) a satisfaction of the guaranteed level of performance, inside terms of boundedness of some optimality indexes. inside particular, the sufficient condition considering a existence of the state feedback controller satisfying the prescribed integral-quadratic index was provided, followed by the sufficient condition considering a existence of the state feedback controller satisfying the given $\mathcal l_2$-gain disturbance rejection constraint. by a proposed design procedures, a optimal control problems dealt with here should be efficiently solved as linear matrix inequality (lmi) optimization problems.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
3534,"anomalies are strange data points; they usually represent an unusual occurrence. anomaly detection was presented from a perspective of wireless sensor networks. different approaches have been taken inside a past, as we will see, not only to identify outliers, but also to establish a statistical properties of a different methods. a usual goal was to show that a idea behind the method was asymptotically efficient and that a metric used was unbiased or maybe biased. this project was based on the work done by [1]. a idea behind the method was based on a principle that a entropy of a data was increased when an anomalous data point was measured. a entropy of a data set was thus to be estimated. inside this report however, preliminary efforts at confirming a results of [1] was presented. to approximate a entropy of a dataset, since no parametric form was assumed, a probability density function of a data set was first estimated with the help of data split method. this estimated pdf value was then plugged-in to a entropy approximation formula to approximate a entropy of a dataset. a data (test signal) used inside this report was gaussian distributed with zero mean and variance 4. results of pdf approximation with the help of a k-nearest neighbour method with the help of a entire dataset, and the data-split method are presented and compared based on how well they approximate a probability density function of the gaussian with similar mean and variance. a number of nearest neighbours chosen considering a purpose of this report was 8. this was arbitrary, but was reasonable since a number of anomalies introduced was expected to be less than this upon data-split. a data-split method was preferred and rightly so.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
9260,"inside this paper we introduce matmpc, an open source software built inside matlab considering nonlinear model predictive control (nmpc). it was designed to facilitate modelling, controller design and simulation considering the wide class of nmpc applications. matmpc has the number of algorithmic modules, including automatic differentiation, direct multiple shooting, condensing, linear quadratic program (qp) solver and globalization. it also supports the unique curvature-like measure of nonlinearity (cmon) mpc algorithm. matmpc has been designed to provide state-of-the-art performance while making a prototyping easy, also with limited programming knowledge. this was achieved by writing each module directly inside matlab api considering c. as the result, matmpc modules should be compiled into mex functions with performance comparable to plain c/c++ solvers. matmpc has been successfully used inside operating systems including windows, linux and os x. selected examples are shown to highlight a effectiveness of matmpc.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
8534,"we study rapidly-rotating boussinesq convection driven by internal heating inside the full sphere. we use the numerical model based on a quasi-geostrophic approximation considering a velocity field, whereas a temperature field was three-dimensional. this approximation allows us to perform simulations considering ekman numbers down to 1e-8, prandtl numbers relevant considering liquid metals (~0.1) and reynolds numbers up to 3e4. persistent zonal flows composed of multiple jets form as the result of a mixing of potential vorticity. considering a largest rayleigh numbers computed, a zonal velocity was larger than a convective velocity despite a presence of boundary friction. a convective structures and a zonal jets widen when a thermal forcing increases. prograde and retrograde zonal jets are dynamically different: inside a prograde jets (which correspond to weak potential vorticity gradients) a convection transports heat efficiently and a mean temperature tends to be homogenised; by contrast, inside a cores of a retrograde jets (which correspond to steep gradients of potential vorticity) a dynamics was dominated by a propagation of rossby waves, resulting inside a formation of steep mean temperature gradients and a dominance of conduction inside a heat transfer process. consequently, inside quasi-geostrophic systems, a width of a retrograde zonal jets controls a efficiency of a heat transfer.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
954,"we study a problem of computing a \textsc{maxima} of the set of $n$ $d$-dimensional points. considering dimensions 2 and 3, there are algorithms to solve a problem with order-oblivious instance-optimal running time. however, inside higher dimensions there was still room considering improvements. we present an algorithm sensitive to a structural entropy of a input set, which improves a running time, considering large classes of instances, on a best solution considering \textsc{maxima} to date considering $d \ge 4$.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17077,"we present the theoretical analysis of a role that strain plays on a electronic structure of chromium nitride crystals. we use lsda+u calculations to study a elastic constants, deforma- tion potentials and strain dependence of electron and hole masses near a fundamental gap. we consider a lowest energy antiferromagnetic models believed to describe crn at low temperatures, and apply strain along different directions. we find relatively large deformation potentials considering all models, and find increasing gaps considering tensile strain along most directions. most interestingly, we find that compressive strains should be able to close a relatively small indirect gap (' 100 mev) at moderate amplitudes ' 1.3%. we also find large and anisotropic changes inside a effective masses with strain, with principal axes closely related to a magnetic ordering of neighboring layers inside a antiferromagnet. it would be interesting to consider a role that these effects may have on typical film growth on different substrates, and a possibility of monitoring optical and transport properties of thin films as strain was applied.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
10021,"with the help of density-functional theory calculations, we analyze a optical absorption properties of lead (pb)-free metal halide perovskites (ab$^{2+}$x$_3$) and double perovskites (ab$^+$b$^{3+}$x$_6$) (a = cs or monovalent organic ion, b$^{2+}$ = non-pb divalent metal, b$^+$ = monovalent metal, b$^{3+}$ = trivalent metal, x = halogen). we show that, if b$^{2+}$ was not sn or ge, pb-free metal halide perovskites exhibit poor optical absorptions because of their indirect bandgap nature. among a nine possible types of pb-free metal halide double perovskites, six have direct bandgaps. of these six types, four show inversion symmetry-induced parity-forbidden or weak transitions between band edges, making them not ideal considering thin-film solar cell application. only one type of pb-free double perovskite shows optical absorption and electronic properties suitable considering solar cell applications, namely those with b$^+$ = in, tl and b$^{3+}$ = sb, bi. our results provide important insights considering designing new metal halide perovskites and double perovskites considering optoelectronic applications.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
13597,"with the help of the new and general method, we prove a existence of random attractor considering a three dimensional stochastic primitive equations defined on the manifold $\d\subset\r^3$ improving a existence of weak attractor considering a deterministic model. furthermore, we show a existence of a invariant measure.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10146,"microscopic theory of a normal-to-superconductor coexistence line of the 2d two-band weyl superconductor subjected to magnetic field was constructed. it was shown that the weyl semi-metal that was nonsuperconducting or having the small critical temperature $t_{c}$ at zero field, might become the superconductor at higher temperature when a magnetic field was tuned to the series of quantized values $h_{n}$. a pairing occurs on landau levels. it was argued that a phenomenon was much easier detectable inside weyl semi - metals than inside parabolic band metals since a quantum limit already has been approaches inside several weyl materials.. an experimental signature of a superconductivity on landau levels was a reduction of magnetoresistivity. this has already been observed inside $cd_{3}as_{2}$ and several other compounds. a novel kind of quantum oscillations of magnetoresistance detected inside $zrte_{5}$ was discussed along these lines.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
17008,"we shall consider the result of fel'dman, where the sharp baker-type lower bound was obtained considering linear forms inside a values of some e-functions. fel'dman's proof was based on an explicit construction of pad√© approximations of a first kind considering these functions. inside a present paper we introduce pad√© approximations of a second kind considering a same functions and use these to obtain the slightly improved version of fel'dman's result.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
13173,"application of humanoid robots has been common inside a field of healthcare and education. it has been recurrently used to improve social behavior and mollify distress level among children with autism, cancer and cerebral palsy. this article discusses a same from the human factors perspective. it shows how people of different age and gender have the different opinion towards a application and acceptance of humanoid robots. additionally, this article highlights a influence of cerebral condition and social interaction on the user behavior and attitude towards humanoid robots. our study performed the literature review and found that (a) children and elderly individuals prefer humanoid robots due to inactive social interaction, (b) a deterministic behavior of humanoid robots should be acknowledged to improve social behavior of autistic children, (c) trust on humanoid robots was highly driven by its application and the user age, gender, and social life.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
18607,"we consider a problem of identifying intermediate variables (or mediators) that regulate a effect of the treatment on the response variable. while there has been significant research on this topic, little work has been done when a set of potential mediators was high-dimensional and when they are interrelated. inside particular, we assume that a causal structure of a treatment, a potential mediators and a response was the directed acyclic graph (dag). high-dimensional dag models have previously been used considering a approximation of causal effects from observational data and methods called ida and joint-ida have been developed considering estimating a effects of single interventions and multiple simultaneous interventions respectively. inside this paper, we propose an ida-type method called mida considering estimating mediation effects from high-dimensional observational data. although ida and joint-ida estimators have been shown to be consistent inside certain sparse high-dimensional settings, their asymptotic properties such as convergence inside distribution and inferential tools inside such settings remained unknown. we prove high-dimensional consistency of mida considering linear structural equation models with sub-gaussian errors. more importantly, we derive distributional convergence results considering mida inside similar high-dimensional settings, which are applicable to ida and joint-ida estimators as well. to a best of our knowledge, these are a first distributional convergence results facilitating inference considering ida-type estimators. these results have been built on our novel theoretical results regarding uniform bounds considering linear regression estimators over varying subsets of high-dimensional covariates, which may be of independent interest. finally, we empirically validate our asymptotic theory and demonstrate a usefulness of mida inside identifying large mediation effects using simulations and application to real data inside genomics.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16415,"schelling's segregation model was the landmark model inside sociology. it shows a counter-intuitive phenomenon that residential segregation between individuals of different groups should emerge even when all involved individuals are tolerant. although a model was widely studied, no pure game-theoretic version where rational agents strategically choose their location exists. we close this gap by introducing and analyzing generalized game-theoretic models of schelling segregation, where a agents should also have individual location preferences. considering our models, we investigate a convergence behavior and a efficiency of their equilibria. inside particular, we prove guaranteed convergence to an equilibrium inside a version which was closest to schelling's original model. moreover, we provide tight bounds on a price of anarchy.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
16508,"we present an idea behind the method to deep approximation of discrete conditional probability distributions. such models have several applications, including generative modeling of audio, image, and video data. our idea behind the method combines two main techniques: dyadic partitioning and graph-based smoothing of a discrete space. by recursively decomposing each dimension into the series of binary splits and smoothing over a resulting distribution with the help of graph-based trend filtering, we impose the strict structure to a model and achieve much higher sample efficiency. we demonstrate a advantages of our model through the series of benchmarks on both synthetic and real-world datasets, inside some cases reducing a error by nearly half inside comparison to other popular methods inside a literature. all of our models are implemented inside tensorflow and publicly available at this https url .",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14972,"infants are experts at playing, with an amazing ability to generate novel structured behaviors inside unstructured environments that lack clear extrinsic reward signals. we seek to mathematically formalize these abilities with the help of the neural network that implements curiosity-driven intrinsic motivation. with the help of the simple but ecologically naturalistic simulated environment inside which an agent should move and interact with objects it sees, we propose the ""world-model"" network that learns to predict a dynamic consequences of a agent's actions. simultaneously, we train the separate explicit ""self-model"" that allows a agent to track a error map of its own world-model, and then uses a self-model to adversarially challenge a developing world-model. we demonstrate that this policy causes a agent to explore novel and informative interactions with its environment, leading to a generation of the spectrum of complex behaviors, including ego-motion prediction, object attention, and object gathering. moreover, a world-model that a agent learns supports improved performance on object dynamics prediction, detection, localization and recognition tasks. taken together, our results are initial steps toward creating flexible autonomous agents that self-supervise inside complex novel physical environments.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4694,"inside a 'big data' era, many real-world applications like search involve a ranking problem considering the large number of items. it was important to obtain effective ranking results and at a same time obtain a results efficiently inside the timely manner considering providing good user experience and saving computational costs. valuable prior research has been conducted considering learning to efficiently rank like a cascade ranking (learning) model, which uses the sequence of ranking functions to progressively filter some items and rank a remaining items. however, most existing research of learning to efficiently rank inside search was studied inside the relatively small computing environments with simulated user queries. this paper presents novel research and thorough study of designing and deploying the cascade model inside the large-scale operational e-commerce search application (cloes), which deals with hundreds of millions of user queries per day with hundreds of servers. a challenge of a real-world application provides new insights considering research: 1). real-world search applications often involve multiple factors of preferences or constraints with respect to user experience and computational costs such as search accuracy, search latency, size of search results and total cpu cost, while most existing search solutions only address one or two factors; 2). effectiveness of e-commerce search involves multiple types of user behaviors such as click and purchase, while most existing cascade ranking inside search only models a click behavior. based on these observations, the novel cascade ranking model was designed and deployed inside an operational e-commerce search application. an extensive set of experiments demonstrate a advantage of a proposed work to address multiple factors of effectiveness, efficiency and user experience inside a real-world application.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5410,"we study the nonlinear coevolving voter model with triadic closure local rewiring. we find three phases with different topological properties and configuration inside a steady state: absorbing consensus phase with the single component, absorbing fragmented phase with two components inside opposite consensus states, and the dynamically active shattered phase with many isolated nodes. this shattered phase, which does not exist considering the coevolving model with global rewiring, has the lifetime that scale exponentially with system size. we characterize a transitions between these phases inside terms of a size of a largest cluster, a number of clusters, and a magnetization. our analysis provides the possible solution to reproduce isolated parts inside adaptive networks and high clustering widely observed inside social systems.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
12541,"considering accommodating more electric vehicles (evs) to battle against fossil fuel emission, a problem of charging station placement was inevitable and could be costly if done improperly. some researches consider the general setup, with the help of conditions such as driving ranges considering planning. however, most of a ev growths inside a next decades will happen inside a urban area, where driving ranges was not a biggest concern. considering such the need, we consider several practical aspects of urban systems, such as voltage regulation cost and protection device upgrade resulting from a large integration of evs. notably, our diversified objective should reveal a trade-off between different factors inside different cities worldwide. to understand a global optimum of large-scale analysis, we add constraint one-by-one to see how to preserve a problem convexity. our sensitivity analysis before and after convexification shows that our idea behind the method was not only universally applicable but also has the small approximation error considering prioritizing a most urgent constraint inside the specific setup. finally, numerical results demonstrate a trade-off, a relationship between different factors and a global objective, and a small approximation error. the unique observation inside this study shows a importance of incorporating a protection device upgrade inside urban system planning on charging stations.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
17334,"we present the systematic density functional theory (dft) plus hubbard $u$ study of structural trends and a stability of different magnetically ordered states across a rare-earth nickelate series, $r$nio$_3$, with $r$ from lu to la. inside particular, we investigate how a magnetic order, a change of a rare-earth ion, and a hubbard interaction $u$ are affecting a bond-length disproportionation between a nickel sites. our results show that structural parameters should be obtained that are inside very good agreement with present experimental data, and that dft+$u$ was inside principle able to capture a most important structural trends across a nickelate series. however, a amplitude of a bond-length disproportionation depends very strongly on a specific value used considering a hubbard $u$ parameter and also on a type of magnetic order imposed inside a calculation. regarding a relative stability of different magnetic orderings, the realistic antiferromagnetic order, consistent with a experimental observations, was favored considering small $u$ values, and becomes more and more favorable compared to a ferromagnetic state towards a end of a series (i.e., towards $r$=pr). nevertheless, it seems that a stability of a ferromagnetic state was generally overestimated within a dft+$u$ calculations. our work provides the profound starting point considering more detailed experimental investigations, and also considering future studies with the help of more advanced computational techniques such as, e.g., dft combined with dynamical mean-field theory.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
838,"we show how an action-dependent baseline should be used by a policy gradient theorem with the help of function approximation, originally presented with action-independent baselines by (sutton et al. 2000).",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16265,"sol-gel transition of carboxylated cellulose nanocrystals was investigated with the help of rheology, saxs, nmr and optical spectroscopies to unveil a distinctive roles of ultrasounds treatment and ions addition. besides cellulose fibers fragmentation, sonication treatment induces fast gelling of a solution. gelation was induced independently on a addition of cations, while a final rheological properties are highly influenced by a type, a concentration as well as on a sequence of a operations since salts must be added before sonication to produce stiff gels. cations with various charge and dimension have been associated to ultrasounds to induce gelation and a gel elastic modulus increase proportionally with a charge over a ion size ratio. saxs analysis of a na+ hydrogel and ca2+ hydrogel to which a ion is added after sonication shows a presence of structurally ordered domains where water was confined as indicated by 1h-nmr investigation of a dynamic of water exchange inside a hydrogels. conversely, separated phases containing essentially free water, characterize a hydrogels obtained by sonication after ca2+ addition, confirming that this ion induces irreversible fiber aggregation. a rheological properties of a hydrogels depend on a duration of a ultrasound treatment and it enables a design of materials programmed with tailored energy dissipation response.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
19952,"some effects of surface tension on fully-nonlinear, long, surface water waves are studied by numerical means. a differences between various solitary waves and their interactions inside subcritical and supercritical surface tension regimes are presented. analytical expressions considering new peaked travelling wave solutions are presented inside a case of critical surface tension. a numerical experiments were performed with the help of the high-accurate finite element method based on smooth cubic splines and a four-stage, classical, explicit runge-kutta method of order four.",0,1,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13964,"as the powerful tool of asynchronous event sequence analysis, point processes have been studied considering the long time and achieved numerous successes inside different fields. among various point process models, hawkes process and its variants attract many researchers inside statistics and computer science these years because they capture a self- and mutually-triggering patterns between different events inside complicated sequences explicitly and quantitatively and are broadly applicable to many practical problems. inside this paper, we describe an open-source toolkit implementing many learning algorithms and analysis tools considering hawkes process model and its variants. our toolkit systematically summarizes recent state-of-the-art algorithms as well as most classic algorithms of hawkes processes, which was beneficial considering both academical education and research. source code should be downloaded from this https url.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
700,"the class of vision problems, less commonly studied, consists of detecting objects inside imagery obtained from physics-based experiments. these objects should span inside 4d (x, y, z, t) and are visible as disturbances (caused due to physical phenomena) inside a image with background distribution being approximately uniform. such objects, occasionally referred to as `events', should be considered as high energy blobs inside a image. unlike a images analyzed inside conventional vision problems, very limited features are associated with such events, and their shape, size and count should vary significantly. this poses the challenge on a use of pre-trained models obtained from supervised approaches. inside this paper, we propose an unsupervised idea behind the method involving iterative clustering based segmentation (ics) which should detect target objects (events) inside real-time. inside this approach, the test image was analyzed over several cycles, and one event was identified per cycle. each cycle consists of a following steps: (1) image segmentation with the help of the modified k-means clustering method, (2) elimination of empty (with no events) segments based on statistical analysis of each segment, (3) merging segments that overlap (correspond to same event), and (4) selecting a strongest event. these four steps are repeated until all a events have been identified. a ics idea behind the method consists of the few hyper-parameters that have been chosen based on statistical study performed over the set of test images. a applicability of ics method was demonstrated on several 2d and 3d test examples.",1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6007,"inside this paper we propose an algorithm and associated data model considering creating the watershed boundary with the help of the 2d flow direction grid. flow direction grids (fdgs) are common abstractions considering hydrodynamic models and are utilized considering delineating physical systems (e.g. watersheds, fluvial, and non-fluvial flow paths). a proposed algorithm and associated data model provides geometric speed increases inside watershed boundary retrieval while keeping storage constraints linear inside comparison to existing techniques. a algorithm called haag shokoufandehs' march (hsm) relies on an existing data structure, a modified nested set model, originally described by celko and applied to hydrodynamic models by haag and shokoufandeh inside 2017. a proposed algorithm creates watershed boundaries by marching around a edges of its' corresponding region, never entering a internal area. inside contrast to existing algorithms that scales inside proportional to a area of a underlying region, a complexity of a hsm algorithm was proportional to a boundary length. results considering the group of tested watersheds (n = 14,718) inside a approximately 36,000 km^2 delaware river watershed show the reduction of between 0 and 99% inside computational complexity with the help of the 30 m dem vs. existing techniques. larger watersheds have the consistent reduction inside a number of (read) operation complexity, with a largest watershed resulting inside approximately 35 million reads with the help of traditional techniques compared to approximately 45 thousand with the help of a hsm algorithm, respectively. modelled estimates of a complexity considering a approximately 6.1 million km^2 amazon river basin show the reduction from 6.7 billion to 1.4 million reads.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17904,"although modern recommendation systems should exploit a structure inside users' item feedback, most are powerless inside a face of new users who provide no structure considering them to exploit. inside this paper we introduce implicitce, an algorithm considering recommending items to new users during their sign-up flow. implicitce works by transforming users' implicit feedback towards auxiliary domain items into an embedding inside a target domain item embedding space. implicitce learns these embedding spaces and transformation function inside an end-to-end fashion and should co-embed users and items with any differentiable similarity function. to train implicitce we explore methods considering maximizing a correlations between model predictions and users' affinities and introduce sample correlation update, the novel and extremely simple training strategy. finally, we show that implicitce trained with sample correlation update outperforms the variety of state of a art algorithms and loss functions on both the large scale twitter dataset and a dblp dataset.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
10475,"from minimal surfaces such as simons' cone and catenoids, with the help of refined lyapunov-schmidt reduction method, we construct new solutions considering the free boundary problem whose free boundary has two components. inside dimension $8$, with the help of variational arguments, we also obtain solutions which are global minimizers of a corresponding energy functional. this shows that savin's theorem was optimal.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15312,"inside this paper, we study a asymptotic behavior of a hermitian-yang-mills flow on the reflexive sheaf. we prove that a limiting reflexive sheaf was isomorphic to a double dual of a graded sheaf associated to a harder-narasimhan-seshadri filtration, this answers the question by bando and siu.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6785,"inside a pursuit of real-time motion planning, the commonly adopted practice was to compute the trajectory by running the planning algorithm on the simplified, low-dimensional dynamical model, and then employ the feedback tracking controller that tracks such the trajectory by accounting considering a full, high-dimensional system dynamics. while this strategy of planning with model mismatch generally yields fast computation times, there are no guarantees of dynamic feasibility, which hampers application to safety-critical systems. building upon recent work that addressed this problem through a lens of hamilton-jacobi (hj) reachability, we devise an algorithmic framework whereby one computes, offline, considering the pair of ""planner"" (i.e., low-dimensional) and ""tracking"" (i.e., high-dimensional) models, the feedback tracking controller and associated tracking bound. this bound was then used as the safety margin when generating motion plans using a low-dimensional model. specifically, we harness a computational tool of sum-of-squares (sos) programming to design the bilinear optimization algorithm considering a computation of a feedback tracking controller and associated tracking bound. a algorithm was demonstrated using numerical experiments, with an emphasis on investigating a trade-off between a increased computational scalability afforded by sos and its intrinsic conservativeness. collectively, our results enable scaling a appealing strategy of planning with model mismatch to systems that are beyond a reach of hj analysis, while maintaining safety guarantees.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1
16823,"we present improved approximation algorithms inside stochastic optimization. we prove that a multi-stage stochastic versions of covering integer programs (such as set cover and vertex cover) admit essentially a same approximation algorithms as their standard (non-stochastic) counterparts; this improves upon work of swamy \& shmoys which shows an approximability that depends multiplicatively on a number of stages. we also present approximation algorithms considering facility location and some of its variants inside a $2$-stage recourse model, improving on previous approximation guarantees. we give the $2.2975$-approximation algorithm inside a standard polynomial-scenario model and an algorithm with an expected per-scenario $2.4957$-approximation guarantee, which was applicable to a more general black-box distribution model.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11788,"we discuss problems with a standard approaches to evaluation considering tasks like visual question answering, and argue that artificial data should be used to address these as the complement to current practice. we demonstrate that with a aid of existing 'deep' linguistic processing technology we are able to create challenging abstract datasets, which enable us to investigate a language understanding abilities of multimodal deep learning models inside detail, as compared to the single performance value on the static and monolithic dataset.",1,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2234,"active learning has long been the topic of study inside machine learning. however, as increasingly complex and opaque models have become standard practice, a process of active learning, too, has become more opaque. there has been little investigation into interpreting what specific trends and patterns an active learning strategy may be exploring. this work expands on a local interpretable model-agnostic explanations framework (lime) to provide explanations considering active learning recommendations. we demonstrate how lime should be used to generate locally faithful explanations considering an active learning strategy, and how these explanations should be used to understand how different models and datasets explore the problem space over time. inside order to quantify a per-subgroup differences inside how an active learning strategy queries spatial regions, we introduce the notion of uncertainty bias (based on disparate impact) to measure a discrepancy inside a confidence considering the model's predictions between one subgroup and another. with the help of a uncertainty bias measure, we show that our query explanations accurately reflect a subgroup focus of a active learning queries, allowing considering an interpretable explanation of what was being learned as points with similar sources of uncertainty have their uncertainty bias resolved. we demonstrate that this technique should be applied to track uncertainty bias over user-defined clusters or automatically generated clusters based on a source of uncertainty.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17018,"thompson sampling was an algorithm considering online decision problems where actions are taken sequentially inside the manner that must balance between exploiting what was known to maximize immediate performance and investing to accumulate new information that may improve future performance. a algorithm addresses the broad range of problems inside the computationally efficient manner and was therefore enjoying wide use. this tutorial covers a algorithm and its application, illustrating concepts through the range of examples, including bernoulli bandit problems, shortest path problems, dynamic pricing, recommendation, active learning with neural networks, and reinforcement learning inside markov decision processes. most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. we will also discuss when and why thompson sampling was or was not effective and relations to alternative algorithms.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4058,"we address a problem of multi-class classification inside a case where a number of classes was very large. we propose the double sampling strategy on top of the multi-class to binary reduction strategy, which transforms a original multi-class problem into the binary classification problem over pairs of examples. a aim of a sampling strategy was to overcome a curse of long-tailed class distributions exhibited inside majority of large-scale multi-class classification problems and to reduce a number of pairs of examples inside a expanded data. we show that this strategy does not alter a consistency of a empirical risk minimization principle defined over a double sample reduction. experiments are carried out on dmoz and wikipedia collections with 10,000 to 100,000 classes where we show a efficiency of a proposed idea behind the method inside terms of training and prediction time, memory consumption, and predictive performance with respect to state-of-the-art approaches.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5576,"visual question answering was the recently proposed artificial intelligence task that requires the deep understanding of both images and texts. inside deep learning, images are typically modeled through convolutional neural networks, and texts are typically modeled through recurrent neural networks. while a requirement considering modeling images was similar to traditional computer vision tasks, such as object recognition and image classification, visual question answering raises the different need considering textual representation as compared to other natural language processing tasks. inside this work, we perform the detailed analysis on natural language questions inside visual question answering. based on a analysis, we propose to rely on convolutional neural networks considering learning textual representations. by exploring a various properties of convolutional neural networks specialized considering text data, such as width and depth, we present our ""cnn inception + gate"" model. we show that our model improves question representations and thus a overall accuracy of visual question answering models. we also show that a text representation requirement inside visual question answering was more complicated and comprehensive than that inside conventional natural language processing tasks, making it the better task to evaluate textual representation methods. shallow models like fasttext, which should obtain comparable results with deep learning models inside tasks like text classification, are not suitable inside visual question answering.",1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11057,"topological data analysis (tda) was the recent and growing branch of statistics devoted to a study of a shape of a data. inside this work we investigate a predictive power of tda inside a context of supervised learning. since topological summaries, most noticeably a persistence diagram, are typically defined inside complex spaces, we adopt the kernel idea behind the method to translate them into more familiar vector spaces. we define the topological exponential kernel, we characterize it, and we show that, despite not being positive semi-definite, it should be successfully used inside regression and classification tasks.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0
8123,"both k-essence and a pressureless perfect fluid develop caustic singularities at finite time. we further explore a connection between a two and show that they belong to a same class of models, which admits a caustic free completion by means of a canonical complex scalar field. specifically, a free massive/self-interacting complex scalar reproduces dynamics of pressureless perfect fluid/shift-symmetric k-essence under certain initial conditions inside a limit of large mass/sharp self-interacting potential. we elucidate the mechanism of resolving caustic singularities inside a complete picture. a collapse time was promoted to complex number. hence, a singularity was not developed inside real time. a same conclusion holds considering the collection of collisionless particles modelled by means of a schroedinger equation, or ultra-light axions (generically, coherent oscillations of bosons inside a bose--einstein condensate state).",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3718,"we solve a classical problem of plateau inside every metric space which was $1$-complemented inside an ultra-completion of itself. this includes all proper metric spaces as well as many locally non-compact metric spaces, inside particular, all dual banach spaces, some non-dual banach spaces such as $l^1$, all hadamard spaces, and many more. our results generalize corresponding results of lytchak and a second author from a setting of proper metric spaces to that of locally non-compact ones. we furthermore solve a dirichlet problem inside a same class of spaces. a main new ingredient inside our proofs was the suitable generalization of a rellich-kondrachov compactness theorem, from which we deduce the result about ultra-limits of sequences of sobolev maps.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8210,"inside this paper we introduce the large-scale hand pose dataset, collected with the help of the novel capture method. existing datasets are either generated synthetically or captured with the help of depth sensors: synthetic datasets exhibit the certain level of appearance difference from real depth images, and real datasets are limited inside quantity and coverage, mainly due to a difficulty to annotate them. we propose the tracking system with six 6d magnetic sensors and inverse kinematics to automatically obtain 21-joints hand pose annotations of depth maps captured with minimal restriction on a range of motion. a capture protocol aims to fully cover a natural hand pose space. as shown inside embedding plots, a new dataset exhibits the significantly wider and denser range of hand poses compared to existing benchmarks. current state-of-the-art methods are evaluated on a dataset, and we demonstrate significant improvements inside cross-benchmark performance. we also show significant improvements inside egocentric hand pose approximation with the cnn trained on a new dataset.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5852,"the practical quantum computer requires quantum bit (qubit) operations with low error rates inside extensible architectures. we study the packaging method that makes it possible to address hundreds of superconducting qubits by means of three-dimensional wires: a large-scale quantum socket. the qubit chip was housed inside the superconducting box, where both box and chip dimensions lead to unwanted modes that should interfere with qubit operations. we theoretically analyze these interference effects inside a context of qubit coherent leakage. we propose two methods to mitigate a resulting errors by detuning a resonance frequency of a modes from a qubit frequency. we perform detailed electromagnetic field simulations indicating that a resonance frequency of a modes increases with a number of installed three-dimensional wires and should be engineered to be significantly higher than a highest qubit frequency. finally, we show preliminary experimental results towards a implementation of the large-scale quantum socket.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
1535,we show the noise-induced transition inside josephson junction with fundamental as well as second harmonic. the periodically modulated multiplicative colored noise should stabilize an unstable configuration inside such the system. a stabilization of a unstable configuration has been captured inside a effective potential of a system obtained by integrating out a high-frequency components of a noise. this was the classical idea behind the method to understand a stability of an unstable configuration due to a presence of such stochasticity inside a system and our numerical analysis confirms a prediction from a analytical calculation.,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
14299,"as one type of complex networks widely-seen inside real-world application, heterogeneous information networks (hins) often encapsulate higher-order interactions that crucially reflect a complex nature among nodes and edges inside real-world data. modeling higher-order interactions inside hin facilitates a user-guided clustering problem by providing an informative collection of signals. at a same time, network motifs have been used extensively to reveal higher-order interactions and network semantics inside homogeneous networks. thus, it was natural to extend a use of motifs to hin, and we tackle a problem of user-guided clustering inside hin by with the help of motifs. we highlight a benefits of comprehensively modeling higher-order interactions instead of decomposing a complex relationships to pairwise interaction. we propose a mochin model which was applicable to arbitrary forms of hin motifs, which was often necessary considering a application scenario inside hins due to their rich and diverse semantics encapsulated inside a heterogeneity. to overcome a curse of dimensionality since a tensor size grows exponentially as a number of nodes increases inside our model, we propose an efficient inference algorithm considering mochin. inside our experiment, mochin surpasses all baselines inside three evaluation tasks under different metrics. a advantage of our model when a supervision was weak was also discussed inside additional experiments.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
5728,"we present the statistical study on a [c i]($^{3} \rm p_{1} \rightarrow {\rm ^3 p}_{0}$), [c i] ($^{3} \rm p_{2} \rightarrow {\rm ^3 p}_{1}$) lines (hereafter [c i] (1$-$0) and [c i] (2$-$1), respectively) and a co (1$-$0) line considering the sample of (ultra)luminous infrared galaxies [(u)lirgs]. we explore a correlations between a luminosities of co (1$-$0) and [c i] lines, and find that $l'_\mathrm{co(1-0)}$ correlates almost linearly with both $l'_ \mathrm{[ci](1-0)}$ and $l'_\mathrm{[ci](2-1)}$, suggesting that [c i] lines should trace total molecular gas mass at least considering (u)lirgs. we also investigate a dependence of $l'_\mathrm{[ci](1-0)}$/$l'_\mathrm{co(1-0)}$, $l'_\mathrm{[ci](2-1)}$/$l'_\mathrm{co(1-0)}$ and $l'_\mathrm{[ci](2-1)}$/$l'_\mathrm{[ci](1-0)}$ on a far-infrared color of 60-to-100 $\mu$m, and find non-correlation, the weak correlation and the modest correlation, respectively. under a assumption that these two carbon transitions are optically thin, we further calculate a [c i] line excitation temperatures, atomic carbon masses, and a mean [c i] line flux-to-h$_2$ mass conversion factors considering our sample. a resulting $\mathrm{h_2}$ masses with the help of these [c i]-based conversion factors roughly agree with those derived from $l'_\mathrm{co(1-0)}$ and co-to-h$_2$ conversion factor.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13747,"imagej was the graphical user interface (gui) driven, public domain, java-based, software package considering general image processing traditionally used mainly inside life sciences fields. a image processing capabilities of imagej are useful and extendable to other scientific fields. here we present astroimagej (aij), which provides an astronomy specific image display environment and tools considering astronomy specific image calibration and data reduction. although aij maintains a general purpose image processing capabilities of imagej, aij was streamlined considering time-series differential photometry, light curve detrending and fitting, and light curve plotting, especially considering applications requiring ultra-precise light curves (e.g., exoplanet transits). aij reads and writes standard fits files, as well as other common image formats, provides fits header viewing and editing, and was world coordinate system (wcs) aware, including an automated interface to a astrometry.net web portal considering plate solving images. aij provides research grade image calibration and analysis tools with the gui driven approach, and easily installed cross-platform compatibility. it enables new users, even at a level of undergraduate student, high school student, or amateur astronomer, to quickly start processing, modeling, and plotting astronomical image data with one tightly integrated software package.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
18894,"quantum technology was increasingly relying on specialised statistical inference methods considering analysing quantum measurement data. this motivates a development of ""quantum statistics"", the field that was shaping up at a overlap of quantum physics and ""classical"" statistics. one of a less investigated topics to date was that of statistical inference considering infinite dimensional quantum systems, which should be seen as quantum counterpart of non-parametric statistics. inside this paper we analyse a asymptotic theory of quantum statistical models consisting of ensembles of quantum systems which are identically prepared inside the pure state. inside a limit of large ensembles we establish a local asymptotic equivalence (lae) of this i.i.d. model to the quantum gaussian white noise model. we use a lae result inside order to establish minimax rates considering a approximation of pure states belonging to hermite-sobolev classes of wave functions. moreover, considering quadratic functional approximation of a same states we note an elbow effect inside a rates, whereas considering testing the pure state the sharp parametric rate was attained over a nonparametric hermite-sobolev class.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
15574,dynamic bloom filters (dbf) were proposed by guo et. al. inside 2010 to tackle a situation where a size of a set to be stored compactly was not known inside advance or should change during a course of a application. we propose the novel competitor to dbf with a following important property that dbf was not able to achieve: our structure was able to maintain the bound on a false positive rate considering a set membership query across all possible sizes of sets that are stored inside it. a new data structure we propose was the dynamic structure that we call dynamic partition bloom filter (dpbf). dpbf was based on our novel concept of the bloom partition tree which was the tree structure with standard bloom filters at a leaves. dpbf was superior to standard bloom filters because it should efficiently handle the large number of unions and intersections of sets of different sizes while controlling a false positive rate. this makes dpbf a first structure to do so to a best of our knowledge. we provide theoretical bounds comparing a false positive probability of dpbf to dbf.,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13251,"we present an algorithm to identify sparse dependence structure inside continuous and non-gaussian probability distributions, given the corresponding set of data. a conditional independence structure of an arbitrary distribution should be represented as an undirected graph (or markov random field), but most algorithms considering learning this structure are restricted to a discrete or gaussian cases. our new idea behind the method allows considering more realistic and accurate descriptions of a distribution inside question, and inside turn better estimates of its sparse markov structure. sparsity inside a graph was of interest as it should accelerate inference, improve sampling methods, and reveal important dependencies between variables. a algorithm relies on exploiting a connection between a sparsity of a graph and a sparsity of transport maps, which deterministically couple one probability measure to another.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7874,"we study a \emph{multiterminal cut} problem, which, given an $n$-vertex graph whose edges are integer-weighted and the set of terminals, asks considering the partition of a vertex set such that each terminal was inside the distinct part, and a total weight of crossing edges was at most $k$. our weapons shall be two classical results known considering decades: \emph{maximum volume minimum ($s,t$)-cuts} by [ford and fulkerson, \emph{flows inside networks}, 1962] and \emph{isolating cuts} by [dahlhaus et al., \emph{siam j. comp.} 23(4):864-894, 1994]. we sharpen these old weapons with a aid of submodular functions, and apply them to this problem, which enable us to design the more elaborated branching scheme on deciding whether the non-terminal vertex was with the terminal or not. this bounded search tree algorithm should be shown to run inside $1.84^k\cdot n^{o(1)}$ time, thereby breaking a $2^k\cdot n^{o(1)}$ barrier. as the by-product, it gives the $1.36^k\cdot n^{o(1)}$ time algorithm considering $3$-terminal cut. a preprocessing applied on non-terminal vertices might be of use considering study of this problem from other aspects.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17353,we study the generalized functional related to a pullback metrics (3). we derive a first variation formula which yield stationary maps. we introduce a stress-energy tensor which was naturally linked to conservation law and yield monotonicity formula using a coarea formula and comparison theorem inside riemannian geometry. the version of this monotonicity inequalities enables us to derive some liouville type results. also we investigate a constant dirichlet boundary value problems and a generalized chern type results considering tension field equation with respect to this functional.,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2749,"a reliability of the system of components depends on reliability of each component. thus, a initial statistical work should be a approximation of a reliability of each component of a system. this was not an easy task because when a system fails, a failure time of the given component should not be observed, that is, censored data. rodrigues et al. (2017) presented the solution considering reliability approximation of components when it was avaliable a system failure time and a status of each component at a time of system failure (if it had failed before, after or it was responsible considering system failure). however, there are situations it may be difficult to identify a status of components at a moment of system failure. such cases are systems with masked causes of failure. since parallel and series systems are a simplest systems, innumerous alternative solutions considering these two systems have been appeared inside a literature. to a best of our knowledge, this seems to be a first work that considers a general case of coherent systems. a three-parameter weibull distribution was considered as a component failure time model. identically distributed failure times was not required restrictions. furthermore, there was no restriction on a subjective choice of prior distributions but preference has been given to continuous prior distributions; these priors represent well a nuances of a environment that a system operates. a statistical work of obtaining quantities of a posterior distribution was supported by a metropolis within gibbs algorithm. with several simulations, a excellent performance of a model is evaluated. we also consider the computer hard-drives real dataset inside order to present a practical relevance of a proposed model.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
17682,"we study gapless quantum spin chains with spin 1/2 and 1: a fredkin and motzkin models. their entangled groundstates are known exactly but not their excitation spectra. we first express a groundstates inside a continuum which allows considering a calculation of spin and entanglement properties inside the unified fashion. doing so, we uncover an emergent conformal-type symmetry, thus consolidating a connection to the widely studied family of lifshitz quantum critical points inside 2d. we then obtain a low lying excited states using large-scale dmrg simulations and find that a dynamical exponent was z = 3.2 inside both cases. other excited states show the different z, indicating that these models have multiple dynamics. moreover, we modify a spin-1/2 model by adding the ferromagnetic heisenberg term, which changes a entire spectrum. we track a resulting non-trivial evolution of a dynamical exponents with the help of dmrg. finally, we exploit an exact map from a quantum hamiltonian to a non-equilibrium dynamics of the classical spin chain to shed light on a quantum dynamics.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
10659,"starting from some linear algebraic data (a weyl-group invariant bilinear form) and some arithmetic data (a bilinear steinberg symbol), we construct the cover of the kac-moody group generalizing a work of matsumoto. specializing our construction over non-archimedean local fields, considering each positive integer n we obtain a notion of $n$-fold metaplectic covers of kac-moody groups. inside this setting, we prove the casselman-shalika type formula considering whittaker functions.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0
18481,"we comprehensively show a topological structure--order of zeros felt by an electron at a positions of other electrons --for a enigmatic filling factor 5/2 at a ""pfaffian-shift"" inside the spherical geometry. the set of linearly independent antisymmetric functions that are constructed from a possible graphs preserving this topological structure provides complete basis considering determining an accurate ground state wave function considering any interaction, inside particular a coulomb interaction. one of a graphs describes $z_2$ para-fermion equivalently a pfaffian wave function, but some of a other graphs also contribute to form a exact ground state considering a coulomb interaction. we further show that our wave function considering a coulomb interaction supports clustering up to half of a composite bosons, signifying the strong-coupling regime.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
18428,"inside this paper we study a effects of hybridization inside a superconducting properties of the two-band system. we consider a cases that these bands are formed by electronic orbitals with angular momentum, such that, a hybridization $v(\mathbf{k})$ among them should be symmetric or antisymmetric under inversion symmetry. we take into account only intra-band attractive interactions inside a two bands and investigate a appearance of an induced inter-band pairing gap. we show that (inter-band) superconducting orderings are induced inside a total absence of attractive interaction between a two bands, which turns out to be completely dependent on a hybridization between them. considering a case of antisymmetric hybridization we show that a induced inter-band superconductivity has the p-wave symmetry.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
1101,"new experiments with spheres-gas (news-g) was the direct dark matter detection experiment with the help of spherical proportional counters (spcs) with light noble gases to search considering low-mass weakly interacting massive particles (wimps). we report a results from a first physics run taken at a laboratoire souterrain de modane (lsm) with sedine, the 60 cm diameter prototype spc operated with the mixture of $\mathrm{ne}+\mathrm{ch}_{4}$ (0.7 %) at 3.1 bars considering the total exposure of $9.7\;\mathrm{kg\cdot days}$. new constraints are set on a spin-independent wimp-nucleon scattering cross-section inside a sub-$\mathrm{gev/c^2}$ mass region. we exclude cross-sections above $4.4 \times \mathrm{10^{-37}\;cm^2}$ at 90 % confidence level (c.l.) considering the 0.5 $\mathrm{gev/c^2}$ wimp. a competitive results obtained with sedine are promising considering a next phase of a news-g experiment: the 140 cm diameter spc to be installed at snolab by summer 2018.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
3676,"inside this paper, we experimentally demonstrate the real-time software defined multiple input multiple output (mimo) visible light communication (vlc) system employing link adaptation of spatial multiplexing and spatial diversity. real-time mimo signal processing was implemented by with the help of a field programmable gate array (fpga) based universal software radio peripheral (usrp) devices. software defined implantation of mimo vlc should assist inside enabling an adaptive and reconfigurable communication system without hardware changes. we measured a error vector magnitude (evm), bit error rate (ber) and spectral efficiency performance considering single carrier m-qam mimo vlc with the help of spatial diversity and spatial multiplexing. results show that spatial diversity mimo vlc improves error performance at a cost of spectral efficiency that spatial multiplexing should enhance. we propose a adaptive mimo solution that both modulation schema and mimo schema are dynamically adapted to a changing channel conditions considering enhancing a error performance and spectral efficiency. a average error-free spectral efficiency of adaptive 2x2 mimo vlc achieved 12 b/s/hz over 2 meters indoor dynamic transmission.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
7735,"a current processes considering building machine learning systems require practitioners with deep knowledge of machine learning. this significantly limits a number of machine learning systems that should be created and has led to the mismatch between a demand considering machine learning systems and a ability considering organizations to build them. we believe that inside order to meet this growing demand considering machine learning systems we must significantly increase a number of individuals that should teach machines. we postulate that we should achieve this goal by making a process of teaching machines easy, fast and above all, universally accessible. while machine learning focuses on creating new algorithms and improving a accuracy of ""learners"", a machine teaching discipline focuses on a efficacy of a ""teachers"". machine teaching as the discipline was the paradigm shift that follows and extends principles of software engineering and programming languages. we put the strong emphasis on a teacher and a teacher's interaction with data, as well as crucial components such as techniques and design principles of interaction and visualization. inside this paper, we present our position regarding a discipline of machine teaching and articulate fundamental machine teaching principles. we also describe how, by decoupling knowledge about machine learning algorithms from a process of teaching, we should accelerate innovation and empower millions of new uses considering machine learning models.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3310,"a recent discovery of a planetary system hosted by a ultracool dwarf star trappist-1 could open new perspectives into a investigation of planetary climates of earth-sized exoplanets, their atmospheres and their possible habitability. inside this paper, we use the simple climate-vegetation energy-balance model to study a climate of a seven trappist-1 planets and a climate dependence on a global albedo, on a fraction of vegetation that could cover their surfaces and on a different greenhouse conditions. a model allows us to investigate whether liquid water could be maintained on a planetary surfaces (i.e., by defining the ""surface water zone"") inside different planetary conditions, with or without a presence of greenhouse effect. it was shown that planet trappist-1d seems to be a most stable from an earth-like perspective, since it resides inside a surface water zone considering the wide range of reasonable values of a model parameters. moreover, according to a model outer planets (f, g and h) cannot host liquid water on their surfaces, even considering earth-like conditions, entering the snowball state. although very simple, a model allows to extract a main features of a trappist-1 planetary climates.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3547,"a local kinematic formulas on complex space forms induce a structure of the commutative algebra on a space $\mathrm{curv}^{\mathrm{u}(n)*}$ of dual unitarily invariant curvature measures. building on a recent results from integral geometry inside complex space forms, we describe this algebra structure explicitly as the polynomial algebra. this was the short way to encode all local kinematic formulas. we then characterize a invariant valuations on complex space forms leaving a space of invariant angular curvature measures fixed.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6425,"standard dynamical system analysis of einstein-maxwell equation inside $f(r)$ theories was considered inside this work. we investigate cosmological dynamics of the uniform magnetic field inside a orthogonal spatially homogeneous (osh) bianchi type i universe with viable $f(r)$ models of gravity. inside this work, a $f(r) = r -\alpha r^n$ and $f(r) = \left( r^b - \lambda\right)^c$ models are examined by with the help of our dynamical system analysis. our results show that both of two $f(r)$ models have the viable cosmological consequence identical to a analysis present inside ref.\cite{amendola:2007nt} considering a flrw background. contrary to ref.\cite{amendola:2007nt}, we discover inside our models that there was an additional anisotropic and non-zero cosmological magnetic fields fixed point emerging before a present of a standard matter epoch. this means that a universe has initially isotropic stage with a intermediated epoch as a anisotropic background and it ends up with a isotropic late-time acceleration. a primordial magnetic fields play the crucial role of a shear evolutions obtained from these two models which have a same scaling of a cosmic time as $\sigma\sim t^{-\frac13}$, instead of $\sigma\sim t^{-1}$ considering a absence of a primordial magnetic cases.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10236,"as the firm varies a price of the product, consumers exhibit reference effects, making purchase decisions based not only on a prevailing price but also a product's price history. we consider a problem of learning such behavioral patterns as the monopolist releases, markets, and prices products. this context calls considering pricing decisions that intelligently trade off between maximizing revenue generated by the current product and probing to gain information considering future benefit. due to dependence on price history, realized demand should reflect delayed consequences of earlier pricing decisions. as such, inference entails attribution of outcomes to prior decisions and effective exploration requires planning price sequences that yield informative future outcomes. despite a considerable complexity of this problem, we offer the tractable systematic approach. inside particular, we frame a problem as one of reinforcement learning and leverage thompson sampling. we also establish the regret bound that provides graceful guarantees on how performance improves as data was gathered and how this depends on a complexity of a demand model. we illustrate merits of a idea behind the method through simulations.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4953,"large-scale deep neural networks (dnns) are both compute and memory intensive. as a size of dnns continues to grow, it was critical to improve a energy efficiency and performance while maintaining accuracy. considering dnns, a model size was an important factor affecting performance, scalability and energy efficiency. weight pruning achieves good compression ratios but suffers from three drawbacks: 1) a irregular network structure after pruning; 2) a increased training complexity; and 3) a lack of rigorous guarantee of compression ratio and inference accuracy. to overcome these limitations, this paper proposes circnn, the principled idea behind the method to represent weights and process neural networks with the help of block-circulant matrices. circnn utilizes a fast fourier transform (fft)-based fast multiplication, simultaneously reducing a computational complexity (both inside inference and training) from o(n2) to o(nlogn) and a storage complexity from o(n2) to o(n), with negligible accuracy loss. compared to other approaches, circnn was distinct due to its mathematical rigor: it should converge to a same effectiveness as dnns without compression. a circnn architecture, the universal dnn inference engine that should be implemented on various hardware/software platforms with configurable network architecture. to demonstrate a performance and energy efficiency, we test circnn inside fpga, asic and embedded processors. our results show that circnn architecture achieves very high energy efficiency and performance with the small hardware footprint. based on a fpga implementation and asic synthesis results, circnn achieves 6-102x energy efficiency improvements compared with a best state-of-the-art results.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3583,"with more and more household objects built on planned obsolescence and consumed by the fast-growing population, hazardous waste recycling has become the critical challenge. given a large variability of household waste, current recycling platforms mostly rely on human operators to analyze a scene, typically composed of many object instances piled up inside bulk. helping them by robotizing a unitary extraction was the key challenge to speed up this tedious process. whereas supervised deep learning has proven very efficient considering such object-level scene understanding, e.g., generic object detection and segmentation inside everyday scenes, it however requires large sets of per-pixel labeled images, that are hardly available considering numerous application contexts, including industrial robotics. we thus propose the step towards the practical interactive application considering generating an object-oriented robotic grasp, requiring as inputs only one depth map of a scene and one user click on a next object to extract. more precisely, we address inside this paper a middle issue of object seg-mentation inside top views of piles of bulk objects given the pixel location, namely seed, provided interactively by the human operator. we propose the twofold framework considering generating edge-driven instance segments. first, we repurpose the state-of-the-art fully convolutional object contour detector considering seed-based instance segmentation by introducing a notion of edge-mask duality with the novel patch-free and contour-oriented loss function. second, we train one model with the help of only synthetic scenes, instead of manually labeled training data. our experimental results show that considering edge-mask duality considering training an encoder-decoder network, as we suggest, outperforms the state-of-the-art patch-based network inside a present application context.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
14101,"we prove a boxing inequality: $$\mathcal{h}^{d-\alpha}_\infty(u) \leq c\alpha(1-\alpha)\int_u \int_{\mathbb{r}^{d} \setminus u} \frac{\mathrm{d}y \, \mathrm{d}z}{|y-z|^{\alpha+d}},$$ considering every $\alpha \in (0,1)$ and every bounded open subset $u \subset \mathbb{r}^d$, where $\mathcal{h}^{d-\alpha}_\infty(u)$ was a hausdorff content of $u$ of dimension $d -\alpha$ and a constant $c > 0$ depends only on $d$. we then show how this approximate implies the trace inequality inside a fractional sobolev space $w^{\alpha, 1}(\mathbb{r}^d)$ that includes sobolev's $l^{\frac{d}{d - \alpha}}$ embedding, its lorentz-space improvement, and hardy's inequality. all these estimates are thus obtained with a appropriate asymptotics as $\alpha$ tends to $0$ and $1$, recovering inside particular a classical inequalities of first order. their counterparts inside a full range $\alpha \in (0, d)$ are also investigated.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9065,"a visual explanation of learned representation of models helps to understand a fundamentals of learning. a attentional models of previous works used to visualize a attended regions over an image or text with the help of their learned weights to confirm their intended mechanism. kim et al. (2016) show that a hadamard product inside multimodal deep networks, which was well-known considering a joint function of visual question answering tasks, implicitly performs an attentional mechanism considering visual inputs. inside this work, we extend their work to show that a hadamard product inside multimodal deep networks performs not only considering visual inputs but also considering textual inputs simultaneously with the help of a proposed gradient-based visualization technique. a attentional effect of hadamard product was visualized considering both visual and textual inputs by analyzing a two inputs and an output of a hadamard product with a proposed method and compared with learned attentional weights of the visual question answering model.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
13511,"inside this paper, we study a energy efficiency (ee) maximization problem inside multiple-input multiple-output (mimo) two-way relay networks with simultaneous wireless information and power transfer (swipt). a network consists of the multiple-antenna amplify-and-forward relay node which provides bidirectional communications between two multiple-antenna transceiver nodes",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
7217,"we introduce an architecture considering large-scale image categorization that enables a end-to-end learning of separate visual features considering a different classes to distinguish. a proposed model consists of the deep cnn shaped like the tree. a stem of a tree includes the sequence of convolutional layers common to all classes. a stem then splits into multiple branches implementing parallel feature extractors, which are ultimately connected to a final classification layer using learned gated connections. these learned gates determine considering each individual class a subset of features to use. such the scheme naturally encourages a learning of the heterogeneous set of specialized features through a separate branches and it allows each class to use a subset of features that are optimal considering its recognition. we show a generality of our proposed method by reshaping several popular cnns from a literature into our proposed architecture. our experiments on a cifar100, cifar10, and synth datasets show that inside each case our resulting model yields the substantial improvement inside accuracy over a original cnn. our empirical analysis also suggests that our scheme acts as the form of beneficial regularization improving generalization performance.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10449,"we derive a 0.01 $\mu$m binned transmission spectrum, between 0.74 and 1.0 $\mu$m, of wasp-80b from low resolution spectra obtained with a fors2 instrument attached to eso's very large telescope. a combination of a fact that wasp-80 was an active star, together with instrumental and telluric factors, introduces correlated noise inside a observed transit light curves, which we treat quantitatively with the help of gaussian processes. comparison of our results together with those from previous studies, to theoretically calculated models reveals an equilibrium temperature inside agreement with a previously measured value of 825k, and the sub-solar metallicity, as well as an atmosphere depleted of molecular species with absorption bands inside a ir ($\gg 5\sigma$). our transmission spectrum alone shows evidence considering additional absorption from a potassium core and wing, whereby its presence was detected from analysis of narrow 0.003 $\mu$m bin light curves ($\gg 5\sigma$). further observations with visible and near-uv filters will be required to expand this spectrum and provide more in-depth knowledge of a atmosphere. these detections are only made possible through an instrument-dependent baseline model and the careful analysis of systematics inside a data.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10160,"we give the rigorous analysis of a statistical behavior of gradients inside the randomly initialized fully connected network n with relu activations. our results show that a empirical variance of a squares of a entries inside a input-output jacobian of n was exponential inside the simple architecture-dependent constant beta, given by a sum of a reciprocals of a hidden layer widths. when beta was large, a gradients computed by n at initialization vary wildly. our idea behind the method complements a mean field theory analysis of random networks. from this point of view, we rigorously compute finite width corrections to a statistics of gradients at a edge of chaos.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
12857,"we investigate the mixed 0-1 conic quadratic optimization problem with indicator variables arising inside mean-risk optimization. a indicator variables are often used to model non-convexities such as fixed charges or cardinality constraints. observing that a problem reduces to the submodular function minimization considering its binary restriction, we derive three classes of strong convex valid inequalities by lifting a polymatroid inequalities on a binary variables. computational experiments demonstrate a effectiveness of a inequalities inside strengthening a convex relaxations and, thereby, improving a solution times considering mean-risk problems with fixed charges and cardinality constraints significantly.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
19485,"this paper presents the class of new algorithms considering distributed statistical approximation that exploit divide-and-conquer approach. we show that one of a key benefits of a divide-and-conquer strategy was robustness, an important characteristic considering large distributed systems. we establish connections between performance of these distributed algorithms and a rates of convergence inside normal approximation, and prove non-asymptotic deviations guarantees, as well as limit theorems, considering a resulting estimators. our techniques are illustrated through several examples: inside particular, we obtain new results considering a median-of-means estimator, as well as provide performance guarantees considering distributed maximum likelihood estimation.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0
4324,"a irreversible turbulent energy cascade epitomizes strongly non-equilibrium systems. at a level of single fluid particles, time irreversibility was revealed by a asymmetry of a rate of kinetic energy change, a lagrangian power, whose moments display the power-law dependence on a reynolds number, as recently shown by xu et al. [h. xu et al, proc. natl. acad. sci. u.s.a. 111, 7558 (2014)]. here lagrangian power statistics are rationalized within a multifractal model of turbulence, whose predictions are shown to agree with numerical and empirical data. multifractal predictions are also tested, considering very large reynolds numbers, inside dynamical models of a turbulent cascade, obtaining remarkably good agreement considering statistical quantities insensitive to a asymmetry and, remarkably, deviations considering those probing a asymmetry. these findings raise fundamental questions concerning time irreversibility inside a infinite-reynolds-number limit of a navier-stokes equations.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11128,"a steadily growing use of license-free frequency bands requires reliable coexistence management considering deterministic medium utilization. considering interference mitigation, proper wireless interference identification (wii) was essential. inside this work we propose a first wii idea behind the method based upon deep convolutional neural networks (cnns). a cnn naively learns its features through self-optimization during an extensive data-driven gpu-based training process. we propose the cnn example which was based upon sensing snapshots with the limited duration of 12.8 {\mu}s and an acquisition bandwidth of 10 mhz. a cnn differs between 15 classes. they represent packet transmissions of ieee 802.11 b/g, ieee 802.15.4 and ieee 802.15.1 with overlapping frequency channels within a 2.4 ghz ism band. we show that a cnn outperforms state-of-the-art wii approaches and has the classification accuracy greater than 95% considering signal-to-noise ratio of at least -5 db.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3421,"random-effects meta-analyses have been widely applied inside evidence synthesis considering various types of medical studies to adequately address between-studies heterogeneity. however, standard inference methods considering average treatment effects (e.g., restricted maximum likelihood estimation) usually underestimate statistical errors and possibly provide highly overconfident results under realistic situations; considering instance, coverage probabilities of confidence intervals should be substantially below a nominal level. a main reason was that these inference methods rely on large sample approximations even though a number of synthesized studies was usually small or moderate inside practice. inside this article we solve this problem with the help of the unified inference method based on a monte carlo conditioning method considering broad application to random-effects meta-analysis. a developed method provides accurate confidence intervals with coverage probabilities that are almost a same as a nominal level. as specific applications, we provide accurate inference procedures considering three types of meta-analysis: conventional univariate meta-analysis considering pairwise treatment comparisons, meta-analysis of diagnostic test accuracy, and multiple treatment comparisons using network meta-analysis. we also illustrate a practical effectiveness of these methods using real data applications.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
6759,"an exhaustive ground-state analysis of extended two-dimensional (2d) correlated spin-electron model consisting of a ising spins localized on nodal lattice sites and mobile electrons delocalized over pairs of decorating sites was performed within a framework of rigorous analytical calculations. a investigated model, defined on an arbitrary 2d doubly decorated lattice, takes into account a kinetic energy of mobile electrons, a nearest-neighbor ising coupling between a localized spins and mobile electrons, a further-neighbor ising coupling between a localized spins and a zeeman energy. a ground-state phase diagrams are examined considering the wide range of model parameters considering both ferromagnetic as well as antiferromagnetic interaction between a nodal ising spins and non-zero value of external magnetic field. it was found that non-zero values of further-neighbor interaction leads to the formation of new quantum states as the consequence of competition between all considered interaction terms. moreover, a new quantum states are accompanied with different magnetic features and thus, several kinds of discontinuous field-driven phase transitions are observed.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
6388,"background and objective: stacking was an ensemble machine learning method that averages predictions from multiple other algorithms, such as generalized linear models and regression trees. the recent iteration of stacking, called super learning, has been developed as the general idea behind the method to black box supervised learning and has seen frequent usage, inside part due to a availability of an r package. i develop super learning inside a sas software system with the help of the new macro, and demonstrate its performance relative to a r package. methods: i follow closely previous work with the help of a r superlearner package and assess a performance of super learning inside the number of domains. i compare a r package with a new sas macro inside the small set of simulations assessing curve fitting inside the prediction model, the set of 14 publicly available datasets to assess cross-validated, expected loss, and data from the randomized trial of job seekers' training to assess a utility of super learning inside causal inference with the help of inverse probability weighting. results: across a simulated data and a publicly available data, a macro performed similarly to a r package, even with the different set of potential algorithms available natively inside r and sas. a example with inverse probability weighting demonstrated a ability of a sas macro to include algorithms developed inside r. conclusions: a super learner macro performs as well as a r package at the number of tasks. further, by extending a macro to include a use of r packages, a macro should leverage both a robust, enterprise oriented procedures inside sas and a nimble, cutting edge packages inside r. inside a spirit of ensemble learning, this macro extends a potential library of algorithms beyond the single software system and provides the simple avenue into machine learning inside sas.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1112,"many processes inside science and engineering should be described by partial differential equations (pdes). traditionally, pdes are derived by considering first principles of physics to derive a relations between a involved physical quantities of interest. the different idea behind the method was to measure a quantities of interest and use deep learning to reverse engineer a pdes which are describing a physical process. inside this paper we use machine learning, and deep learning inside particular, to discover pdes hidden inside complex data sets from measurement data. we include examples of data from the known model problem, and real data from weather station measurements. we show how necessary transformations of a input data amounts to coordinate transformations inside a discovered pde, and we elaborate on feature and model selection. it was shown that a dynamics of the non-linear, second order pde should be accurately described by an ordinary differential equation which was automatically discovered by our deep learning algorithm. even more interestingly, we show that similar results apply inside a context of more complex simulations of a swedish temperature distribution.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
333,"inside this note, let $\a$ be the finitary hereditary abelian category with enough projectives. by with the help of a associativity formula of hall algebras, we give the new and simple proof of a main theorem inside \cite{yan}, which states that a bridgeland's hall algebra of 2-cyclic complexes of projective objects inside $\a$ was isomorphic to a drinfeld double hall algebra of $\a$. inside the similar way, we give the simplification of a key step inside a proof of theorem 4.11 inside \cite{lp}.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
17843,"walking quadruped robots face challenges inside positioning their feet and lifting their legs during gait cycles over uneven terrain. a robot laika was under development as the quadruped with the flexible, actuated spine designed to assist with foot movement and balance during these gaits. this paper presents a first set of hardware designs considering a spine of laika, the physical prototype of those designs, and tests inside both hardware and simulations that show a prototype's capabilities. laika's spine was the tensegrity structure, used considering its advantages with weight and force distribution, and represents a first working prototype of the tensegrity spine considering the quadruped robot. a spine bends by adjusting a lengths of a cables that separate its vertebrae, and twists with the help of an actuated rotating vertebra at its center. a current prototype of laika has stiff legs attached to a spine, and was used as the test setup considering evaluation of a spine itself. this work shows a advantages of laika's spine by demonstrating a spine lifting each of a robot's four feet, both as the form of balancing and as the precursor considering the walking gait. these foot motions, with the help of specific combinations of bending and rotation movements of a spine, are measured inside both simulation and hardware experiments. hardware data are used to calibrate a simulations, such that a simulations should be used considering control of balancing or gait cycles inside a future. future work will attach actuated legs to laika's spine, and examine balancing and gait cycles when combined with leg movements.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
15058,"a performance of neural network (nn)-based language models was steadily improving due to a emergence of new architectures, which are able to learn different natural language characteristics. this paper presents the novel framework, which shows that the significant improvement should be achieved by combining different existing heterogeneous models inside the single architecture. this was done through 1) the feature layer, which separately learns different nn-based models and 2) the mixture layer, which merges a resulting model features. inside doing so, this architecture benefits from a learning capabilities of each model with no noticeable increase inside a number of model parameters or a training time. extensive experiments conducted on a penn treebank (ptb) and a large text compression benchmark (ltcb) corpus showed the significant reduction of a perplexity when compared to state-of-the-art feedforward as well as recurrent neural network architectures.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6929,"we elaborate the quantum model considering a meaning associated with corpora of written documents, like a pages forming a world wide web. to that end, we are guided by how physicists constructed quantum theory considering microscopic entities, which unlike classical objects cannot be fully represented inside our spatial theater. we suggest that the similar construction needs to be carried out by linguists and computational scientists, to capture a full meaning carried by collections of documental entities. more precisely, we show how to associate the quantum-like 'entity of meaning' to the 'language entity formed by printed documents', considering a latter as a collection of traces that are left by a former, inside specific results of search actions that we describe as measurements. inside other words, we offer the perspective where the collection of documents, like a web, was described as a space of manifestation of the more complex entity - a qweb - which was a object of our modeling, drawing its inspiration from previous studies on operational-realistic approaches to quantum physics and quantum modeling of human cognition and decision-making. we emphasize that the consistent qweb model needs to account considering a observed correlations between words appearing inside printed documents, e.g., co-occurrences, as a latter would depend on a 'meaning connections' existing between a concepts that are associated with these words. inside that respect, we show that both 'context and interference (quantum) effects' are required to explain a probabilities calculated by counting a relative number of documents containing certain words and co-ocurrrences of words.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
575,"a problem of fake news has gained the lot of attention as it was claimed to have had the significant impact on 2016 us presidential elections. fake news was not the new problem and its spread inside social networks was well-studied. often an underlying assumption inside fake news discussion was that it was written to look like real news, fooling a reader who does not check considering reliability of a sources or a arguments inside its content. through the unique study of three data sets and features that capture a style and a language of articles, we show that this assumption was not true. fake news inside most cases was more similar to satire than to real news, leading us to conclude that persuasion inside fake news was achieved through heuristics rather than a strength of arguments. we show overall title structure and a use of proper nouns inside titles are very significant inside differentiating fake from real. this leads us to conclude that fake news was targeted considering audiences who are not likely to read beyond titles and was aimed at creating mental associations between entities and claims.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
11942,"inside this paper, we argue that a future of artificial intelligence research resides inside two keywords: integration and embodiment. we support this claim by analyzing a recent advances of a field. regarding integration, we note that a most impactful recent contributions have been made possible through a integration of recent machine learning methods (based inside particular on deep learning and recurrent neural networks) with more traditional ones (e.g. monte-carlo tree search, goal babbling exploration or addressable memory systems). regarding embodiment, we note that a traditional benchmark tasks (e.g. visual classification or board games) are becoming obsolete as state-of-the-art learning algorithms idea behind the method or even surpass human performance inside most of them, having recently encouraged a development of first-person 3d game platforms embedding realistic physics. building upon this analysis, we first propose an embodied cognitive architecture integrating heterogenous sub-fields of artificial intelligence into the unified framework. we demonstrate a utility of our idea behind the method by showing how major contributions of a field should be expressed within a proposed framework. we then claim that benchmarking environments need to reproduce ecologically-valid conditions considering bootstrapping a acquisition of increasingly complex cognitive skills through a concept of the cognitive arms race between embodied agents.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14769,"nowadays, robots become the companion inside everyday life. to be well-accepted by humans, robots should efficiently understand meanings of their partners' motions and body language, and respond accordingly. learning concepts by imitation brings them this ability inside the user-friendly way. this paper presents the fast and robust model considering incremental learning of concepts by imitation (iloci). inside iloci, observed multimodal spatio-temporal demonstrations are incrementally abstracted and generalized based on both their perceptual and functional similarities during a imitation. inside this method, perceptually similar demonstrations are abstracted by the dynamic model of mirror neuron system. an incremental method was proposed to learn their functional similarities through the limited number of interactions with a teacher. learning all concepts together by a proposed memory rehearsal enables robot to utilize a common structural relations among concepts which not only expedites a learning process especially at a initial stages, but also improves a generalization ability and a robustness against discrepancies between observed demonstrations. performance of iloci was assessed with the help of standard lasa handwriting benchmark data set. a results show efficiency of iloci inside concept acquisition, recognition and generation inside addition to its robustness against variability inside demonstrations.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
4895,"inside this paper we introduce a deep kernelized autoencoder, the neural network model that allows an explicit approximation of (i) a mapping from an input space to an arbitrary, user-specified kernel space and (ii) a back-projection from such the kernel space to input space. a proposed method was based on traditional autoencoders and was trained through the new unsupervised loss function. during training, we optimize both a reconstruction accuracy of input samples and a alignment between the kernel matrix given as prior and a inner products of a hidden representations computed by a autoencoder. kernel alignment provides control over a hidden representation learned by a autoencoder. experiments have been performed to evaluate both reconstruction and kernel alignment performance. additionally, we applied our method to emulate kpca on the denoising task obtaining promising results.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1018,"this work proposes the solution considering a longitudinal and lateral control problem of urban autonomous vehicles with the help of the gain scheduling lpv control approach. with the help of a kinematic and dynamic vehicle models, the linear parameter varying (lpv) representation was adopted and the cascade control methodology was proposed considering controlling both vehicle behaviours. inside particular, considering a control design, a use of both models separately lead to solve two lpv lmi-lqr problems. furthermore, to achieve a desired levels of performance, an idea behind the method based on cascade design of a a kinematic and dynamic controllers has been proposed. this cascade control scheme was based on a idea that a dynamic closed loop behaviour was designed to be faster than a kinematic closed loop one. a obtained gain scheduling lpv control approach, jointly with the trajectory generation module, has presented suitable results inside the simulated city driving scenario.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
2008,"inside typical reinforcement learning (rl), a environment was assumed given and a goal of a learning was to identify an optimal policy considering a agent taking actions through its interactions with a environment. inside this paper, we extend this setting by considering a environment was not given, but controllable and learnable through its interaction with a agent at a same time. this extension was motivated by environment design scenarios inside a real-world, including game design, shopping space design and traffic signal design. theoretically, we find the dual markov decision process (mdp) w.r.t. a environment to that w.r.t. a agent, and derive the policy gradient solution to optimizing a parametrized environment. furthermore, discontinuous environments are addressed by the proposed general generative framework. our experiments on the maze game design task show a effectiveness of a proposed algorithms inside generating diverse and challenging mazes against various agent settings.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11748,"a classic double bubble theorem says that a least-perimeter way to enclose and separate two prescribed volumes inside $\mathbb{r}^n$ was a standard double bubble. we seek a optimal double bubble inside $\mathbb{r}^n$ with density, which we assume to be strictly log-convex. considering $n=1$ we show that a solution was sometimes two contiguous intervals and sometimes three contiguous intervals. inside higher dimensions, we think that a solution was sometimes the standard double bubble and sometimes concentric spheres (e.g. considering one volume small and a other large).",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17186,"geodesic balls inside the simply connected space forms $\mathbb{s}^n$, $\mathbb{r}^{n}$ or $\mathbb{h}^{n}$ are distinguished manifolds considering comparison inside bounded riemannian geometry. inside this paper we show that they have a maximum possible boundary volume among miao-tam critical metrics with connected boundary provided that a boundary of a manifold was an einstein hypersurface. inside a same spirit we also extend the rigidity theorem due to boucher et al. \cite{bou} and shen \cite{shen} to $n$-dimensional static metrics with positive constant scalar curvature, which provides another proof of the partial answer to a cosmic no-hair conjecture previously obtained by chru≈õciel \cite{chrus}.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13257,"exploiting a deep generative model's remarkable ability of learning a data-manifold structure, some recent researches proposed the geometric data interpolation method based on a geodesic curves on a learned data-manifold. however, this interpolation method often gives poor results due to the topological difference between a model and a dataset. a model defines the family of simply-connected manifolds, whereas a dataset generally contains disconnected regions or holes that make them non-simply-connected. to compensate this difference, we propose the novel density regularizer that make a interpolation path circumvent a holes denoted by low probability density. we confirm that our method gives consistently better interpolation results from a experiments with real-world image datasets.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
9011,"a hubble catalog of variables (hcv) project aims to identify a variable sources inside a hubble source catalog (hsc), which includes about 92 million objects with over 300 million measurements detected by a wfpc2, acs and wfc3 cameras on board of a hubble space telescope (hst), by with the help of an automated pipeline containing the set of detection and validation algorithms. all a hsc sources with more than the predefined number of measurements inside the single filter/instrument combination are pre-processed to correct systematic effect and to remove a bad measurements. a corrected data are used to compute the number of variability indexes to determine a variability status of each source. a final variable source catalog will contain variables stars, active galactic nuclei (agns), supernovae (sns) or even new types of variables, reaching an unprecedented depth (v$\leq$27 mag). at a end of a project, a first release of a hcv will be available at a mikulski archive considering space telescopes (mast) and a esa hubble science archives. a hcv pipeline will be deployed at a space telescope science institute (stsci) so that an updated hcv may be generated following future releases of hsc.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
17595,"this book chapter introduces regression approaches and regression adjustment considering approximate bayesian computation (abc). regression adjustment adjusts parameter values after rejection sampling inside order to account considering a imperfect match between simulations and observations. imperfect match between simulations and observations should be more pronounced when there are many summary statistics, the phenomenon coined as a curse of dimensionality. because of this imperfect match, credibility intervals obtained with regression approaches should be inflated compared to true credibility intervals. a chapter presents a main concepts underlying regression adjustment. the theorem that compares theoretical properties of posterior distributions obtained with and without regression adjustment was presented. last, the practical application of regression adjustment inside population genetics shows that regression adjustment shrinks posterior distributions compared to rejection approaches, which was the solution to avoid inflated credibility intervals.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
3327,"psychiatric neuroscience was increasingly aware of a need to define psychopathology inside terms of abnormal neural computation. a central tool inside this endeavour was a fitting of computational models to behavioural data. a most prominent example of this procedure was fitting reinforcement learning (rl) models to decision-making data collected from mentally ill and healthy subject populations. these models are generative models of a decision-making data themselves, and a parameters we seek to infer should be psychologically and neurobiologically meaningful. currently, a gold standard idea behind the method to this inference procedure involves monte-carlo sampling, which was robust but computationally intensive---rendering additional procedures, such as cross-validation, impractical. searching considering point estimates of model parameters with the help of optimization procedures remains the popular and interesting option. on the novel testbed simulating parameter approximation from the common rl task, we investigated a effects of smooth vs. boundary constraints on parameter approximation with the help of interior point and deterministic direct search algorithms considering optimization. ultimately, we show that a use of boundary constraints should lead to substantial truncation effects. our results discourage a use of boundary constraints considering these applications.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2207,"retrieving spoken content with spoken queries, or query-by- example spoken term detection (std), was attractive because it makes possible a matching of signals directly on a acoustic level without transcribing them into text. here, we propose an end-to-end query-by-example std model based on an attention-based multi-hop network, whose input was the spoken query and an audio segment containing several utterances; a output states whether a audio segment includes a query. a model should be trained inside either the supervised scenario with the help of labeled data, or inside an unsupervised fashion. inside a supervised scenario, we find that a attention mechanism and multiple hops improve performance, and that a attention weights indicate a time span of a detected terms. inside a unsupervised setting, a model mimics a behavior of a existing query-by-example std system, yielding performance comparable to a existing system but with the lower search time complexity.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16719,"to harness a complexity of their high-dimensional bodies during sensorimotor development, infants are guided by patterns of freezing and freeing of degrees of freedom. considering instance, when learning to reach, infants free a degrees of freedom inside their arm proximodistally, i.e. from joints that are closer to a body to those that are more distant. here, we formulate and study computationally a hypothesis that such patterns should emerge spontaneously as a result of the family of stochastic optimization processes (evolution strategies with covariance-matrix adaptation), without an innate encoding of the maturational schedule. inside particular, we present simulated experiments with an arm where the computational learner progressively acquires reaching skills through adaptive exploration, and we show that the proximodistal organization appears spontaneously, which we denote pdff (proximodistal freezing and freeing of degrees of freedom). we also compare this emergent organization between different arm morphologies -- from human-like to quite unnatural ones -- to study a effect of different kinematic structures on a emergence of pdff. keywords: human motor learning; proximo-distal exploration; stochastic optimization; modelling; evolution strategies; cross-entropy methods; policy search; morphology.}",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
17968,"a results of a probabilistic analysis of a direct numerical simulations of irregular unidirectional deep-water waves are discussed. it was shown that an occurrence of large-amplitude soliton-like groups represents an extraordinary case, which was able to increase noticeably a probability of high waves even inside moderately rough sea conditions. a ensemble of wave realizations should be large enough to take these rare events into account. thus we provide the striking example when long-living coherent structures make a water wave statistics extreme.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6468,"markov chain monte carlo based bayesian data analysis has now become a method of choice considering analyzing and interpreting data inside almost all disciplines of science. inside astronomy, over a last decade, we have also seen the steady increase inside a number of papers that employ monte carlo based bayesian analysis. new, efficient monte carlo based methods are continuously being developed and explored. inside this review, we first explain a basics of bayesian theory and discuss how to set up data analysis problems within this framework. next, we provide an overview of various monte carlo based methods considering performing bayesian data analysis. finally, we discuss advanced ideas that enable us to tackle complex problems and thus hold great promise considering a future. we also distribute downloadable computer software (available at this https url ) that implements some of a algorithms and examples discussed here.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
10642,"we consider the generalization of elliptic multiple zeta values, which we call twisted elliptic multiple zeta values. these arise as iterated integrals on an elliptic curve from which the rational lattice has been removed. at a cusp, twisted elliptic multiple zeta values are shown to degenerate to cyclotomic multiple zeta values inside a same way as elliptic multiple zeta values degenerate to classical multiple zeta values. we investigate properties of twisted elliptic multiple zeta values and consider them inside a context of a non-planar part of a four-point one-loop open-string amplitude.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
6432,"we consider a problem of identity testing and recovering (that is, interpolating) of the ""hidden"" monic polynomials $f$, given an oracle access to $f(x)^e$ considering $x\in\mathbb f_q$, where $\mathbb f_q$ was a finite field of $q$ elements and an extension fields access was not permitted. a naive interpolation algorithm needs $de+1$ queries, where $d =\max\{{\rm deg}\ f, {\rm deg }\ g\}$ and thus requires $ de<q$. considering the prime $q = p$, we design an algorithm that was asymptotically better inside certain cases, especially when $d$ was large. a algorithm was based on the result of independent interest inside spirit of additive combinatorics. it gives an upper bound on a number of values of the rational function of large degree, evaluated on the short sequence of consecutive integers, that belong to the small subgroup of $\mathbb f_p^*$.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
9344,"legged robots should outperform wheeled machines considering most navigation tasks across unknown and rough terrains. considering such tasks, visual feedback was the fundamental asset to provide robots with terrain-awareness. however, robust dynamic locomotion on difficult terrains with real-time performance guarantees remains the challenge. indeed, a computational effort demanded by visual processing limits a potential considering realtime control and planning strategies. inside this paper, we present the real-time, dynamic foothold adaptation strategy based on visual feedback. our method adjusts a landing position of a feet inside the fully reactive manner, with the help of only on-board computers and sensors. a correction was computed and executed continuously along a swing phase trajectory of each leg. to efficiently adapt a landing position, we implement the self-supervised foothold classifier based on the convolutional neural network (cnn). a training set was automatically generated by the heuristic algorithm that jointly evaluates terrain morphology, kinematics, and leg collisions. our method results inside an up to 200 times faster computation with respect to a full-blown heuristics. our goal was to react to visual stimuli from a environment, bridging a gap between blind reactive locomotion and purely vision-based planning strategies. we assess a performance of our method on a dynamic quadruped robot hyq, executing static and dynamic gaits (at speeds up to 0.5 m/s) inside both simulated and real scenarios; a benefit of safe foothold adaptation was clearly demonstrated by a overall robot behavior.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
7522,"we prove two rigidity theorems considering maps between riemannian manifolds. first, we prove that the lipschitz map $f:m\to n$ between two oriented riemannian manifolds, whose differential was almost everywhere an orientation-preserving isometry, was an isometric immersion. this theorem is previously proved with the help of regularity theory considering conformal maps; we give the new, simple proof, by generalizing a piola identity considering a cofactor operator. second, we prove that if there exists the sequence of mapping $f_n:m\to n$, whose differentials converge inside $l^p$ to a set of orientation-preserving isometries, then there exists the subsequence converging to an isometric immersion. these results are generalizations of celebrated rigidity theorems by liouville (1850) and reshetnyak (1967) from euclidean to riemannian settings. finally, we describe applications of these theorems to non-euclidean elasticity and to convergence notions of manifolds.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3575,"turbulent spots surrounded by laminar flow are the landmark of transitional shear flows, but a dependence of their kinematic properties on spatial structure was poorly understood. we here investigate this dependence inside pipe flow considering reynolds numbers between 1500 and 5000. we compute spatially localized relative periodic orbits inside long pipes and show that their upstream and downstream fronts decay exponentially towards a laminar profile. this allows to model a fronts by employing a linearized navier-stokes equations, and a resulting model yields a spatial decay rate and a front velocity profiles of a periodic orbits as the function of reynolds number, azimuthal wave number and propagation speed. inside addition, when applied to the localized turbulent puff, a model was shown to accurately approximate a spatial decay rate of its upstream and downstream tails. our study provides insight into a relationship between a kinematics and spatial structure of localized turbulence and more generally into a physics of localization.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18555,"advertisements are unavoidable inside modern society. times square was notorious considering its incessant display of advertisements. its popularity was worldwide and smaller cities possess miniature versions of a display, such as pittsburgh and its digital works inside oakland on forbes avenue. tokyo's ginza district recently rose to popularity due to its upscale shops and constant onslaught of advertisements to pedestrians. advertisements arise inside other mediums as well. considering example, they aid popular streaming services, such as spotify, hulu, and youtube tv gather significant streams of revenue to reduce a cost of monthly subscriptions considering consumers. ads provide an additional source of money considering companies and entire industries to allocate resources toward alternative business motives. they are attractive to companies and nearly unavoidable considering consumers. one challenge considering advertisers was examining the advertisement's effectiveness or usefulness inside conveying the message to their targeted demographics. rather than constructing the single, static image of content, the video advertisement possesses hundreds of frames of data with varying scenes, actors, objects, and complexity. therefore, measuring effectiveness of video advertisements was important to impacting the billion-dollar industry. this paper explores a combination of human-annotated features and common video processing techniques to predict effectiveness ratings of advertisements collected from youtube. this task was seen as the binary (effective vs. non-effective), four-way, and five-way machine learning classification task. a first findings inside terms of accuracy and inference on this dataset, as well as some of a first ad research, on the small dataset are presented. accuracies of 84\%, 65\%, and 55\% are reached on a binary, four-way, and five-way tasks respectively.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10890,"with machine learning successfully applied to new daunting problems almost every day, general ai starts looking like an attainable goal. however, most current research focuses instead on important but narrow applications, such as image classification or machine translation. we believe this to be largely due to a lack of objective ways to measure progress towards broad machine intelligence. inside order to fill this gap, we propose here the set of concrete desiderata considering general ai, together with the platform to test machines on how well they satisfy such desiderata, while keeping all further complexities to the minimum.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
9926,"a increase of vehicle inside highways may cause traffic congestion as well as inside a normal roadways. predicting a traffic flow inside highways especially, was demanded to solve this congestion problem. predictions on time-series multivariate data, such as inside a traffic flow dataset, have been largely accomplished through various approaches. a idea behind the method with conventional prediction algorithms, such as with support vector machine (svm), was only capable of accommodating predictions that are independent inside each time unit. hence, a sequential relationships inside this time series data was hardly explored. continuous conditional random field (ccrf) was one of probabilistic graphical model (pgm) algorithms which should accommodate this problem. a neighboring aspects of sequential data such as inside a time series data should be expressed by ccrf so that its predictions are more reliable. inside this article, the novel idea behind the method called dm-ccrf was adopted by modifying a ccrf prediction algorithm to strengthen a probability of a predictions made by a baseline regressor. a result shows that dm-ccrf was superior inside performance compared to ccrf. this was validated by a error decrease of a baseline up to 9% significance. this was twice a standard ccrf performance which should only decrease baseline error by 4.582% at most.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
711,we present the generalization bound considering feedforward neural networks inside terms of a product of a spectral norm of a layers and a frobenius norm of a weights. a generalization bound was derived with the help of the pac-bayes analysis.,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7585,"existing approaches to online convex optimization (oco) make sequential one-slot-ahead decisions, which lead to (possibly adversarial) losses that drive subsequent decision iterates. their performance was evaluated by a so-called regret that measures a difference of losses between a online solution and a best yet fixed overall solution inside hindsight. a present paper deals with online convex optimization involving adversarial loss functions and adversarial constraints, where a constraints are revealed after making decisions, and should be tolerable to instantaneous violations but must be satisfied inside a long term. performance of an online algorithm inside this setting was assessed by: i) a difference of its losses relative to a best dynamic solution with one-slot-ahead information of a loss function and a constraint (that was here termed dynamic regret); and, ii) a accumulated amount of constraint violations (that was here termed dynamic fit). inside this context, the modified online saddle-point (mosp) scheme was developed, and proved to simultaneously yield sub-linear dynamic regret and fit, provided that a accumulated variations of per-slot minimizers and constraints are sub-linearly growing with time. mosp was also applied to a dynamic network resource allocation task, and it was compared with a well-known stochastic dual gradient method. under various scenarios, numerical experiments demonstrate a performance gain of mosp relative to a state-of-the-art.",1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1
5857,"decades of psychological research have been aimed at modeling how people learn features and categories. a empirical validation of these theories was often based on artificial stimuli with simple representations. recently, deep neural networks have reached or surpassed human accuracy on tasks such as identifying objects inside natural images. these networks learn representations of real-world stimuli that should potentially be leveraged to capture psychological representations. we find that state-of-the-art object classification networks provide surprisingly accurate predictions of human similarity judgments considering natural images, but fail to capture some of a structure represented by people. we show that the simple transformation that corrects these discrepancies should be obtained through convex optimization. we use a resulting representations to predict a difficulty of learning novel categories of natural images. our results extend a scope of psychological experiments and computational modeling by enabling tractable use of large natural stimulus sets.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12369,"the magnetic impurity coupled to the superconductor gives rise to the yu-shiba-rusinov (ysr) state in a superconducting energy gap. with increasing exchange coupling a excitation energy of this state eventually crosses zero and a system switches to the ysr groundstate with bound quasiparticles screening a impurity spin by $\hbar/2$. here we explore inas nanowire double quantum dots tunnel coupled to the superconductor and demonstrate ysr screening of spin-1/2 and spin-1 states. gating a double dot through 9 different charge states, we show that a honeycomb pattern of zero-bias conductance peaks, archetypal of double dots coupled to normal leads, was replaced by lines of zero-energy ysr states. these enclose regions of ysr-screened dot spins displaying distinctive spectral features, and their characteristic shape and topology change markedly with tunnel coupling strengths. we find excellent agreement with the simple zero-bandwidth approximation, and with numerical renormalization group calculations considering a two-orbital anderson model.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
514,"inside this paper, we study neural cognition inside social network. the stochastic model was introduced and shown to incorporate two well-known models inside pavlovian conditioning and social networks as special case, namely rescorla-wagner model and friedkin-johnsen model. a interpretation and comparison of these model are discussed. we consider two cases when a disturbance was independent identical distributed considering all time and when a distribution of a random variable evolves according to the markov chain. we show that a systems considering both cases are mean square stable and a expectation of a states converges to consensus.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
19668,"injuries have the great impact on professional soccer, due to their large influence on team performance and a considerable costs of rehabilitation considering players. existing studies inside a literature provide just the preliminary understanding of which factors mostly affect injury risk, while an evaluation of a potential of statistical models inside forecasting injuries was still missing. inside this paper, we propose the multi-dimensional idea behind the method to injury forecasting inside professional soccer that was based on gps measurements and machine learning. by with the help of gps tracking technology, we collect data describing a training workload of players inside the professional soccer club during the season. we then construct an injury forecaster and show that it was both accurate and interpretable by providing the set of case studies of interest to soccer practitioners. our idea behind the method opens the novel perspective on injury prevention, providing the set of simple and practical rules considering evaluating and interpreting a complex relations between injury risk and training performance inside professional soccer.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8790,"inside this paper, the comparison of reinforcement learning algorithms and their performance on the robot box pushing task was provided. a robot box pushing problem was structured as both the single-agent problem and also the multi-agent problem. the q-learning algorithm was applied to a single-agent box pushing problem, and three different q-learning algorithms are applied to a multi-agent box pushing problem. both sets of algorithms are applied on the dynamic environment that was comprised of static objects, the static goal location, the dynamic box location, and dynamic agent positions. the simulation environment was developed to test a four algorithms, and their performance was compared through graphical explanations of test results. a comparison shows that a newly applied reinforcement algorithm out-performs a previously applied algorithms on a robot box pushing problem inside the dynamic environment.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
4165,"we show that considering any c^0 jordan curve c inside a sphere at infinity of h^3, there exists an embedded $h$-plane p_h inside h^3 with asymptotic boundary c considering any h inside (-1,1). as the corollary, we proved that any quasi-fuchsian hyperbolic 3-manifold m=sxr contains an h-surface s_h inside a homotopy class of a core surface s considering any h inside (-1,1). we also proved that considering any c^1 jordan curve j inside a sphere at infinity, there exists the unique minimizing h-plane p_h with asymptotic boundary j considering the generic h inside (-1,1).",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5246,"we establish a well-posedness of the coupled micro-macro parabolic-elliptic system modeling a interplay between two pressures inside the gas-liquid mixture close to equilibrium that was filling the porous media with distributed microstructures. additionally, we prove the local stability approximate considering a inverse micro-macro robin problem, potentially useful inside identifying quantitatively the micro-macro interfacial robin transfer coefficient given microscopic measurements on accessible fixed interfaces. to tackle a solvability issue we use two-scale energy estimates and two-scale regularity/compactness arguments cast inside a schauder's fixed point theorem. the number of auxiliary problems, regularity, and scaling arguments are used inside ensuring a suitable fr√©chet differentiability of a solution and a structure of a inverse stability estimate.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15987,"across smart-grid and smart-city applications, there are problems where an ensemble of agents was to be controlled such that both a aggregate behaviour and individual-level perception of a system's performance are acceptable. inside many applications, traditional pi control was used to regulate aggregate ensemble performance. our principal contribution inside this note was to demonstrate that pi control may not be always suitable considering this purpose, and inside some situations may lead to the loss of ergodicity considering closed-loop systems. building on this observation, the theoretical framework was proposed to both analyse and design control systems considering a regulation of large scale ensembles of agents with the probabilistic intent. examples are given to illustrate our results.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
12378,"we give the microscopic derivation of time-dependent correlation functions of a $1d$ cubic nonlinear schr√∂dinger equation (nls) from many-body quantum theory. a starting point of our proof was our previous work on a time-independent problem and work of a second author on a corresponding problem on the finite lattice. an important new obstacle inside our analysis was a need to work with the cutoff inside a number of particles, which breaks a gaussian structure of a free quantum field and prevents a use of a wick theorem. we overcome it by a means of complex analytic methods. our methods apply to a nonlocal nls with bounded convolution potential. inside a periodic setting, we also consider a local nls, arising from short-range interactions inside a many-body setting. to that end, we need a dispersion of a nls inside a form of periodic strichartz estimates inside $x^{s,b}$ spaces.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11221,"we consider a abelian-higgs model inside 2+1 dimensions with instanton-monopole defects. this model was closely related to a phases of quantum anti-ferromagnets. inside a presence of $\mathbb{z}_2$ preserving monopole operators, there are two confining ground states inside a monopole phase, corresponding to a valence bond solid (vbs) phase of quantum magnets. we show that a domain-wall carries the 't hooft anomaly inside this case. a anomaly should be saturated by, e.g., charge-conjugation breaking on a wall or by a domain wall theory becoming gapless (a gapless model that saturates a anomaly was $su(2)_1$ wzw). either way a fundamental scalar particles (i.e. spinons) which are confined inside a bulk are deconfined on a domain-wall. this $\mathbb{z}_2$ phase should be realized either with spin-1/2 on the rectangular lattice, or spin-1 on the square lattice. inside both cases a domain wall contains spin-1/2 particles (which are absent inside a bulk). we discuss a possible relation to recent lattice simulations of domain walls inside vbs. we further generalize a discussion to abrikosov-nielsen-olsen (ano) vortices inside the dual superconductor of a abelian-higgs model inside 3+1 dimensions, and to a easy-plane limit of anti-ferromagnets. inside a latter case a wall should undergo the variant of a bkt transition (consistent with a anomalies) while a bulk was still gapped. a same was true considering a easy-axis limit of anti-ferromagnets. we also touch upon some analogies to yang-mills theory.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
6037,"after thirty years of a discovery of a fundamental plane, explanations to a tilt of a fundamental plane with respect to a virial plane still suffer from a need of fine-tuning. inside this paper, we try to explore a origin of this tilt from a perspective of modified newtonian dynamics (mond) by applying a 16 coma galaxies available inside thomas et al.[1]. based on a mass models that should reproduce de vaucouleurs' law closely, we find that a tilt of a traditional fundamental plane was naturally explained by a simple form of a mondian interpolating function, if we assume the well motivated choice of anisotropic velocity distribution, and adopt a kroupa or salpeter stellar mass-to-light ratio. our analysis does not necessarily rule out the varying stellar mass-to-light ratio.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3305,"considering binary $[n,k,d]$ linear locally repairable codes (lrcs), two new upper bounds on $k$ are derived. a first one applies to lrcs with disjoint local repair groups, considering general values of $n,d$ and locality $r$, containing some previously known bounds as special cases. a second one was based on solving an optimization problem and applies to lrcs with arbitrary structure of local repair groups. particularly, an explicit bound was derived from a second bound when $d\geq 5$. the specific comparison shows this explicit bound outperforms a cadambe-mazumdar bound considering $5\leq d\leq 8$ and large values of $n$. moreover, the construction of binary linear lrcs with $d\geq6$ attaining our second bound was provided.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
5203,inside this paper we study a a.e. exponential strong summability problem considering a rectangular partial sums of double trigonometric fourier series of a functions from $l\log l$ .,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4708,"matrix completion models are among a most common formulations of recommender systems. recent works have showed the boost of performance of these techniques when introducing a pairwise relationships between users/items inside a form of graphs, and imposing smoothness priors on these graphs. however, such techniques do not fully exploit a local stationarity structures of user/item graphs, and a number of parameters to learn was linear w.r.t. a number of users and items. we propose the novel idea behind the method to overcome these limitations by with the help of geometric deep learning on graphs. our matrix completion architecture combines graph convolutional neural networks and recurrent neural networks to learn meaningful statistical graph-structured patterns and a non-linear diffusion process that generates a known ratings. this neural network system requires the constant number of parameters independent of a matrix size. we apply our method on both synthetic and real datasets, showing that it outperforms state-of-the-art techniques.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15668,"early detection of cyber-attacks was crucial considering the safe and reliable operation of a smart grid. inside a literature, outlier detection schemes making sample-by-sample decisions and online detection schemes requiring perfect attack models have been proposed. inside this paper, we formulate a online attack/anomaly detection problem as the partially observable markov decision process (pomdp) problem and propose the universal robust online detection algorithm with the help of a framework of model-free reinforcement learning (rl) considering pomdps. numerical studies illustrate a effectiveness of a proposed rl-based algorithm inside timely and accurate detection of cyber-attacks targeting a smart grid.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8623,"presented data contains a record of five spreading campaigns that occurred inside the virtual world platform. users distributed avatars between each other during a campaigns. a processes varied inside time and range and were either incentivized or not incentivized. campaign data was accompanied by events. a data should be used to build the multilayer network to place a campaigns inside the wider context. to a best of a authors knowledge, a study was a first publicly available dataset containing the complete real multilayer social network together, along with five complete spreading processes inside it.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
3370,"deep reinforcement learning (rl) algorithms should learn complex robotic skills from raw sensory inputs, but have yet to achieve a kind of broad generalization and applicability demonstrated by deep learning methods inside supervised domains. we present the deep rl method that was practical considering real-world robotics tasks, such as robotic manipulation, and generalizes effectively to never-before-seen tasks and objects. inside these settings, ground truth reward signals are typically unavailable, and we therefore propose the self-supervised model-based approach, where the predictive model learns to directly predict a future from raw sensory readings, such as camera images. at test time, we explore three distinct goal specification methods: designated pixels, where the user specifies desired object manipulation tasks by selecting particular pixels inside an image and corresponding goal positions, goal images, where a desired goal state was specified with an image, and image classifiers, which define spaces of goal states. our deep predictive models are trained with the help of data collected autonomously and continuously by the robot interacting with hundreds of objects, without human supervision. we demonstrate that visual mpc should generalize to never-before-seen objects---both rigid and deformable---and solve the range of user-defined object manipulation tasks with the help of a same model.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
16580,"inside this paper, we prove existence of optimal complementary dual codes (lcd codes) over large finite fields. we also give methods to generate orthogonal matrices over finite fields and then apply them to construct lcd codes. construction methods include random sampling inside a orthogonal group, code extension, matrix product codes and projection over the self-dual basis.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
2166,"cyclic pursuit frameworks, which are built upon pursuit interactions between neighboring agents inside the cycle graph, provide an efficient way to create useful global behaviors inside the collective of autonomous robots. previous work had considered cyclic pursuit with the constant bearing (cb) pursuit law, and demonstrated a existence of circling equilibria considering a corresponding dynamics. inside this work, we propose the beacon-referenced version of a cb pursuit law, wherein the stationary beacon provides an additional reference considering a individual agents inside the collective. when implemented inside the cyclic framework, we show that a resulting dynamics admit relative equilibria corresponding to the circling orbit around a beacon, with a circling radius and a distribution of agents along a orbit determined by parameters of a proposed pursuit law. we also derive necessary conditions considering stability of a circling equilibria, which provides the guide considering parameter selection. finally, by introducing the change of variables, we demonstrate a existence of the family of invariant manifolds related to spiraling motions around a beacon which preserve a ""pure shape"" of a collective, and study a reduced dynamics on the representative manifold.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1
9477,"how should we effectively encode evolving information over dynamic graphs into low-dimensional representations? inside this paper, we propose dyrep, an inductive deep representation learning framework that learns the set of functions to efficiently produce low-dimensional node embeddings that evolves over time. a learned embeddings drive a dynamics of two key processes namely, communication and association between nodes inside dynamic graphs. these processes exhibit complex nonlinear dynamics that evolve at different time scales and subsequently contribute to a update of node embeddings. we employ the time-scale dependent multivariate point process model to capture these dynamics. we devise an efficient unsupervised learning procedure and demonstrate that our idea behind the method significantly outperforms representative baselines on two real-world datasets considering a problem of dynamic link prediction and event time prediction.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7551,"we consider a problem of approximating a reachable set of the discrete-time polynomial system from the semialgebraic set of initial conditions under general semialgebraic set constraints. assuming inclusion inside the given simple set like the box or an ellipsoid, we provide the method to compute certified outer approximations of a reachable set. a proposed method consists of building the hierarchy of relaxations considering an infinite-dimensional moment problem. under certain assumptions, a optimal value of this problem was a volume of a reachable set and a optimum solution was a restriction of a lebesgue measure on this set. then, one should outer approximate a reachable set as closely as desired with the hierarchy of super level sets of increasing degree polynomials. considering each fixed degree, finding a coefficients of a polynomial boils down to computing a optimal solution of the convex semidefinite program. when a degree of a polynomial approximation tends to infinity, we provide strong convergence guarantees of a super level sets to a reachable set. we also present some application examples together with numerical results.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
6661,"dirac structures are geometric objects that generalize both poisson structures and presymplectic structures on manifolds. they naturally appear inside a formulation of constrained mechanical systems. inside this paper, we show that a evolution equa- tions considering nonequilibrium thermodynamics admit an intrinsic formulation inside terms of dirac structures, both on a lagrangian and a hamiltonian settings. inside absence of irreversible processes these dirac structures reduce to canonical dirac structures associated to canonical symplectic forms on phase spaces. our geometric formulation of nonequilibrium thermodynamic thus consistently extends a geometric formulation of mechanics, to which it reduces inside absence of irreversible processes. a dirac structures are associated to a variational formulation of nonequilibrium thermodynamics developed inside gay-balmaz and yoshimura [2016a,b] and are induced from the nonlinear nonholonomic constraint given by a expression of a entropy production of a system.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15261,"we study the general online linear optimization problem(olo). at each round, the subset of objects from the fixed universe of $n$ objects was chosen, and the linear cost associated with a chosen subset was incurred. to measure a performance of our algorithms, we use a notion of regret which was a difference between a total cost incurred over all iterations and a cost of a best fixed subset inside hindsight. we consider full information and bandit feedback considering this problem. this problem was equivalent to olo on a $\{0,1\}^n$ hypercube. a exp2 algorithm and its bandit variant are commonly used strategies considering this problem. it is previously unknown if it was possible to run exp2 on a hypercube inside polynomial time. inside this paper, we present the polynomial time algorithm called polyexp considering olo on a hypercube. we show that our algorithm was equivalent to both exp2 on $\{0,1\}^n$ as well as online mirror descent(omd) with entropic regularization on $[0,1]^n$ and bernoulli sampling. we consider $l_\infty$ adversarial losses. we show polyexp achieves expected regret bounds that are the factor of $\sqrt{n}$ better than exp2 inside all a three settings. because of a equivalence of these algorithms, this implies an improvement on exp2's regret bounds. we also show matching regret lower bounds. finally, we show how to use polyexp on a $\{-1,+1\}^n$ hypercube, solving an open problem inside bubeck et al (colt 2012).",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7177,"we show how an ensemble of $q^*$-functions should be leveraged considering more effective exploration inside deep reinforcement learning. we build on well established algorithms from a bandit setting, and adapt them to a $q$-learning setting. we propose an exploration strategy based on upper-confidence bounds (ucb). our experiments show significant gains on a atari benchmark.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19061,"a advances inside conic optimization have led to its increased utilization considering modeling data uncertainty. inside particular, conic mean-risk optimization gained prominence inside probabilistic and robust optimization. whereas a corresponding conic models are solved efficiently over convex sets, their discrete counterparts are intractable. inside this paper, we give the highly effective successive quadratic upper-bounding procedure considering discrete mean-risk minimization problems. a procedure was based on the reformulation of a mean-risk problem through a perspective of its convex quadratic term. computational experiments conducted on a network interdiction problem with stochastic capacities show that a proposed idea behind the method yields solutions within 1-2% of optimality inside the small fraction of a time required by exact search algorithms. we demonstrate a value of a proposed idea behind the method considering constructing efficient frontiers of flow-at-risk vs. interdiction cost considering varying confidence levels.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
18095,"we present the novel framework that enables efficient probabilistic inference inside large-scale scientific models by allowing a execution of existing domain-specific simulators as probabilistic programs, resulting inside highly interpretable posterior inference. our framework was general purpose and scalable, and was based on the cross-platform probabilistic execution protocol through which an inference engine should control simulators inside the language-agnostic way. we demonstrate a technique inside particle physics, on the scientifically accurate simulation of a tau lepton decay, which was the key ingredient inside establishing a properties of a higgs boson. high-energy physics has the rich set of simulators based on quantum field theory and a interaction of particles inside matter. we show how to use probabilistic programming to perform bayesian inference inside these existing simulator codebases directly, inside particular conditioning on observable outputs from the simulated particle detector to directly produce an interpretable posterior distribution over decay pathways. inference efficiency was achieved using inference compilation where the deep recurrent neural network was trained to parameterize proposal distributions and control a stochastic simulator inside the sequential importance sampling scheme, at the fraction of a computational cost of markov chain monte carlo sampling.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
10335,"conventional superconductivity, as used inside this review, refers to electron-phonon coupled superconducting electron-pairs described by bcs theory. unconventional superconductivity refers to superconductors where a cooper pairs are not bound together by phonon exchange but instead by exchange of some other kind, e. g. spin fluctuations inside the superconductor with magnetic order either coexistent or nearby inside a phase diagram. such unconventional superconductivity has been known experimentally since heavy fermion cecu2si2, with its strongly correlated 4f electrons, is discovered to superconduct below 0.6 k inside 1979. since a discovery of unconventional superconductivity inside a layered cuprates inside 1986, a study of these materials saw tc jump to 164 k by 1994. further progress inside high temperature superconductivity would be aided by understanding a cause of such unconventional pairing. this review compares a fundamental properties of 9 unconventional superconducting classes of materials - from 4f-electron heavy fermions to organic superconductors to classes where only three known members exist to a cuprates with over 200 examples, with a hope that common features will emerge to aid theory explain (and predict!) these phenomena. inside addition, three new emerging classes of superconductors (topological, interfacial [e. g. fese on srtio3], and h2s under high pressure) are briefly covered, even though their conventionality was not yet fully determined.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
16202,"we consider a global optimization of the function over the continuous domain. at every evaluation attempt, we should observe a function at the chosen point inside a domain and we reap a reward of a value observed. we assume that drawing these observations are expensive and noisy. we frame it as the continuum-armed bandit problem with the gaussian process prior on a function. inside this regime, most algorithms have been developed to minimize some form of regret. contrary to this popular norm, inside this paper, we study a convergence of a sequential point $\boldsymbol{x}^t$ to a global optimizer $\boldsymbol{x}^*$ considering a thompson sampling approach. under some assumptions and regularity conditions, we show an exponential rate of convergence to a true optimal.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0
19298,"we calculate a full counting statistics (fcs) of the subsystem energy inside free fermionic systems by means of a grassmann variables. we demonstrate that a generating function of these systems should be written as the determinant formula with respect to a hamiltonian couplings and by employing a bell's polynomials, we derive exact formulas considering a subsystem energy moments. inside addition, we discuss a same quantities inside a quantum xy spin chain, and we demonstrate that at a critical regimes a fluctuations of a energy moments decay like the power-law as we expect from a conformal field theory arguments, while inside non-critical regimes, a decay was exponential. furthermore, we discuss a full counting statistics of subsystem energy inside a quantum xx chain.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
18092,"the physical model of the three-dimensional flow of the viscous bubbly fluid inside an intermediate regime between bubble formation and breakage was presented. a model was based on mechanics and thermodynamics of the single bubble coupled to a dynamics of the viscous fluid as the whole, and takes into account multiple physical effects, including gravity, viscosity, and surface tension. dimensionless versions of a resulting nonlinear model are obtained, and values of dimensionless parameters are estimated considering typical magma flows inside horizontal subaerial lava fields and vertical volcanic conduits. exact solutions of a resulting system of nonlinear equations corresponding to equilibrium flows and traveling waves are analyzed inside a one-dimensional setting. generalized su-gardner-type perturbation analysis was employed to study approximate solutions of a model inside a long-wave ansatz. simplified nonlinear partial differential equations (pde) satisfied by a leading terms of a perturbation solutions are systematically derived. it was shown that considering specific classes of perturbations, approximate solutions of a bubbly fluid model arise from solutions of a classical diffusion, burgers, variable-coefficient burgers, and korteweg-de vries equations.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6907,"embedded systems inside safety-critical environments are continuously required to deliver more performance and functionality, while expected to provide verified safety guarantees. nonetheless, platform-wide software verification (required considering safety) was often expensive. therefore, design methods that enable utilization of components such as real-time operating systems (rtos), without requiring their correctness to guarantee safety, was necessary. inside this paper, we propose the design idea behind the method to deploy safe-by-design embedded systems. to attain this goal, we rely on the small core of verified software to handle faults inside applications and rtos and recover from them while ensuring that timing constraints of safety-critical tasks are always satisfied. faults are detected by monitoring a application timing and fault-recovery was achieved using full platform restart and software reload, enabled by a short restart time of embedded systems. schedulability analysis was used to ensure that a timing constraints of critical plant control tasks are always satisfied inside spite of faults and consequent restarts. we derive schedulability results considering four restart-tolerant task models. we use the simulator to evaluate and compare a performance of a considered scheduling models.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
18502,"accessing web archives raises the number of issues caused by their temporal characteristics. additional knowledge was needed to find and understand older texts. especially entities mentioned inside texts are subject to change. most severe inside terms of information retrieval are name changes. inside order to find entities that have changed their name over time, search engines need to be aware of this evolution. we tackle this problem by analyzing wikipedia inside terms of entity evolutions mentioned inside articles. we present statistical data on excerpts covering name changes, which will be used to discover similar text passages and extract evolution knowledge inside future work.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8161,"a electronic health record (ehr) contains the large amount of multi-dimensional and unstructured clinical data of significant operational and research value. distinguished from previous studies, our idea behind the method embraces the double-annotated dataset and strays away from obscure ""black-box"" models to comprehensive deep learning models. inside this paper, we present the novel neural attention mechanism that not only classifies clinically important findings. specifically, convolutional neural networks (cnn) with attention analysis are used to classify radiology head computed tomography reports based on five categories that radiologists would account considering inside assessing acute and communicable findings inside daily practice. a experiments show that our cnn attention models outperform non-neural models, especially when trained on the larger dataset. our attention analysis demonstrates a intuition behind a classifier's decision by generating the heatmap that highlights attended terms used by a cnn model; this was valuable when potential downstream medical decisions are to be performed by human experts or a classifier information was to be used inside cohort construction such as considering epidemiological studies.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18066,"$ \def\vecc#1{\boldsymbol{#1}} $we design the polynomial time algorithm that considering any weighted undirected graph $g = (v, e,\vecc w)$ and sufficiently large $\delta > 1$, partitions $v$ into subsets $v_1, \ldots, v_h$ considering some $h\geq 1$, such that $\bullet$ at most $\delta^{-1}$ fraction of a weights are between clusters, i.e. \[ w(e - \cup_{i = 1}^h e(v_i)) \lesssim \frac{w(e)}{\delta};\] $\bullet$ a effective resistance diameter of each of a induced subgraphs $g[v_i]$ was at most $\delta^3$ times a average weighted degree, i.e. \[ \max_{u, v \in v_i} \mathsf{reff}_{g[v_i]}(u, v) \lesssim \delta^3 \cdot \frac{|v|}{w(e)} \quad \text{ considering all } i=1, \ldots, h.\] inside particular, it was possible to remove one percent of weight of edges of any given graph such that each of a resulting connected components has effective resistance diameter at most a inverse of a average weighted degree. our proof was based on the new connection between effective resistance and low conductance sets. we show that if a effective resistance between two vertices $u$ and $v$ was large, then there must be the low conductance cut separating $u$ from $v$. this implies that very mildly expanding graphs have constant effective resistance diameter. we believe that this connection could be of independent interest inside algorithm design.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15079,"this paper explores conformal prediction inside a learning under privileged information (lupi) paradigm. we use a svm+ realization of lupi inside an inductive conformal predictor, and apply it to a mnist benchmark dataset and three datasets inside drug discovery. a results show that with the help of privileged information produces valid models and improves efficiency compared to standard svm, however a improvement varies between a tested datasets and was not substantial inside a drug discovery applications. more importantly, with the help of svm+ inside the conformal prediction framework enables valid prediction intervals at specified significance levels.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16326,"temporal point processes have been widely applied to model event sequence data generated by online users. inside this paper, we consider a problem of how to design a optimal control policy considering point processes, such that a stochastic system driven by a point process was steered to the target state. inside particular, we exploit a key insight to view a stochastic optimal control problem from a perspective of optimal measure and variational inference. we further propose the convex optimization framework and an efficient algorithm to update a policy adaptively to a current system state. experiments on synthetic and real-world data show that our algorithm should steer a user activities much more accurately and efficiently than other stochastic control methods.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,1
11527,"model selection inside mixed models based on a conditional distribution was appropriate considering many practical applications and has been the focus of recent statistical research. inside this paper we introduce a r-package caic4 that allows considering a computation of a conditional akaike information criterion (caic). computation of a conditional aic needs to take into account a uncertainty of a random effects variance and was therefore not straightforward. we introduce the fast and stable implementation considering a calculation of a caic considering linear mixed models estimated with lme4 and additive mixed models estimated with gamm4 . furthermore, caic4 offers the stepwise function that allows considering the fully automated stepwise selection scheme considering mixed models based on a conditional aic. examples of many possible applications are presented to illustrate a practical impact and easy handling of a package.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7001,"inside visual question answering (vqa), an algorithm must answer text-based questions about images. while multiple datasets considering vqa have been created since late 2014, they all have flaws inside both their content and a way algorithms are evaluated on them. as the result, evaluation scores are inflated and predominantly determined by answering easier questions, making it difficult to compare different methods. inside this paper, we analyze existing vqa algorithms with the help of the new dataset. it contains over 1.6 million questions organized into 12 different categories. we also introduce questions that are meaningless considering the given image to force the vqa system to reason about image content. we propose new evaluation schemes that compensate considering over-represented question-types and make it easier to study a strengths and weaknesses of algorithms. we analyze a performance of both baseline and state-of-the-art vqa models, including multi-modal compact bilinear pooling (mcb), neural module networks, and recurrent answering units. our experiments establish how attention helps certain categories more than others, determine which models work better than others, and explain how simple models (e.g. mlp) should surpass more complex models (mcb) by simply learning to answer large, easy question categories.",1,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13058,"with a large-scale penetration of a internet, considering a first time, humanity has become linked by the single, open, communications platform. harnessing this fact, we report insights arising from the unified internet activity and location dataset of an unparalleled scope and accuracy drawn from over the trillion (1.5$\times 10^{12}$) observations of end-user internet connections, with temporal resolution of just 15min over 2006-2012. we first apply this dataset to a expansion of a internet itself over 1,647 urban agglomerations globally. we find that unique ip per capita counts reach saturation at approximately one ip per three people, and take, on average, 16.1 years to achieve; eclipsing a estimated 100- and 60- year saturation times considering steam-power and electrification respectively. next, we use intra-diurnal internet activity features to up-scale traditional over-night sleep observations, producing a first global approximate of over-night sleep duration inside 645 cities over 7 years. we find statistically significant variation between continental, national and regional sleep durations including some evidence of global sleep duration convergence. finally, we approximate a relationship between internet concentration and economic outcomes inside 411 oecd regions and find that a internet's expansion was associated with negative or positive productivity gains, depending strongly on sectoral considerations. to our knowledge, our study was a first of its kind to use online/offline activity of a entire internet to infer social science insights, demonstrating a unparalleled potential of a internet as the social data-science platform.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0
10854,"we build deep rl agents that execute declarative programs expressed inside formal language. a agents learn to ground a terms inside this language inside their environment, and should generalize their behavior at test time to execute new programs that refer to objects that were not referenced during training. a agents develop disentangled interpretable representations that allow them to generalize to the wide variety of zero-shot semantic tasks.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18450,"massive black-hole binaries, formed when galaxies merge, are among a primary sources of gravitational waves targeted by ongoing pulsar timing array (pta) experiments and a upcoming space-based lisa interferometer. however, their formation and merger rates are still highly uncertain. recent upper limits on a stochastic gravitational-wave background obtained by ptas are starting being inside marginal tension with theoretical models considering a pairing and orbital evolution of these systems. this tension should be resolved by assuming that these binaries are more eccentric or interact more strongly with a environment (gas and stars) than expected, or by accounting considering possible selection biases inside a construction of a theoretical models. however, another (pessimistic) possibility was that these binaries do not merge at all, but stall at large ($\sim$ pc) separations. we explore this extreme scenario by with the help of the galaxy-formation semi-analytic model including massive black holes (isolated and inside binaries), and show that future generations of ptas will detect a stochastic gravitational-wave background from a massive black-hole binary population within $10-15$ years of observations, even inside a ""nightmare scenario"" inside which all binaries stall at a hardening radius. moreover, we argue that this scenario was too pessimistic, because our model predicts a existence of the sub-population of binaries with small mass ratios ($q \lesssim 10^{-3}$) that should merge within the hubble time simply as the result of gravitational-wave emission. this sub-population will be observable with large signal-to-noise ratios by future ptas thanks to next-generation radio telescopes such as ska or fast, and possibly by lisa.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4736,"current recommender systems exploit user and item similarities by collaborative filtering. some advanced methods also consider a temporal evolution of item ratings as the global background process. however, all prior methods disregard a individual evolution of the user's experience level and how this was expressed inside a user's writing inside the review community. inside this paper, we model a joint evolution of user experience, interest inside specific item facets, writing style, and rating behavior. this way we should generate individual recommendations that take into account a user's maturity level (e.g., recommending art movies rather than blockbusters considering the cinematography expert). as only item ratings and review texts are observables, we capture a user's experience and interests inside the latent model learned from her reviews, vocabulary and writing style. we develop the generative hmm-lda model to trace user evolution, where a hidden markov model (hmm) traces her latent experience progressing over time -- with solely user reviews and ratings as observables over time. a facets of the user's interest are drawn from the latent dirichlet allocation (lda) model derived from her reviews, as the function of her (again latent) experience level. inside experiments with five real-world datasets, we show that our model improves a rating prediction over state-of-the-art baselines, by the substantial margin. we also show, inside the use-case study, that our model performs well inside a assessment of user experience levels.",1,0,0,1,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0
17142,"it becomes increasingly popular to perform mediation analysis considering complex data from sophisticated experimental studies. inside this paper, we present granger mediation analysis (gma), the new framework considering causal mediation analysis of multiple time series. this framework was motivated by the functional magnetic resonance imaging (fmri) experiment where we are interested inside estimating a mediation effects between the randomized stimulus time series and brain activity time series from two brain regions. a stable unit treatment assumption considering causal mediation analysis was thus unrealistic considering this type of time series data. to address this challenge, our framework integrates two types of models: causal mediation analysis across a variables and vector autoregressive models across a temporal observations. we further extend this framework to handle multilevel data to address individual variability and correlated errors between a mediator and a outcome variables. these models not only provide valid causal mediation considering time series data but also model a causal dynamics across time. we show that a modeling parameters inside our models are identifiable, and we develop computationally efficient methods to maximize a likelihood-based optimization criteria. simulation studies show that our method reduces a approximation bias and improve statistical power, compared to existing approaches. on the real fmri data set, our idea behind the method not only infers a causal effects of brain pathways but accurately captures a feedback effect of a outcome region on a mediator region.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0
622,"with the help of back-propagation and its variants to train deep networks was often problematic considering new users. issues such as exploding gradients, vanishing gradients, and high sensitivity to weight initialization strategies often make networks difficult to train, especially when users are experimenting with new architectures. here, we present local representation alignment (lra), the training procedure that was much less sensitive to bad initializations, does not require modifications to a network architecture, and should be adapted to networks with highly nonlinear and discrete-valued activation functions. furthermore, we show that one variation of lra should start with the null initialization of network weights and still successfully train networks with the wide variety of nonlinearities, including tanh, relu-6, softplus, signum and others that may draw their inspiration from biology. the comprehensive set of experiments on mnist and a much harder fashion mnist data sets show that lra should be used to train networks robustly and effectively, succeeding even when back-propagation fails and outperforming other alternative learning algorithms, such as target propagation and feedback alignment.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15021,"many bosonic (fermionic) fractional quantum hall states, such as laughlin, moore-read and read-rezayi wavefunctions, belong to the special class of orthogonal polynomials: a jack polynomials (times the vandermonde determinant). this fundamental observation allows to point out two different recurrence relations considering a coefficients of a permanent (slater) decomposition of a bosonic (fermionic) states. here we provide an explicit fock space representation considering these wavefunctions by introducing the two-body squeezing operator which represents them as the jastrow operator applied to reference states, which are inside general simple periodic one dimensional patterns. remarkably, this operator representation was a same considering bosons and fermions, and a different nature of a two recurrence relations was an outcome of particle statistics.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
9715,"node-perturbation learning was the type of statistical gradient descent algorithm that should be applied to problems where a objective function was not explicitly formulated, including reinforcement learning. it estimates a gradient of an objective function by with the help of a change inside a object function inside response to a perturbation. a value of a objective function considering an unperturbed output was called the baseline. cho et al. proposed node-perturbation learning with the noisy baseline. inside this paper, we report on building a statistical mechanics of cho's model and on deriving coupled differential equations of order parameters that depict learning dynamics. we also show how to derive a generalization error by solving a differential equations of order parameters. on a basis of a results, we show that cho's results are also apply inside general cases and show some general performances of cho's model.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1220,"confidence intervals are the popular way to visualize and analyze data distributions. unlike p-values, they should convey information both about statistical significance as well as effect size. however, very little work exists on applying confidence intervals to multivariate data. inside this paper we define confidence intervals considering multivariate data that extend a one-dimensional definition inside the natural way. inside our definition every variable was associated with its own confidence interval as usual, but the data vector should be outside of the few of these, and still be considered to be within a confidence area. we analyze a problem and show that a resulting confidence areas retain a good qualities of their one-dimensional counterparts: they are informative and easy to interpret. furthermore, we show that a problem of finding multivariate confidence intervals was hard, but provide efficient approximate algorithms to solve a problem.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
10661,"low-pressure gaseous tpcs are well suited detectors to correlate a directions of nuclear recoils to a galactic dark matter (dm) halo. indeed, inside addition to providing the measure of a energy deposition due to a elastic scattering of the dm particle on the nucleus inside a target gas, they allow considering a reconstruction of a track of a recoiling nucleus. inside order to exclude a background events originating from radioactive decays on a surfaces of a detector materials within a drift volume, efforts are ongoing to precisely localize a track nuclear recoil inside a drift volume along a axis perpendicular to a cathode plane. we report here a implementation of a measure of a signal induced on a cathode by a motion of a primary electrons toward a anode inside the mimac chamber. as the validation, we performed an independent measurement of a drift velocity of a electrons inside a considered gas mixture, correlating inside time a cathode signal with a measure of a arrival times of a electrons on a anode.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
2794,"variability inside a light curves of spotted, rotating stars was often non-sinusoidal and quasi-periodic --- spots move on a stellar surface and have finite lifetimes, causing stellar flux variations to slowly shift inside phase. the strictly periodic sinusoid therefore cannot accurately model the rotationally modulated stellar light curve. physical models of stellar surfaces have many drawbacks preventing effective inference, such as highly degenerate or high-dimensional parameter spaces. inside this work, we test an appropriate effective model: the gaussian process with the quasi-periodic covariance kernel function. this highly flexible model allows sampling of a posterior probability density function of a periodic parameter, marginalising over a other kernel hyperparameters with the help of the markov chain monte carlo approach. to test a effectiveness of this method, we infer rotation periods from 333 simulated stellar light curves, demonstrating that a gaussian process method produces periods that are more accurate than both the sine-fitting periodogram and an autocorrelation function method. we also demonstrate that it works well on real data, by inferring rotation periods considering 275 kepler stars with previously measured periods. we provide the table of rotation periods considering these 1132 kepler objects of interest and their posterior probability density function samples. because this method delivers posterior probability density functions, it will enable hierarchical studies involving stellar rotation, particularly those involving population modelling, such as inferring stellar ages, obliquities inside exoplanet systems, or characterising star-planet interactions. a code used to implement this method was available online.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
17721,"we revisit a physical properties of global and local monopoles and discuss their implications inside a dynamics of monopole networks. inside particular, we review a velocity-dependent one-scale (vos) model considering global and local monopoles and propose physically motivated changes to its equations. we suggest the new form considering a acceleration term of a evolution equation of a root-mean-squared velocity and show that, with this change, a vos model was able to describe a results of radiation and matter era numerical simulations of global monopole networks with the single value of a acceleration parameter $k$, thus resolving a tension previously found inside a literature. we also show that a fact that a energy of global monopoles was not localized within their cores affects their dynamics and, thus, a hubble damping terms inside a vos equations. we study a ultra-relativistic linear scaling regime predicted by a vos equations and demonstrate that it cannot be attained either on radiation or matter eras and, thus, cannot arise from a cosmological evolution of the global monopole network. we also briefly discuss a implications of our findings considering a vos model considering local monopoles.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19777,"future cosmic microwave background (cmb) satellite missions aim to use a $b$ mode polarization to measure a tensor-to-scalar ratio $r$ with the sensitivity of about $10^{-3}$. achieving this goal will not only require sufficient detector array sensitivity but also unprecedented control of all systematic errors inherent to cmb polarization measurements. since polarization measurements derive from differences between observations at different times and from different sensors, detector response mismatches introduce leakages from intensity to polarization and thus lead to the spurious $b$ mode signal. because a expected primordial $b$ mode polarization signal was dwarfed by a known unpolarized intensity signal, such leakages could contribute substantially to a final error budget considering measuring $r.$ with the help of simulations we approximate a magnitude and angular spectrum of a spurious $b$ mode signal resulting from bandpass mismatch between different detectors. it was assumed here that a detectors are calibrated, considering example with the help of a cmb dipole, so that their sensitivity to a primordial cmb signal has been perfectly matched. consequently a mismatch inside a frequency bandpass shape between detectors introduces difference inside a relative calibration of galactic emission components. we simulate with the help of the range of scanning patterns being considered considering future satellite missions. we find that a spurious contribution to $r$ from reionization bump on large angular scales ($\ell < 10$) was $\approx 10^{-3}$ assuming large detector arrays and 20 percent of a sky masked. we show how a amplitude of a leakage depends on a angular coverage per pixels that results from a scan pattern.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14295,"we report a observation of magnetic domains inside a exotic, antiferromagnetically ordered all-in-all-out state of nd$_2$zr$_2$o$_7$, induced by spin canting. a all-in-all-out state should be realized by ising-like spins on the pyrochlore lattice and was established inside nd$_2$zr$_2$o$_7$ below 0.31 k considering external magnetic fields up to 0.14 t. two different spin arrangements should fulfill this configuration which leads to a possibility of magnetic domains. a all-in-all-out domain structure should be controlled by an external magnetic field applied parallel to a [111] direction. this was the result of different spin canting mechanism considering a two all-in-all-out configurations considering such the direction of a magnetic field. a change of a domain structure was observed through the hysteresis inside a magnetic susceptibility. no hysteresis occurs, however, inside case a external magnetic field was applied along [100].",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
10294,"inside 2015 a us federal government sponsored the dengue forecasting competition with the help of historical case data from iquitos, peru and san juan, puerto rico. competitors were evaluated on several aspects of out-of-sample forecasts including a targets of peak week, peak incidence during that week and total season incidence across each of several seasons. our team is one of a top performers of that competition, outperforming all other teams inside multiple targets/locals. inside this paper we report on our methodology, the large component of which, surprisingly, ignores a known biology of epidemics at large---in particular relationships between dengue transmission and environmental factors---and instead relies on flexible nonparametric nonlinear gaussian process (gp) regression fits that ""memorize"" a trajectories of past seasons, and then ""match"" a dynamics of a unfolding season to past ones inside real-time. our phenomenological idea behind the method has advantages inside situations where disease dynamics are less well understood, e.g., at sites with shorter histories of disease (such as iquitos), or where measurements and forecasts of ancillary covariates like precipitation are unavailable and/or where a strength of association with cases are as yet unknown. inside particular, we show that a gp idea behind the method generally outperforms the more classical generalized linear (autoregressive) model (glm) that we developed to utilize abundant covariate information. we illustrate variations of our method(s) on a two benchmark locales alongside the full summary of results submitted by other contest competitors.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
8514,"multidimensional numerical simulations of the homogeneous, chemically reactive gas were used to study ignition, flame stability, and deflagration-to-detonation transition (ddt) inside the supersonic combustor. a configuration studied is the rectangular channel with the supersonic inflow of stoichiometric ethylene-oxygen and the transimissive outflow boundary. a calculation was initialized with the velocity inside a computational domain equal to that of a inflow, which was held constant considering a duration of a calculation. a compressible reactive navier-stokes equations were solved by the high-order numerical algorithm on an adapting mesh. this paper describes two calculations, one with the mach 3 inflow and one with mach 5.25. inside a mach 3 case, a fuel-oxidizer mixture does not ignite and a flow reaches the steady-state oblique shock train structure. inside a mach 5.25 case, ignition occurs inside a boundary layers and a flame front becomes unstable due to the rayleigh-taylor instability at a interface between a burned and unburned gas. growth of a reaction front and expansion of a burned gas compress and preheat a unburned gas. ddt occurs inside several locations, initiating both at a flame front and inside a unburned gas, due to an energy-focusing mechanism. a growth of a flame instability that leads to ddt was analyzed with the help of a atwood number parameter.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15101,"searching considering two-dimensional (2d) realistic materials able to realize room-temperature quantum spin hall (qsh) effects was currently the growing field. here, we through ab initio calculations to identify arsenene oxide, aso, as an excellent candidate, which demonstrates high stability, flexibility, and tunable spin-orbit coupling (soc) gaps. inside contrast to known pristine or functionalized arsenene, a maximum nontrivial band gap of aso reaches 89 mev, and should be further enhanced to 130 mev under biaxial strain. by sandwiching 2d aso between bn sheets, we propose the quantum well inside which a band topology of aso was preserved with the sizeable band gap. considering that aso having fully oxidized surfaces are naturally stable against surface oxidization and degradation, this functionality provides the viable strategy considering designing topological quantum devices operating at room temperature.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
9640,"fake news may be intentionally created to promote economic, political and social interests, and should lead to negative impacts on humans beliefs and decisions. hence, detection of fake news was an emerging problem that has become extremely prevalent during a last few years. most existing works on this topic focus on manual feature extraction and supervised classification models leveraging the large number of labeled (fake or real) articles. inside contrast, we focus on content-based detection of fake news articles, while assuming that we have the small amount of labels, made available by manual fact-checkers or automated sources. we argue this was the more realistic setting inside a presence of massive amounts of content, most of which cannot be easily factchecked. to that end, we represent collections of news articles as multi-dimensional tensors, leverage tensor decomposition to derive concise article embeddings that capture spatial/contextual information about each news article, and use those embeddings to create an article-by-article graph on which we propagate limited labels. results on three real-world datasets show that our method performs on par or better than existing models that are fully supervised, inside that we achieve better detection accuracy with the help of fewer labels. inside particular, our proposed method achieves 75.43% of accuracy with the help of only 30% of labels of the public dataset while an svm-based classifier achieved 67.43%. furthermore, our method achieves 70.92% of accuracy inside the large dataset with the help of only 2% of labels.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11154,"representation learning has become an invaluable idea behind the method considering learning from symbolic data such as text and graphs. however, while complex symbolic datasets often exhibit the latent hierarchical structure, state-of-the-art methods typically learn embeddings inside euclidean vector spaces, which do not account considering this property. considering this purpose, we introduce the new idea behind the method considering learning hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an n-dimensional poincar√© ball. due to a underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. we introduce an efficient algorithm to learn a embeddings based on riemannian optimization and show experimentally that poincar√© embeddings outperform euclidean embeddings significantly on data with latent hierarchies, both inside terms of representation capacity and inside terms of generalization ability.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6419,"principal component analysis continues to be the powerful tool inside dimension reduction of high dimensional data. we assume the variance-diverging model and use a high-dimension, low-sample-size asymptotics to show that even though a principal component directions are not consistent, a sample and prediction principal component scores should be useful inside revealing a population structure. we further show that these scores are biased, and a bias was asymptotically decomposed into rotation and scaling parts. we propose methods of bias-adjustment that are shown to be consistent and work well inside a finite but high dimensional situations with small sample sizes. a potential advantage of bias-adjustment was demonstrated inside the classification setting.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
19133,"sparsity has been widely recognized as crucial considering efficient optimization inside graph-based slam. because a sparsity and structure of a slam graph reflect a set of incorporated measurements, many methods considering sparsification have been proposed inside hopes of reducing computation. these methods often focus narrowly on reducing edge count without regard considering structure at the global level. such structurally-naive techniques should fail to produce significant computational savings, even after aggressive pruning. inside contrast, simple heuristics such as measurement decimation and keyframing are known empirically to produce significant computation reductions. to demonstrate why, we propose the quantitative metric called elimination complexity (ec) that bridges a existing analytic gap between graph structure and computation. ec quantifies a complexity of a primary computational bottleneck: a factorization step of the gauss-newton iteration. with the help of this metric, we show rigorously that decimation and keyframing impose favorable global structures and therefore achieve computation reductions on a order of $r^2/9$ and $r^3$, respectively, where $r$ was a pruning rate. we additionally present numerical results showing ec provides the good approximation of computation inside both batch and incremental (isam2) optimization and demonstrate that pruning methods promoting globally-efficient structure outperform those that do not.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
13787,"there possibly exists some direct, non-gravitational coupling between dark energy and dark matter. this possibility should be seriously tested by with the help of observations, which requires us to understand such the scenario from a aspects of both expansion history and growth of structure. it was found that once calculating a perturbations inside a interacting dark energy (ide) scenario, considering most cases a curvature perturbation on superhorizon scales was divergent, which was the catastrophe considering a ide cosmology. we found the solution to this issue, which was to establish an effective theory to treat a dark energy perturbations totally based on a basic facts of dark energy. this scheme generalizes a parametrized post-friedmann framework of uncoupled dark energy and should be used to cure a instability of a ide cosmology. a whole parameter space of ide models should henceforward be explored by observational data. a ide scenario should thus be tested or falsified with current and future observational data by with the help of a ppf approach. we expect that a future highly accurate observational data would offer a certain answer to a question whether there was the direct coupling between dark energy and dark matter.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19865,"we generalize cartan's logarithmic derivative of the smooth map from the manifold into the lie group $g$ to smooth maps into the homogeneous space $m=g/h$, and determine a global monodromy obstruction to reconstructing such maps from infinitesimal data. a logarithmic derivative of a embedding of the submanifold $\sigma \subset m$ becomes an invariant of $\sigma $ under symmetries of a ""klein geometry"" $m$ whose analysis was taken up inside a article, ""lie algebroid invariants considering subgeometry"".",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17762,"based on generalization and extension of previous work [phys. rev. lett. {\bf 112}, 067201 (2014)] to multiple independent markovian baths we will compute a charge and spin current statistics of a open hubbard model with weak system-bath coupling up to next-to-leading order inside a coupling parameter. a physical results are related to those considering a $xxz$ model inside a analogous setup implying the certain universality which potentially holds inside this class of nonequilibrium models.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
8620,"we investigate a use of maximum distance separable (mds) codes to cache popular content to reduce a download delay of wireless content delivery. inside particular, we consider the cellular system where devices roam inside an out of the cell according to the poisson random process. popular content was cached inside the limited number of a mobile devices with the help of an mds code and should be downloaded from a mobile devices with the help of device-to-device communication. we derive an analytical expression considering a delay incurred inside downloading content from a wireless network and show that distributed caching with the help of mds codes should dramatically reduce a download delay with respect to a scenario where content was always downloaded from a base station and to a case of uncoded distributed caching.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
13538,"we address a problem of making human motion capture inside a wild more practical by with the help of the small set of inertial sensors attached to a body. since a problem was heavily under-constrained, previous methods either use the large number of sensors, which was intrusive, or they require additional video input. we take the different idea behind the method and constrain a problem by: (i) making use of the realistic statistical body model that includes anthropometric constraints and (ii) with the help of the joint optimization framework to fit a model to orientation and acceleration measurements over multiple frames. a resulting tracker sparse inertial poser (sip) enables 3d human pose approximation with the help of only 6 sensors (attached to a wrists, lower legs, back and head) and works considering arbitrary human motions. experiments on a recently released tnt15 dataset show that, with the help of a same number of sensors, sip achieves higher accuracy than a dataset baseline without with the help of any video data. we further demonstrate a effectiveness of sip on newly recorded challenging motions inside outdoor scenarios such as climbing or jumping over the wall.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15790,"a position vector field x was a most elementary and natural geometric object on the euclidean submanifold $m$. a position vector field plays very important roles inside mathematics as well as inside physics. similarly, a tangential component x^t of a position vector field was a most natural vector field tangent to a euclidean submanifold $m$. we simply call a vector field x^t a \textit{canonical vector field} of a euclidean submanifold m. inside earlier articles, we investigated euclidean submanifolds whose canonical vector fields are concurrent, concircular, or torse-forming. inside this article we study euclidean submanifolds with conformal canonical vector field. inside particular, we characterize such submanifolds. several applications are also given. inside a last section we present three global results on complete euclidean submanifolds with conformal canonical vector field.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17291,"schumann resonance transients which propagate around a globe should potentially generate the correlated background inside widely separated gravitational wave detectors. we show that due to a distribution of lightning hotspots around a globe these transients have characteristic time lags, and this feature should be useful to further suppress such the background, especially inside searches of a stochastic gravitational-wave background. the brief review of a corresponding literature on schumann resonances and lightnings was also given.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
10135,"to solve a puzzle of metamagnetic phenomena inside heavy fermion systems, we have compared paramagnetic isostructural ce and yb systems, ceni$_2$ge$_2$ and ybni$_2$ge$_2$, both of which are located near the magnetic instability. a most intriguing result was discovery of the metamagnetic-like transition inside both systems from magnetization measurements inside the pulsed magnetic fields. this was a first observation of a metamagnetism considering isomorphic ce and yb paramagnetic systems. similar to other metamagnets, a metamagnetic transition fields considering both compounds are well scaled by a temperature $t_{\chi}^{\rm max}$, at which a magnetic susceptibility shows the maximum. inside addition, considering ceni$_2$ge$_2$ the peak of nonlinear susceptibility $\chi_3$ appears at approximately $t_{\chi}^{\rm max}/2$, as considering other heavy-fermion metamagnets. inside contrast, ybni$_2$ge$_2$ shows only the sign change considering $\chi_3$ at $t_{\chi}^{\rm max}$, as observed inside itinerant metamagnets located near a ferromagnetic critical point. a metamagnetism of ceni$_2$ge$_2$ corresponds to the typical kondo lattice system, whereas that of ybni$_2$ge$_2$ was similar to a nearly ferromagnetic itinerant systems. other possibilities considering a metamagnetic behavior of ybni$_2$ge$_2$ are also discussed.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
12317,"future generation of gravitational wave detectors will have a sensitivity to detect gravitational wave events at redshifts far beyond any detectable electromagnetic sources. we show that if a observed event rate was greater than one event per year at redshifts z > 40, then a probability distribution of primordial density fluctuations must be significantly non-gaussian or a events originate from primordial black holes. a nature of a excess events should be determined from a redshift distribution of a merger rate.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16077,"we explore considering a first time a effect of self-interacting dark matter (sidm) on a dark matter (dm) and baryonic distribution inside massive galaxies formed inside hydrodynamical cosmological simulations, including explicit baryonic physics treatment. the novel implementation of super-massive black hole (smbh) formation and evolution was used, as inside tremmel et al.(2015, 2016), allowing to explicitly follow smbh dynamics at a center of galaxies. the high sidm constant cross-section was chosen, $\sigma$=10 $\rm cm^2/gr$, to amplify differences from cdm models. milky way-like galaxies form the shallower dm density profile inside sidm than they do inside cdm, with differences already at 20 kpc scales. this demonstrates that even considering a most massive spirals a effect of sidm dominates over a adiabatic contraction due to baryons. strikingly, a dynamics of smbhs differs inside a sidm and reference cdm case. smbhs inside massive spirals have sunk to a centre of their host galaxy inside both a sidm and cdm run, while inside less massive galaxies about 80$\%$ of a smbh population was off-centered inside a sidm case, as opposed to a cdm case inside which $\sim$90$\%$ of smbhs have reached their host's centre. smbhs are found as far as $\sim$9 kpc away from a centre of their host sidm galaxy. this difference was due to a increased dynamical friction timescale caused by a lower dm density inside sidm galaxies compared to cdm, resulting inside 'core stalling'. this pilot work highlights a importance of simulating inside the full hydrodynamical context different dm models combined to smbh physics to study their influence on galaxy formation.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15540,"inside this paper, we present an online adaptive pca algorithm that was able to compute a full dimensional eigenspace per new time-step of sequential data. a algorithm was based on the one-step update rule that considers all second order correlations between previous samples and a new time-step. our algorithm has o(n) complexity per new time-step inside its deterministic mode and o(1) complexity per new time-step inside its stochastic mode. we test our algorithm on the number of time-varying datasets of different physical phenomena. explained variance curves indicate that our technique provides an excellent approximation to a original eigenspace computed with the help of standard pca inside batch mode. inside addition, our experiments show that a stochastic mode, despite its much lower computational complexity, converges to a same eigenspace computed with the help of a deterministic mode.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
136,"signed networks allow to model positive and negative relationships. we analyze existing extensions of spectral clustering to signed networks. it turns out that existing approaches do not recover a ground truth clustering inside several situations where either a positive or a negative network structures contain no noise. our analysis shows that these problems arise as existing approaches take some form of arithmetic mean of a laplacians of a positive and negative part. as the solution we propose to use a geometric mean of a laplacians of positive and negative part and show that it outperforms a existing approaches. while a geometric mean of matrices was computationally expensive, we show that eigenvectors of a geometric mean should be computed efficiently, leading to the numerical scheme considering sparse matrices which was of independent interest.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14924,"gravity was the new instrument to coherently combine a light of a european southern observatory very large telescope interferometer to form the telescope with an equivalent 130 m diameter angular resolution and the collecting area of 200 m$^2$. a instrument comprises fiber fed integrated optics beam combination, high resolution spectroscopy, built-in beam analysis and control, near-infrared wavefront sensing, phase-tracking, dual beam operation and laser metrology [...]. this article gives an overview of gravity and reports on a performance and a first astronomical observations during commissioning inside 2015/16. we demonstrate phase tracking on stars as faint as m$_k$ ~ 10 mag, phase-referenced interferometry of objects fainter than m$_k$ ~ 15 mag with the limiting magnitude of m$_k$ ~ 17 mag, minute long coherent integrations, the visibility accuracy of better than 0.25 %, and spectro-differential phase and closure phase accuracy better than 0.5¬∞, corresponding to the differential astrometric precision of better than 10 microarcseconds ({\mu}as). a dual-beam astrometry, measuring a phase difference of two objects with laser metrology, was still under commissioning. first observations show residuals as low as 50 {\mu}as when following objects over several months. we illustrate a instrument performance with a observations of archetypical objects considering a different instrument modes. examples include a galactic center supermassive black hole and its fast orbiting star s2 considering phase referenced dual beam observations and infrared wavefront sensing, a high mass x-ray binary bp cru and a active galactic nucleus of pds 456 considering few {\mu}as spectro-differential astrometry, a t tauri star s cra considering the spectro-differential visibility analysis, {\xi} tel and 24 cap considering high accuracy visibility observations, and {\eta} car considering interferometric imaging with gravity.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
19360,"atomic sized two-level systems (tlss) inside amorphous dielectrics are known as the major source of loss inside superconducting devices. inside addition, individual tls are known to induce large frequency shifts due to strong coupling to a devices. however, inside a presence of the broad ensemble of tlss these shifts are symmetrically canceled out and not observed inside the typical single-tone spectroscopy experiment. we introduce the two-tone spectroscopy on a normal modes of the pair of coupled superconducting coplanar waveguide resonators to reveal this effect. together with an appropriate saturation model this enables us to extract a average single-photon rabi frequency of dominant tlss to be $\omega_0/2\pi \approx 79 $ khz. at high photon numbers we observe an enhanced frequency shift due to nonlinear kinetic inductance when with the help of a two-tone method and approximate a value of a nonlinear coefficient as $k/2\pi \approx -1\times 10^{-4}$ hz/photon. furthermore, a life-time of each resonance should be controlled (increased) by pumping of a other mode as demonstrated both experimentally and theoretically.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
6429,"block coordinate update (bcu) methods enjoy low per-update computational complexity because every time only one or the few block variables would need to be updated among possibly the large number of blocks. they are also easily parallelized and thus have been particularly popular considering solving problems involving large-scale dataset and/or variables. inside this paper, we propose the primal-dual bcu method considering solving linearly constrained convex program inside multi-block variables. a method was an accelerated version of the primal-dual algorithm proposed by a authors, which applies randomization inside selecting block variables to update and establishes an $o(1/t)$ convergence rate under weak convexity assumption. we show that a rate should be accelerated to $o(1/t^2)$ if a objective was strongly convex. inside addition, if one block variable was independent of a others inside a objective, we then show that a algorithm should be modified to achieve the linear rate of convergence. a numerical experiments show that a accelerated method performs stably with the single set of parameters while a original method needs to tune a parameters considering different datasets inside order to achieve the comparable level of performance.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0
18479,"we propose the novel idea behind the method to vietnamese word segmentation. our idea behind the method was based on a single classification ripple down rules methodology (compton and jansen, 1990), where rules are stored inside an exception structure and new rules are only added to correct segmentation errors given by existing rules. experimental results on a benchmark vietnamese treebank show that our idea behind the method outperforms previous state-of-the-art approaches jvnsegmenter, vntokenizer, dongdu and uetsegmenter inside terms of both accuracy and performance speed. our code was open-source and available at: this https url.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9580,"this paper shows a cfd investigation of pressure drop and thermal performance of aluminum oxide nano fluid in square channel inserted the cylinder with and without fin with the help of two-phase method. a constant heat flux on outer wall and laminar flow regime are applied inside a considered domain with the help of finite volume method. a results depict that a enhance of nano-particles volume concentration and reynolds number have dramatic effects on heat transfer coefficient enhancement. moreover, a increase of nano-particle diameter has opposite impact on heat transfer efficiency. a passive way leads to higher pressure drops. considering all fluids under consideration, pressure drop escalated with reynolds number. injecting nano-particles into a water causes to increase inside pressure drop and this impact was more significant inside high nano-particle volume fraction.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2279,"we present a study of a dependence of galaxy clustering on luminosity and stellar mass inside a redshift range 2$<$z$<$3.5 with the help of 3236 galaxies with robust spectroscopic redshifts from a vimos ultra deep survey (vuds). we measure a two-point real-space correlation function $w_p(r_p)$ considering four volume-limited stellar mass and four luminosity, m$_{uv}$ absolute magnitude selected, sub-samples. we find that a scale dependent clustering amplitude $r_0$ significantly increases with increasing luminosity and stellar mass indicating the strong galaxy clustering dependence on these properties. this corresponds to the strong relative bias between these two sub-samples of $\delta$b/b$^*$=0.43. fitting the 5-parameter hod model we find that a most luminous and massive galaxies occupy a most massive dark matter haloes with $\langle$m$_h$$\rangle$ = 10$^{12.30}$ h$^{-1}$ m$_{\odot}$. similar to a trends observed at lower redshift, a minimum halo mass m$_{min}$ depends on a luminosity and stellar mass of galaxies and grows from m$_{min}$ =10$^{9.73}$ h$^{-1}$m$_{\odot}$ to m$_{min}$=10$^{11.58}$ h$^{-1}$m$_{\odot}$ from a faintest to a brightest among our galaxy sample, respectively. we find a difference between these halo masses to be much more pronounced than was observed considering local galaxies of similar properties. moreover, at z~3, we observe that a masses at which the halo hosts, on average, one satellite and one central galaxy was m$_1$$\approx$4m$_{min}$ over all luminosity ranges, significantly lower than observed at z~0 indicating that a halo satellite occupation increases with redshift. a luminosity and stellar mass dependence was also reflected inside a measurements of a large scale galaxy bias, which we model as b$_{g,hod}$($>$l)=1.92+25.36(l/l$^*$)$^{7.01}$. we conclude our study with measurements of a stellar-to-halo mass ratio (shmr).",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12836,"sequence generative adversarial networks (seqgan) have been used to improve conditional sequence generation tasks, considering example, chit-chat dialogue generation. to stabilize a training of seqgan, monte carlo tree search (mcts) or reward at every generation step (regs) was used to evaluate a goodness of the generated subsequence. mcts was computationally intensive, but a performance of regs was worse than mcts. inside this paper, we propose stepwise gan (stepgan), inside which a discriminator was modified to automatically assign scores quantifying a goodness of each subsequence at every generation step. stepgan has significantly less computational costs than mcts. we demonstrate that stepgan outperforms previous gan-based methods on both synthetic experiment and chit-chat dialogue generation.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11354,"electronic medical records contain multi-format electronic medical data that consist of an abundance of medical knowledge. facing with patient's symptoms, experienced caregivers make right medical decisions based on their professional knowledge that accurately grasps relationships between symptoms, diagnosis and corresponding treatments. inside this paper, we aim to capture these relationships by constructing the large and high-quality heterogenous graph linking patients, diseases, and drugs (pdd) inside emrs. specifically, we propose the novel framework to extract important medical entities from mimic-iii (medical information mart considering intensive care iii) and automatically link them with a existing biomedical knowledge graphs, including icd-9 ontology and drugbank. a pdd graph presented inside this paper was accessible on a web using a sparql endpoint, and provides the pathway considering medical discovery and applications, such as effective treatment recommendations.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3169,"radar presents the promising alternative to lidar and vision inside autonomous vehicle applications, being able to detect objects at long range under the variety of weather conditions. however, distinguishing between occupied and free space from the raw radar scan was notoriously difficult. we consider a challenge of learning an inverse sensor model (ism) mapping the raw radar observation to occupancy probabilities inside the discretised space. we frame this problem as the segmentation task, utilising the deep neural network that was able to learn an inherently probabilistic ism from raw sensor data considers scene context. inside doing so our idea behind the method explicitly accounts considering a heteroscedastic aleatoric uncertainty considering radar that arises due to complex interactions between occlusion and sensor noise. our network was trained with the help of only partial occupancy labels generated from lidar and able to successfully distinguish between occupied and free space. we evaluate our idea behind the method on five hours of data recorded inside the dynamic urban environment and show that it significantly outperforms classical constant false-alarm rate (cfar) filtering approaches inside light of challenging noise artefacts whilst identifying space that was inherently uncertain because of occlusion.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
17031,"clustering of data points was the fundamental tool inside data analysis. we consider points $x$ inside the relaxed metric space, where a triangle inequality holds within the constant factor. a {\em cost} of clustering $x$ by $q$ was $v(q)=\sum_{x\in x} d_{xq}$. two basic tasks, parametrized by $k \geq 1$, are {\em cost estimation}, which returns (approximate) $v(q)$ considering queries $q$ such that $|q|=k$ and {\em clustering}, which returns an (approximate) minimizer of $v(q)$ of size $|q|=k$. with very large data sets $x$, we seek efficient constructions of small samples that act as surrogates to a full data considering performing these tasks. existing constructions that provide quality guarantees are either worst-case, and unable to benefit from structure of real data sets, or make explicit strong assumptions on a structure. we show here how to avoid both these pitfalls with the help of adaptive designs. at a core of our design was a {\em one2all} construction of multi-objective probability-proportional-to-size (pps) samples: given the set $m$ of centroids and $\alpha \geq 1$, one2all efficiently assigns probabilities to points so that a clustering cost of {\em each} $q$ with cost $v(q) \geq v(m)/\alpha$ should be estimated well from the sample of size $o(\alpha |m|\epsilon^{-2})$. considering cost queries, we should obtain worst-case sample size $o(k\epsilon^{-2})$ by applying one2all to the bicriteria approximation $m$, but we adaptively balance $|m|$ and $\alpha$ to further reduce sample size. considering clustering, we design an adaptive wrapper that applies the base clustering algorithm to the sample $s$. our wrapper uses a smallest sample that provides statistical guarantees that a quality of a clustering on a sample carries over to a full data set. we demonstrate experimentally a huge gains of with the help of our adaptive instead of worst-case methods.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4016,"we study the non-linear and non-local evolution equation considering curves obtained as a sharp interface limit of the phase-field model considering crawling motion of eukaryotic cells on the substrate. we establish uniqueness of solutions to a sharp interface limit equation inside a so-called subcritical parameter regime. a proof relies on the gr√∂nwall approximate considering the specially chosen weighted $l^2$ norm. next, as persistent motion of crawling cells was of central interest to biologists we study a existence of traveling wave solutions. we prove that traveling wave solutions exist inside a supercritical parameter regime provided a non-linear term of a sharp interface limit equation possesses certain asymmetry (related, e.g., to myosin contractility). finally, we numerically investigate traveling wave solutions and simulate their dynamics. due to non-uniqueness of solutions of a sharp interface limit equation we simulate the related, singularly perturbed pde system which was uniquely solvable. our simulations predict instability of traveling wave solutions and capture both bipedal wandering cell motion as well as rotating cell motion; these behaviors qualitatively agree with recent experimental and theoretical fidings.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6246,"inside 1822, cauchy presented a idea of traction vector that contains both a normal and tangential components of a internal surface forces per unit area and gave a tetrahedron argument to prove a existence of stress tensor. these great achievements form a main part of a foundation of continuum mechanics. considering about two centuries, some versions of tetrahedron argument and the few other proofs of a existence of stress tensor are presented inside every text on continuum mechanics, fluid mechanics, and a relevant subjects. inside this article, we show a birth, importance, and location of these cauchy's achievements, then by presenting a formal tetrahedron argument inside detail, considering a first time, we extract some fundamental challenges. these conceptual challenges are related to a result of applying a conservation of linear momentum to any mass element, a order of magnitude of a surface and volume terms, a definition of traction vectors on a surfaces that pass through a same point, a approximate processes inside a derivation of stress tensor, and some others. inside the comprehensive review, we present a different tetrahedron arguments and a proofs of a existence of stress tensor, discuss a challenges inside each one, and classify them inside two general approaches. inside a first idea behind the method that was followed inside most texts, a traction vectors do not exactly define on a surfaces that pass through a same point, so most of a challenges hold. but inside a second approach, a traction vectors are defined on a surfaces that pass exactly through a same point, therefore some of a relevant challenges are removed. we also study a improved works of hamel and backus, and indicate that a original work of backus removes most of a challenges. this article shows that a foundation of continuum mechanics was not the finished subject and there are still some fundamental challenges.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19258,"nonequilibrium dynamics inside correlated materials has attracted attention due to a possibility of characterizing, tuning, and creating complex ordered states. to understand a photoinduced microscopic dynamics, especially a linkage under realistic pump conditions between transient states and remnant elementary excitations, we performed nonperturbative simulations of various time-resolved spectroscopies. we used a mott antiferromagnetic insulator as the model platform. a transient dynamics of multi-particle excitations should be attributed to a interplay between floquet virtual states and the modification of a density of states, inside which interactions induce the spectral weight transfer. with the help of an autocorrelation of a time-dependent spectral function, we show that resonance of a virtual states with a upper hubbard band inside a mott insulator provides a route towards manipulating a electronic distribution and modifying charge and spin excitations. our results link transient dynamics to a nature of many-body excitations and provide an opportunity to design nonequilibrium states of matter using tuned laser pulses.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
7835,"a local induction equation, or a binormal flow on space curves was the well-known model of deformation of space curves as it describes a dynamics of vortex filaments, and a complex curvature was governed by a nonlinear schr√∂dinger equation. inside this paper, we present its discrete analogue, namely, the model of deformation of discrete space curves by a discrete nonlinear schr√∂dinger equation. we also present explicit formulas considering both smooth and discrete curves inside terms of tau functions of a two-component kp hierarchy.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15241,"modeling variability inside tensor decomposition methods was one of a challenges of source separation. one possible solution to account considering variations from one data set to another, jointly analysed, was to resort to a parafac2 model. however, so far imposing constraints on a mode with variability has not been possible. inside a following manuscript, the relaxation of a parafac2 model was introduced, that allows considering imposing nonnegativity constraints on a varying mode. an algorithm to compute a proposed flexible parafac2 model was derived, and its performance was studied on both synthetic and chemometrics data.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
9778,"robots will eventually be part of every household. it was thus critical to enable algorithms to learn from and be guided by non-expert users. inside this paper, we bring the human inside a loop, and enable the human teacher to give feedback to the learning agent inside a form of natural language. we argue that the descriptive sentence should provide the much stronger learning signal than the numeric reward inside that it should easily point to where a mistakes are and how to correct them. we focus on a problem of image captioning inside which a quality of a output should easily be judged by non-experts. we propose the hierarchical phrase-based captioning model trained with policy gradients, and design the feedback network that provides reward to a learner by conditioning on a human-provided feedback. we show that by exploiting descriptive feedback our model learns to perform better than when given independently written human captions.",1,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18901,"multiwinner voting rules are used to select the small representative subset of candidates or items from the larger set given a preferences of voters. however, if candidates have sensitive attributes such as gender or ethnicity (when selecting the committee), or specified types such as political leaning (when selecting the subset of news items), an algorithm that chooses the subset by optimizing the multiwinner voting rule may be unbalanced inside its selection -- it may under or over represent the particular gender or political orientation inside a examples above. we introduce an algorithmic framework considering multiwinner voting problems when there was an additional requirement that a selected subset should be ""fair"" with respect to the given set of attributes. our framework provides a flexibility to (1) specify fairness with respect to multiple, non-disjoint attributes (e.g., ethnicity and gender) and (2) specify the score function. we study a computational complexity of this constrained multiwinner voting problem considering monotone and submodular score functions and present several approximation algorithms and matching hardness of approximation results considering various attribute group structure and types of score functions. we also present simulations that suggest that adding fairness constraints may not affect a scores significantly when compared to a unconstrained case.",1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12,"inside this paper, we analyze a fundamental conditions considering low-rank tensor completion given a separation or tensor-train (tt) rank, i.e., ranks of unfoldings. we exploit a algebraic structure of a tt decomposition to obtain a deterministic necessary and sufficient conditions on a locations of a samples to ensure finite completability. specifically, we propose an algebraic geometric analysis on a tt manifold that should incorporate a whole rank vector simultaneously inside contrast to a existing idea behind the method based on a grassmannian manifold that should only incorporate one rank component. our proposed technique characterizes a algebraic independence of the set of polynomials defined based on a sampling pattern and a tt decomposition, which was instrumental to obtaining a deterministic condition on a sampling pattern considering finite completability. inside addition, based on a proposed analysis, assuming that a entries of a tensor are sampled independently with probability $p$, we derive the lower bound on a sampling probability $p$, or equivalently, a number of sampled entries that ensures finite completability with high probability. moreover, we also provide a deterministic and probabilistic conditions considering unique completability.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0
10303,"polarized topics often spark discussion and debate on social media. recent studies have shown that polarized debates have the specific clustered structure inside a endorsement net- work, which indicates that users direct their endorsements mostly to ideas they already agree with. understanding these polarized discussions and exposing social media users to content that broadens their views was of paramount importance. a contribution of this demonstration was two-fold. (i) the tool to visualize retweet networks about controversial issues on twitter. by with the help of our visualization, users should understand how polarized discussions are shaped on twitter, and explore a positions of a various actors. (ii) the solution to reduce polarization of such discussions. we do so by exposing users to information which presents the contrarian point of view. users should visually inspect our recommendations and understand why and how these would play out inside terms of a retweet network. our demo (this https url homepage) provides one of a first steps inside developing automated tools that aid users explore, and possibly escape, their echo chambers. a ideas inside a demo should also aid content providers design tools to broaden their reach to people with different political and ideological backgrounds.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
4968,"inside this paper we develop the numerical method to solve nonlinear optimal control problems with final-state constraints. specifically, we extend a projection operator based netwon's method considering trajectory optimization (pronto), which is proposed by hauser considering unconstrained optimal control problems. while inside a standard method final-state constraints should be only approximately handled by means of the terminal penalty, inside this work we propose the methodology to meet a constraints exactly. moreover, our method guarantees recursive feasibility of a final-state constraint. this was an appealing property especially inside realtime applications inside which one would like to be able to stop a computation even if a desired tolerance has not been reached, but still satisfy a constraints. following a same conceptual idea of pronto, a proposed strategy was based on two main steps which (differently from a standard scheme) preserve a feasibility of a final-state constraints: (i) solve the quadratic approximation of a nonlinear problem to find the descent direction, and (ii) get the (feasible) trajectory by means of the feedback law (which turns out to be the nonlinear projection operator). to find a (feasible) descent direction we take advantage of final-state constrained linear quadratic optimal control methods, while a second step was performed by suitably designing the constrained version of a trajectory tracking projection operator. a effectiveness of a proposed strategy was tested on a optimal state transfer of an inverted pendulum.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
12481,"inside this work, we study a information theoretic converse considering a index coding problem. we generalize a definition considering a alignment chain, introduced by maleki et al., to capture more flexible relations among interfering messages at each receiver. based on this, we derive improved converse results considering a single-server centralized index coding problem. a new bounds uniformly outperform a maximum acyclic induced subgraph bound, and should be useful considering large problems, considering which a generally tighter polymatroidal bound becomes computationally impractical. we then extend these new bounds to a multi-server distributed index coding problem. we also present the separate, but related result where we identify the smaller centralized index coding instance compared to those identified inside a literature, considering which non-shannon-type inequalities are necessary to give the tighter converse.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
5419,"considering both indoor and outdoor environments, we propose an efficient and novel method considering different scales and sparse 3d point clouds registration that cannot be handled by a current popular icp approaches. our algorithm efficiently detects a scale difference between point clouds and uses a keyframes to approximate a relative pose considering calculating a scale difference. a algorithm applies the filter and computes a final transformation which coverages to the global minimum. a good approximation of transform and scale helps inside a calculation of a covariance matrix with the help of the closed form solution efficiently. this covariance between point clouds helps inside a approximation of information matrix considering pose-graph slam.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
18298,"a problem of planar registration consists inside finding a transformation that better aligns two point sets. inside our setting, a search domain was a set of planar rigid transformations and a objective function was a sum of a distances between each point of a transformed source set and a destination set. we propose the novel branch and bound (bnb) method considering finding a globally optimal solution. a algorithm recursively splits a search domain into boxes and computes an upper and the lower bound considering a minimum value of a restricted problem. we present two main contributions. first, we define two lower bounds. a cheap bound consists of a sum of a minimum distances between each point of source point set, transformed according to current box, and all a candidate points inside a destination point set. a relaxation bound corresponds to a solution of the concave relaxation of a objective function based on a linearization of a distance. inside large boxes, a cheap bound was the better approximation of a function minimum, while, inside small boxes, a relaxation bound was much more accurate. second, we present the queue-based algorithm that considerably speeds up a computation.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
14604,"inside this paper, we present an integrated solution to memory-efficient environment modeling by an autonomous mobile robot equipped with the laser range-finder. majority of nowadays approaches to autonomous environment modeling, called exploration, employs occupancy grids as environment representation where a working space was divided into small cells each storing information about a corresponding piece of a environment inside a form of the probabilistic approximate of its state. inside contrast, a presented idea behind the method uses the polygonal representation of a explored environment which consumes much less memory, enables fast planning and decision-making algorithms and it was thus reliable considering large-scale environments. simultaneous localization and mapping (slam) has been integrated into a presented framework to correct odometry errors and to provide accurate position estimates. this involves also the refinement of a already generated environment model inside case of loop closure, i.e. when a robot detects that it revisited an already explored place. a framework has been implemented inside robot operating system (ros) and tested with the real robot inside various environments. a experiments show that a polygonal representation with slam integrated should be used inside a real world as it was fast, memory efficient and accurate. moreover, a refinement should be executed inside real-time during a exploration process.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
9748,"generative models, either by simple clustering algorithms or deep neural network architecture, have been developed as the probabilistic approximation method considering dimension reduction or to model a underlying properties of data structures. although their apparent use has largely been limited to image recognition and classification, generative machine learning algorithms should be the powerful tool considering travel behaviour research. inside this paper, we examine a generative machine learning idea behind the method considering analyzing multiple discrete-continuous (mdc) travel behaviour data to understand a underlying heterogeneity and correlation, increasing a representational power of such travel behaviour models. we show that generative models are conceptually similar to choice selection behaviour process through information entropy and variational bayesian inference. specifically, we consider the restricted boltzmann machine (rbm) based algorithm with multiple discrete-continuous layer, formulated as the variational bayesian inference optimization problem. we systematically describe a proposed machine learning algorithm and develop the process of analyzing travel behaviour data from the generative learning perspective. we show parameter stability from model analysis and simulation tests on an open dataset with multiple discrete-continuous dimensions and the size of 293,330 observations. considering interpretability, we derive analytical methods considering conditional probabilities as well as elasticities. our results indicate that latent variables inside generative models should accurately represent joint distribution consistently w.r.t multiple discrete-continuous variables. lastly, we show that our model should generate statistically similar data distributions considering travel forecasting and prediction.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
184,"inside this paper, a idea of the new artificial intelligence based optimization algorithm, which was inspired from a nature of vortex, has been provided briefly. as also the bio-inspired computation algorithm, a idea was generally focused on the typical vortex flow / behavior inside nature and inspires from some dynamics that are occurred inside a sense of vortex nature. briefly, a algorithm was also the swarm-oriented evolutional problem solution approach; because it includes many methods related to elimination of weak swarm members and trying to improve a solution process by supporting a solution space using new swarm members. inside order have better idea about success of a algorithm; it has been tested using some benchmark functions. at this point, a obtained results show that a algorithm should be an alternative to a literature inside terms of single-objective optimization solution ways. vortex optimization algorithm (voa) was a name suggestion by a authors; considering this new idea of intelligent optimization approach.",1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
12380,"this paper describes the novel idea behind the method to software engineering derived from a ""sp theory of intelligence"" and its realisation inside a ""sp computer model"". despite superficial appearances, it was shown that many of a key ideas inside software engineering have counterparts inside a structure and workings of a sp system. potential benefits of this new idea behind the method to software engineering include: a automation or semi-automation of software development, with support considering programming of a sp system where necessary; allowing programmers to concentrate on 'world-oriented' parallelism, without worries about parallelism to speed up processing; support considering a long-term goal of programming a sp system using written or spoken natural language; reducing or eliminating a distinction between 'design' and 'implementation'; reducing or eliminating operations like compiling or interpretation; reducing or eliminating a need considering verification of software; reducing a need considering validation of software; no formal distinction between program and database; a potential considering substantial reductions inside a number of types of data file and a number of computer languages; benefits considering version control; and reducing technical debt.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3824,"magnetic skyrmions are localized nanometric spin textures with quantized winding numbers as a topological invariant. rapidly increasing attention has been paid to a investigations of skyrmions since their experimental discovery inside 2009, due both to a fundamental properties and a promising potential inside spintronics based applications. however, controlled creation of skyrmions remains the pivotal challenge towards technological applications. here, we report that skyrmions should be created locally by electric field inside a magnetoelectric helimagnet cu$\mathsf{_2}$oseo$\mathsf{_3}$. with the help of lorentz transmission electron microscopy, we successfully write skyrmions inside situ from the helical spin background. our discovery was highly coveted since it implies that skyrmionics should be integrated into contemporary field effect transistor based electronic technology, where very low energy dissipation should be achieved, and thus realizes the large step forward to its practical applications.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
337,"inside this paper we propose the neural network model with the novel sequential attention layer that extends soft attention by assigning weights to words inside an input sequence inside the way that takes into account not just how well that word matches the query, but how well surrounding words match. we evaluate this idea behind the method on a task of reading comprehension (on a who did what and cnn datasets) and show that it dramatically improves the strong baseline--the stanford reader--and was competitive with a state of a art.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2329,"inside this paper we prove an asymptotic behavior considering a radial eigenvalues to a dirichlet $p$-laplacian problem $-\delta_p\,u = \lambda\,|u|^{p-2}u$ inside $\omega$, $u=0$ on $\partial\omega$, where $\omega$ was an annular domain $\omega=\omega_{r,\overline{r}}$ inside $\mathbb{r}^n$.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4445,"shape memory materials have a capability to recover their original shape after the significant amount of deformation when they are subjected to certain stimuli, considering instance, heat or magnetic fields. however, their performance was often limited by a energetics and geometry of a martensitic-austenitic phase transformation. here, we report a discovery of the unique shape memory behavior inside cafe2as2, which exhibits unprecedented superelasticity with over 13% recoverable strain, over 3 gpa yield strength, repeatable stress-strain response even at a micrometer scale, and cryogenic linear shape memory effects near 50 k. these properties are acheived through the reversible uni-axial phase transformation mechanism, a tetragonal/orthorhombic-to-collapsed tetragonal phase transformation. our results offer a possibility of developing cryogenic linear technologies with the high precision and high actuation power per-unit-volume considering deep space exploration, and more broadly, suggest the mechanistic path to the whole new class of shape memory materials, thcr2si2-structured intermetallic compounds.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
7958,"weyl fermions are shown to exist in the parabolic band, where a kinetic energy of carriers was given by a non-relativistic schroedinger equation. there are fermi arcs as the direct consequence of a folding of the ring shaped fermi surface in a first brillouin zone. our results stem from a decomposition of a kinetic energy into a sum of a square of a weyl state, a coupling to a local magnetic field and a rashba interaction. a weyl fermions break a time and reflection symmetries present inside a kinetic energy, thus allowing considering a onset of the weak three-dimensional magnetic field around a layer. this field brings topological stability to a current carrying states through the chern number. inside a special limit that a weyl state becomes gapless this magnetic interaction was shown to be purely attractive, thus suggesting a onset of the superconducting condensate of zero helicity states.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
14212,"metric learning aims at learning the distance which was consistent with a semantic meaning of a samples. a problem was generally solved by learning an embedding considering each sample such that a embeddings of samples of a same category are compact while a embeddings of samples of different categories are spread-out inside a feature space. we study a features extracted from a second last layer of the deep neural network based classifier trained with a cross entropy loss on top of a softmax layer. we show that training classifiers with different temperature values of softmax function leads to features with different levels of compactness. leveraging these insights, we propose the ""heating-up"" strategy to train the classifier with increasing temperatures, leading a corresponding embeddings to achieve state-of-the-art performance on the variety of metric learning benchmarks.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2650,inside this paper we investigate a special subsequence of a halton sequence indexed by $\lfloor\beta n\rfloor$ with $\beta \in \mathbb{r}$ and prove the metric almost low- discrepancy result.,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
17685,"we investigate a magnetic and superconducting properties inside electron-doped li$_x$hfncl. hfncl was the band insulator that undergoes an insulator to superconductor transition upon doping at $x\approx0.13$. a persistence of a insulating state considering $x<0.13$ was due to an anderson transition probably related to li disorder. inside a metallic and superconducting phase, li$_x$hfncl was the prototype two-dimensional two-valley electron gas with parabolic bands. by performing the model random phase approximation idea behind the method as well as first-principles range-separated heyd-scuseria-ernzerhof (hse06) calculations, we find that a spin susceptibility $\chi_s$ was strongly enhanced inside a low doping regime by a electron-electron interaction. furthermore, inside a low doping limit, a exchange interaction renormalizes a intervalley electron-phonon coupling and results inside the strong increase of a superconducting critical temperature considering $x<0.15$. on a contrary, considering $x>0.15$, $t_c$ was approximately constant, inside agreement with experiments. at $x=0.055$ we found that $t_c$ should be as large as 40 k, suggesting that a synthesis of cleaner samples of li$_x$hfncl could remove a anderson insulating state competing with superconductivity and generate the high-$t_c$ superconductor.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0
10668,"we use techniques from functorial quantum field theory to provide the geometric description of a parity anomaly inside fermionic systems coupled to background gauge and gravitational fields on odd-dimensional spacetimes. we give an explicit construction of the geometric cobordism bicategory which incorporates general background fields inside the stack, and together with a theory of symmetric monoidal bicategories we use it to provide a concrete forms of invertible extended quantum field theories which capture anomalies inside both a path integral and hamiltonian frameworks. specialising this situation by with the help of a extension of a atiyah-patodi-singer index theorem to manifolds with corners due to loya and melrose, we obtain the new hamiltonian perspective on a parity anomaly. we compute explicitly a 2-cocycle of a projective representation of a gauge symmetry on a quantum state space, which was defined inside the parity-symmetric way by suitably augmenting a standard chiral fermionic fock spaces with lagrangian subspaces of zero modes of a dirac hamiltonian that naturally appear inside a index theorem. we describe a significance of our constructions considering a bulk-boundary correspondence inside the large class of time-reversal invariant gauge-gravity symmetry-protected topological phases of quantum matter with gapless charged boundary fermions, including a standard topological insulator inside 3+1 dimensions.",0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
1386,"inside this paper we introduce zhusuan, the python probabilistic programming library considering bayesian deep learning, which conjoins a complimentary advantages of bayesian methods and deep learning. zhusuan was built upon tensorflow. unlike existing deep learning libraries, which are mainly designed considering deterministic neural networks and supervised tasks, zhusuan was featured considering its deep root into bayesian inference, thus supporting various kinds of probabilistic models, including both a traditional hierarchical bayesian models and recent deep generative models. we use running examples to illustrate a probabilistic programming on zhusuan, including bayesian logistic regression, variational auto-encoders, deep sigmoid belief networks and bayesian recurrent neural networks.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
13391,"we introduce the method considering modeling disk galaxies designed to take full advantage of data from integral field spectroscopy (ifs). a method fits equilibrium models to simultaneously reproduce a surface brightness, rotation and velocity dispersion profiles of the galaxy. a models are fully self-consistent 6d distribution functions considering the galaxy with the sersic-profile stellar bulge, exponential disk and parametric dark matter halo, generated by an updated version of galactics. by creating realistic flux-weighted maps of a kinematic moments (flux, mean velocity and dispersion), we simultaneously fit photometric and spectroscopic data with the help of both maximum-likelihood and bayesian (mcmc) techniques. we apply a method to the gama spiral galaxy (g79635) with kinematics from a sami galaxy survey and deep $g$- and $r$-band photometry from a vst-kids survey, comparing parameter constraints with those from traditional 2d bulge-disk decomposition. our method returns broadly consistent results considering shared parameters, while constraining a mass-to-light ratios of stellar components and reproducing a hi-inferred circular velocity well beyond a limits of a sami data. while a method was tailored considering fitting integral field kinematic data, it should use other dynamical constraints like central fibre dispersions and \hi circular velocities, and was well-suited considering modelling galaxies with the combination of deep imaging and hi and/or optical spectra (resolved or otherwise). our implementation (magrite) was computationally efficient and should generate well-resolved models and kinematic maps inside under the minute on modern processors.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
16183,"we consider steady nonlinear free surface flow past an arbitrary bottom topography inside three dimensions, concentrating on a shape of a wave pattern that forms on a surface of a fluid. assuming ideal fluid flow, a problem was formulated with the help of the boundary integral method and discretised to produce the nonlinear system of algebraic equations. a jacobian of this system was dense due to integrals being evaluated over a entire free surface. to overcome a computational difficulty and large memory requirements, the jacobian-free newton krylov (jfnk) method was utilised. with the help of the block-banded approximation of a jacobian from a linearised system as the preconditioner considering a jfnk scheme, we find significant reductions inside computational time and memory required considering generating numerical solutions. these improvements also allow considering the larger number of mesh points over a free surface and a bottom topography. we present the range of numerical solutions considering both subcritical and supercritical regimes, and considering the variety of bottom configurations. we discuss nonlinear features of a wave patterns as well as their relationship to ship wakes.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5127,"object detection aims to identify instances of semantic objects of the certain class inside images or videos. a success of state-of-the-art approaches was attributed to a significant progress of object proposal and convolutional neural networks (cnns). most promising detectors involve multi-task learning with an optimization objective of softmax loss and regression loss. a first was considering multi-class categorization, while a latter was considering improving localization accuracy. however, few of them attempt to further investigate a hardness of distinguishing different sorts of distracting background regions (i.e., negatives) from true object regions (i.e., positives). to improve a performance of classifying positive object regions vs. the variety of negative background regions, we propose to incorporate triplet embedding into learning objective. a triplet units are formed by assigning each negative region to the meaningful object class and establishing class- specific negatives, followed by triplets construction. over a benchmark pascal voc 2007, a proposed triplet em- bedding has improved a performance of well-known fastrcnn model with the map gain of 2.1%. inside particular, a state-of-the-art idea behind the method ohem should benefit from a triplet embedding and has achieved the map improvement of 1.2%.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12411,"we propose deepmapping, the novel registration framework with the help of deep neural networks (dnns) as auxiliary functions to align multiple point clouds from scratch to the globally consistent frame. we use dnns to model a highly non-convex mapping process that traditionally involves hand-crafted data association, sensor pose initialization, and global refinement. our key novelty was that properly defining unsupervised losses to ""train"" these dnns through back-propagation was equivalent to solving a underlying registration problem, yet enables fewer dependencies on good initialization as required by icp. our framework contains two dnns: the localization network that estimates a poses considering input point clouds, and the map network that models a scene structure by estimating a occupancy status of global coordinates. this allows us to convert a registration problem to the binary occupancy classification, which should be solved efficiently with the help of gradient-based optimization. we further show that deepmapping should be readily extended to address a problem of lidar slam by imposing geometric constraints between consecutive point clouds. experiments are conducted on both simulated and real datasets. qualitative and quantitative comparisons demonstrate that deepmapping often enables more robust and accurate global registration of multiple point clouds than existing techniques. our code was available at this http url.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
2927,"deep convnets have been shown to be effective considering a task of human pose approximation from single images. however, several challenging issues arise inside a video-based case such as self-occlusion, motion blur, and uncommon poses with few or no examples inside training data sets. temporal information should provide additional cues about a location of body joints and aid to alleviate these issues. inside this paper, we propose the deep structured model to approximate the sequence of human poses inside unconstrained videos. this model should be efficiently trained inside an end-to-end manner and was capable of representing appearance of body joints and their spatio-temporal relationships simultaneously. domain knowledge about a human body was explicitly incorporated into a network providing effective priors to regularize a skeletal structure and to enforce temporal consistency. a proposed end-to-end architecture was evaluated on two widely used benchmarks (penn action dataset and jhmdb dataset) considering video-based pose estimation. our idea behind the method significantly outperforms a existing state-of-the-art methods.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9408,"today's social media platforms enable to spread both authentic and fake news very quickly. some approaches have been proposed to automatically detect such ""fake"" news based on their content, but it was difficult to agree on universal criteria of authenticity (which should be bypassed by adversaries once known). besides, it was obviously impossible to have each news item checked by the human. inside this paper, we the mechanism to limit a spread of fake news which was not based on content. it should be implemented as the plugin on the social media platform. a principle was as follows: the team of fact-checkers reviews the small number of news items (the most popular ones), which enables to have an approximation of each user's inclination to share fake news items. then, with the help of the bayesian approach, we approximate a trustworthiness of future news items, and treat accordingly those of them that pass the certain ""untrustworthiness"" threshold. we then evaluate a effectiveness and overhead of this technique on the large twitter graph. we show that having the few thousands users exposed to one given news item enables to reach the very precise approximation of its reliability. we thus identify more than 99% of fake news items with no false positives. a performance impact was very small: a induced overhead on a 90th percentile latency was less than 3%, and less than 8% on a throughput of user operations.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
15862,"we formulate the microscopic linear response theory of nonequilibrium magnonic torques and magnon pumping applicable to multiple-magnonic-band uniform ferromagnets with dzyaloshinskii-moriya interactions. from a linear response theory, we identify a extrinsic and intrinsic contributions where a latter was expressed using a berry curvature of magnonic bands. we observe that inside a presence of the time-dependent magnetization dzyaloshinskii-moriya interactions should act as fictitious electric fields acting on magnons. we study various current responses to this fictitious field and analyze a role of berry curvature. after identifying a magnon-mediated contribution to a equilibrium dzyaloshinskii-moriya interaction, we also establish a onsager reciprocity between a magnon-mediated torques and heat pumping. we apply our theory to a magnonic heat pumping and torque responses inside honeycomb and kagome lattice ferromagnets.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
9832,"the self-doping effect between outer and inner cuo$_2$ planes (ops and ips) inside multi-layer cuprate superconductors was studied. when one considers the three-layer tight-binding model of a hg-based three-layer cuprate derived from a first principle calculations, a electron concentration gets to be large inside a op compared to ip. this was inconsistent with a experimental fact that more hole carriers tend to be introduced into a op than ip.we investigate the three-layer hubbard model with a two-particle self-consistent idea behind the method considering multi-layer systems to incorporate electron correlations. we observe that a double occupancy (antiferromagnetic instability) inside a ip decreases (increases) more than a op, and also reveal that more electrons tend to be introduced into a ip than op to obtain a energy gain from a on-site hubbard interaction. these results are consistent with a experimental facts, and this electron distribution between a op and ip should be interpreted as the self-doping effect arising from strong electron correlations.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
13347,"to apply a framework of topology of sustainable management (tsm) by heitzig et al. (2016) to dynamical models, we connect it to viability theory (vt) using the variant definition of a former. this enables us to use a saint-pierre algorithm to approximate a main partition of tsm, improving a operationalization of tsm. furthermore, we present an extension of a algorithm to compute implicitly defined capture basins, the notion from vt that was more elaborated inside a article, as these come up inside tsm. we use the low- complexity model coupling environmental and socio-economic dynamics to demonstrate a applicability of this approach. two common problems of estimations inside vt are critical considering this example: (i) an unbounded state space and (ii) highly varying time scales. we solve both by introducing appropriate coordinate transformations. these solutions are applicable considering general systems, too.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
14193,"we consider a approximation accuracy of individual strength parameters of the thurstone choice model when each input observation consists of the choice of one item from the set of two or more items (so called top-1 lists). this model accommodates a well-known choice models such as a luce choice model considering comparison sets of two or more items and a bradley-terry model considering pair comparisons. we provide the tight characterization of a mean squared error of a maximum likelihood parameter estimator. we also provide similar characterizations considering parameter estimators defined by the rank-breaking method, which amounts to deducing one or more pair comparisons from the comparison of two or more items, assuming independence of these pair comparisons, and maximizing the likelihood function derived under these assumptions. we also consider the related binary classification problem where each individual parameter takes value from the set of two possible values and a goal was to correctly classify all items within the prescribed classification error.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
17943,"reinforcement learning and a evolutionary strategy are two major approaches inside addressing complicated control problems. both are strong contenders and have their own devotee communities. both groups have been very active inside developing new advances inside their own domain and devising, inside recent years, leading-edge techniques to address complex continuous control tasks. here, inside a context of deep reinforcement learning, we formulate the parallelized version of a proximal policy optimization method and the deep deterministic policy gradient method. moreover, we conduct the thorough comparison between a state-of-the-art techniques inside both camps fro continuous control; evolutionary methods and deep reinforcement learning methods. a results show there was no consistent winner.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7139,"we consider the second order equation with the linear ""elastic"" part and the nonlinear damping term depending on the power of a norm of a velocity. we investigate a asymptotic behavior of solutions, after rescaling them suitably inside order to take into account a decay rate and bound their energy away from zero.we find the rather unexpected dichotomy phenomenon. solutions with finitely many fouriercomponents are asymptotic to solutions of a linearized equationwithout damping, and exhibit some sort of equipartition of theenergy among a components. solutions with infinitely manyfourier components tend to zero weakly but not strongly. we showalso that a limit of a energy of solutions depends only on thenumber of their fourier components.the proof of our results was inspired by a analysis of asimplified model which we devise through an averaging procedure,and whose solutions exhibit a same asymptotic properties as thesolutions to a original equation.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11631,"inside this work, we propose the new data visualization and clustering technique considering discovering discriminative structures inside high-dimensional data. this technique, referred to as cpca++, utilizes a fact that a interesting features of the ""target"" dataset may be obscured by high variance components during traditional pca. by analyzing what was referred to as the ""background"" dataset (i.e., one that exhibits a high variance principal components but not a interesting structures), our technique was capable of efficiently highlighting a structure that was unique to a ""target"" dataset. similar to another recently proposed algorithm called ""contrastive pca"" (cpca), a proposed cpca++ method identifies important dataset specific patterns that are not detected by traditional pca inside the wide variety of settings. however, a proposed cpca++ method was significantly more efficient than cpca, because it does not require a parameter sweep inside a latter approach. we applied a cpca++ method to a problem of image splicing localization. inside this application, we utilize authentic edges as a background dataset and a spliced edges as a target dataset. a proposed method was significantly more efficient than state-of-the-art methods, as a former does not require iterative updates of filter weights using stochastic gradient descent and backpropagation, nor a training of the classifier. furthermore, a cpca++ method was shown to provide performance scores comparable to a state-of-the-art multi-task fully convolutional network (mfcn).",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1008,"many recent algorithms considering reinforcement learning are model-free and founded on a bellman equation. here we present the method founded on a costate equation and models of a state dynamics. we use a costate -- a gradient of cost with respect to state -- to improve a policy and also to ""focus"" a model, training it to detect and mimic those features of a environment that are most relevant to its task. we show that this method should handle difficult time-optimal control problems, driving deterministic or stochastic mechanical systems quickly to the target. on these tasks it works well compared to deep deterministic policy gradient, the recent bellman method. and because it creates the model, a costate method should also learn from mental practice.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8765,"crn/mgo(111) multilayers modeled using \textit{ab initio} calculations give rise to nanoscale, scalable, spatially separated two-dimensional electron and hole gases (2deg+2dhg), each confined to its own crn interface. due to a cr $3d^3$ configuration, both electron and hole gases are based on correlated transition metal layers involving bands of $3d$ character. transport calculations predict each subsystem will have the large thermopower, on a order of 250 $\mu$$v /k$ at room temperature. these heterostructures combine the large thermoelectric efficiency with scalable nanoscale conducting sheets; considering example, operating at the temperature difference of 50k, 40 bilayers could produce the 1 v voltage with the film thickness of 100 nm.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
368,"we report the detailed study of a transport coefficients of $\beta$-bi$_4$i$_4$ quasi-one dimensional topological insulator. electrical resistivity, thermoelectric power, thermal conductivity and hall coefficient measurements are consistent with a possible appearance of the charge density wave order at low temperatures. both electrons and holes contribute to a conduction inside $\beta$-bi$_4$i$_4$ and a dominant type of charge carrier changes with temperature as the consequence of temperature-dependent carrier densities and mobilities. measurements of resistivity and seebeck coefficient under hydrostatic pressure up to 2 gpa show the shift of a charge density wave order to higher temperatures suggesting the strongly one-dimensional character at ambient pressure. surprisingly, superconductivity was induced inside $\beta$-bi$_4$i$_4$ above 10 gpa with of 4.0 k which was slightly decreasing upon increasing a pressure up to 20 gpa. chemical characterisation of a pressure-treated samples shows amorphization of $\beta$-bi$_4$i$_4$ under pressure and rules out decomposition into bi and bii$_3$ at room-temperature conditions.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
8158,"we consider a problem of the neural network being requested to classify images (or other inputs) without making implicit use of the ""protected concept"", that was the concept that should not play any role inside a decision of a network. typically these concepts include information such as gender or race, or other contextual information such as image backgrounds that might be implicitly reflected inside unknown correlations with other variables, making it insufficient to simply remove them from a input features. inside other words, making accurate predictions was not good enough if those predictions rely on information that should not be used: predictive performance was not a only important metric considering learning systems. we apply the method developed inside a context of domain adaptation to address this problem of ""being right considering a right reason"", where we request the classifier to make the decision inside the way that was entirely 'agnostic' to the given protected concept (e.g. gender, race, background etc.), even if this could be implicitly reflected inside other attributes using unknown correlations. after defining a concept of an 'agnostic model', we demonstrate how a domain-adversarial neural network should remove unwanted information from the model with the help of the gradient reversal layer.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
13101,we show that every effective action of the compact lie group $k$ on the unit sphere $s^n$ admits an explicit orbit whose principal curvatures are bounded from above by $4\sqrt{14}$.,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
16909,"mlpack was an open-source c++ machine learning library with an emphasis on speed and flexibility. since its original inception inside 2007, it has grown to be the large project implementing the wide variety of machine learning algorithms, from standard techniques such as decision trees and logistic regression to modern techniques such as deep neural networks as well as other recently-published cutting-edge techniques not found inside any other library. mlpack was quite fast, with benchmarks showing mlpack outperforming other libraries' implementations of a same methods. mlpack has an active community, with contributors from around a world---including some from pust. this short paper describes a goals and design of mlpack, discusses how a open-source community functions, and shows an example usage of mlpack considering the simple data science problem.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
484,"diffuse interstellar bands (dibs) hold the lot of information about a state and a structure of a ism. structure should most directly be observed by extensive spectroscopic surveys, including surveys of stars where dibs are especially important, as they are conveniently found inside all observed bands. large surveys lack a quality of spectra to detect weak dibs, so many spectra from small regions on a sky have to be combined before the sufficient signal-to-noise ratio (snr) was achieved. however, a clumpiness of a dib clouds was unknown, which poses the problem, as a measured properties should end up being averaged over the too large area. we use the technique called gaussian processes to accurately measure profiles of interstellar absorption lines inside 145 high snr and high resolution spectra of hot stars. together with bayesian mcmc idea behind the method we also get reliable estimates of a uncertainties. we derive scales at which column densities of 18 dibs, ch, ch$^+$, ca i, and ca ii show some spatial correlation. this correlation scale was associated with a size of a ism clouds. scales expressed as a angle on a sky vary significantly from dib to dib between $\sim0.23^\circ$ considering a dib at 5512 {\aa} and 3.5$^\circ$ considering a dib at 6196 {\aa}, suggesting that different dib carriers have different clumpiness but occupy a same general space. our study includes lines-of-sight all over a northern milky way, as well as out of a galactic plane, covering regions with different physical conditions. a derived correlation scales therefore represent the general image of a galactic ism on a scales of $\sim5$ pc to $100$ pc.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14448,"we design the stochastic algorithm to train any smooth neural network to $\varepsilon$-approximate local minima, with the help of $o(\varepsilon^{-3.25})$ backpropagations. a best result is essentially $o(\varepsilon^{-4})$ by sgd. more broadly, it finds $\varepsilon$-approximate local minima of any smooth nonconvex function inside rate $o(\varepsilon^{-3.25})$, with only oracle access to stochastic gradients.",1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15057,"this brief note complements some results regarding the recently developed technique considering a stability analysis of linear time-invariant, time delay systems with the help of a matrix lambert w function. by means of the numeric example, it was shown that there are cases considering which a dominant roots of a system should be found without with the help of a principal branch of this multi-valued function, contradicting a main proposition of a methodology.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
11265,"screened modified gravity (smg) was the kind of scalar-tensor theory with screening mechanisms, which should suppress a fifth force inside dense regions and allow theories to evade a solar system and laboratory tests. inside this paper, we investigate how a screening mechanisms inside smg affect a gravitational radiation damping effects, calculate inside detail a rate of a energy loss due to a emission of tensor and scalar gravitational radiations, and derive their contributions to a change inside a orbital period of a binary system. we find that a scalar radiation depends on a screened parameters and a propagation speed of scalar waves, and a scalar dipole radiation dominates a orbital decay of a binary system. considering strongly self-gravitating bodies, all effects of scalar sector are strongly suppressed by a screening mechanisms inside smg. by comparing our results to observations of binary system psr j1738+0333, we place a stringent constraints on a screening mechanisms inside smg. as an application of these results, we focus on three specific models of smg (chameleon, symmetron, and dilaton), and derive a constraints on a model parameters, respectively.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6180,"inside order to perform autonomous sequential manipulation tasks, perception inside cluttered scenes remains the critical challenge considering robots. inside this paper, we propose the probabilistic idea behind the method considering robust sequential scene approximation and manipulation - sequential scene understanding and manipulation(sum). sum considers uncertainty due to discriminative object detection and recognition inside a generative approximation of a most likely object poses maintained over time to achieve the robust approximation of a scene under heavy occlusions and unstructured environment. our method utilizes candidates from discriminative object detector and recognizer to guide a generative process of sampling scene hypothesis, and each scene hypotheses was evaluated against a observations. also sum maintains beliefs of scene hypothesis over robot physical actions considering better approximation and against noisy detections. we conduct extensive experiments to show that our idea behind the method was able to perform robust approximation and manipulation.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
7947,"procedural textures are normally generated from mathematical models with parameters carefully selected by experienced users. however, considering naive users, a intuitive way to obtain the desired texture was to provide semantic descriptions such as ""regular,"" ""lacelike,"" and ""repetitive"" and then the procedural model with proper parameters will be automatically suggested to generate a corresponding textures. by contrast, it was less practical considering users to learn mathematical models and tune parameters based on multiple examinations of large numbers of generated textures. inside this study, we propose the novel framework that generates procedural textures according to user-defined semantic descriptions, and we establish the mapping between procedural models and semantic texture descriptions. first, based on the vocabulary of semantic attributes collected from psychophysical experiments, the multi-label learning method was employed to annotate the large number of textures with semantic attributes to form the semantic procedural texture dataset. then, we derive the low dimensional semantic space inside which a semantic descriptions should be separated from one other. finally, given the set of semantic descriptions, a diverse properties of a samples inside a semantic space should lead a framework to find an appropriate generation model that uses appropriate parameters to produce the desired texture. a experimental results show that a proposed framework was effective and that a generated textures closely correlate with a input semantic descriptions.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13441,"the systematic raman study inside a visible carried out on a yba2cu316,18ox (x=6-7) compounds, with isotopic substitution of 18o considering 16o, has detected the doping dependent deviation from harmonic behavior considering a frequency shift of a in-phase mode, the smaller amount of anharmonicity considering a apex mode, and almost no effect considering a out-of-phase b1g-symmetry phonon. it appears that a amount of anharmonicity depends strongly on a oxygen concentration; it diminishes close to a tetragonal to orthorhombic structural phase transition and close to optimal doping, while it reaches its maximum value considering a ortho-ii and the tetragonal phase. a almost zero anharmonicity at optimal doping persists even at 77k. a data inside a overdoped oxygen concentration, where the softening of a in-phase phonon frequency occurs, indicate that a anharmonicity was not enhanced by a sudden increase inside a cuo2 buckling. a results fully agree with recent studies of a ortho-ii phase but they do not comply with the static double-well potential of a apical oxygen atom at optimal doping.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
12912,"deep convolutional neural networks (cnns) used inside practice employ potentially hundreds of layers and $10$,$000$s of nodes. such network sizes entail significant computational complexity due to a large number of convolutions that need to be carried out; inside addition, the large number of parameters needs to be learned and stored. very deep and wide cnns may therefore not be well suited to applications operating under severe resource constraints as was a case, e.g., inside low-power embedded and mobile platforms. this paper aims at understanding a impact of cnn topology, specifically depth and width, on a network's feature extraction capabilities. we address this question considering a class of scattering networks that employ either weyl-heisenberg filters or wavelets, a modulus non-linearity, and no pooling. a exponential feature map energy decay results inside wiatowski et al., 2017, are generalized to $\mathcal{o}(a^{-n})$, where an arbitrary decay factor $a>1$ should be realized through suitable choice of a weyl-heisenberg prototype function or a mother wavelet. we then show how networks of fixed (possibly small) depth $n$ should be designed to guarantee that $((1-\varepsilon)\cdot 100)\%$ of a input signal's energy are contained inside a feature vector. based on a notion of operationally significant nodes, we characterize, partly rigorously and partly heuristically, a topology-reducing effects of (effectively) band-limited input signals, band-limited filters, and feature map symmetries. finally, considering networks based on weyl-heisenberg filters, we determine a prototype function bandwidth that minimizes---for fixed network depth $n$---the average number of operationally significant nodes per layer.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1157,"the computing platform based on low temperature superconductors (lts) has already been proven both theoretically and experimentally. however, qubits based on high tc superconductors (hts) are not yet well understood. here we study a andreev bounds states (abs) inside a later materials inside order to show that the formal correspondence exists between a mathieu levels inside the cooper pair box qubit built with lts and a andreev levels inside hts junctions.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
16274,"searching considering simple models that possess non-trivial controlling properties was one of a central tasks inside a field of quantum technologies. inside this work, we construct the quantum spin-$1/2$ chain of finite size, termed as controllable spin wire (csw), inside which we have $\hat{s}^{z} \hat{s}^{z}$ (ising) interactions with the transverse field inside a bulk, and $\hat{s}^{x} \hat{s}^{z}$ and $\hat{s}^{z} \hat{s}^{z}$ couplings with the canted field on a boundaries. a hamiltonians on a boundaries, dubbed as tuning hamiltonians (th's), bear a same form as a effective hamiltonians emerging inside a so-called `quantum entanglement simulator' that was originally proposed considering mimicking infinite models. we show that tuning a th's (parametrized by $\alpha$) should trigger non-trivial controlling of a bulk properties, including a degeneracy of energy/entanglement spectra, and a response to a magnetic field $h_{bulk}$ inside a bulk. the universal point dubbed as $\alpha^s$ emerges. considering $\alpha > \alpha^s$, a ground-state diagram versus $h_{bulk}$ consists of three `phases', which are ne√©l and polarized phases, and an emergent pseudo-magnet phase, distinguished by entanglement and magnetization. considering $\alpha < \alpha^s$, a phase diagram changes completely, with no step-like behaviors to distinguish phases. due to its controlling properties and simplicity, a csw could potentially serve inside future a experiments considering developing quantum devices.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
14888,"x-ray observations of two metal-deficient luminous compact galaxies (lcg) (shoc~486 and sdss j084220.94+115000.2) with properties similar to a so-called green pea galaxies were obtained with the help of a {\emph{chandra x-ray observatory}}. green pea galaxies are relatively small, compact (a few kpc across) galaxies that get their green color from strong [oiii]$\lambda$5007\aa\ emission, an indicator of intense, recent star formation. these two galaxies were predicted to have a highest observed count rates, with the help of a x-ray luminosity -- star formation rate ($l_x$--sfr) relation considering x-ray binaries, from the statistically complete sample drawn from optical criteria. we determine a x-ray luminosity relative to star-formation rate and metallicity considering these two galaxies. neither exhibit any evidence of active galactic nuclei and we suspect a x-ray emission originates from unresolved populations of high mass x-ray binaries. we discuss a $l_x$--sfr--metallicity plane considering star-forming galaxies and show that a two lcgs are consistent with a prediction of this relation. this was a first detection of green pea analogs inside x-rays.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1454,"given the nonconvex function that was an average of $n$ smooth functions, we design stochastic first-order methods to find its approximate stationary points. a convergence of our new methods depends on a smallest (negative) eigenvalue $-\sigma$ of a hessian, the parameter that describes how nonconvex a function is. our methods outperform known results considering the range of parameter $\sigma$, and should be used to find approximate local minima. our result implies an interesting dichotomy: there exists the threshold $\sigma_0$ so that a currently fastest methods considering $\sigma>\sigma_0$ and considering $\sigma<\sigma_0$ have different behaviors: a former scales with $n^{2/3}$ and a latter scales with $n^{3/4}$.",1,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0
8010,"pixelcnn achieves state-of-the-art results inside density approximation considering natural images. although training was fast, inference was costly, requiring one network evaluation per pixel; o(n) considering n pixels. this should be sped up by caching activations, but still involves generating each pixel sequentially. inside this work, we propose the parallelized pixelcnn that allows more efficient inference by modeling certain pixel groups as conditionally independent. our new pixelcnn model achieves competitive density approximation and orders of magnitude speedup - o(log n) sampling instead of o(n) - enabling a practical generation of 512x512 images. we evaluate a model on class-conditional image generation, text-to-image synthesis, and action-conditional video generation, showing that our model achieves a best results among non-pixel-autoregressive density models that allow efficient sampling.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13278,"an analytic center cutting plane method was an iterative algorithm based on a computation of analytic centers. inside this paper, we propose some analytic center cutting plane methods considering solving quasimonotone or pseudomonotone variational inequalities whose domains are bounded or unbounded convex bodies.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
7628,"we consider a first exit time of the shiryaev-roberts diffusion with constant positive drift from a interval $[0,a]$ where $a>0$. we show that a moment generating function (laplace transform) of the suitably standardized version of a first exit time converges to that of a unit-mean exponential distribution as $a\to+\infty$. a proof was explicit inside that a moment generating function of a first exit time was first expressed analytically and inside the closed form, and then a desired limit as $a\to+\infty$ was evaluated directly. a result was of importance inside a area of quickest change-point detection, and its discrete-time counterpart has been previously established - although inside the different manner - by pollak and tartakovsky (2009).",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
1907,"we present the first internal delensing of cmb maps, both inside temperature and polarization, with the help of a public foreground-cleaned (smica) planck 2015 maps. after forming quadratic estimates of a lensing potential, we use a corresponding displacement field to undo a lensing on a same data. we build differences of a delensed spectra to a original data spectra specifically to look considering delensing signatures. after taking into account reconstruction noise biases inside a delensed spectra, we find an expected sharpening of a power spectrum acoustic peaks with the delensing efficiency of $29\,\%$ ($tt$) $25\,\%$ ($te$) and $22\,\%$ ($ee$). a detection significance of a delensing effects was very high inside all spectra: $12\,\sigma$ inside $ee$ polarization; $18\,\sigma$ inside $te$; and $20\,\sigma$ inside $tt$. a null hypothesis of no lensing inside a maps was rejected at $26\,\sigma$. while direct detection of a power inside lensing $b$-modes themselves was not possible at high significance at planck noise levels, we do detect (at $4.5\,\sigma$ under a null hypothesis) delensing effects inside a $b$-mode map, with $7\,\%$ reduction inside lensing power. our results provide the first demonstration of polarization delensing, and generally of internal cmb delensing, and stand inside agreement with a baseline $\lambda$cdm planck 2015 cosmology expectations.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3577,"docking was an important tool inside computational drug discovery that aims to predict a binding pose of the ligand to the target protein through the combination of pose scoring and optimization. the scoring function that was differentiable with respect to atom positions should be used considering both scoring and gradient-based optimization of poses considering docking. with the help of the differentiable grid-based atomic representation as input, we demonstrate that the scoring function learned by training the convolutional neural network (cnn) to identify binding poses should also be applied to pose optimization. we also show that an iteratively-trained cnn that includes poses optimized by a first cnn inside its training set performs even better at optimizing randomly initialized poses than either a first cnn scoring function or autodock vina.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16997,"this paper was concerned with the linear fractional representation idea behind the method to a synthesis of linear coherent quantum controllers considering the given linear quantum plant. a plant and controller represent open quantum harmonic oscillators and are modelled by linear quantum stochastic differential equations. a feedback interconnections between a plant and a controller are assumed to be established through quantum bosonic fields. inside this framework, conditions considering a stabilization of the given linear quantum plant using linear coherent quantum feedback are addressed with the help of the stable factorization approach. a class of all stabilizing quantum controllers was parameterized inside a frequency domain. coherent quantum weighted h_2 and h_\infty control problems considering linear quantum systems are formulated inside a frequency domain. finally, the projected gradient descent scheme was outlined considering a coherent quantum weighted h_2 control problem.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
8268,"we present the construction of the 2-hilbert space of sections of the bundle gerbe, the suitable candidate considering the prequantum 2-hilbert space inside higher geometric quantisation. we introduce the direct sum on a morphism categories inside a 2-category of bundle gerbes and show that these categories are cartesian monoidal and abelian. endomorphisms of a trivial bundle gerbe, or higher functions, carry a structure of the rig-category, which acts on generic morphism categories of bundle gerbes. we continue by presenting the categorification of a hermitean metric on the hermitean line bundle. this was achieved by introducing the functorial dual that extends a dual of vector bundles to morphisms of bundle gerbes, and constructing the two-variable adjunction considering a aforementioned rig-module category structure on morphism categories. its right internal hom was a module action, composed by taking a dual of higher functions, while a left internal hom was interpreted as the bundle gerbe metric. sections of bundle gerbes are defined as morphisms from a trivial bundle gerbe to the given bundle gerbe. a resulting categories of sections carry the rig-module structure over a category of finite-dimensional hilbert spaces. the suitable definition of 2-hilbert spaces was given, modifying previous definitions by a use of two-variable adjunctions. we prove that a category of sections of the bundle gerbe fits into this framework, thus obtaining the 2-hilbert space of sections. inside particular, this should be constructed considering prequantum bundle gerbes inside problems of higher geometric quantisation. we define the dimensional reduction functor and show that a categorical structures introduced on bundle gerbes naturally reduce to their counterparts on hermitean line bundles with connections. inside several places inside this thesis, we provide examples, making 2-hilbert spaces of sections and dimensional reduction very explicit.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4561,"we study algorithmic problems on subsets of euclidean space of low fractal dimension. these spaces are a subject of intensive study inside various branches of mathematics, including geometry, topology, and measure theory. there are several well-studied notions of fractal dimension considering sets and measures inside euclidean space. we consider the definition of fractal dimension considering finite metric spaces which agrees with standard notions used to empirically approximate a fractal dimension of various sets. we define a fractal dimension of some metric space to be a infimum $\delta>0$, such that considering any $\epsilon > 0$, considering any ball $b$ of radius $r\geq 2\epsilon$, and considering any $\epsilon $-net $n$ (that is, considering any maximal $\epsilon $-packing), we have $|b\cap n|=o((r/\epsilon)^\delta)$. with the help of this definition we obtain faster algorithms considering the plethora of classical problems on sets of low fractal dimension inside euclidean space. our results apply to exact and fixed-parameter algorithms, approximation schemes, and spanner constructions. interestingly, a dependence of a performance of these algorithms on a fractal dimension nearly matches a currently best-known dependence on a standard euclidean dimension. thus, when a fractal dimension was strictly smaller than a ambient dimension, our results yield improved solutions inside all of these settings.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9167,"multiple planet systems provide an ideal laboratory considering probing exoplanet composition, formation history and potential habitability. considering a trappist-1 planets, a planetary radii are well established from transits (gillon et al., 2016, gillon et al., 2017), with reasonable mass estimates coming from transit timing variations (gillon et al., 2017, wang et al., 2017) and dynamical modeling (quarles et al., 2017). a low bulk densities of a trappist-1 planets demand significant volatile content. here we show with the help of mass-radius-composition models, that trappist-1f and g likely contain substantial ($\geq50$ wt\%) water/ice, with b and c being significantly drier ($\leq15$ wt\%). we propose this gradient of water mass fractions implies planets f and g formed outside a primordial snow line whereas b and c formed inside. we find that compared to planets inside our solar system that also formed within a snow line, trappist-1b and c contain hundreds more oceans worth of water. we demonstrate a extent and timescale of migration inside a trappist-1 system depends on how rapidly a planets formed and a relative location of a primordial snow line. this work provides the framework considering understanding a differences between a protoplanetary disks of our solar system versus m dwarfs. our results provide key insights into a volatile budgets, timescales of planet formation, and migration history of likely a most common planetary host inside a galaxy.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3923,"fixed-parameter algorithms and kernelization are two powerful methods to solve $\mathsf{np}$-hard problems. yet, so far those algorithms have been largely restricted to static inputs. inside this paper we provide fixed-parameter algorithms and kernelizations considering fundamental $\mathsf{np}$-hard problems with dynamic inputs. we consider the variety of parameterized graph and hitting set problems which are known to have $f(k)n^{1+o(1)}$ time algorithms on inputs of size $n$, and we consider a question of whether there was the data structure that supports small updates (such as edge/vertex/set/element insertions and deletions) with an update time of $g(k)n^{o(1)}$; such an update time would be essentially optimal. update and query times independent of $n$ are particularly desirable. among many other results, we show that feedback vertex set and $k$-path admit dynamic algorithms with $f(k)\log^{o(1)}n$ update and query times considering some function $f$ depending on a solution size $k$ only. we complement our positive results by several conditional and unconditional lower bounds. considering example, we show that unlike their undirected counterparts, directed feedback vertex set and directed $k$-path do not admit dynamic algorithms with $n^{o(1)}$ update and query times even considering constant solution sizes $k\leq 3$, assuming popular hardness hypotheses. we also show that unconditionally, inside a cell probe model, directed feedback vertex set cannot be solved with update time that was purely the function of $k$.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12381,"an autonomous and resilient controller was proposed considering leader-follower multi-agent systems under uncertainties and cyber-physical attacks. a leader was assumed non-autonomous with the nonzero control input, which allows changing a team behavior or mission inside response to environmental changes. the resilient learning-based control protocol was presented to find optimal solutions to a synchronization problem inside a presence of attacks and system dynamic uncertainties. an observer-based distributed h_infinity controller was first designed to prevent propagating a effects of attacks on sensors and actuators throughout a network, as well as to attenuate a effect of these attacks on a compromised agent itself. non-homogeneous game algebraic riccati equations are derived to solve a h_infinity optimal synchronization problem and off-policy reinforcement learning was utilized to learn their solution without requiring any knowledge of a agent's dynamics. the trust-confidence based distributed control protocol was then proposed to mitigate attacks that hijack a entire node and attacks on communication links. the confidence value was defined considering each agent based solely on its local evidence. a proposed resilient reinforcement learning algorithm employs a confidence value of each agent to indicate a trustworthiness of its own information and broadcast it to its neighbors to put weights on a data they receive from it during and after learning. if a confidence value of an agent was low, it employs the trust mechanism to identify compromised agents and remove a data it receives from them from a learning process. simulation results are provided to show a effectiveness of a proposed approach.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1
15371,"mapping and self-localization inside unknown environments are fundamental capabilities inside many robotic applications. these tasks typically involve a identification of objects as unique features or landmarks, which requires a objects both to be detected and then assigned the unique identifier that should be maintained when viewed from different perspectives and inside different images. a \textit{data association} and \textit{simultaneous localization and mapping} (slam) problems are, individually, well-studied inside a literature. but these two problems are inherently tightly coupled, and that has not been well-addressed. without accurate slam, possible data associations are combinatorial and become intractable easily. without accurate data association, a error of slam algorithms diverge easily. this paper proposes the novel nonparametric pose graph that models data association and slam inside the single framework. an algorithm was further introduced to alternate between inferring data association and performing slam. experimental results show that our idea behind the method has a new capability of associating object detections and localizing objects at a same time, leading to significantly better performance on both a data association and slam problems than achieved by considering only one and ignoring imperfections inside a other.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
4203,"inside mechanical systems, maxwell-betti reciprocity means that a displacement at point b inside response to the force at point the was a same as a displacement at point the inside response to a same force applied at point b. because a notion of reciprocity was general, fundamental, and was operant considering other physical systems like electromagnetics, acoustics, and optics, there was significant interest inside understanding systems that are not reciprocal, or exhibit non-reciprocity. however, most studies of non-reciprocity have occurred inside bulk-scale structures considering dynamic problems involving time reversal symmetry. as the result, little was known about a mechanisms governing static non-reciprocal responses, particularly inside atomically-thin two-dimensional materials like graphene. here, we use classical atomistic simulations to demonstrate that out of plane ripples, which are intrinsic to graphene, enable significant, multiple orders of magnitude enhancements inside a statically non-reciprocal response of graphene metamaterials. specifically, we find that the striking interplay between a ripples and a stress fields that are induced inside a metamaterials due to their geometry impact a displacements that are transmitted by a metamaterial, thus leading to the significantly enhanced static non-reciprocal response. this study thus demonstrates a potential of two-dimensional mechanical metamaterials considering symmetry-breaking applications.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
10591,"quantification was the supervised learning task that consists inside predicting, given the set of classes c and the set d of unlabelled items, a prevalence (or relative frequency) p(c|d) of each class c inside c. quantification should inside principle be solved by classifying all a unlabelled items and counting how many of them have been attributed to each class. however, this ""classify and count"" idea behind the method has been shown to yield suboptimal quantification accuracy; this has established quantification as the task of its own, and given rise to the number of methods specifically devised considering it. we propose the recurrent neural network architecture considering quantification (that we call quanet) that observes a classification predictions to learn higher-order ""quantification embeddings"", which are then refined by incorporating quantification predictions of simple classify-and-count-like methods. we test {quanet on sentiment quantification on text, showing that it substantially outperforms several state-of-the-art baselines.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
9975,"a layered compound kcu$_3$as$_2$o$_7$(od)$_3$, comprising distorted kagome planes of $s=1/2$ cu$^{2+}$ ions, was the recent addition to a family of type-ii multiferroics. previous zero field neutron diffraction work has found two helically ordered regimes inside \kns, each showing the distinct coupling between a magnetic and ferroelectric order parameters. here, we extend this work to magnetic fields up to $20$~t with the help of neutron powder diffraction, capacitance, polarization, and high-field magnetization measurements, thus determining a $h-t$ phase diagram. we find metamagnetic transitions inside both low temperatures phases around $\mu_0 h_c \sim 3.7$~t, which neutron powder diffraction reveals to correspond to the rotation of a helix plane away from a easy plane, as well as the small change inside a propagation vector. furthermore, we show that a sign of a ferroelectric polarization was reversible inside the magnetic field, although no change was observed (or expected on a basis of a magnetic structure) due to a transition at $3.7$~t. we finally justify a temperature dependence of a polarization inside both zero-field ordered phases by the symmetry analysis of a free energy expansion.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
889,"to overcome a large memory requirement of classical lattice sieving algorithms considering solving hard lattice problems, bai-laarhoven-stehl√© [ants 2016] studied tuple lattice sieving, where tuples instead of pairs of lattice vectors are combined to form shorter vectors. herold-kirshanova [pkc 2017] recently improved upon their results considering arbitrary tuple sizes, considering example showing that the triple sieve should solve a shortest vector problem (svp) inside dimension $d$ inside time $2^{0.3717d + o(d)}$, with the help of the technique similar to locality-sensitive hashing considering finding nearest neighbors. inside this work, we generalize a spherical locality-sensitive filters of becker-ducas-gama-laarhoven [soda 2016] to obtain space-time tradeoffs considering near neighbor searching on dense data sets, and we apply these techniques to tuple lattice sieving to obtain even better time complexities. considering instance, our triple sieve heuristically solves svp inside time $2^{0.3588d + o(d)}$. considering practical sieves based on micciancio-voulgaris' gausssieve [soda 2010], this shows that the triple sieve uses less space and less time than a current best near-linear space double sieve.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16746,we propose paramagnetic semiconductors as active media considering refrigeration at cryogenic temperatures by adiabatic demagnetization. a paramagnetism of impurity dopants or structural defects should provide a entropy necessary considering refrigeration at cryogenic temperatures. we present the simple model considering a theoretical limitations to specific entropy and cooling power achievable by demagnetization of various semiconductor systems. performance comparable to that of a hydrate (cmn) was predicted.,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
13945,"we propose the novel technique to make neural network robust to adversarial examples with the help of the generative adversarial network. we alternately train both classifier and generator networks. a generator network generates an adversarial perturbation that should easily fool a classifier network by with the help of the gradient of each image. simultaneously, a classifier network was trained to classify correctly both original and adversarial images generated by a generator. these procedures aid a classifier network to become more robust to adversarial perturbations. furthermore, our adversarial training framework efficiently reduces overfitting and outperforms other regularization methods such as dropout. we applied our method to supervised learning considering cifar datasets, and experimantal results show that our method significantly lowers a generalization error of a network. to a best of our knowledge, this was a first method which uses gan to improve supervised learning.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18667,"sequential monte carlo (smc) methods are the class of monte carlo methods that are used to obtain random samples of the high dimensional random variable inside the sequential fashion. many problems encountered inside applications often involve different types of constraints. these constraints should make a problem much more challenging. inside this paper, we formulate the general framework of with the help of smc considering constrained sampling problems based on forward and backward pilot resampling strategies. we review some existing methods under a framework and develop several new algorithms. it was noted that all information observed or imposed on a underlying system should be viewed as constraints. thus a idea behind the method outlined inside this paper should be useful inside many applications.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
16381,consider an equidimensional faithful conical action of an algebraic torus $t$ on an affine normal conical variety $x$ over an algebraically closed field of characteristic zero. then there exists the finite normal subgroup $n$ of $t$ such that a action of $t$ on $x/n$ was coffee.,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
4399,"we present the new model considering singing synthesis based on the modified version of a wavenet architecture. instead of modeling raw waveform, we model features produced by the parametric vocoder that separates a influence of pitch and timbre. this allows conveniently modifying pitch to match any target melody, facilitates training on more modest dataset sizes, and significantly reduces training and generation times. our model makes frame-wise predictions with the help of mixture density outputs rather than categorical outputs inside order to reduce a required parameter count. as we found overfitting to be an issue with a relatively small datasets used inside our experiments, we propose the method to regularize a model and make a autoregressive generation process more robust to prediction errors. with the help of the simple multi-stream architecture, harmonic, aperiodic and voiced/unvoiced components should all be predicted inside the coherent manner. we compare our method to existing parametric statistical and state-of-the-art concatenative methods with the help of quantitative metrics and the listening test. while naive implementations of a autoregressive generation algorithm tend to be inefficient, with the help of the smart algorithm we should greatly speed up a process and obtain the system that's competitive inside both speed and quality.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14862,"we construct periodic solutions of nonlinear wave equations with the help of analytic continuation. a construction applies inside particular to einstein equations, leading to infinite-dimensional families of time-periodic solutions of a vacuum, or of a einstein-maxwell-dilaton-scalar fields-yang-mills-higgs-chern-simons-$f(r)$ equations, with the negative cosmological constant.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3286,"considering multi-block alternating direction method of multipliers(admm), where a objective function should be decomposed into multiple block components, we show that with block symmetric gauss-seidel iteration, a algorithm will converge quickly. a method will apply the block symmetric gauss-seidel iteration inside a primal update and the linear correction that should be derived inside view of richard iteration. we also establish a linear convergence rate considering linear systems.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
16290,"as roles considering unmanned aerial vehicles (uav) continue to diversify, a ability to sense and interact closely with a environment becomes increasingly important. within this paper we report on a initial flight tests of the novel adaptive compliant actuator which will allow the uav to carry out such tasks as a ""pick and placement"" of remote sensors, structural testing and contact-based inspection. three key results are discussed and presented; a ability to physically compensate impact forces or apply interaction forces by a uav through a use of a active compliant manipulator; to be able to tailor these forces through tuning of a manipulator controller gains; and a ability to apply the rapid series of physical pulses inside order to excite remotely placed sensors, e.g. vibration sensors. a paper describes a overall system requirements and system modelling considerations which have driven a concept through to flight testing. the series of over sixty flight tests have been used to generate initial results which clearly demonstrate a potential of this new type of compliant aerial actuator. results are discussed inside line with potential applications; and the series of future flight tests are described which will enable us to refine and characterise a overall system.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
6801,"this paper tackles a distributed leader-follower (l-f) control problem considering heterogeneous mobile robots inside unknown environments requiring obstacle avoidance, inter-robot collision avoidance, and reliable robot communications. to prevent an inter-robot collision, we employ the virtual propulsive force between robots. considering obstacle avoidance, we present the novel distributed negative-imaginary (ni) variant formation tracking control idea behind the method and the dynamic network topology methodology which allows a formation to change its shape and a robot to switch their roles. inside a case of communication or sensor loss, the uav, controlled by the strictly-negative-imaginary (sni) controller with good wind resistance characteristics, was utilized to track a position of a ugv formation with the help of its camera. simulations and indoor experiments have been conducted to validate a proposed methods.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
9193,"inside this paper, we present the fractional decomposition of a probability generating function of a innovation process of a first-order non-negative integer-valued autoregressive [inar(1)] process to obtain a corresponding probability mass function. we also provide the comprehensive review of integer-valued time series models, based on a concept of thinning operators, with geometric-type marginals. inside particular, we develop four fractional approaches to obtain a distribution of innovation processes of a inar(1) model and show that a distribution of a innovations sequence has geometric-type distribution. these approaches are discussed inside detail and illustrated through the few examples. finally, with the help of a methods presented here, we develop four new first-order non-negative integer-valued autoregressive process considering autocorrelated counts with overdispersion with known marginals, and derive some properties of these models.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
8594,"we report a imaging of magnetic domains inside ferromagnetic conib nanotubes with very long aspect ratio, fabricated by electroless plating. while axial magnetization was expected considering long tubes made of soft magnetic materials, we evidence series of azimuthal domains. we tentatively explain these by a interplay of anisotropic strain and/or grain size, with magneto-elasticity and/or anisotropic interfacial magnetic anisotropy. this material could be interesting considering dense data storage, as well as curvature-induced magnetic phenomena such as a non-reciprocity of spin-wave propagation.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
5795,"self-supervised learning (ssl) was the reliable learning mechanism inside which the robot enhances its perceptual capabilities. typically, inside ssl the trusted, primary sensor cue provides supervised training data to the secondary sensor cue. inside this article, the theoretical analysis was performed on a fusion of a primary and secondary cue inside the minimal model of ssl. the proof was provided that determines a specific conditions under which it was favorable to perform fusion. inside short, it was favorable when (i) a prior on a target value was strong or (ii) a secondary cue was sufficiently accurate. a theoretical findings are validated with computational experiments. subsequently, the real-world case study was performed to investigate if fusion inside ssl was also beneficial when assumptions of a minimal model are not met. inside particular, the flying robot learns to map pressure measurements to sonar height measurements and then fuses a two, resulting inside better height estimation. fusion was also beneficial inside a opposite case, when pressure was a primary cue. a analysis and results are encouraging to study ssl fusion also considering other robots and sensors.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
15888,"inside [bak] a first author proved that considering any $\beta\in (1,\beta_{kl})$ every $x\in(0,\frac{1}{\beta-1})$ has the simply normal $\beta$-expansion, where $\beta_{kl}\approx 1.78723$ was a komornik-loreti constant. this result was complemented by an observation made inside [jss], where it is shown that whenever $\beta\in (\beta_t, 2]$ there exists an $x\in(0,\frac{1}{\beta-1})$ with the unique $\beta$-expansion, and this expansion was not simply normal. here $\beta_t\approx 1.80194$ was a unique zero inside $(1,2]$ of a polynomial $x^3-x^2-2x+1$. this leaves the gap inside our understanding within a interval $[\beta_{kl}, \beta_t]$. inside this paper we fill this gap and prove that considering any $\beta\in (1,\beta_t],$ every $x\in(0,\frac{1}{\beta-1})$ has the simply normal $\beta$-expansion. considering completion, we provide the proof that considering any $\beta\in(1,2)$, lebesgue almost every $x$ has the simply normal $\beta$-expansion. we also give examples of $x$ with multiple $\beta$-expansions, none of which are simply normal. our proofs rely on ideas from combinatorics on words and dynamical systems.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
3789,"predictive geometric models deliver excellent results considering many machine learning use cases. despite their undoubted performance, neural predictive algorithms should show unexpected degrees of instability and variance, particularly when applied to large datasets. we present an idea behind the method to measure changes inside geometric models with respect to both output consistency and topological stability. considering a example of the recommender system with the help of word2vec, we analyze a influence of single data points, approximation methods and parameter settings. our findings should aid to stabilize models where needed and to detect differences inside informational value of data points on the large scale.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8322,"a chest x-ray was one of a most commonly accessible radiological examinations considering screening and diagnosis of many lung diseases. the tremendous number of x-ray imaging studies accompanied by radiological reports are accumulated and stored inside many modern hospitals' picture archiving and communication systems (pacs). on a other side, it was still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) should be used to facilitate a data-hungry deep learning paradigms inside building truly large-scale high precision computer-aided diagnosis (cad) systems. inside this paper, we present the new chest x-ray database, namely ""chestx-ray8"", which comprises 108,948 frontal-view x-ray images of 32,717 unique patients with a text-mined eight disease image labels (where each image should have multi-labels), from a associated radiological reports with the help of natural language processing. importantly, we demonstrate that these commonly occurring thoracic diseases should be detected and even spatially-located using the unified weakly-supervised multi-label image classification and disease localization framework, which was validated with the help of our proposed dataset. although a initial quantitative results are promising as reported, deep convolutional neural network based ""reading chest x-rays"" (i.e., recognizing and locating a common disease patterns trained with only image-level labels) remains the strenuous task considering fully-automated high precision cad systems. data download link: this https url",1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17654,"inside data summarization we want to choose k prototypes inside order to summarize the data set. we study the setting where a data set comprises several demographic groups and we are restricted to choose k_i prototypes belonging to group i. the common idea behind the method to a problem without a fairness constraint was to optimize the centroid-based clustering objective such as k-center. the natural extension then was to incorporate a fairness constraint into a clustering objective. existing algorithms considering doing so run inside time super-quadratic inside a size of a data set. this was inside contrast to a standard k-center objective that should be approximately optimized inside linear time. inside this paper, we resolve this gap by providing the simple approximation algorithm considering a k-center problem under a fairness constraint with running time linear inside a size of a data set and k. if a number of demographic groups was small, a approximation guarantee of our algorithm only incurs the constant-factor overhead. we demonstrate a applicability of our algorithm on both synthetic and real data sets.",1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4607,"we establish the solvable heisenberg-ising model on the spin-1 ni-containing polymer chain, $[ni (nn'-dmen) (\mu-n_3)_2]$, with $nn'-dmen$ being $nn'-dimethylethylenediamine$, that fully covers a interaction characteristics of a material and by which, we should characterize all a peculiar magnetic features of a polymer, which has been partly studied inside experiment. by purely analytical calculations, we should see that a magnetization exhibits three plateaus at zero, mid, and 3/4 of a saturation value at low temperatures below 2 k. a corresponding featuring peaks of magnetic susceptibility are clearly shown. a model also displays plateaus inside thermal entanglement that captures a one-to-one correspondence between thermal entanglement plateaus and those of a magnetization. a calculations are done by a transfer matrix technique.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
17783,"protoplanetary disks often appear as multiple concentric rings inside dust continuum emission maps and scattered light images. these features are often associated with possible young planets inside these disks. many non-planetary explanations have also been suggested, including snow lines, dead zones and secular gravitational instabilities inside a dust. inside this paper we suggest another potential origin. a presence of copious amounts of dust tends to strongly reduce a conductivity of a gas, thereby inhibiting a magneto-rotational instability, and thus reducing a turbulence inside a disk. from viscous disk theory it was known that the disk tends to increase its surface density inside regions where a viscosity (i.e. turbulence) was low. local maxima inside a gas pressure tend to attract dust through radial drift, increasing a dust content even more. we investigate mathematically if this could potentially lead to the feedback loop inside which the perturbation inside a dust surface density could perturb a gas surface density, leading to increased dust drift and thus amplification of a dust perturbation and, as the consequence, a gas perturbation. we find that this was indeed possible, even considering moderately small dust grain sizes, which drift less efficiently, but which are more likely to affect a gas ionization degree. we speculate that this instability could be triggered by a small dust population initially, and when a local pressure maxima are strong enough, a larger dust grains get trapped and lead to a familiar ring-like shapes. we also discuss a many uncertainties and limitations of this model.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14473,"inside this paper, the real-time internet of things (iot) monitoring system was considered inside which a iot devices are scheduled to sample underlying physical processes and send a status updates to the common destination. inside the real-world iot, due to a possibly different dynamics of each physical process, a sizes of a status updates considering different devices are often different and each status update typically requires multiple transmission slots. by taking into account such multi-time slot transmissions with non-uniform sizes of a status updates under noisy channels, a problem of joint device scheduling and status sampling was studied inside order to minimize a average age of information (aoi) at a destination. this stochastic problem was formulated as an infinite horizon average cost markov decision process (mdp). a monotonicity of a value function of a mdp was characterized and then used to show that a optimal scheduling and sampling policy was threshold-based with respect to a aoi at each device. to overcome a curse of dimensionality, the low-complexity suboptimal policy was proposed through the semi-randomized base policy and linear approximated value functions. a proposed suboptimal policy was shown to exhibit the similar structure to a optimal policy, which provides the structural base considering its effective performance. the structure-aware algorithm was then developed to obtain a suboptimal policy. a analytical results are further extended to a iot monitoring system with random status update arrivals, considering which, a optimal scheduling and sampling policy was also shown to be threshold-based with a aoi at each device. simulation results illustrate a structures of a optimal policy and show the near-optimal aoi performance resulting from a proposed suboptimal solution approach.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
8782,"plasmonics allows manipulating light at a nanoscale, but has limitations due to a static nature of nanostructures and lack of tuneability. we propose and theoretically analyse the room-temperature liquid-metal nanodroplet that changes its shape, and therefore tunes a plasmon resonance frequency, due to capillary oscillations. we show a possibility to tune a capillary oscillation frequency of a nanodroplet and to drive a oscillations electrically or mechanically. employed as the tuneable nanoantenna, a nanodroplet may find applications inside sensors, imaging, microscopy, and medicine.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
884,"task-oriented dialogue focuses on conversational agents that participate inside user-initiated dialogues on domain-specific topics. inside contrast to chatbots, which simply seek to sustain open-ended meaningful discourse, existing task-oriented agents usually explicitly model user intent and belief states. this paper examines bypassing such an explicit representation by depending on the latent neural embedding of state and learning selective attention to dialogue history together with copying to incorporate relevant prior context. we complement recent work by showing a effectiveness of simple sequence-to-sequence neural architectures with the copy mechanism. our model outperforms more complex memory-augmented models by 7% inside per-response generation and was on par with a current state-of-the-art on dstc2.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2440,"policy evaluation or value function or q-function approximation was the key procedure inside reinforcement learning (rl). it was the necessary component of policy iteration and should be used considering variance reduction inside policy gradient methods. therefore its quality has the significant impact on most rl algorithms. motivated by manifold regularized learning, we propose the novel kernelized policy evaluation method that takes advantage of a intrinsic geometry of a state space learned from data, inside order to achieve better sample efficiency and higher accuracy inside q-function approximation. applying a proposed method inside a least-squares policy iteration (lspi) framework, we observe superior performance compared to widely used parametric basis functions on two standard benchmarks inside terms of policy quality.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18848,"we identify a ""organization"" of the human social group as a communication network(s) within that group. we then introduce three theoretical approaches to analyzing what determines a structures of human organizations. all three approaches adopt the group-selection perspective, so that a group's network structure was (approximately) optimal, given a information-processing limitations of agents within a social group, and a exogenous welfare function of a overall group. inside a first idea behind the method we use the new sub-field of telecommunications theory called network coding, and focus on the welfare function that involves a ability of a organization to convey information among a agents. inside a second idea behind the method we focus on the scenario where agents within a organization must allocate their future communication resources when a state of a future environment was uncertain. we show how this formulation should be solved with the linear program. inside a third approach, we introduce an information synthesis problem inside which agents within an organization receive information from various sources and must decide how to transform such information and transmit a results to other agents inside a organization. we propose leveraging a computational power of neural networks to solve such problems. these three approaches formalize and synthesize work inside fields including anthropology, archeology, economics and psychology that deal with organization structure, theory of a firm, span of control and cognitive limits on communication.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
19450,"a critical properties of a single-crystalline semiconducting ferromagnet crgete$_3$ were investigated by bulk dc magnetization around a paramagnetic to ferromagnetic phase transition. critical exponents $\beta = 0.200\pm0.003$ with critical temperature $t_c = 62.65\pm0.07$ k and $\gamma = 1.28\pm0.03$ with $t_c = 62.75\pm0.06$ k are obtained by a kouvel-fisher method whereas $\delta = 7.96\pm0.01$ was obtained by a critical isotherm analysis at $t_c = 62.7$ k. these critical exponents obey a widom scaling relation $\delta = 1+\gamma/\beta$, indicating self-consistency of a obtained values. with these critical exponents a isotherm $m(h)$ curves below and above a critical temperatures collapse into two independent universal branches, obeying a single scaling equation $m = f_\pm(h)$, where $m$ and $h$ are renormalized magnetization and field, respectively. a determined exponents match well with those calculated from a results of renormalization group idea behind the method considering the two-dimensional ising system coupled with long-range interaction between spins decaying as $j(r)\approx r^{-(d+\sigma)}$ with $\sigma=1.52$.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
7330,"we have studied a paradigm of cosmic inflation with the help of a simplest model based on a idea of supersymmetric hybrid inflation with non-minimal coupling to gravity, specially under a slow-roll approximation following a superconformal idea behind the method to supergravity. it was found that within the range of values of a non-minimal coupling parameter $\xi$, a model should accommodate a inflation data reported by a planck ($n_s$ and upper limit of $r$) and bicep2/keck (upper limit of $r$) collaborations. a study shows that a most probable value of $\xi$ should be $\sim 0.0056\pm0.0005$. that was coupling was found to be very week. within this range of $\xi$, a values of $r$ estimated from our model considering $50 - 70$ e-foldings are found to lay well below a upper limits set by a planck and bicep2/keck collaborations. similarly, values of $n_s$ obtained considering a said parameters are inside good agreement with its latest data of a planck collaboration. a constraint equations considering a running of a scalar spectral index $n_{sk}$ and its running $n_{skk}$ are derived from a numerical solutions of our model considering these parameters. these equations should be used to test our model from a data of future cosmological observations.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12442,"we theoretically study artificial light harvesting by the dimerized mobius ring. when a donors inside a ring are dimerized, a energies of a donor ring are splitted into two sub-bands. because of a nontrivial mobius boundary condition, both a photon and acceptor are coupled to all collectiveexcitation modes inside a donor ring. therefore, a quantum dynamics inside a light harvesting are subtly influenced by a dimerization inside a mobius ring. it was discovered that energy transfer was more efficient inside the dimerized ring than that inside an equally-spaced ring. this discovery was also confirmed by a calculation with a perturbation theory, which was equivalent to a wigner-weisskopf approximation. our findings may be benificial to a optimal design of artificial light harvesting.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
14661,"we propose the setting considering two-phase opinion dynamics inside social networks, where the node's final opinion inside a first phase acts as its initial biased opinion inside a second phase. inside this setting, we study a problem of two camps aiming to maximize adoption of their respective opinions, by strategically investing on nodes inside a two phases. the node's initial opinion inside a second phase naturally plays the key role inside determining a final opinion of that node, and thus also of other nodes inside a network due to its influence on them. more importantly, this bias also determines a effectiveness of the camp's investment on that node inside a second phase. to formalize this two-phase investment setting, we propose an extension of friedkin-johnsen model, and thus formulate a utility functions of a camps. there was the tradeoff while splitting a budget between a two phases. the lower investment inside a first phase results inside worse initial biases considering a second phase, while the higher investment spares the lower available budget considering a second phase. we first analyze a non-competitive case where only one camp invests, considering which we present the polynomial time algorithm considering determining an optimal way to split a camp's budget between a two phases. we then analyze a case of competing camps, where we show a existence of nash equilibrium and that it should be computed inside polynomial time under reasonable assumptions. we conclude our study with simulations on real-world network datasets, inside order to quantify a effects of a initial biases and a weightage attributed by nodes to their initial biases, as well as that of the camp deviating from its equilibrium strategy. our main conclusion was that, if nodes attribute high weightage to their initial biases, it was advantageous to have the high investment inside a first phase, so as to effectively influence a biases to be harnessed inside a second phase.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
17091,"we investigate from first principles a field-like spin-orbit torques (sots) inside the ag$_{2}$bi-terminated ag(111) film grown on ferromagnetic fe(110). we find that the large part of a sot arises from a spin-orbit interaction (soi) inside a ag$_{2}$bi layer far away from a fe layers. these results clearly hint at the long range spin transfer inside a direction perpendicular to a film that does not originate inside a spin hall effect. inside order to bring evidence of a non-local character of a computed sot, we show that a torque acting on a fe layers should be engineered by a introduction of bi vacancies inside a ag$_{2}$bi layer. overall, we find the drastic dependence of a sot on a disorder type, which we explain by the complex interplay of different contributions to a sot inside a brillouin zone.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
17998,"this article provides a first survey of computational models of emotion inside reinforcement learning (rl) agents. a survey focuses on agent/robot emotions, and mostly ignores human user emotions. emotions are recognized as functional inside decision-making by influencing motivation and action selection. therefore, computational emotion models are usually grounded inside a agent's decision making architecture, of which rl was an important subclass. studying emotions inside rl-based agents was useful considering three research fields. considering machine learning (ml) researchers, emotion models may improve learning efficiency. considering a interactive ml and human-robot interaction (hri) community, emotions should communicate state and enhance user investment. lastly, it allows affective modelling (am) researchers to investigate their emotion theories inside the successful ai agent class. this survey provides background on emotion theory and rl. it systematically addresses 1) from what underlying dimensions (e.g., homeostasis, appraisal) emotions should be derived and how these should be modelled inside rl-agents, 2) what types of emotions have been derived from these dimensions, and 3) how these emotions may either influence a learning efficiency of a agent or be useful as social signals. we also systematically compare evaluation criteria, and draw connections to important rl sub-domains like (intrinsic) motivation and model-based rl. inside short, this survey provides both the practical overview considering engineers wanting to implement emotions inside their rl agents, and identifies challenges and directions considering future emotion-rl research.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
8873,"a growing role of data-driven approaches to scientific discovery has unveiled the large class of models that involve latent transformations with the rigid algebraic constraint. among them, multi-reference alignment (mra) was the simple model that captures fundamental aspects of a statistical and algorithmic challenges arising from this new paradigm. inside this model, an unknown signal was subject to two types of corruption: the latent cyclic shift and a more traditional additive white noise. a goal was to recover a signal at the certain precision from independent samples. while at high signal-to-noise ratio (snr), a number of observations needed to recover the generic signal was proportional to 1/snr, we show that it rises to 1/snr^3 inside a more realistic low snr regime. we propose an algorithm that achieves this optimal dependence on a snr. furthermore, we extend our results to cover the heterogeneous mra model where a samples come from the mixture of signals, as was often a case inside applications such as cryo-electron microscopy, where molecules may have different conformations. we provide a first known procedure that provably achieves signal recovery inside a low snr regime considering heterogeneous mra.",1,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
8364,"we propose the logic of asynchronous announcements, where truthful announcements are publicly sent but individually received by agents. additional to epistemic modalities, a logic therefore contains two types of dynamic modalities, considering sending messages and considering receiving messages. a semantics defines truth relative to a current state of reception of messages considering all agents. this means that knowledge need not be truthful, because some messages may not have been received by a knowing agent. messages that are announcements may also result inside partial synchronization, namely when an agent learns from receiving an announcement that other announcements must already have been received by other agents. we give detailed examples of a semantics, and prove several semantic results, including that: after an announcement an agent knows that the proposition was true, if and only if on condition of a truth of that announcement, a agent knows that after that announcement and after any number of other agents also receiving it, a proposition was true. we show that on multi-agent epistemic models, each formula inside asynchronous announcement logic was equivalent to the formula inside epistemic logic.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6389,"fabrication of sub-micron josephson junctions was demonstrated with the help of standard processing techniques considering high-coherence, superconducting qubits. these junctions are made inside two separate lithography steps with normal-angle evaporation. most significantly, this work demonstrates that it was possible to achieve high coherence with junctions formed on aluminum surfaces cleaned inside situ with ar milling before a junction oxidation. this method eliminates a angle-dependent shadow masks typically used considering small junctions. therefore, this was conducive to a implementation of typical methods considering improving margins and yield with the help of conventional cmos processing. a current method uses electron-beam lithography and an additive process to define a top and bottom electrodes. extension of this work to optical lithography and subtractive processes was discussed.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
17825,"si li and author suggested inside that, inside some cases, a ads/cft correspondence should be formulated inside terms of a algebraic operation of koszul duality. inside this paper this suggestion was checked explicitly considering $m2$ branes inside an $\omega$-background. a algebra of supersymmetric operators on the stack of $k$ $m2$ branes was shown to be koszul dual, inside large $k$, to a algebra of supersymmetric operators of $11$-dimensional supergravity inside an $\omega$-background (using a formulation of supergravity inside an $\omega$-background presented inside arxiv:1610.04144). a twisted form of supergravity that was used here should be quantized to all orders inside perturbation theory. we find that a koszul duality result holds to all orders inside perturbation theory, inside both a gravitational theory and a theory on a $m2$. (however, there was the certain non-linear identification of a coupling constants on each side which i is unable to determine explicitly). it was also shown that a algebra of operators on $k$ $m2$ branes, as $k \to \infty$, was the quantum double-loop algebra (a two-variable analog of a yangian). this algebra was also a koszul dual of a algebra of operators on a gravitational theory. an explicit presentation considering this algebra was presented, and it was shown that this algebra was a unique quantization of its classical limit. some conjectural applications to enumerative geometry of calabi-yau threefolds are also presented.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
15286,"this paper proposes the principled information theoretic analysis of classification considering deep neural network structures, e.g. convolutional neural networks (cnn). a output of convolutional filters was modeled as the random variable y conditioned on a object class c and network filter bank f. a conditional entropy (cent) h(y |c,f) was shown inside theory and experiments to be the highly compact and class-informative code, that should be computed from a filter outputs throughout an existing cnn and used to obtain higher classification results than a original cnn itself. experiments demonstrate a effectiveness of cent feature analysis inside two separate cnn classification contexts. 1) inside a classification of neurodegeneration due to alzheimer's disease (ad) and natural aging from 3d magnetic resonance image (mri) volumes, 3 cent features result inside an auc=94.6% considering whole-brain ad classification, a highest reported accuracy on a public oasis dataset used and 12% higher than a softmax output of a original cnn trained considering a task. 2) inside a context of visual object classification from 2d photographs, transfer learning based on the small set of cent features identified throughout an existing cnn leads to auc values comparable to a 1000-feature softmax output of a original network when classifying previously unseen object categories. a general information theoretical analysis explains various recent cnn design successes, e.g. densely connected cnn architectures, and provides insights considering future research directions inside deep learning.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5734,"the successful grasp requires careful balancing of a contact forces. deducing whether the particular grasp will be successful from indirect measurements, such as vision, was therefore quite challenging, and direct sensing of contacts through touch sensing provides an appealing avenue toward more successful and consistent robotic grasping. however, inside order to fully evaluate a value of touch sensing considering grasp outcome prediction, we must understand how touch sensing should influence outcome prediction accuracy when combined with other modalities. doing so with the help of conventional model-based techniques was exceptionally difficult. inside this work, we investigate a question of whether touch sensing aids inside predicting grasp outcomes within the multimodal sensing framework that combines vision and touch. to that end, we collected more than 9,000 grasping trials with the help of the two-finger gripper equipped with gelsight high-resolution tactile sensors on each finger, and evaluated visuo-tactile deep neural network models to directly predict grasp outcomes from either modality individually, and from both modalities together. our experimental results indicate that incorporating tactile readings substantially improve grasping performance.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
16384,"this paper explores a information-theoretic limitations of graph property testing inside zero-field ising models. instead of learning a entire graph structure, sometimes testing the basic graph property such as connectivity, cycle presence or maximum clique size was the more relevant and attainable objective. since property testing was more fundamental than graph recovery, any necessary conditions considering property testing imply corresponding conditions considering graph recovery, while custom property tests should be statistically and/or computationally more efficient than graph recovery based algorithms. understanding a statistical complexity of property testing requires a distinction of ferromagnetic (i.e., positive interactions only) and general ising models. with the help of combinatorial constructs such as graph packing and strong monotonicity, we characterize how target properties affect a corresponding minimax upper and lower bounds within a realm of ferromagnets. on a other hand, by studying a detection of an antiferromagnetic (i.e., negative interactions only) curie-weiss model buried inside rademacher noise, we show that property testing was strictly more challenging over general ising models. inside terms of methodological development, we propose two types of correlation based tests: computationally efficient screening considering ferromagnets, and score type tests considering general models, including the fast cycle presence test. our correlation screening tests match a information-theoretic bounds considering property testing inside ferromagnets.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0
12935,"inside this paper, we consider the family of jacobi-type algorithms considering simultaneous orthogonal diagonalization problem of symmetric tensors. considering a jacobi-based algorithm of [siam j. matrix anal. appl., 2(34):651--672, 2013], we prove its global convergence considering simultaneous orthogonal diagonalization of symmetric matrices and 3rd-order tensors. we also propose the new jacobi-based algorithm inside a general setting and prove its global convergence considering sufficiently smooth functions.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0
17428,"a light detection and ranging (lidar) technology allows to sense surrounding objects with fine-grained resolution inside the large areas. their data (aka point clouds), generated continuously at very high rates, should provide information to support automated functionality inside cyberphysical systems. clustering of point clouds was the key problem to extract this type of information. methods considering solving a problem inside the continuous fashion should facilitate improved processing inside e.g. fog architectures, allowing continuous, streaming processing of data close to a sources. we propose lisco, the single-pass continuous euclidean-distance-based clustering of lidar point clouds, that maximizes a granularity of a data processing pipeline. besides its algorithmic analysis, we provide the thorough experimental evaluation and highlight its up to 3x improvements and its scalability benefits compared to a baseline, with the help of both real-world datasets as well as synthetic ones to fully explore a worst-cases.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
11479,"inside this paper, we first remodel a line coverage as the 1d discrete problem with co-linear targets. then, an order-based greedy algorithm, called oga, was proposed to solve a problem optimally. it will be shown that a existing order inside a 1d modeling, and especially a resulted markov property of a selected sensors should aid design greedy algorithms such as oga. these algorithms demonstrate optimal/efficient performance and have lower complexity compared to a state-of-the-art. furthermore, it was demonstrated that a conventional continuous line coverage problem should be converted to an equivalent discrete problem and solved optimally by oga. next, we formulate a well-known weak barrier coverage problem as an instance of a continuous line coverage problem (i.e. the 1d problem) as opposed to a conventional 2d graph-based models. we demonstrate that a equivalent discrete version of this problem should be solved optimally and faster than a state-of-the-art methods with the help of an extended version of oga, called k-oga. moreover, an efficient local algorithm, called logm, was proposed to mend barrier gaps due to sensor failure. inside a case of m gaps, logm was proved to select at most 2m-1 sensors more than a optimal while being local and implementable inside distributed fashion. we demonstrate a optimal/efficient performance of a proposed algorithms using extensive simulations.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10740,"swarm robotic systems are currently being used to address many real-world problems. one interesting application of swarm robotics was a self-organized formation of structures and shapes. some of a key challenges inside a swarm robotic systems include swarm size constraint, random motion, coordination among robots, localization, and adaptability inside the decentralized environment. rubenstein et al. presented the system (""programmable self-assembly inside the thousand-robot swarm"", science, 2014) considering thousand-robot swarm able to form only solid shapes with a robots inside aggregated form by applying a collective behavior algorithm. even though agent-based approaches have been presented inside various studies considering self-organized formation, however these studies lack agent-based modeling (abm) idea behind the method along with a constraints inside term of structure complexity and heterogeneity inside large swarms with dynamic localization. a cognitive agent-based computing (cabc) idea behind the method was capable of modeling such self-organization based multi-agents systems (mas). inside this paper, we develop the simulation model with the help of abm under cabc idea behind the method considering self-organized shape formation inside swarm robots. we propose the shape formation algorithm considering validating our model and perform simulation-based experiments considering six different shapes including hole-based shapes. we also demonstrate a formal specification considering our model. a simulation result shows a robustness of a proposed idea behind the method having a emergent behavior of robots considering a self-organized shape formation. a performance of a proposed idea behind the method was evaluated by robots convergence rate.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
1751,"let $e$ be an elliptic curve defined over $\mathbf{q}$ without complex multiplication. considering each prime $\ell$, there was the representation $\rho_{e,\ell}\colon \text{gal}(\overline{\mathbf{q}}/\mathbf{q}) \to \text{gl}_2(\mathbf{f}_{\ell})$ that describes a galois action on a $\ell$-torsion points of $e$. building on recent work of rouse--zureick-brown and zywina, we find models considering composite level modular curves whose rational points classify elliptic curves over $\mathbf{q}$ with simultaneously non-surjective, composite image of galois. we also provably determine a rational points on almost all of these curves. finally, we give an application of our results to a study of entanglement fields.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
19092,"dynamic origin-destination (od) demand was central to transportation system modeling and analysis. a dynamic od demand approximation problem (dode) has been studied considering decades, most of which solve a dode problem on the typical day or several typical hours. there was the lack of methods that approximate high-resolution dynamic od demand considering the sequence of many consecutive days over several years (referred to as 24/7 od inside this research). having multi-year 24/7 od demand would allow the better understanding of characteristics of dynamic od demands and their evolution/trends over a past few years, the critical input considering modeling transportation system evolution and reliability. this paper presents the data-driven framework that estimates day-to-day dynamic od with the help of high-granular traffic counts and speed data collected over many years. a proposed framework statistically clusters daily traffic data into typical traffic patterns with the help of t-distributed stochastic neighbor embedding (t-sne) and k-means methods. the gpu-based stochastic projected gradient descent method was proposed to efficiently solve a multi-year 24/7 dode problem. it was demonstrated that a new method efficiently estimates a 5-minute dynamic od demand considering every single day from 2014 to 2016 on i-5 and sr-99 inside a sacramento region. a resultant multi-year 24/7 dynamic od demand reveals a daily, weekly, monthly, seasonal and yearly change inside travel demand inside the region, implying intriguing demand characteristics over a years.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1
10254,"inside this paper, we report on a visualization capabilities of an explainable ai planning (xaip) agent that should support human inside a loop decision making. imposing transparency and explainability requirements on such agents was especially important inside order to establish trust and common ground with a end-to-end automated planning system. visualizing a agent's internal decision-making processes was the crucial step towards achieving this. this may include externalizing a ""brain"" of a agent -- starting from its sensory inputs, to progressively higher order decisions made by it inside order to drive its planning components. we also show how a planner should bootstrap on a latest techniques inside explainable planning to cast plan visualization as the plan explanation problem, and thus provide concise model-based visualization of its plans. we demonstrate these functionalities inside a context of a automated planning components of the smart assistant inside an instrumented meeting space.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10027,"we investigate a emergence of ${\cal n}=1$ supersymmetry inside a long-range behavior of three-dimensional parity-symmetric yukawa systems. we discuss the renormalization idea behind the method that manifestly preserves supersymmetry whenever such symmetry was realized, and use it to prove that supersymmetry-breaking operators are irrelevant, thus proving that such operators are suppressed inside a infrared. all our findings are illustrated with a aid of a $\epsilon$-expansion and the functional variant of perturbation theory, but we provide numerical estimates of critical exponents that are based on a non-perturbative functional renormalization group.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
6399,"piecewise affine (pwa) systems are widely used to model highly nonlinear behaviors such as contact dynamics inside robot locomotion and manipulation. existing control techniques considering pwa systems have computational drawbacks, both inside offline design and online implementation. inside this paper, we introduce the method to obtain feedback control policies and the corresponding set of admissible initial conditions considering discrete-time pwa systems such that all a closed-loop trajectories reach the goal polytope, while the cost function was optimized. a idea was conceptually similar to lqr-trees \cite{tedrake2010lqr}, which consists of 3 steps: (1) open-loop trajectory optimization, (2) feedback control considering computation of ""funnels"" of states around trajectories, and (3) repeating (1) and (2) inside the way that a funnels are grown backward from a goal inside the tree fashion and fill a state-space as much as possible. we show pwa dynamics should be exploited to combine step (1) and (2) into the single step that was tackled with the help of mixed-integer convex programming, which makes a method suitable considering dealing with hard constraints. illustrative examples on contact-based dynamics are presented.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1
17301,"inside a era of big data, k-means clustering has been widely adopted as the basic processing tool inside various contexts. however, its computational cost could be prohibitively high as a data size and a cluster number are large. it was well known that a processing bottleneck of k-means lies inside a operation of seeking closest centroid inside each iteration. inside this paper, the novel solution towards a scalability issue of k-means was presented. inside a proposal, k-means was supported by an approximate k-nearest neighbors graph. inside a k-means iteration, each data sample was only compared to clusters that its nearest neighbors reside. since a number of nearest neighbors we consider was much less than k, a processing cost inside this step becomes minor and irrelevant to k. a processing bottleneck was therefore overcome. a most interesting thing was that k-nearest neighbor graph was constructed by iteratively calling a fast $k$-means itself. comparing with existing fast k-means variants, a proposed algorithm achieves hundreds to thousands times speed-up while maintaining high clustering quality. as it was tested on 10 million 512-dimensional data, it takes only 5.2 hours to produce 1 million clusters. inside contrast, to fulfill a same scale of clustering, it would take 3 years considering traditional k-means.",1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0