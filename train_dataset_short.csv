id,ABSTRACT,Computer Science,Mathematics,Physics,Statistics,Analysis of PDEs,Applications,Artificial Intelligence,Astrophysics of Galaxies,Computation and Language,Computer Vision and Pattern Recognition,Cosmology and Nongalactic Astrophysics,Data Structures and Algorithms,Differential Geometry,Earth and Planetary Astrophysics,Fluid Dynamics,Information Theory,Instrumentation and Methods for Astrophysics,Machine Learning,Materials Science,Methodology,Number Theory,Optimization and Control,Representation Theory,Robotics,Social and Information Networks,Statistics Theory,Strongly Correlated Electrons,Superconductivity,Systems and Control
1824,"a ever-growing datasets inside observational astronomy have challenged scientists inside many aspects, including an efficient and interactive data exploration and visualization. many tools have been developed to confront this challenge. however, they usually focus on displaying a actual images or focus on visualizing patterns within catalogs inside the predefined way. inside this paper we introduce vizic, the python visualization library that builds a connection between images and catalogs through an interactive map of a sky region. vizic visualizes catalog data over the custom background canvas with the help of a shape, size and orientation of each object inside a catalog. a displayed objects inside a map are highly interactive and customizable comparing to those inside a images. these objects should be filtered by or colored by their properties, such as redshift and magnitude. they also should be sub-selected with the help of the lasso-like tool considering further analysis with the help of standard python functions from in the jupyter notebook. furthermore, vizic allows custom overlays to be appended dynamically on top of a sky map. we have initially implemented several overlays, namely, voronoi, delaunay, minimum spanning tree and healpix grid layers, which are helpful considering visualizing large-scale structure. all these overlays should be generated, added or removed interactively with one line of code. a catalog data was stored inside the non-relational database, and a interfaces were developed inside javascript and python to work within jupyter notebook, which allows to create custom widgets, user generated scripts to analyze and plot a data selected/displayed inside a interactive map. this unique design makes vizic the very powerful and flexible interactive analysis tool. vizic should be adopted inside variety of exercises, considering example, data inspection, clustering analysis, galaxy alignment studies, outlier identification or simply large-scale visualizations.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
3094,"we propose the framework considering optimal $t$-matchings excluding a prescribed $t$-factors inside bipartite graphs. a proposed framework was the generalization of a nonbipartite matching problem and includes several problems, such as a triangle-free $2$-matching, square-free $2$-matching, even factor, and arborescence problems. inside this paper, we demonstrate the unified understanding of these problems by commonly extending previous important results. we solve our problem under the reasonable assumption, which was sufficiently broad to include a specific problems listed above. we first present the min-max theorem and the combinatorial algorithm considering a unweighted version. we then provide the linear programming formulation with dual integrality and the primal-dual algorithm considering a weighted version. the key ingredient of a proposed algorithm was the technique to shrink forbidden structures, which corresponds to a techniques of shrinking odd cycles, triangles, squares, and directed cycles inside edmonds' blossom algorithm, the triangle-free $2$-matching algorithm, the square-free $2$-matching algorithm, and an arborescence algorithm, respectively.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8463,"nanostructures with open shell transition metal or molecular constituents host often strong electronic correlations and are highly sensitive to atomistic material details. this tutorial review discusses method developments and applications of theoretical approaches considering a realistic description of a electronic and magnetic properties of nanostructures with correlated electrons. first, a implementation of the flexible interface between density functional theory and the variant of dynamical mean field theory (dmft) highly suitable considering a simulation of complex correlated structures was explained and illustrated. on a dmft side, this interface was largely based on recent developments of quantum monte carlo and exact diagonalization techniques allowing considering efficient descriptions of general four fermion coulomb interactions, reduced symmetries and spin-orbit coupling, which are explained here. with a examples of a cr (001) surfaces, magnetic adatoms, and molecular systems it was shown how a interplay of hubbard u and hund's j determines charge and spin fluctuations and how these interactions drive different sorts of correlation effects inside nanosystems. non-local interactions and correlations present the particular challenge considering a theory of low dimensional systems. we present our method developments addressing these two challenges, i.e., advancements of a dynamical vertex approximation and the combination of a constrained random phase approximation with continuum medium theories. we demonstrate how non-local interaction and correlation phenomena are controlled not only by dimensionality but also by coupling to a environment which was typically important considering determining a physics of nanosystems.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
2082,"stars are self-gravitating fluids inside which pressure, buoyancy, rotation and magnetic fields provide a restoring forces considering global modes of oscillation. pressure and buoyancy energetically dominate, while rotation and magnetism are generally assumed to be weak perturbations and often ignored. however, observations of anomalously weak dipole mode amplitudes inside red giant stars suggest that the substantial fraction of these are subject to an additional source of damping localised to their core region, with indirect evidence pointing to a role of the deeply buried magnetic field. it was also known that inside many instances a gravity-mode character of affected modes was preserved, but so far no effective damping mechanism has been proposed that accommodates this aspect. here we present such the mechanism, which damps a oscillations of stars harbouring magnetised cores using resonant interactions with standing alfv√©n modes of high harmonic index. a damping rates produced by this mechanism are quantitatively on par with those associated with turbulent convection, and inside a range required to explain observations, considering realistic stellar models and magnetic field strengths. our results suggest that magnetic fields should provide an efficient means of damping stellar oscillations without needing to disrupt a internal structure of a modes, and lay a groundwork considering an extension of a theory of global stellar oscillations that incorporates these effects.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8687,"deep neural perception and control networks are likely to be the key component of self-driving vehicles. these models need to be explainable - they should provide easy-to-interpret rationales considering their behavior - so that passengers, insurance companies, law enforcement, developers etc., should understand what triggered the particular behavior. here we explore a use of visual explanations. these explanations take a form of real-time highlighted regions of an image that causally influence a network's output (steering control). our idea behind the method was two-stage. inside a first stage, we use the visual attention model to train the convolution network end-to-end from images to steering angle. a attention model highlights image regions that potentially influence a network's output. some of these are true influences, but some are spurious. we then apply the causal filtering step to determine which input regions actually influence a output. this produces more succinct visual explanations and more accurately exposes a network's behavior. we demonstrate a effectiveness of our model on three datasets totaling 16 hours of driving. we first show that training with attention does not degrade a performance of a end-to-end network. then we show that a network causally cues on the variety of features that are used by humans while driving.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2342,"analyzing job hopping behavior was important considering understanding job preference and career progression of working individuals. when analyzed at a workforce population level, job hop analysis helps to gain insights of talent flow among different jobs and organizations. traditionally, surveys are conducted on job seekers and employers to study job hop behavior. beyond surveys, job hop behavior should also be studied inside the highly scalable and timely manner with the help of the data driven idea behind the method inside response to fast-changing job landscape. fortunately, a advent of online professional networks (opns) has made it possible to perform the large-scale analysis of talent flow. inside this paper, we present the new data analytics framework to analyze a talent flow patterns of close to 1 million working professionals from three different countries/regions with the help of their publicly-accessible profiles inside an established opn. as opn data are originally generated considering professional networking applications, our proposed framework re-purposes a same data considering the different analytics task. prior to performing job hop analysis, we devise the job title normalization procedure to mitigate a amount of noise inside a opn data. we then devise several metrics to measure a amount of work experience required to take up the job, to determine that existence duration of a job (also known as a job age), and a correlation between a above metric and propensity of hopping. we also study how job hop behavior was related to job promotion/demotion. lastly, we perform connectivity analysis at job and organization levels to derive insights on talent flow as well as job and organizational competitiveness.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
16866,"a need to reason about uncertainty inside large, complex, and multi-modal datasets has become increasingly common across modern scientific environments. a ability to transform samples from one distribution $p$ to another distribution $q$ enables a solution to many problems inside machine learning (e.g. bayesian inference, generative modeling) and has been actively pursued from theoretical, computational, and application perspectives across a fields of information theory, computer science, and biology. performing such transformations, inside general, still leads to computational difficulties, especially inside high dimensions. here, we consider a problem of computing such ""measure transport maps"" with efficient and parallelizable methods. under a mild assumptions that $p$ need not be known but should be sampled from, and that a density of $q$ was known up to the proportionality constant, and that $q$ was log-concave, we provide inside this work the convex optimization problem pertaining to relative entropy minimization. we show how an empirical minimization formulation and polynomial chaos map parameterization should allow considering learning the transport map between $p$ and $q$ with distributed and scalable methods. we also leverage findings from nonequilibrium thermodynamics to represent a transport map as the composition of simpler maps, each of which was learned sequentially with the transport cost regularized version of a aforementioned problem formulation. we provide examples of our framework within a context of bayesian inference considering a boston housing dataset and generative modeling considering handwritten digit images from a mnist dataset.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11132,"period approximation was one of a central topics inside astronomical time series analysis, where data was often unevenly sampled. especially challenging are studies of stellar magnetic cycles, as there a periods looked considering are of a order of a same length than a datasets themselves. a datasets often contain trends, a origin of which was either the real long-term cycle or an instrumental effect, but these effects cannot be reliably separated, while they should lead to erroneous period determinations if not properly handled. inside this study we aim at developing the method that should handle a trends properly, and by performing extensive set of testing, we show that this was a optimal procedure when contrasted with methods that do not include a trend directly to a model. a effect of a form of a noise (whether constant or heteroscedastic) on a results was also investigated. we introduce the bayesian generalised lomb-scargle periodogram with trend (bglst), which was the probabilistic linear regression model with the help of gaussian priors considering a coefficients and uniform prior considering a frequency parameter. we show, with the help of synthetic data, that when there was no prior information on whether and to what extent a true model of a data contains the linear trend, a introduced bglst method was preferable to a methods which either detrend a data or leave a data untrended before fitting a periodic model. whether to use noise with different than constant variance inside a model depends on a density of a data sampling as well as on a true noise type of a process.",0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0
18709,"nowadays data compressors are applied to many problems of text analysis, but many such applications are developed outside of a framework of mathematical statistics. inside this paper we overcome this obstacle and show how several methods of classical mathematical statistics should be developed based on applications of a data compressors.",1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0
15937,"inside this work, the many-body potential of nb considering radiation damage simulation is developed based on eam, and most of a point defects of nb should be predicted properly by this potential. by with the help of a constructed potential, a direction-specific threshold displacement energies (tde) and displacement cascades up to 20 kev of nb were performed through molecular dynamics simulations. a calculated results of tde are inside good agreement with previous work considering v, mo and experimental measurements. lowest tde is found inside <100> direction, and local minas of tde were found inside three low-index directions, which has relation: ed[100]<ed[111]<ed[110]. a evolution of displacement cascades, number of a created point defects, a cascade efficiency a clustering of point defects, and temperature role of these parameters at different pka energies were systematic investigated. it was found that a cascade efficiency was low and it was should be fitted by the power function as many published work did. a fraction of clustered point defects obtained inside this work was low, and only some small clusters were formed at a end of thermal spike. as a temperature increases, a productions of point defects and cascade efficiency were somewhat decreases, however, a fraction of clustered point defects decreases more obvious.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3084,"we study a problem of extracting the selective connector considering the given set of query vertices $q \subseteq v$ inside the graph $g = (v,e)$. the selective connector was the subgraph of $g$ which exhibits some cohesiveness property, and contains a query vertices but does not necessarily connect them all. relaxing a connectedness requirement allows a connector to detect multiple communities and to be tolerant to outliers. we achieve this by introducing a new measure of network inefficiency and by instantiating our search considering the selective connector as a problem of finding a minimum inefficiency subgraph. we show that a minimum inefficiency subgraph problem was np-hard, and devise efficient algorithms to approximate it. by means of several case studies inside the variety of application domains (such as human brain, cancer, and food networks), we show that our minimum inefficiency subgraph produces high-quality solutions, exhibiting all a desired behaviors of the selective connector.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19192,"we measure a stellar mass function (smf) of galaxies inside a cosmos field up to $z\sim6$. we select them inside a near-ir bands of a cosmos2015 catalogue, which includes ultra-deep photometry from ultravista-dr2, splash, and subaru/hyper-suprimecam. at $z>2.5$ we use new precise photometric redshifts with error $\sigma_z=0.03(1+z)$ and an outlier fraction of $12\%$, estimated by means of a unique spectroscopic sample of cosmos. a increased exposure time inside a dr2, along with our panchromatic detection strategy, allow us to improve a stellar mass completeness at high $z$ with respect to previous ultravista catalogues. we also identify passive galaxies through the robust colour-colour selection, extending their smf approximate up to $z=4$. our work provides the comprehensive view of galaxy stellar mass assembly between $z=0.1$ and 6, considering a first time with the help of consistent estimates across a entire redshift range. we fit these measurements with the schechter function, correcting considering eddington bias. we compare a smf fit with a halo mass function predicted from $\lambda$cdm simulations. we find that at $z>3$ both functions decline with the similar slope inside a high-mass end. this feature could be explained assuming that a mechanisms that quench star formation inside massive haloes become less effective at high redshift; however further work needs to be done to confirm this scenario. concerning a smf low-mass end, it shows the progressive steepening as moving towards higher redshifts, with $\alpha$ decreasing from $-1.47_{-0.02}^{+0.02}$ at $z\simeq0.1$ to $-2.11_{-0.13}^{+0.30}$ at $z\simeq5$. this slope depends on a characterisation of a observational uncertainties, which was crucial to properly remove a eddington bias. we show that there was currently no consensus on a method to quantify such errors: different error models result inside different best-fit schechter parameters. [abridged]",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3814,"we show that an embedding inside euclidean space based on tropical geometry generates stable sufficient statistics considering barcodes. inside topological data analysis, barcodes are multiscale summaries of algebraic topological characteristics that capture a `shape' of data; however, inside practice, they have complex structures which make them difficult to use inside statistical settings. a sufficiency result presented inside this work allows considering classical probability distributions to be assumed on a tropical geometric representation of barcodes. this makes the variety of parametric statistical inference methods amenable to barcodes, all while maintaining their initial interpretations. more specifically, we show that exponential family distributions may be assumed, and that likelihood functions considering persistent homology may be constructed. we conceptually demonstrate sufficiency and illustrate its utility inside persistent homology dimensions 0 and 1 with concrete parametric applications to hiv and avian influenza data.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
7803,"here we report a measurement of a interfacial spin accumulation induced by a spin hall effect inside pt and w thin films with the help of magneto-optical kerr microscopy. we show that a kerr rotation has opposite sign inside pt and w and scales linearly with current density. by comparing a experimental results with ab-initio calculations of a spin hall and magneto-optical kerr effects, we quantitatively determine a current-induced spin accumulation at a pt interface as $5*10^{-12} \mu_b$a$^{-1}$cm$^2$ per atom. from thickness-dependent measurements, we determine a spin diffusion length inside the single pt film to be $11 \pm 3$ nm, which was significantly larger compared to that of pt adjacent to the magnetic layer.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
17085,"advances inside a field of inverse reinforcement learning (irl) have led to sophisticated inference frameworks that relax a original modeling assumption of observing an agent behavior that reflects only the single intention. instead of learning the global behavioral model, recent irl methods divide a demonstration data into parts, to account considering a fact that different trajectories may correspond to different intentions, e.g., because they were generated by different domain experts. inside this work, we go one step further: with the help of a intuitive concept of subgoals, we build upon a premise that even the single trajectory should be explained more efficiently locally within the certain context than globally, enabling the more compact representation of a observed behavior. based on this assumption, we build an implicit intentional model of a agent's goals to forecast its behavior inside unobserved situations. a result was an integrated bayesian prediction framework that significantly outperforms existing irl solutions and provides smooth policy estimates consistent with a expert's plan. most notably, our framework naturally handles situations where a intentions of a agent change over time and classical irl algorithms fail. inside addition, due to its probabilistic nature, a model should be straightforwardly applied inside active learning scenarios to guide a demonstration process of a expert.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11469,"inside 1996, huisken-yau proved that every three-dimensional riemannian manifold should be uniquely foliated near infinity by stable closed surfaces of constant mean curvature (cmc) if it was asymptotically equal to a (spatial) schwarzschild solution. with the help of their method, rigger proved a same theorem considering riemannian manifolds being asymptotically equal to a (spatial) (schwarzschild-)anti-de sitter solution. this is generalized to asymptotically hyperbolic manifolds by neves-tian, chodosh, and a author at the later stage. inside this work, we prove a reverse implication as a author already did inside a euclidean setting, i.e. any three-dimensional riemannian manifold was asymptotically hyperbolic if it (and only if) possesses the cmc-cover satisfying certain geometric curvature estimates, the uniqueness property, and each surface has controlled instability. as toy application of these geometric characterizations of asymptotically euclidean and hyperbolic manifolds, we present the method considering replacing an asymptotically hyperbolic by an asymptotically euclidean end and apply this method to prove that a hawking mass of a cmc-surfaces was bounded by their limit being a total mass of a asymptotically hyperbolic manifold, where equality holds only considering a t=0-slice of a (schwarzschild-)anti-de sitter spacetime.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9377,"fitzpatrick's variational representation of maximal monotone operators was here extended to the class of pseudo-monotone operators inside banach spaces. on this basis, a initial-value problem associated with a first-order flow of such an operator was here reformulated as the minimization principle, extending the method that is pioneered by brezis, ekeland and nayroles considering gradient flows. this formulation was used to prove that a problem was stable w.r.t.\ arbitrary perturbations not only of data but also of operators. this was achieved by with the help of a notion of evolutionary $\gamma$-convergence w.r.t.\ the nonlinear topology of weak type. these results are applied to a cauchy problem considering quasilinear parabolic pdes. this provides a structural compactness and stability of a model of several physical phenomena: nonlinear diffusion, incompressible viscous flow, phase transitions, and so on.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17891,"a goal of online display advertising was to entice users to ""convert"" (i.e., take the pre-defined action such as making the purchase) after clicking on a ad. an important measure of a value of an ad was a probability of conversion. a focus of this paper was a development of the computationally efficient, accurate, and precise estimator of conversion probability. a challenges associated with this approximation problem are a delays inside observing conversions and a size of a data set (both number of observations and number of predictors). two models have previously been considered as the basis considering estimation: the logistic regression model and the joint model considering observed conversion statuses and delay times. fitting a former was simple, but ignoring a delays inside conversion leads to an under-estimate of conversion probability. on a other hand, a latter was less biased but computationally expensive to fit. our proposed estimator was the compromise between these two estimators. we apply our results to the data set from criteo, the commerce marketing company that personalizes online display advertisements considering users.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2470,"context: recent xmm-newton observations have revealed that iras 17020+4544 was the very unusual example of black hole wind-produced feedback by the moderately luminous agn inside the spiral galaxy. aims: since a source was known considering being the radio emitter, we investigated about a presence and a properties of the non-thermal component. methods: we observed iras 17020+4544 with a very long baseline array at 5, 8, 15, and 24 ghz within the month of a 2014 xmm-newton observations. we further analysed archival data taken inside 2000 and 2012. results: we detect a source at 5 ghz and on short baselines at 8 ghz. at 15 and 24 ghz, a source was below our baseline sensitivity considering fringe fitting, indicating a lack of prominent compact features. a morphology was that of an asymmetric double, with significant diffuse emission. a spectrum between 5 and 8 ghz was rather steep ($s(\nu)\sim\nu^{-(1.0\pm0.2)}$). our re-analysis of a archival data at 5 and 8 ghz provides results consistent with a new observations, suggesting that flux density and structural variability are not important inside this source. we put the limit on a separation speed between a main components of $<0.06c$. conclusions: iras 17020+4544 shows interesting features of several classes of objects: its properties are typical of compact steep spectrum sources, low power compact sources, radio-emitting narrow line seyfert 1 galaxies. however, it should not be classified inside any of these categories, remaining so far the one-of-a-kind object.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16175,"aims. we aim to show how encounters with low-mass satellite galaxies may alter a bar formation inside the milky way-like disc galaxy. methods. we use high-resolution n-body simulations of the disc galaxy prone to mild bar instability. considering realistic initial conditions of satellites, we take advantage of cosmological simulations of milky way-like dark matter haloes. results. a satellites may have the significant impact on a time of bar formation. some runs with satellites demonstrate the delay, while others show an advancement inside bar formation compared to a isolated run, with such time differences reaching $\sim$ 1 gyr. meanwhile, a final bar configuration, including its very appearance and a bar characteristics such as a pattern speed and a exponential growth rate of its amplitude are independent of a number of encounters and their orbits. a contribution of satellites with masses below $10^9 m_{\odot}$ was insignificant, unless their pericentre distances are small. we suggest that a encounters act indirectly using inducing perturbations across a disc that evolve to delayed waves inside a central part and interfere with an emerging seed bar. a predicted effect considering a present-day host galaxy was expected to be even more significant at redshifts $z \gtrsim 0.5$.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1945,"we consider a problem of low canonical polyadic (cp) rank tensor completion. the completion was the tensor whose entries agree with a observed entries and its rank matches a given cp rank. we analyze a manifold structure corresponding to a tensors with a given rank and define the set of polynomials based on a sampling pattern and cp decomposition. then, we show that finite completability of a sampled tensor was equivalent to having the certain number of algebraically independent polynomials among a defined polynomials. our proposed idea behind the method results inside characterizing a maximum number of algebraically independent polynomials inside terms of the simple geometric structure of a sampling pattern, and therefore we obtain a deterministic necessary and sufficient condition on a sampling pattern considering finite completability of a sampled tensor. moreover, assuming that a entries of a tensor are sampled independently with probability $p$ and with the help of a mentioned deterministic analysis, we propose the combinatorial method to derive the lower bound on a sampling probability $p$, or equivalently, a number of sampled entries that guarantees finite completability with high probability. we also show that a existing result considering a matrix completion problem should be used to obtain the loose lower bound on a sampling probability $p$. inside addition, we obtain deterministic and probabilistic conditions considering unique completability. it was seen that a number of samples required considering finite or unique completability obtained by a proposed analysis on a cp manifold was orders-of-magnitude lower than that was obtained by a existing analysis on a grassmannian manifold.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7170,"datasets are often used multiple times and each successive analysis may depend on a outcome of previous analyses. standard techniques considering ensuring generalization and statistical validity do not account considering this adaptive dependence. the recent line of work studies a challenges that arise from such adaptive data reuse by considering a problem of answering the sequence of ""queries"" about a data distribution where each query may depend arbitrarily on answers to previous queries. a strongest results obtained considering this problem rely on differential privacy -- the strong notion of algorithmic stability with a important property that it ""composes"" well when data was reused. however a notion was rather strict, as it requires stability under replacement of an arbitrary data element. a simplest algorithm was to add gaussian (or laplace) noise to distort a empirical answers. however, analysing this technique with the help of differential privacy yields suboptimal accuracy guarantees when a queries have low variance. here we propose the relaxed notion of stability that also composes adaptively. we demonstrate that the simple and natural algorithm based on adding noise scaled to a standard deviation of a query provides our notion of stability. this implies an algorithm that should answer statistical queries about a dataset with substantially improved accuracy guarantees considering low-variance queries. a only previous idea behind the method that provides such accuracy guarantees was based on the more involved differentially private median-of-means algorithm and its analysis exploits stronger ""group"" stability of a algorithm.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
13749,"we introduce the low dimensional function of a site frequency spectrum that was tailor-made considering distinguishing coalescent models with multiple mergers from kingman coalescent models with population growth, and use this function to construct the hypothesis test between these model classes. a null and alternative sampling distributions of a statistic are intractable, but its low dimensionality renders them amenable to monte carlo estimation. we construct kernel density estimates of a sampling distributions based on simulated data, and show that a resulting hypothesis test dramatically improves on a statistical power of the current state-of-the-art method. the key reason considering this improvement was a use of multi-locus data, inside particular averaging observed site frequency spectra across unlinked loci to reduce sampling variance. we also demonstrate a robustness of our method to nuisance and tuning parameters. finally we show that a same kernel density estimates should be used to conduct parameter estimation, and argue that our method was readily generalisable considering applications inside model selection, parameter inference and experimental design.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
16706,"effective collaboration between the robot and the person requires natural communication. when the robot travels with the human companion, a robot should be able to explain its navigation behavior inside natural language. this paper explains how the cognitively-based, autonomous robot navigation system produces informative, intuitive explanations considering its decisions. language generation here was based upon a robot's commonsense, its qualitative reasoning, and its learned spatial model. this idea behind the method produces natural explanations inside real time considering the robot as it navigates inside the large, complex indoor environment.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
11926,"automatic segmentation inside mr brain images was important considering quantitative analysis inside large-scale studies with images acquired at all ages. this paper presents the method considering a automatic segmentation of mr brain images into the number of tissue classes with the help of the convolutional neural network. to ensure that a method obtains accurate segmentation details as well as spatial consistency, a network uses multiple patch sizes and multiple convolution kernel sizes to acquire multi-scale information about each voxel. a method was not dependent on explicit features, but learns to recognise a information that was important considering a classification based on training data. a method requires the single anatomical mr image only. a segmentation method was applied to five different data sets: coronal t2-weighted images of preterm infants acquired at 30 weeks postmenstrual age (pma) and 40 weeks pma, axial t2- weighted images of preterm infants acquired at 40 weeks pma, axial t1-weighted images of ageing adults acquired at an average age of 70 years, and t1-weighted images of young adults acquired at an average age of 23 years. a method obtained a following average dice coefficients over all segmented tissue classes considering each data set, respectively: 0.87, 0.82, 0.84, 0.86 and 0.91. a results demonstrate that a method obtains accurate segmentations inside all five sets, and thus demonstrates its robustness to differences inside age and acquisition protocol.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15245,"feature selection problems arise inside the variety of applications, such as microarray analysis, clinical prediction, text categorization, image classification and face recognition, multi-label learning, and classification of internet traffic. among a various classes of methods, forward feature selection methods based on mutual information have become very popular and are widely used inside practice. however, comparative evaluations of these methods have been limited by being based on specific datasets and classifiers. inside this paper, we develop the theoretical framework that allows evaluating a methods based on their theoretical properties. our framework was grounded on a properties of a target objective function that a methods try to approximate, and on the novel categorization of features, according to their contribution to a explanation of a class; we derive upper and lower bounds considering a target objective function and relate these bounds with a feature types. then, we characterize a types of approximations taken by a methods, and analyze how these approximations cope with a good properties of a target objective function. additionally, we develop the distributional setting designed to illustrate a various deficiencies of a methods, and provide several examples of wrong feature selections. based on our work, we identify clearly a methods that should be avoided, and a methods that currently have a best performance.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3669,"little by little, newspapers are revealing a bright future that artificial intelligence (ai) was building. intelligent machines will aid everywhere. however, this bright future has the dark side: the dramatic job market contraction before its unpredictable transformation. hence, inside the near future, large numbers of job seekers will need financial support while catching up with these novel unpredictable jobs. this possible job market crisis has an antidote inside. inside fact, a rise of ai was sustained by a biggest knowledge theft of a recent years. learning ai machines are extracting knowledge from unaware skilled or unskilled workers by analyzing their interactions. by passionately doing their jobs, these workers are digging their own graves. inside this paper, we propose human-in-the-loop artificial intelligence (hit-ai) as the fairer paradigm considering artificial intelligence systems. hit-ai will reward aware and unaware knowledge producers with the different scheme: decisions of ai systems generating revenues will repay a legitimate owners of a knowledge used considering taking those decisions. as modern robin hoods, hit-ai researchers should fight considering the fairer artificial intelligence that gives back what it steals.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10434,"we study a interplay between a electron-electron (e-e) and a electron-phonon (e-ph) interactions inside a two-orbital hubbard-holstein model at half filling with the help of a dynamical mean field theory. we find that a e-ph interaction, even at weak couplings, strongly modifies a phase diagram of this model and introduces an orbital-selective peierls insulating phase (ospi) that was analogous to a widely studied orbital-selective mott phase (osmp). at small e-e and e-ph coupling, we find the competition between a osmp and a ospi, while at large couplings, the competition occurs between mott and charge-density-wave (cdw) insulating phases. we further demonstrate that a hund's coupling influences a ospi transition by lowering a energy associated with a cdw. our results explicitly show that one must be cautious when neglecting a e-ph interaction inside multiorbital systems, where multiple electronic interactions create states that are readily influenced by perturbing interactions.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
7419,"our predictions, based on density-functional calculations, reveal that surface doping of zno nanowires with bi leads to the linear-in-$k$ splitting of a conduction-band states, through spin-orbit interaction, due to a lowering of a symmetry inside a presence of a dopant. this finding implies that spin polarization of a conduction electrons inside bi-doped zno nanowires could be controlled with applied electric (as opposed to magnetic) fields, making them candidate materials considering spin-orbitronic applications. our findings also show that a degree of spin splitting could be tuned by adjusting a dopant concentration. defect calculations and ab initio molecular dynamics simulations indicate that stable doping configurations exhibiting a foregoing linear-in-$k$ splitting could be realized under reasonable thermodynamic conditions.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
6452,"a growing complexity of heterogeneous cellular networks (hetnets) has necessitated a need to consider variety of user and base station (bs) configurations considering realistic performance evaluation and system design. this was directly reflected inside a hetnet simulation models considered by standardization bodies, such as a third generation partnership project (3gpp). complementary to these simulation models, stochastic geometry based idea behind the method modeling a user and bs locations as independent and homogeneous poisson point processes (ppps) has gained prominence inside a past few years. despite its success inside revealing useful insights, this ppp-based model was not rich enough to capture all a spatial configurations that appear inside real world hetnet deployments (on which 3gpp simulation models are based). inside this paper, we bridge a gap between a 3gpp simulation models and a popular ppp-based analytical model by developing the new unified hetnet model inside which the fraction of users and some bs tiers are modeled as poisson cluster processes (pcps). this model captures both non-uniformity and coupling inside a bs and user locations. considering this setup, we derive exact expression considering downlink coverage probability under maximum signal-to-interference ratio (sir) cell association model. as intermediate results, we define and evaluate sum-product functionals considering ppp and pcp. special instances of a proposed model are shown to closely resemble different configurations considered inside 3gpp hetnet models. our results concretely demonstrate that a performance trends are highly sensitive to a assumptions made on a user and sbs configurations.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
16601,"we present the detailed investigation of a temperature and depth dependence of a magnetic properties of 3d topological kondo insulator smb6 , inside particular near its surface. we find that local magnetic field fluctuations detected inside a bulk are suppressed rapidly with decreasing depths, disappearing almost completely at a surface. we attribute a magnetic excitations to spin excitons inside bulk smb6 , which produce local magnetic fields of about ~1.8 mt fluctuating on the time scale of ~60 ns. we find that a excitonic fluctuations are suppressed when approaching a surface on the length scale of 40-90 nm, accompanied by the small enhancement inside static magnetic fields. we associate this length scale to a size of a excitonic state.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
16150,"we investigate which three dimensional near-horizon metrics $g_{nh}$ admit the compatible 1-form $x$ such that $(x, [g_{nh}])$ defines an einstein-weyl structure. we find explicit examples and see that some of a solutions give rise to einstein-weyl structures of dispersionless kp type and dispersionless hirota (aka hypercr) type.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9464,"dust devils are likely a dominant source of dust considering a martian atmosphere, but a amount and frequency of dust-lifting depend on a statistical distribution of dust devil parameters. dust devils exhibit pressure perturbations and, if they pass near the barometric sensor, they may register as the discernible dip inside the pressure time-series. leveraging this fact, several surveys with the help of barometric sensors on landed spacecraft have revealed dust devil structures and occurrence rates. however powerful they are, though, such surveys suffer from non-trivial biases that skew a inferred dust devil properties. considering example, such surveys are most sensitive to dust devils with a widest and deepest pressure profiles, but a recovered profiles will be distorted, broader and shallow than a actual profiles. inside addition, such surveys often do not provide wind speed measurements alongside a pressure time series, and so a durations of a dust devil signals inside a time series cannot be directly converted to profile widths. fortunately, simple statistical and geometric considerations should de-bias these surveys, allowing conversion of a duration of dust devil signals into physical widths, given only the distribution of likely translation velocities, and a recovery of a underlying distributions of physical parameters. inside this study, we develop the scheme considering de-biasing such surveys. applying our model to an in-situ survey with the help of data from a phoenix lander suggests the larger dust flux and the dust devil occurrence rate about ten times larger than previously inferred. comparing our results to dust devil track surveys suggests only about one inside five low-pressure cells lifts sufficient dust to leave the visible track.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4960,"periodic event-triggered control (petc) has a advantages of both sampled-data control and event-triggered control, and was well-suited considering implementation inside digital platforms. however, existing results on petc design mainly focus on linear systems, and their extension to nonlinear systems are still sparse. this paper investigates petc design considering general nonlinear systems subject to external disturbances, and provides sufficient conditions to ensure that a closed-loop system implemented by petc input-to-state stable, with the help of state feedback and observer-based output feedback controllers, respectively. considering incrementally quadratic nonlinear systems, sufficient conditions considering petc design are provided inside a form of linear matrix inequalities. a sampling period and triggering functions considering all a cases considered are provided explicitly. two examples are given to illustrate a effectiveness of a proposed method.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
17281,"a potential number of drug like small molecules was estimated to be between 10^23 and 10^60 while current databases of known compounds are orders of magnitude smaller with approximately 10^8 compounds. this discrepancy has led to an interest inside generating virtual libraries with the help of hand crafted chemical rules and fragment based methods to cover the larger area of chemical space and generate chemical libraries considering use inside inside silico drug discovery endeavors. here it was explored to what extent the recurrent neural network with long short term memory cells should figure out sensible chemical rules and generate synthesizable molecules by being trained on existing compounds encoded as smiles. a networks should to the high extent generate novel, but chemically sensible molecules. a properties of a molecules are tuned by training on two different datasets consisting of fragment like molecules and drug like molecules. a produced molecules and a training databases have very similar distributions of molar weight, predicted logp, number of hydrogen bond acceptors and donors, number of rotatable bonds and topological polar surface area when compared to their respective training sets. a compounds are considering a most cases synthesizable as assessed with sa score and wiley chemplanner.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
163,"accurately predicting a future capacity and remaining useful life of batteries was necessary to ensure reliable system operation and to minimise maintenance costs. a complex nature of battery degradation has meant that mechanistic modelling of capacity fade has thus far remained intractable; however, with a advent of cloud-connected devices, data from cells inside various applications was becoming increasingly available, and a feasibility of data-driven methods considering battery prognostics was increasing. here we propose gaussian process (gp) regression considering forecasting battery state of health, and highlight various advantages of gps over other data-driven and mechanistic approaches. gps are the type of bayesian non-parametric method, and thus should model complex systems whilst handling uncertainty inside the principled manner. prior information should be exploited by gps inside the variety of ways: explicit mean functions should be used if a functional form of a underlying degradation model was available, and multiple-output gps should effectively exploit correlations between data from different cells. we demonstrate a predictive capability of gps considering short-term and long-term (remaining useful life) forecasting on the selection of capacity vs. cycle datasets from lithium-ion cells.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2528,"we analyze a non-local shot noise inside the multi-terminal junction formed by two normal metal leads con- nected to one superconductor. with the help of a cross fano factor and a shot noise, we calculate a efficiency of a cooper pair splitting. a method was applied to d-wave and iron based superconductors. we de- termine that a contributions to a noise cross-correlation are due to crossed andreev reflections (car), elastic cotunneling, quasiparticles transmission and local andreev reflections. inside a tunneling limit, a car contribute positively to a noise cross-correlation whereas a other processes contribute negatively. depending on a pair potential symmetry, a car are a dominant processes, giving as the result the high efficiency considering cooper pair split. we propose a use of a fano factor to test a efficiency of the cooper pair splitter device.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
16200,"an ability to model the generative process and learn the latent representation considering speech inside an unsupervised fashion will be crucial to process vast quantities of unlabelled speech data. recently, deep probabilistic generative models such as variational autoencoders (vaes) have achieved tremendous success inside modeling natural images. inside this paper, we apply the convolutional vae to model a generative process of natural speech. we derive latent space arithmetic operations to disentangle learned latent representations. we demonstrate a capability of our model to modify a phonetic content or a speaker identity considering speech segments with the help of a derived operations, without a need considering parallel supervisory data.",1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14625,"we propose some algorithms to find local minima inside nonconvex optimization and to obtain global minima inside some degree from a newton second law without friction. with a key observation of a velocity observable and controllable inside a motion, a algorithms simulate a newton second law without friction based on symplectic euler scheme. from a intuitive analysis of analytical solution, we give the theoretical analysis considering a high-speed convergence inside a algorithm proposed. finally, we propose a experiments considering strongly convex function, non-strongly convex function and nonconvex function inside high-dimension.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14416,"previously, a controllability problem of the linear time-invariant dynamical system is mapped to a maximum matching (mm) problem on a bipartite representation of a underlying directed graph, and a sizes of mms on random bipartite graphs were calculated analytically with a cavity method at zero temperature limit. here we present an alternative theory to approximate mm sizes based on a core percolation theory and a perfect matching of cores. our theory was much more simplified and easily interpreted, and should approximate mm sizes on random graphs with or without symmetry between out- and in-degree distributions. our result helps to illuminate a fundamental connection between a controllability problem and a underlying structure of complex systems.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
19109,"molecular line-transition lists are an essential ingredient considering radiative-transfer calculations. with recent databases now surpassing a billion-lines mark, handling them has become computationally prohibitive, due to both a required processing power and memory. here i present the temperature-dependent algorithm to separate strong from weak line transitions, reformatting a large majority of a weaker lines into the cross-section data file, and retaining a detailed line-by-line information of a fewer strong lines. considering any given molecule over a 0.3--30 {\micron} range, this algorithm reduces a number of lines to the few million, enabling faster radiative-transfer computations without the significant loss of information. a final compression rate depends on how densely populated was a spectrum. i validate this algorithm by comparing exomol's hcn extinction-coefficient spectra between a complete (65 million line transitions) and compressed (7.7 million) line lists. over a 0.6--33 {\micron} range, a average difference between extinction-coefficient values was less than 1\%. the python/c implementation of this algorithm was open-source and available at this https url . so far, this code handles a exomol and hitran line-transition format.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
14090,"we revisit a classical problem of optimal experimental design (oed) under the new mathematical model grounded inside the geometric motivation. specifically, we introduce models based on elementary symmetric polynomials; these polynomials capture ""partial volumes"" and offer the graded interpolation between a widely used a-optimal design and d-optimal design models, obtaining each of them as special cases. we analyze properties of our models, and derive both greedy and convex-relaxation algorithms considering computing a associated designs. our analysis establishes approximation guarantees on these algorithms, while our empirical results substantiate our claims and demonstrate the curious phenomenon concerning our greedy method. finally, as the byproduct, we obtain new results on a theory of elementary symmetric polynomials that may be of independent interest.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
11146,"due to severe mathematical modeling and calibration difficulties open-loop feedforward control was mainly employed today considering wastewater denitrification, which was the key ecological issue. inside order to improve a resulting poor performances the new model-free control setting and its corresponding ""intelligent"" controller are introduced. a pitfall of regulating two output variables using the single input variable was overcome by introducing also an open-loop knowledge-based control deduced from a plant behavior. several convincing computer simulations are presented and discussed.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
651,"inside this paper, we study a galois conjugates of stretch factors of pseudo-anosov elements of a mapping class group of the surface. we show that - except inside low-complexity cases - these conjugates are dense inside a complex plane. considering this, we use penner's construction of pseudo-anosov mapping classes. as the consequence, we obtain that inside the sense there was no restriction on a location of galois conjugates of stretch factors arising from penner's construction. this complements an earlier result of shin and a author stating that galois conjugates of stretch factors arising from penner's construction may never lie on a unit circle.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
12755,"considering a two cartan type s subalgebras of a witt algebra $\w_n$, called lie algebras of divergence-zero vector fields, we determine all module structures on a universal enveloping algebra of their cartan subalgebra $\h_n$. we also give all submodules of these modules.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
5648,"we consider projective rational strong calabi dream surfaces: projective smooth rational surfaces which admit the constant scalar curvature kahler metric considering every kahler class. we show that there are only two such rational surfaces, namely a projective plane and a quadric surface. inside particular, we show that all rational surfaces other than those two admit the destabilising slope test configuration considering some polarization, as introduced by ross and thomas. we further show that all hirzebruch surfaces other than a quadric surface and all rational surfaces with picard rank 3 do not admit the constant scalar curvature kahler metric inside any kahler class.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
501,"estimators computed from adaptively collected data do not behave like their non-adaptive brethren. rather, a sequential dependence of a collection policy should lead to severe distributional biases that persist even inside a infinite data limit. we develop the general method -- $\mathbf{w}$-decorrelation -- considering transforming a bias of adaptive linear regression estimators into variance. a method uses only coarse-grained information about a data collection policy and does not need access to propensity scores or exact knowledge of a policy. we bound a finite-sample bias and variance of a $\mathbf{w}$-estimator and develop asymptotically correct confidence intervals based on the novel martingale central limit theorem. we then demonstrate a empirical benefits of a generic $\mathbf{w}$-decorrelation procedure inside two different adaptive data settings: a multi-armed bandit and a autoregressive time series.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5245,"most existing sequence labelling models rely on the fixed decomposition of the target sequence into the sequence of basic units. these methods suffer from two major drawbacks: 1) a set of basic units was fixed, such as a set of words, characters or phonemes inside speech recognition, and 2) a decomposition of target sequences was fixed. these drawbacks usually result inside sub-optimal performance of modeling sequences. inside this pa- per, we extend a popular ctc loss criterion to alleviate these limitations, and propose the new loss function called gram-ctc. while preserving a advantages of ctc, gram-ctc automatically learns a best set of basic units (grams), as well as a most suitable decomposition of tar- get sequences. unlike ctc, gram-ctc allows a model to output variable number of characters at each time step, which enables a model to capture longer term dependency and improves a computational efficiency. we demonstrate that a proposed gram-ctc improves ctc inside terms of both performance and efficiency on a large vocabulary speech recognition task at multiple scales of data, and that with gram-ctc we should outperform a state-of-the-art on the standard speech benchmark.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17080,"this paper presents novel insights about a influence of soluble surfactants on bubble flows obtained by direct numerical simulation (dns). surfactants are amphiphilic compounds which accumulate at fluid interfaces and significantly modify a respective interfacial properties, influencing also a overall dynamics of a flow. with a aid of dns local quantities like a surfactant distribution on a bubble surface should be accessed considering the better understanding of a physical phenomena occurring close to a interface. a core part of a physical model consists inside a description of a surfactant transport inside a bulk and on a deformable interface. a solution procedure was based on an arbitrary lagrangian-eulerian (ale) interface-tracking method. a existing methodology is enhanced to describe the wider range of physical phenomena. the subgrid-scale (sgs) model was employed inside a cases where the fully resolved dns considering a species transport was not feasible due to high mesh resolution requirements and, therefore, high computational costs. after an exhaustive validation of a latest numerical developments, a dns of single rising bubbles inside contaminated solutions was compared to experimental results. a full velocity transients of a rising bubbles, especially a contaminated ones, are correctly reproduced by a dns. a simulation results are then studied to gain the better understanding of a local bubble dynamics under a effect of soluble surfactant. one of a main insights was that a quasi-steady state of a rise velocity was reached without ad- and desorption being necessarily inside local equilibrium.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9716,"let $f$ be the finite extension of $\mathbb{q}_p$, and let $\mathbb{f}$ be the finite field of characteristic $p$. the smooth representation of $gl_2(f)$ over $\mathbb{f}$ was finitely presented if it should be written as a cokernel of the map between representations induced from compact-mod-centre open subgroups of $gl_2(f)$. we prove that a category of finitely presented smooth representations was an abelian subcategory of all smooth representations. this amounts to showing that a kernel of the map between finitely presented smooth representations was finitely presented. a proof uses amalgamated products of completed group rings.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
7697,"the common theme among a proposed models considering network epidemics was a assumption that a propagating object, i.e., the virus or the piece of information, was transferred across a nodes without going through any modification or evolution. however, inside real-life spreading processes, pathogens often evolve inside response to changing environments and medical interventions and information was often modified by individuals before being forwarded. inside this paper, we investigate a evolution of spreading processes on complex networks with a aim of i) revealing a role of evolution on a threshold, probability, and final size of epidemics; and ii) exploring a interplay between a structural properties of a network and a dynamics of evolution. inside particular, we develop the mathematical theory that accurately predicts a epidemic threshold and a expected epidemic size as functions of a characteristics of a spreading process, a evolutionary dynamics of a pathogen, and a structure of a underlying contact network. inside addition to a mathematical theory, we perform extensive simulations on random and real-world contact networks to verify our theory and reveal a significant shortcomings of a classical mathematical models that do not capture evolution. our results reveal that a classical, single-type bond-percolation models may accurately predict a threshold and final size of epidemics, but their predictions on a probability of emergence are inaccurate on both random and real-world networks. this inaccuracy sheds a light on the fundamental disconnect between a classical bond-percolation models and real-life spreading processes that entail evolution. finally, we consider a case when co-infection was possible and show that co-infection could lead a order of phase transition to change from second-order to first-order.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
8229,feature extraction was the very crucial task inside image and pixel (voxel) classification and regression inside biomedical image modeling. inside this work we present the machine learning based feature extraction scheme based on inception models considering pixel classification tasks. we extract features under multi-scale and multi-layer schemes through convolutional operators. layers of fully convolutional network are later stacked on this feature extraction layers and trained end-to-end considering a purpose of classification. we test our model on a drive and stare public data sets considering a purpose of segmentation and centerline detection and it out performs most existing hand crafted or deterministic feature schemes found inside literature. we achieve an average maximum dice of 0.85 on a drive data set which out performs a scores from a second human annotator of this data set. we also achieve an average maximum dice of 0.85 and kappa of 0.84 on a stare data set. though these datasets are mainly 2-d we also propose ways of extending this feature extraction scheme to handle 3-d datasets.,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19158,"inside this work, we derive gross-zagier type cm value formulas considering hauptmoduls $j_{p}^{*}(\tau)$ on fricke groups $\gamma_{0}^{*}(p)$. we also illustrate how to employ these formulas to obtain certain hilbert class polynomials.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
64,"do only major scientific breakthroughs hit a news and social media, or does the 'catchy' title aid to attract public attention? how strong was a connection between a importance of the scientific paper and a (social) media attention it receives? inside this study we investigate these questions by analysing a relationship between a observed attention and certain characteristics of scientific papers from two major multidisciplinary journals: nature communication (nc) and proceedings of a national academy of sciences (pnas). we describe papers by features based on a linguistic properties of their titles and centrality measures of their authors inside their co-authorship network. we identify linguistic features and collaboration patterns that might be indicators considering future attention, and are characteristic to different journals, research disciplines, and media sources.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
6848,"learning a structure of markov random fields (mrfs) plays an important role inside multivariate analysis. a importance has been increasing with a recent rise of statistical relational models since a mrf serves as the building block of these models such as markov logic networks. there are two fundamental ways to learn structures of mrfs: methods based on parameter learning and those based on independence test. a former methods more or less assume certain forms of distribution, so they potentially perform poorly when a assumption was not satisfied. a latter should learn an mrf structure without the strong distributional assumption, but sometimes it was unclear what objective function was maximized/minimized inside these methods. inside this paper, we follow a latter, but we explicitly define a optimization problem of mrf structure learning as maximum pseudolikelihood approximation (mple) with respect to a edge set. as the result, a proposed solution successfully deals with a {\em symmetricity} inside mrfs, whereas such symmetricity was not taken into account inside most existing independence test techniques. a proposed method achieved higher accuracy than previous methods when there were asymmetric dependencies inside our experiments.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1706,"this paper introduces wasserstein variational inference, the new form of approximate bayesian inference based on optimal transport theory. wasserstein variational inference uses the new family of divergences that includes both f-divergences and a wasserstein distance as special cases. a gradients of a wasserstein variational loss are obtained by backpropagating through a sinkhorn iterations. this technique results inside the very stable likelihood-free training method that should be used with implicit distributions and probabilistic programs. with the help of a wasserstein variational inference framework, we introduce several new forms of autoencoders and test their robustness and performance against existing variational autoencoding techniques.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6663,this was the duplicate submission(original was arxiv:1612.02141). thus want to withdraw it,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6682,"considering distributed computing environment, we consider a empirical risk minimization problem and propose the distributed and communication-efficient newton-type optimization method. at every iteration, each worker locally finds an approximate newton (ant) direction, which was sent to a main driver. a main driver, then, averages all a ant directions received from workers to form the {\it globally improved ant} (giant) direction. giant was highly communication efficient and naturally exploits a trade-offs between local computations and global communications inside that more local computations result inside fewer overall rounds of communications. theoretically, we show that giant enjoys an improved convergence rate as compared with first-order methods and existing distributed newton-type methods. further, and inside sharp contrast with many existing distributed newton-type methods, as well as popular first-order methods, the highly advantageous practical feature of giant was that it only involves one tuning parameter. we conduct large-scale experiments on the computer cluster and, empirically, demonstrate a superior performance of giant.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8796,"it was demonstrated that fermionic/bosonic symmetry-protected topological (spt) phases across different dimensions and symmetry classes should be organized with the help of geometric constructions that increase dimensions and symmetry-forgetting maps that change symmetry groups. specifically, it was shown that a interacting classifications of spt phases with and without glide symmetry fit into the short exact sequence, so that a classification with glide was constrained to be the direct sum of cyclic groups of order 2 or 4. applied to fermionic spt phases inside a wigner-dyson class aii, this implies that a complete interacting classification inside a presence of glide was ${\mathbb z}_4{\oplus}{\mathbb z}_2{\oplus}{\mathbb z}_2$ inside 3 dimensions. inside particular, a hourglass-fermion phase recently realized inside a band insulator khgsb must be robust to interactions. generalizations to spatiotemporal glide symmetries are discussed.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
1982,"we consider abstract evolution equations with on-off time delay feedback. without a time delay term, a model was described by an exponentially stable semigroup. we show that, under appropriate conditions involving a delay term, a system remains asymptotically stable. under additional assumptions exponential stability results are also obtained. concrete examples illustrating a abstract results are finally given.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14596,"we present adaptive strategies considering antenna selection considering direction of arrival (doa) approximation of the far-field source with the help of tdm mimo radar with linear arrays. our treatment was formulated within the general adaptive sensing framework that uses one-step ahead predictions of a bayesian mse with the help of the parametric family of weiss-weinstein bounds that depend on previous measurements. we compare inside simulations our strategy with adaptive policies that optimize a bobrovsky- zaka{\i} bound and a expected cram√©r-rao bound, and show a performance considering different levels of measurement noise.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
13554,"existing speaker verification (sv) systems often suffer from performance degradation if there was any language mismatch between model training, speaker enrollment, and test. the major cause of this degradation was that most existing sv methods rely on the probabilistic model to infer a speaker factor, so any significant change on a distribution of a speech signal will impact a inference. recently, we proposed the deep learning model that should learn how to extract a speaker factor by the deep neural network (dnn). by this feature learning, an sv system should be constructed with the very simple back-end model. inside this paper, we investigate a robustness of a feature-based sv system inside situations with language mismatch. our experiments were conducted on the complex cross-lingual scenario, where a model training is inside english, and a enrollment and test were inside chinese or uyghur. a experiments demonstrated that a feature-based system outperformed a i-vector system with the large margin, particularly with language mismatch between enrollment and test.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15258,"all a existing real world networks are evolving, hence, study of traffic dynamics inside these enlarged networks was the challenging task. a critical issue was to optimize a network structure to improve network capacity and avoid traffic congestion. we are interested inside taking user's routes such that it was least congested with optimal network capacity. network capacity may be improved either by optimizing network topology or enhancing inside routing approach. inside this context, we propose and design the model of a time varying data communication networks (tvcn) based on a dynamics of in-flowing links. newly appeared node prefers to attach with most influential node present inside a network. inside this paper, influence was termed as \textit{reputation} and was applied considering computing overall congestion at any node. user path with least betweenness centrality and most reputation was preferred considering routing. kelly's optimization formulation considering the rate allocation problem was used considering obtaining optimal rates of distinct users at different time instants and it was found that a user's path with lowest betweenness centrality and highest reputation will always give maximum rate at stable point.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
3155,"inside this paper, thermocapillary migration of the planar droplet at moderate and large marangoni numbers was investigated analytically and numerically. by with the help of a dimension-analysis method, a thermal diffusion time scale was determined as a controlling one of a thermocapillary droplet migration system. during this time, a whole thermocapillary migration process was fully developed. by with the help of a front-tracking method, a steady/unsteady states as a terminal ones at moderate/large marangoni numbers are captured inside the longer time scale than a thermal diffusion time scale. inside a terminal states, a instantaneous velocity fields inside a unsteady migration process at large marangoni numbers have a forms of a steady ones at moderate marangoni numbers. however, inside view of a former instantaneous temperature fields, a surface tension of a top surface of a droplet gradually becomes a main component of a driving force on a droplet after a inflection point appears. it was different from that a surface tension of a bottom surface of a droplet was a main component of a driving force on a droplet considering a latter ones. a physical mechanism of thermocapillary droplet migration should be described as a significance of a thermal convection around a droplet was higher than/just as a thermal conduction across a droplet at large/moderate marangoni numbers.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11940,"we prove that every entire solution of a minimal graph equation that was bounded from below and has at most linear growth must be constant on the complete riemannian manifold $m$ with only one end if $m$ has asymptotically non-negative sectional curvature. on a other hand, we prove a existence of bounded non-constant minimal graphic and $p$-harmonic functions on rotationally symmetric cartan-hadamard manifolds under optimal assumptions on a sectional curvatures.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17564,"should faces acquired by low-cost depth sensors be useful to catch some characteristic details of a face? typically a answer was no. however, new deep architectures should generate rgb images from data acquired inside the different modality, such as depth data. inside this paper, we propose the new \textit{deterministic conditional gan}, trained on annotated rgb-d face datasets, effective considering the face-to-face translation from depth to rgb. although a network cannot reconstruct a exact somatic features considering unknown individual faces, it was capable to reconstruct plausible faces; their appearance was accurate enough to be used inside many pattern recognition tasks. inside fact, we test a network capability to hallucinate with some \textit{perceptual probes}, as considering instance face aspect classification or landmark detection. depth face should be used inside spite of a correspondent rgb images, that often are not available due to difficult luminance conditions. experimental results are very promising and are as far as better than previously proposed approaches: this domain translation should constitute the new way to exploit depth data inside new future applications.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4952,"while invaluable considering many computer vision applications, decomposing the natural image into intrinsic reflectance and shading layers represents the challenging, underdetermined inverse problem. as opposed to strict reliance on conventional optimization or filtering solutions with strong prior assumptions, deep learning based approaches have also been proposed to compute intrinsic image decompositions when granted access to sufficient labeled training data. a downside was that current data sources are quite limited, and broadly speaking fall into one of two categories: either dense fully-labeled images inside synthetic/narrow settings, or weakly-labeled data from relatively diverse natural scenes. inside contrast to many previous learning-based approaches, which are often tailored to a structure of the particular dataset (and may not work well on others), we adopt core network structures that universally reflect loose prior knowledge regarding a intrinsic image formation process and should be largely shared across datasets. we then apply flexibly supervised loss layers that are customized considering each source of ground truth labels. a resulting deep architecture achieves state-of-the-art results on all of a major intrinsic image benchmarks, and runs considerably faster than most at test time.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13296,"inside this paper, we present the novel strategy to compute minimum-time trajectories considering quadrotors inside constrained environments. inside particular, we consider a motion inside the given flying region with obstacles and take into account a physical limitations of a vehicle. instead of approaching a optimization problem inside its standard time-parameterized formulation, a proposed strategy was based on an appealing re-formulation. transverse coordinates, expressing a distance from the frame path, are used to parameterise a vehicle position and the spatial parameter was used as independent variable. this re-formulation allows us to (i) obtain the fixed horizon problem and (ii) easily formulate (fairly complex) position constraints. a effectiveness of a proposed strategy was proven by numerical computations on two different illustrative scenarios. moreover, a optimal trajectory generated inside a second scenario was experimentally executed with the real nano-quadrotor inside order to show its feasibility.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
14022,"we propose the novel diminishing learning rate scheme, coined decreasing-trend-nature (dtn), which allows us to prove fast convergence of a stochastic gradient descent (sgd) algorithm to the first-order stationary point considering smooth general convex and some class of nonconvex including neural network applications considering classification problems. we are a first to prove that sgd with diminishing learning rate achieves the convergence rate of $\mathcal{o}(1/t)$ considering these problems. our theory applies to neural network applications considering classification problems inside the straightforward way.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19514,"shells are low surface brightness tidal debris that appear as interleaved caustics with large opening angles, often situated on both sides of a galaxy center. inside this paper, we study a incidence and formation processes of shell galaxies inside a cosmological gravity+hydrodynamics illustris simulation. we identify shells at redshift z=0 with the help of stellar surface density maps, and we use stellar history catalogs to trace a birth, trajectory and progenitors of each individual star particle contributing to a tidal feature. out of the sample of a 220 most massive galaxies inside illustris ($\mathrm{m}_{\mathrm{200crit}}>6\times10^{12}\,\mathrm{m}_{\odot}$), $18\%\pm3\%$ of a galaxies exhibit shells. this fraction increases with increasing mass cut: higher mass galaxies are more likely to have stellar shells. furthermore, a fraction of massive galaxies that exhibit shells decreases with increasing redshift. we find that shell galaxies observed at redshift $z=0$ form preferentially through relatively major mergers ($\gtrsim$1:10 inside stellar mass ratio). progenitors are accreted on low angular momentum orbits, inside the preferred time-window between $\sim$4 and 8 gyrs ago. our study indicates that, due to dynamical friction, more massive satellites are allowed to probe the wider range of impact parameters at accretion time, while small companions need almost purely radial infall trajectories inside order to produce shells. we also find the number of special cases, as the consequence of a additional complexity introduced by a cosmological setting. these include galaxies with multiple shell-forming progenitors, satellite-of-satellites also forming shells, or satellites that fail to produce shells due to multiple major mergers happening inside quick succession.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16058,"a number of scientific articles has grown rapidly over a years and there are no signs that this growth will slow down inside a near future. because of this, it becomes increasingly difficult to keep up with a latest developments inside the scientific field. to address this problem, we present here an idea behind the method to aid researchers learn about a latest developments and findings by extracting inside the normalized form core claims from scientific articles. this normalized representation was the controlled natural language of english sentences called aida, which has been proposed inside previous work as the method to formally structure and organize scientific findings and discourse. we show how such aida sentences should be automatically extracted by detecting a core claim of an article, checking considering aida compliance, and - if necessary - transforming it into the compliant sentence. while our algorithm was still far from perfect, our results indicate that a different steps are feasible and they support a claim that aida sentences might be the promising idea behind the method to improve scientific communication inside a future.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7115,"we investigate a problem of learning discrete, undirected graphical models inside the differentially private way. we show that a idea behind the method of releasing noisy sufficient statistics with the help of a laplace mechanism achieves the good trade-off between privacy, utility, and practicality. the naive learning algorithm that uses a noisy sufficient statistics ""as is"" outperforms general-purpose differentially private learning algorithms. however, it has three limitations: it ignores knowledge about a data generating process, rests on uncertain theoretical foundations, and exhibits certain pathologies. we develop the more principled idea behind the method that applies a formalism of collective graphical models to perform inference over a true sufficient statistics within an expectation-maximization framework. we show that this learns better models than competing approaches on both synthetic data and on real human mobility data used as the case study.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11426,"inside this work, we consider a problem of combining link, content and temporal analysis considering community detection and prediction inside evolving networks. such temporal and content-rich networks occur inside many real-life settings, such as bibliographic networks and question answering forums. most of a work inside a literature (that uses both content and structure) deals with static snapshots of networks, and they do not reflect a dynamic changes occurring over multiple snapshots. incorporating dynamic changes inside a communities into a analysis should also provide useful insights about a changes inside a network such as a migration of authors across communities. inside this work, we propose chimera, the shared factorization model that should simultaneously account considering graph links, content, and temporal analysis. this idea behind the method works by extracting a latent semantic structure of a network inside multidimensional form, but inside the way that takes into account a temporal continuity of these embeddings. such an idea behind the method simplifies temporal analysis of a underlying network by with the help of a embedding as the surrogate. the consequence of this simplification was that it was also possible to use this temporal sequence of embeddings to predict future communities. we present experimental results illustrating a effectiveness of a approach.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0
13317,"one of a key challenges of visual perception was to extract abstract models of 3d objects and object categories from visual measurements, which are affected by complex nuisance factors such as viewpoint, occlusion, motion, and deformations. starting from a recent idea of viewpoint factorization, we propose the new idea behind the method that, given the large number of images of an object and no other supervision, should extract the dense object-centric coordinate frame. this coordinate frame was invariant to deformations of a images and comes with the dense equivariant labelling neural network that should map image pixels to their corresponding object coordinates. we demonstrate a applicability of this method to simple articulated objects and deformable objects such as human faces, learning embeddings from random synthetic transformations or optical flow correspondences, all without any manual supervision.",1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1909,"we consider a following generalization of a binary search problem. the search strategy was required to locate an unknown target node $t$ inside the given tree $t$. upon querying the node $v$ of a tree, a strategy receives as the reply an indication of a connected component of $t\setminus\{v\}$ containing a target $t$. a cost of querying each node was given by the known non-negative weight function, and a considered objective was to minimize a total query cost considering the worst-case choice of a target. designing an optimal strategy considering the weighted tree search instance was known to be strongly np-hard, inside contrast to a unweighted variant of a problem which should be solved optimally inside linear time. here, we show that weighted tree search admits the quasi-polynomial time approximation scheme: considering any $0 \textless{} \varepsilon \textless{} 1$, there exists the $(1+\varepsilon)$-approximation strategy with the computation time of $n^{o(\log n / \varepsilon^2)}$. thus, a problem was not apx-hard, unless $np \subseteq dtime(n^{o(\log n)})$. by applying the generic reduction, we obtain as the corollary that a studied problem admits the polynomial-time $o(\sqrt{\log n})$-approximation. this improves previous $\hat o(\log n)$-approximation approaches, where a $\hat o$-notation disregards $o(\mathrm{poly}\log\log n)$-factors.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10149,"complex activity recognition was challenging due to a inherent uncertainty and diversity of performing the complex activity. normally, each instance of the complex activity has its own configuration of atomic actions and their temporal dependencies. we propose inside this paper an atomic action-based bayesian model that constructs allen's interval relation networks to characterize complex activities with structural varieties inside the probabilistic generative way: by introducing latent variables from a chinese restaurant process, our idea behind the method was able to capture all possible styles of the particular complex activity as the unique set of distributions over atomic actions and relations. we also show that local temporal dependencies should be retained and are globally consistent inside a resulting interval network. moreover, network structure should be learned from empirical data. the new dataset of complex hand activities has been constructed and made publicly available, which was much larger inside size than any existing datasets. empirical evaluations on benchmark datasets as well as our in-house dataset demonstrate a competitiveness of our approach.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18379,"we consider a minimization of composite objective functions composed of a expectation of quadratic functions and an arbitrary convex function. we study a stochastic dual averaging algorithm with the constant step-size, showing that it leads to the convergence rate of o(1/n) without strong convexity assumptions. this thus extends earlier results on least-squares regression with a euclidean geometry to (a) all convex regularizers and constraints, and (b) all geome-tries represented by the bregman divergence. this was achieved by the new proof technique that relates stochastic and deterministic recursions.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0
16649,"a highly-adaptive-lasso(hal)-tmle was an efficient estimator of the pathwise differentiable parameter inside the statistical model that at minimal (and possibly only) assumes that a sectional variation norm of a true nuisance parameters are finite. it relies on an initial estimator (hal-mle) of a nuisance parameters by minimizing a empirical risk over a parameter space under a constraint that sectional variation norm was bounded by the constant, where this constant should be selected with cross-validation. inside a formulation of a halmle this sectional variation norm corresponds with a sum of absolute value of coefficients considering an indicator basis. due to its reliance on machine learning, statistical inference considering a tmle has been based on its normal limit distribution, thereby potentially ignoring the large second order remainder inside finite samples. inside this article, we present four methods considering construction of the finite sample 0.95-confidence interval that use a nonparametric bootstrap to approximate a finite sample distribution of a hal-tmle or the conservative distribution dominating a true finite sample distribution. we prove that it consistently estimates a optimal normal limit distribution, while its approximation error was driven by a performance of a bootstrap considering the well behaved empirical process. we demonstrate our general inferential methods considering 1) nonparametric approximation of a average treatment effect based on observing on each unit the covariate vector, binary treatment, and outcome, and considering 2) nonparametric approximation of a integral of a square of a multivariate density of a data distribution.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
3925,"one of a major drawbacks of modularized task-completion dialogue systems was that each module was trained individually, which presents several challenges. considering example, downstream modules are affected by earlier modules, and a performance of a entire system was not robust to a accumulated errors. this paper presents the novel end-to-end learning framework considering task-completion dialogue systems to tackle such issues. our neural dialogue system should directly interact with the structured database to assist users inside accessing information and accomplishing certain tasks. a reinforcement learning based dialogue manager offers robust capabilities to handle noises caused by other components of a dialogue system. our experiments inside the movie-ticket booking domain show that our end-to-end system not only outperforms modularized dialogue system baselines considering both objective and subjective evaluation, but also was robust to noises as demonstrated by several systematic experiments with different error granularity and rates specific to a language understanding module.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2923,"let $(\sigma,g)$ be the compact riemannian surface without boundary and $\lambda_1(\sigma)$ be a first eigenvalue of a laplace-beltrami operator $\delta_g$. let $h$ be the positive smooth function on $\sigma$. define the functional $$j_{\alpha,\beta}(u)=\frac{1}{2}\int_\sigma(|\nabla_gu|^2-\alpha u^2)dv_g-\beta\log\int_\sigma he^udv_g$$ on the function space $\mathcal{h}=\left\{u\in w^{1,2}(\sigma): \int_\sigma udv_g=0\right\}$. if $\alpha<\lambda_1(\sigma)$ and $j_{\alpha,8\pi}$ has no minimizer on $\mathcal{h}$, then we calculate a infimum of $j_{\alpha,8\pi}$ on $\mathcal{h}$ by with the help of a method of blow-up analysis. as the consequence, we give the sufficient condition under which the kazdan-warner equation has the solution. if $\alpha\geq \lambda_1(\sigma)$, then $\inf_{u\in\mathcal{h}}j_{\alpha,8\pi}(u)=-\infty$. if $\beta>8\pi$, then considering any $\alpha\in\mathbb{r}$, there holds $\inf_{u\in\mathcal{h}}j_{\alpha,\beta}(u)=-\infty$. moreover, we consider a same problem inside a case that $\alpha$ was large, where higher order eigenvalues are involved.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8239,"a electric field gradient (efg) tensor at a $^{75}$as site couples to a orbital occupations of a as p-orbitals and was the sensitive probe of local nematicity inside bafe$_2$as$_2$. we use nuclear magnetic resonance to measure a nuclear quadrupolar splittings and find that a efg asymmetry responds linearly to a presence of the strain field inside a paramagnetic phase. we extract a nematic susceptibility from a slope of this linear response as the function of temperature and find that it diverges near a structural transition inside agreement with other measures of a bulk nematic susceptibility. our work establishes an alternative method to extract a nematic susceptibility which, inside contrast to transport methods, should be extended in a superconducting state.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
13768,"confidence was the fundamental concept inside statistics, but there was the tendency to misinterpret it as probability. inside this paper, i argue that an intuitively and mathematically more appropriate interpretation of confidence was through belief/plausibility functions, inside particular, those that satisfy the certain validity property. given their close connection with confidence, it was natural to ask how the valid belief/plausibility function should be constructed directly. a inferential model (im) framework provides such the construction, and here i prove the complete-class theorem stating that, considering every nominal confidence region, there exists the valid im whose plausibility regions are contained by a given confidence region. this characterization has implications considering statistics understanding and communication, and highlights a importance of belief functions and a im framework.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
6931,"star formation inside galaxies relies on a availability of cold, dense gas, which, inside turn, relies on factors internal and external to a galaxies. inside order to provide the simple model considering how star formation was regulated by various physical processes inside galaxies, we analyse data at redshift $z=0$ from the hydrodynamical cosmological simulation that includes prescriptions considering star formation and stellar evolution, active galactic nuclei (agn), and their associated feedback processes. this model should determine a star formation rate (sfr) as the function of galaxy stellar mass, gas mass, black hole mass, and environment. we find that gas mass was a most important quantity controlling star formation inside low-mass galaxies, and star-forming galaxies inside dense environments have higher sfr than their counterparts inside a field. inside high-mass galaxies, we find that black holes more massive than $\sim10^{7.5}$ m$_\odot$ should be triggered to quench star formation inside their host; this mass scale was emergent inside our simulations. furthermore, this black hole mass corresponds to the galaxy bulge mass $\sim2\times10^{10}$ m$_\odot$, consistent with a mass at which galaxies start to become dominated by early types ($\sim3\times10^{10}$ m$_\odot$, as previously shown inside observations by kauffmann et al.). finally, we demonstrate that our model should reproduce well a sfr measured from observations of galaxies inside a gama and alfalfa surveys.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19809,"microwave plasma discharges working at low pressure are nowadays the well-developed technique mainly used to provide radiations at different wavelengths. a aim of this work was to show that those discharges are an efficient windowless vuv photon source considering planetary atmospheric photochemistry experiments. to do this, we use the surfatron-type discharge with the neon gas flow inside a mbar pressure range coupled to the photochemical reactor. working inside a vuv range allows to focus on nitrogen-dominated atmospheres ({\lambda}<100nm). a experimental setup makes sure that no other energy sources (electrons, metastable atoms) than a vuv photons interact with a reactive medium. neon owns two resonance lines at 73.6 and 74.3 nm which behave differently regarding a pressure or power conditions. inside parallel, a vuv photon flux emitted at 73.6 nm has been experimentally estimated inside different conditions of pressure and power and varies inside the large range between 2x1013 ph.s-1.cm-2 and 4x1014 ph.s-1.cm-2 which was comparable to the vuv synchrotron photon flux. our first case study was a atmosphere of titan and its n2-ch4 atmosphere. with this vuv source, a production of hcn and c2n2, two major titan compounds, was detected, ensuring a suitability of a source considering atmospheric photochemistry experiments.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18526,"we study detection methods considering multivariable signals under dependent noise. a main focus was on three-dimensional signals, i.e. on signals inside a space-time domain. examples considering such signals are multifaceted. they include geographic and climatic data as well as image data, that are observed over the fixed time horizon. we assume that a signal was observed as the finite block of noisy samples whereby we are interested inside detecting changes from the given reference signal. our detector statistic was based on the sequential partial sum process, related to classical signal decomposition and reconstruction approaches applied to a sampled signal. we show that this detector process converges weakly under a no change null hypothesis that a signal coincides with a reference signal, provided that a spatial-temporal partial sum process associated to a random field of a noise terms disturbing a sampled signal con- verges to the brownian motion. more generally, we also establish a limiting distribution under the wide class of local alternatives that allows considering smooth as well as discontinuous changes. our results also cover extensions to a case that a reference signal was unknown. we conclude with an extensive simulation study of a detection algorithm.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
6745,"domain adaptation refers to a problem of leveraging labeled data inside the source domain to learn an accurate model inside the target domain where labels are scarce or unavailable. the recent idea behind the method considering finding the common representation of a two domains was using domain adversarial training (ganin & lempitsky, 2015), which attempts to induce the feature extractor that matches a source and target feature distributions inside some feature space. however, domain adversarial training faces two critical limitations: 1) if a feature extraction function has high-capacity, then feature distribution matching was the weak constraint, 2) inside non-conservative domain adaptation (where no single classifier should perform well inside both a source and target domains), training a model to do well on a source domain hurts performance on a target domain. inside this paper, we address these issues through a lens of a cluster assumption, i.e., decision boundaries should not cross high-density data regions. we propose two novel and related models: 1) a virtual adversarial domain adaptation (vada) model, which combines domain adversarial training with the penalty term that punishes a violation a cluster assumption; 2) a decision-boundary iterative refinement training with the teacher (dirt-t) model, which takes a vada model as initialization and employs natural gradient steps to further minimize a cluster assumption violation. extensive empirical results demonstrate that a combination of these two models significantly improve a state-of-the-art performance on a digit, traffic sign, and wi-fi recognition domain adaptation benchmarks.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4239,"probabilistic integration of the continuous dynamical system was the way of systematically introducing model error, at scales no larger than errors introduced by standard numerical discretisation, inside order to enable thorough exploration of possible responses of a system to inputs. it was thus the potentially useful idea behind the method inside the number of applications such as forward uncertainty quantification, inverse problems, and data assimilation. we extend a convergence analysis of probabilistic integrators considering deterministic ordinary differential equations, as proposed by conrad et al.\ (\textit{stat.\ comput.}, 2016), to establish mean-square convergence inside a uniform norm on discrete- or continuous-time solutions under relaxed regularity assumptions on a driving vector fields and their induced flows. specifically, we show that randomised high-order integrators considering globally lipschitz flows and randomised euler integrators considering dissipative vector fields with polynomially-bounded local lipschitz constants all have a same mean-square convergence rate as their deterministic counterparts, provided that a variance of a integration noise was not of higher order than a corresponding deterministic integrator. these and similar results are proven considering probabilistic integrators where a random perturbations may be state-dependent, non-gaussian, or non-centred random variables.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
18657,"we theoretically investigate pump-probe optical responses inside a two-dimensional extended hubbard model describing cuprates by with the help of the time-dependent lanczos method. at half filling, pumping generates photoinduced absorptions in a mott gap. the part of low-energy absorptions was attributed to a independent propagation of photoinduced holons and doublons. a spectral weight just below a mott gap increases with decreasing a on-site coulomb interaction $u$. we find that a next-nearest-neighbor coulomb interaction $v_1$ enhances this $u$ dependence, indicating a presence of biexcitonic contributions formed by two holon-doublon pairs. photo-pumping inside hole-doped systems also induces spectral weights below remnant mott-gap excitations, being consistent with recent experiments. a induced weights are less sensitive to $v_1$ and may be related to a formation of the biexcitonic state inside a presence of hole carriers.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
2177,"let $(m,g)$ be the pseudo-riemannian manifold of signature $(p,q)$. we construct mutually quasi-inverse equivalences between a groupoid of bundles of weakly-faithful complex clifford modules on $(m,g)$ and a groupoid of reduced complex lipschitz structures on $(m,g)$. as an application, we show that $(m,g)$ admits the bundle of irreducible complex clifford modules if and only if it admits either the $spin^{c}(p,q)$ structure (when $p+q$ was odd) or the $pin^{c}(p,q)$ structure (when $p+q$ was even). when $p-q\equiv_8 3,4,6, 7$, we compare with a classification of bundles of irreducible real clifford modules which we obtained inside previous work. a results obtained inside this note form the counterpart of a classification of bundles of faithful complex clifford modules which is previously given by t. friedrich and a. trautman.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10516,"if the significant fraction of a dark matter inside a universe was made of an ultra-light scalar field, named fuzzy dark matter (fdm) with the mass $m_a$ of a order of $10^{-22}-10^{-21}$ ev, then its de broglie wavelength was large enough to impact a physics of large scale structure formation. inside particular, a associated cut-off inside a linear matter power spectrum modifies a structure of a intergalactic medium (igm) at a scales probed by a lyman-$\alpha$ forest of distant quasars. we study this effect by making use of dedicated cosmological simulations which take into account a hydrodynamics of a igm. we explore heuristically a amplitude of quantum pressure considering a fdm masses considered here and conclude that quantum effects should not modify significantly a non-linear evolution of matter density at a scales relevant to a measured lyman-$\alpha$ flux power, and considering $m_a \geq 10^{-22}$ ev. we derive the scaling law between $m_a$ and a mass of a well-studied thermal warm dark matter (wdm) model that was best adapted to a lyman-$\alpha$ forest data, and differs significantly from a one infered by the simple linear extrapolation. by comparing fdm simulations with a lyman-$\alpha$ flux power spectra determined from a boss survey, and marginalizing over relevant nuisance parameters, we exclude fdm masses inside a range $10^{-22} \leq m_a < 2.3\times 10^{-21}$ ev at 95 % cl. adding higher-resolution lyman-$\alpha$ spectra extends a exclusion range up to $2.9\times 10^{-21}$ ev. this provides the significant constraint on fdm models tailored to solve a ""small-scale problems"" of $\lambda$cdm.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12454,"we study quasar proximity zones inside a redshift range $5.77 \leq z \leq 6.54$ by homogeneously analyzing $34$ medium resolution spectra, encompassing both archival and newly obtained data, and exploiting recently updated systemic redshift and magnitude measurements. whereas previous studies found strong evolution of proximity zone sizes with redshift, and argued that this provides evidence considering the rapidly evolving intergalactic medium (igm) neutral fraction during reionization, we measure the much shallower trend $\propto(1+z)^{-1.44}$. we compare our measured proximity zone sizes to predictions from hydrodynamical simulations post-processed with one-dimensional radiative transfer, and find good agreement between observations and theory irrespective of a ionization state of a ambient igm. this insensitivity to igm ionization state has been previously noted, and results from a fact that a definition of proximity zone size as a first drop of a smoothed quasar spectrum below a $10\%$ flux transmission level probes locations where a ionizing radiation from a quasar was an order of magnitude larger than a expected ultraviolet ionizing background that sets a neutral fraction of a igm. our analysis also uncovered three objects with exceptionally small proximity zones (two have $r_p < 1$proper mpc), which constitute outliers from a observed distribution and are challenging to explain with our radiative transfer simulations. we consider various explanations considering their origin, such as strong absorption line systems associated with a quasar or patchy reionization, but find that a most compelling scenario was that these quasars have been shining considering $\lesssim 10^5$yr.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18355,"a electric coupling between surface ions and bulk ferroelectricity gives rise to the continuum of mixed states inside ferroelectric thin films, exquisitely sensitive to temperature and external factors, such as applied voltage and oxygen pressure. here we develop a comprehensive analytical description of these coupled ferroelectric and ionic (""ferroionic"") states by combining a ginzburg-landau-devonshire description of a ferroelectric properties of a film with langmuir adsorption model considering a electrochemical reaction at a film surface. we explore a thermodynamic and kinetic characteristics of a ferroionic states as the function of temperature, film thickness, and external electric potential. these studies provide the new insight into mesoscopic properties of ferroelectric thin films, whose surface was exposed to chemical environment as screening charges supplier.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
1863,"traditional image clustering methods take the two-step approach, feature learning and clustering, sequentially. however, recent research results demonstrated that combining a separated phases inside the unified framework and training them jointly should achieve the better performance. inside this paper, we first introduce fully convolutional auto-encoders considering image feature learning and then propose the unified clustering framework to learn image representations and cluster centers jointly based on the fully convolutional auto-encoder and soft $k$-means scores. at initial stages of a learning procedure, a representations extracted from a auto-encoder may not be very discriminative considering latter clustering. we address this issue by adopting the boosted discriminative distribution, where high score assignments are highlighted and low score ones are de-emphasized. with a gradually boosted discrimination, clustering assignment scores are discriminated and cluster purities are enlarged. experiments on several vision benchmark datasets show that our methods should achieve the state-of-the-art performance.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
12838,"this survey presents a main results achieved considering a influence maximization problem inside social networks. this problem was well studied inside a literature and, thanks to its recent applications, some of which currently deployed on a field, it was receiving more and more attention inside a scientific community. a problem should be formulated as follows: given the graph, with each node having the certain probability of influencing its neighbors, select the subset of vertices so that a number of nodes inside a network that are influenced was maximized. starting from this model, we introduce a main theoretical developments and computational results that have been achieved, taking into account different diffusion models describing how a information spreads throughout a network, various ways inside which a sources of information could be placed, and how to tackle a problem inside a presence of uncertainties affecting a network. finally, we present one of a main application that has been developed and deployed exploiting tools and techniques previously discussed.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
8773,"predictable feature analysis (pfa) (richthofer, wiskott, icmla 2015) was an algorithm that performs dimensionality reduction on high dimensional input signal. it extracts those subsignals that are most predictable according to the certain prediction model. we refer to these extracted signals as predictable features. inside this work we extend a notion of pfa to take supplementary information into account considering improving its predictions. such information should be the multidimensional signal like a main input to pfa, but was regarded external. that means it won't participate inside a feature extraction - no features get extracted or composed of it. features will be exclusively extracted from a main input such that they are most predictable based on themselves and a supplementary information. we refer to this enhanced pfa as pfax (pfa extended). even more important than improving prediction quality was to observe a effect of supplementary information on feature selection. pfax transparently provides insight how a supplementary information adds to prediction quality and whether it was valuable at all. finally we show how to invert that relation and should generate a supplementary information such that it would yield the certain desired outcome of a main signal. we apply this to the setting inspired by reinforcement learning and let a algorithm learn how to control an agent inside an environment. with this method it was feasible to locally optimize a agent's state, i.e. reach the certain goal that was near enough. we are preparing the follow-up paper that extends this method such that also global optimization was feasible.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
2555,"sufficient and necessary conditions considering a stability of positive feedback interconnections of negative imaginary systems are derived using an integral quadratic constraint (iqc) approach. a iqc framework accommodates distributed-parameter systems with irrational transfer function representations, while generalising existing results inside a literature and allowing exploitation of flexibility at zero and infinite frequencies to reduce conservatism inside a analysis. a main results manifest a important property that a negative imaginariness of systems gives rise to the certain form of iqcs on positive frequencies that are bounded away from zero and infinity. two additional sets of iqcs on a dc and instantaneous gains of a systems are shown to be sufficient and necessary considering closed-loop stability along the homotopy of systems.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
4376,when the flow was not allowed to be reoriented a maximum residual flow problem with $k$-arc destruction was known to be $np$-hard considering $k=2$. we show that when the flow was allowed to be adaptive a problem becomes polynomial considering every fixed $k$.,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15800,"machine learning (ml) plays an ever-increasing role inside advanced automotive functionality considering driver assistance and autonomous operation; however, its adequacy from a perspective of safety certification remains controversial. inside this paper, we analyze a impacts that a use of ml as an implementation idea behind the method has on iso 26262 safety lifecycle and ask what could be done to address them. we then provide the set of recommendations on how to adapt a standard to accommodate ml.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
8844,"dynamic epidemic models have proven valuable considering public health decision makers as they provide useful insights into a understanding and prevention of infectious diseases. however, inference considering these types of models should be difficult because a disease spread was typically only partially observed e.g. inside form of reported incidences inside given time periods. this chapter discusses how to perform likelihood-based inference considering partially observed markov epidemic models when it was relatively easy to generate samples from a markov transmission model while a likelihood function was intractable. a first part of a chapter reviews a theoretical background of inference considering partially observed markov processes (pomp) using iterated filtering. inside a second part of a chapter a performance of a method and associated practical difficulties are illustrated on two examples. inside a first example the simulated outbreak data set consisting of a number of newly reported cases aggregated by week was fitted to the pomp where a underlying disease transmission model was assumed to be the simple markovian sir model. a second example illustrates possible model extensions such as seasonal forcing and over-dispersion inside both, a transmission and observation model, which should be used, e.g., when analysing routinely collected rotavirus surveillance data. both examples are implemented with the help of a r-package pomp (king et al., 2016) and a code was made available online.",0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
11419,"motivated by a spin-triplet superconductor sr2ruo4, a thermal hall conductivity was investigated considering several pairing symmetries with broken time-reversal symmetry. inside a chiral p-wave phase with the fully opened quasiparticle excitation gap, a temperature dependence of a thermal hall conductivity has the temperature linear term associated with a topological property directly, and an exponential term, which shows the drastic change around a lifshitz transition. examining f-wave states as alternative candidates with $\bm d=\delta_0\hat{z}(k_x^2-k_y^2)(k_x\pm ik_y)$ and $\bm d=\delta_0\hat{z}k_xk_y(k_x\pm ik_y)$ with gapless quasiparticle excitations, we study a temperature dependence of a thermal hall conductivity, where considering a former state a thermal hall conductivity has the quadratic dependence on temperature, originating from a linear dispersions, inside addition to linear and exponential behavior. a obtained result may enable us to distinguish between a chiral p-wave and f-wave states inside sr2ruo4.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
11116,"the lack of understanding of human biology creates the hurdle considering a development of precision medicines. to overcome this hurdle we need to better understand a potential synergy between the given investigational treatment (vs. placebo or active control) and various demographic or genetic factors, disease history and severity, etc., with a goal of identifying those patients at increased risk of exhibiting clinically meaningful treatment benefit. considering this reason, we propose a vg method, which combines a idea of an individual treatment effect (ite) from virtual twins (foster, et al., 2011) with a unbiased variable selection and cutoff value determination algorithm from guide (loh, et al., 2015). simulation results show a vg method has less variable selection bias than virtual twins and higher statistical power than guide interaction inside a presence of prognostic variables with strong treatment effects. type i error and predictive performance of virtual twins, guide and vg are compared through a use of simulation studies. results obtained after retrospectively applying vg to data from the clinical trial also are discussed.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16947,"spectral clustering has found extensive use inside many areas. most traditional spectral clustering algorithms work inside three separate steps: similarity graph construction; continuous labels learning; discretizing a learned labels by k-means clustering. such common practice has two potential flaws, which may lead to severe information loss and performance degradation. first, predefined similarity graph might not be optimal considering subsequent clustering. it was well-accepted that similarity graph highly affects a clustering results. to this end, we propose to automatically learn similarity information from data and simultaneously consider a constraint that a similarity matrix has exact c connected components if there are c clusters. second, a discrete solution may deviate from a spectral solution since k-means method was well-known as sensitive to a initialization of cluster centers. inside this work, we transform a candidate solution into the new one that better approximates a discrete one. finally, those three subtasks are integrated into the unified framework, with each subtask iteratively boosted by with the help of a results of a others towards an overall optimal solution. it was known that a performance of the kernel method was largely determined by a choice of kernels. to tackle this practical problem of how to select a most suitable kernel considering the particular data set, we further extend our model to incorporate multiple kernel learning ability. extensive experiments demonstrate a superiority of our proposed method as compared to existing clustering approaches.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1047,"data available across a web was largely unstructured. offers published by multiple sources like banks, digital wallets, merchants, etc., are one of a most accessed advertising data inside today's world. this data gets accessed by millions of people on the daily basis and was easily interpreted by humans, but since it was largely unstructured and diverse, with the help of an algorithmic way to extract meaningful information out of these offers was hard. identifying a essential offer entities (for instance, its amount, a product on which a offer was applicable, a merchant providing a offer, etc.) from these offers plays the vital role inside targeting a right customers to improve sales. this work presents and evaluates various existing named entity recognizer (ner) models which should identify a required entities from offer feeds. we also propose the novel hybrid ner model constructed by two-level stacking of conditional random field, bidirectional lstm and spacy models at a first level and an svm classifier at a second. a proposed hybrid model has been tested on offer feeds collected from multiple sources and has shown better performance inside a offer domain when compared to a existing models.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
9157,"photometric stereo was the method considering estimating a normal vectors of an object from images of a object under varying lighting conditions. motivated by several recent works that extend photometric stereo to more general objects and lighting conditions, we study the new robust idea behind the method to photometric stereo that utilizes dictionary learning. specifically, we propose and analyze two approaches to adaptive dictionary regularization considering a photometric stereo problem. first, we propose an image preprocessing step that utilizes an adaptive dictionary learning model to remove noise and other non-idealities from a image dataset before estimating a normal vectors. we also propose an alternative model where we directly apply a adaptive dictionary regularization to a normal vectors themselves during estimation. we study a practical performance of both methods through extensive simulations, which demonstrate a state-of-the-art performance of both methods inside a presence of noise.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
443,"learning graphical models from data was an important problem with wide applications, ranging from genomics to a social sciences. nowadays datasets often have upwards of thousands---sometimes tens or hundreds of thousands---of variables and far fewer samples. to meet this challenge, we have developed the new r package called sparsebn considering learning a structure of large, sparse graphical models with the focus on bayesian networks. while there are many existing software packages considering this task, this package focuses on a unique setting of learning large networks from high-dimensional data, possibly with interventions. as such, a methods provided place the premium on scalability and consistency inside the high-dimensional setting. furthermore, inside a presence of interventions, a methods implemented here achieve a goal of learning the causal network from data. additionally, a sparsebn package was fully compatible with existing software packages considering network analysis.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0
2434,"this paper addresses a problem of output voltage regulation considering multiple dc/dc converters connected to the microgrid, and prescribes the scheme considering sharing power among different sources. this architecture was structured inside such the way that it admits quantifiable analysis of a closed-loop performance of a network of converters; a analysis simplifies to studying closed-loop performance of an equivalent {\em single-converter} system. a proposed architecture allows considering a proportion inside which a sources provide power to vary with time; thus overcoming limitations of our previous designs. additionally, a proposed control framework was suitable to both centralized and decentralized implementations, i.e., a same control architecture should be employed considering voltage regulation irrespective of a availability of common load-current (or power) measurement, without a need to modify controller parameters. a performance becomes quantifiably better with better communication of a demanded load to all a controllers at all a converters (in a centralized case); however guarantees viability when such communication was absent. case studies comprising of battery, pv and generic sources are presented and demonstrate a enhanced performance of prescribed optimal controllers considering voltage regulation and power sharing.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
16074,"critical node problems involve identifying the subset of critical nodes from an undirected graph whose removal results inside optimizing the pre-defined measure over a residual graph. as useful models considering the variety of practical applications, these problems are computational challenging. inside this paper, we study a classic critical node problem (cnp) and introduce an effective memetic algorithm considering solving cnp. a proposed algorithm combines the double backbone-based crossover operator (to generate promising offspring solutions), the component-based neighborhood search procedure (to find high-quality local optima) and the rank-based pool updating strategy (to guarantee the healthy population). specially, a component-based neighborhood search integrates two key techniques, i.e., two-phase node exchange strategy and node weighting scheme. a double backbone-based crossover extends a idea of general backbone-based crossovers. extensive evaluations on 42 synthetic and real-world benchmark instances show that a proposed algorithm discovers 21 new upper bounds and matches 18 previous best-known upper bounds. we also demonstrate a relevance of our algorithm considering effectively solving the variant of a classic cnp, called a cardinality-constrained critical node problem. finally, we investigate a usefulness of each key algorithmic component.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1867,"we view intersection handling on autonomous vehicles as the reinforcement learning problem, and study its behavior inside the transfer learning setting. we show that the network trained on one type of intersection generally was not able to generalize to other intersections. however, the network that was pre-trained on one intersection and fine-tuned on another performs better on a new task compared to training inside isolation. this network also retains knowledge of a prior task, even though some forgetting occurs. finally, we show that a benefits of fine-tuning hold when transferring simulated intersection handling knowledge to the real autonomous vehicle.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
4284,"we consider the probit model without covariates, but a latent gaussian variables having compound symmetry covariance structure with the single parameter characterizing a common correlation. we study a parameter approximation problem under such one-parameter probit models. as the surprise, we demonstrate that a likelihood function does not yield consistent estimates considering a correlation. we then formally prove a parameter's nonestimability by deriving the non-vanishing minimax lower bound. this counter-intuitive phenomenon provides an interesting insight that one bit information of a latent gaussian variables was not sufficient to consistently recover their correlation. on a other hand, we further show that trinary data generated from a gaussian variables should consistently approximate a correlation with parametric convergence rate. thus we reveal the phase transition phenomenon regarding a discretization of latent gaussian variables while preserving a estimability of a correlation.",0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0
4598,inside this paper we develop a notion of screen isoparametric hypersurface considering null hypersurfaces of robertson-walker spacetimes. with the help of this formalism we derive cartan identities considering a screen principal curvatures of null screen hypersurfaces inside lorentzian space forms and provide the local characterization of such hypersurfaces.,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17666,"autonomous unmanned aerial vehicles (uavs) that should execute aggressive (i.e., high-speed and high-acceleration) maneuvers have attracted significant attention inside a past few years. inside this paper, we propose the novel control law considering accurate tracking of aggressive quadcopter trajectories. a proposed method tracks position and yaw angle with their derivatives of up to fourth order, specifically, a position, velocity, acceleration, jerk, and snap along with a yaw angle, yaw rate and yaw acceleration. two key aspects of a proposed method are a following. first, a controller exploits a differential flatness of a quadcopter dynamics to generate feedforward inputs considering attitude rate and attitude acceleration inside order to track a jerk and snap references. a tracking was enabled by direct control of body torque with the help of closed-loop control of all four propeller speeds based on optical encoders attached to a motors. second, a controller utilizes a incremental nonlinear dynamic inversion (indi) method considering accurate tracking of linear and angular accelerations despite external disturbances. hence, no prior modeling of aerodynamic effects was required. we rigorously analyze a proposed controller through response analysis, and we demonstrate it inside experiments. a proposed control law enables the 1-kg quadcopter uav to track complex 3d trajectories, reaching speeds up to 8.2 m/s and accelerations up to 2g, while keeping a root-mean-square tracking error down to 4 cm, inside the flight volume that was roughly 6.5 m long, 6.5 m wide, and 1.5 m tall. we also demonstrate a robustness of a controller by attaching the drag plate to a uav inside flight tests and by pulling on a uav with the rope during hover.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1
16289,"a main objective of this thesis was a study of a evolution under a ricci flow of surfaces with singularities of cone type. the second objective, emerged from a techniques we use, was a study of families of ricci flow solitons inside dimension 2 and 3. a ricci flow was an evolution equation considering riemannian manifolds, introduced by r. hamilton inside 1982. it was from a achievements made by g. perelman with this technique inside 2002 when a ricci flow has been established inside the discipline itself, generating the great interest inside a community. this thesis contains four original results. first result was the complete classification of solitons inside smooth and cone surfaces. this classification completes a preceding results found by hamilton, chow and wu and others, and we obtain explicit descriptions of all solitons inside dimension 2. second result was the geometrization of cone surfaces by ricci flow. this result, which uses a aforementioned first result, extends a theory of hamilton to a singular case. this was a most comprehensive result inside a thesis, considering which we use and develop analysis and pde techniques, as well as comparison geometry techniques. third result was a existence of the ricci flow that removes cone singularities. this clearly exposes a non-uniqueness of solutions to a flow , inside analogy to a ricci flow with cusps of p. topping. a fourth result was a construction of the new expanding gradient ricci soliton inside dimension 3. just as we do with solitons on cone surfaces, we give an explicit construction with the help of techniques of phase portraits. we also prove that this was a only soliton with its topology and its lower bound of a curvature, and besides this was the critical case amongst all expanding solitons inside dimension 3 with curvature bounded below.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4982,"self-attentive feed-forward sequence models have been shown to achieve impressive results on sequence modeling tasks, thereby presenting the compelling alternative to recurrent neural networks (rnns) which has remained a de-facto standard architecture considering many sequence modeling problems to date. despite these successes, however, feed-forward sequence models like a transformer fail to generalize inside many tasks that recurrent models handle with ease (e.g. copying when a string lengths exceed those observed at training time). moreover, and inside contrast to rnns, a transformer model was not computationally universal, limiting its theoretical expressivity. inside this paper we propose a universal transformer which addresses these practical and theoretical shortcomings and we show that it leads to improved performance on several tasks. instead of recurring over a individual symbols of sequences like rnns, a universal transformer repeatedly revises its representations of all symbols inside a sequence with each recurrent step. inside order to combine information from different parts of the sequence, it employs the self-attention mechanism inside every recurrent step. assuming sufficient memory, its recurrence makes a universal transformer computationally universal. we further employ an adaptive computation time (act) mechanism to allow a model to dynamically adjust a number of times a representation of each position inside the sequence was revised. beyond saving computation, we show that act should improve a accuracy of a model. our experiments show that on various algorithmic tasks and the diverse set of large-scale language understanding tasks a universal transformer generalizes significantly better and outperforms both the vanilla transformer and an lstm inside machine translation, and achieves the new state of a art on a babi linguistic reasoning task and a challenging lambada language modeling task.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
13694,"the type iin supernova (sn) was dominated by a interaction of sn ejecta with a circumstellar medium (csm). some sne iin (e.g., sn 2006jd) have episodes of re-brightening (""bumps"") inside their light curves. we present iptf13z, the sn iin discovered by a intermediate palomar transient factory (iptf) and characterised by several bumps inside its light curve. we analyse this peculiar behaviour trying to infer a properties of a csm and of a sn explosion, as well as a nature of its progenitor star. we obtained multi-band optical photometry considering over 1000 days after discovery with a p48 and p60 telescopes at palomar observatory. we obtained low-resolution optical spectra inside a same period. we did an archival search considering progenitor outbursts. we analyse our photometry and spectra, and compare iptf13z to other sne iin. the simple analytical model was used to approximate properties of a csm. iptf13z is the sn iin showing the light curve with five bumps during its decline phase. a bumps had amplitudes between 0.4 and 0.9 mag and durations between 20 and 120 days. a most prominent bumps appeared inside all our different optical bands. a spectra showed typical sn iin characteristics, with emission lines of h$\alpha$ (with broad component fwhm ~$10^{3}-10^{4} ~{\rm ~km ~s^{-1}}$ and narrow component fwhm ~$10^2 \rm ~km ~s^{-1}$) and he i, but also with fe ii, ca ii, na i d and h$\beta$ p-cygni profiles (with velocities of ~$10^{3}$ ${\rm ~km ~s^{-1}}$). the pre-explosion outburst is identified lasting $\gtrsim 50$ days, with $m_r \approx -15$ mag around 210 days before discovery. large, variable progenitor mass-loss rates (~> 0.01 $m_{\odot} \rm ~yr^{-1}$) and csm densities (~> 10$^{-16}$ g cm$^{-3}$) are derived. we suggest that a light curve bumps of iptf13z arose from sn ejecta interacting with denser regions inside a csm, possibly produced by a eruptions of the luminous blue variable star.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15015,we give an introduction to a mckay correspondence and its connection to quotients of $\mathbb{c}^n$ by finite reflection groups. this yields the natural construction of noncommutative resolutions of a discriminants of these reflection groups. this paper was an extended version of e.f.'s talk with a same title delivered at a icra.,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
595,"we present an attention based visual analysis framework to compute grasp-relevant information inside order to guide grasp planning with the help of the multi-fingered robotic hand. our idea behind the method uses the computational visual attention model to locate regions of interest inside the scene, and uses the deep convolutional neural network to detect grasp type and point considering the sub-region of a object presented inside the region of interest. we demonstrate a proposed framework inside object grasping tasks, inside which a information generated from a proposed framework was used as prior information to guide a grasp planning. results show that a proposed framework should not only speed up grasp planning with more stable configurations, but also was able to handle unknown objects. furthermore, our framework should handle cluttered scenarios. the new grasp type dataset (gtd) that considers 6 commonly used grasp types and covers 12 household objects was also presented.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
8339,"community structure was an important area of research. it has received the considerable attention from a scientific community. despite its importance, one of a key problems inside locating information about community detection was a diverse spread of related articles across various disciplines. to a best of our knowledge, there was no current comprehensive review of recent literature which uses the scientometric analysis with the help of complex networks analysis covering all relevant articles from a web of science (wos). here we present the visual survey of key literature with the help of citespace. a idea was to identify emerging trends besides with the help of network techniques to examine a evolution of a domain. towards that end, we identify a most influential, central, as well as active nodes with the help of scientometric analyses. we examine authors, key articles, cited references, core subject categories, key journals, institutions, as well as countries. a exploration of a scientometric literature of a domain reveals that yong wang was the pivot node with a highest centrality. additionally, we have observed that mark newman was a most highly cited author inside a network. we have also identified that a journal, ""reviews of modern physics"" has a strongest citation burst. inside terms of cited documents, an article by andrea lancichinetti has a highest centrality score. we have also discovered that a origin of a key publications inside this domain was from a united states. whereas scotland has a strongest and longest citation burst. additionally, we have found that a categories of ""computer science"" and ""engineering"" lead other categories based on frequency and centrality respectively.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
17552,"the fundamental component of a game theoretic idea behind the method to distributed control was a design of local utility functions. inside part i of this work we showed how to systematically design local utilities so as to maximize a induced worst case performance. a purpose of a present manuscript was to specialize a general results obtained inside part i to the class of monotone submodular, supermodular and set covering problems. inside a case of set covering problems, we show how any distributed algorithm capable of computing the nash equilibrium inherits the performance certificate matching a well known 1-1/e approximation of nemhauser. relative to a class of submodular maximization problems considered here, we show how a performance offered by a game theoretic idea behind the method improves on existing approximation algorithms. we briefly discuss a algorithmic complexity of computing (pure) nash equilibria and show how our idea behind the method generalizes and subsumes previously fragmented results inside a area of optimal utility design. two applications and corresponding numerics are presented: a vehicle target assignment problem and the coverage problem arising inside distributed caching considering wireless networks.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
14641,"we classify and characterize three dimensional $u(1)$ quantum spin liquids (deconfined $u(1)$ gauge theories) with global symmetries. these spin liquids have an emergent gapless photon and emergent electric/magnetic excitations (which we assume are gapped). we first discuss inside great detail a case with time reversal and $so(3)$ spin rotational symmetries. we find there are 15 distinct such quantum spin liquids based on a properties of bulk excitations. we show how to interpret them as gauged symmetry-protected topological states (spts). some of these states possess fractional response to an external $so(3)$ gauge field, due to which we dub them ""fractional topological paramagnets"". we identify 11 other anomalous states that should be grouped into 3 anomaly classes. a classification was further refined by weakly coupling these quantum spin liquids to bosonic symmetry protected topological (spt) phases with a same symmetry. this refinement does not modify a bulk excitation structure but modifies universal surface properties. taking this refinement into account, we find there are 168 distinct such $u(1)$ quantum spin liquids. after this warm-up we provide the general framework to classify symmetry enriched $u(1)$ quantum spin liquids considering the large class of symmetries. as the more complex example, we discuss $u(1)$ quantum spin liquids with time reversal and $z_2$ symmetries inside detail. based on a properties of a bulk excitations, we find there are 38 distinct such spin liquids that are anomaly-free. there are also 37 anomalous $u(1)$ quantum spin liquids with this symmetry. finally, we briefly discuss a classification of $u(1)$ quantum spin liquids enriched by some other symmetries.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
11616,"we derive an explicit formula, as well as an efficient procedure, considering constructing the generalized jacobian considering a projector of the given square matrix onto a birkhoff polytope, i.e., a set of doubly stochastic matrices. to guarantee a high efficiency of our procedure, the semismooth newton method considering solving a dual of a projection problem was proposed and efficiently implemented. extensive numerical experiments are presented to demonstrate a merits and effectiveness of our method by comparing its performance against other powerful solvers such as a commercial software gurobi and a academic code pproj [{\sc hager and zhang}, siam journal on optimization, 26 (2016), pp.~1773--1798]. inside particular, our algorithm was able to solve a projection problem with over one billion variables and nonnegative constraints to the very high accuracy inside less than 15 minutes on the modest desktop computer. more importantly, based on our efficient computation of a projections and their generalized jacobians, we should design the highly efficient augmented lagrangian method (alm) considering solving the class of convex quadratic programming (qp) problems constrained by a birkhoff polytope. a resulted alm was demonstrated to be much more efficient than gurobi inside solving the collection of qp problems arising from a relaxation of quadratic assignment problems.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
17330,"determining wavelength-dependent exoplanet radii measurements was an excellent way to probe a composition of exoplanet atmospheres. inside light of this, borsa et al. (2016) sought to develop the technique to obtain such measurements by comparing ground-based transmission spectra to a expected brightness variations during an exoplanet transit. however, we demonstrate herein that this was not possible due to a transit light curve normalisation necessary to remove a effects of a earth's atmosphere on a ground-based observations. this was because a recoverable exoplanet radius was set by a planet-to-star radius ratio within a transit light curve; we demonstrate this both analytically and with simulated planet transits, as well as through the reanalysis of a hd 189733b data.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
19148,"experience inside exploring our own solar system has shown that direct investigation of planetary bodies with the help of space probes invariably yields scientific knowledge not otherwise obtainable. inside a case of exoplanets, such direct investigation may be required to confirm inferences made by astronomical observations, especially with regard to planetary interiors, surface processes, geological evolution, and possible biology. this will necessitate transporting sophisticated scientific instruments across interstellar space, and some proposed methods considering achieving this with flight-times measured inside decades are reviewed. it was concluded that, with a possible exception of very lightweight (and thus scientifically limited) probes accelerated to velocities of ~0.1c with powerful earth-based lasers, achieving such the capability may have to wait until a development of the space-based civilization capable of leveraging a material and energy resources of a solar system.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
4800,"influence maximization (im), which selects the set of $k$ users (called seeds) to maximize a influence spread over the social network, was the fundamental problem inside the wide range of applications such as viral marketing and network monitoring. existing im solutions fail to consider a highly dynamic nature of social influence, which results inside either poor seed qualities or long processing time when a network evolves. to address this problem, we define the novel im query named stream influence maximization (sim) on social streams. technically, sim adopts a sliding window model and maintains the set of $k$ seeds with a largest influence value over a most recent social actions. next, we propose a influential checkpoints (ic) framework to facilitate continuous sim query processing. a ic framework creates the checkpoint considering each window slide and ensures an $\varepsilon$-approximate solution. to improve its efficiency, we further devise the sparse influential checkpoints (sic) framework which selectively keeps $o(\frac{\log{n}}{\beta})$ checkpoints considering the sliding window of size $n$ and maintains an $\frac{\varepsilon(1-\beta)}{2}$-approximate solution. experimental results on both real-world and synthetic datasets confirm a effectiveness and efficiency of our proposed frameworks against a state-of-the-art im approaches.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
15347,"after a first direct detection of gravitational waves (gw), detection of stochastic background of gws was an important next step, and a first gw event suggests that it was within a reach of a second-generation ground-based gw detectors. such the gw signal was typically tiny, and should be detected by cross-correlating a data from two spatially separated detectors if a detector noise was uncorrelated. it has been advocated, however, that a global magnetic fields inside a earth-ionosphere cavity produce a environmental disturbances at low-frequency bands, known as schumann resonances, which potentially couple with gw detectors. inside this paper, we present the simple analytical model to approximate its impact on a detection of stochastic gws. a model crucially depends on a geometry of a detector pair through a directional coupling, and we investigate a basic properties of a correlated magnetic noise based on a analytic expressions. a model reproduces a major trend of a recently measured global correlation between a gw detectors using magnetometer. a estimated values of a impact of correlated noise also match those obtained from a measurement. finally, we give an implication to a detection of stochastic gws including upcoming detectors, kagra and ligo india. a model suggests that ligo hanford-virgo and virgo-kagra pairs are possibly less sensitive to a correlated noise, and should achieve the better sensitivity to a stochastic gw signal inside a most pessimistic case.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
6478,training the deep convolutional neural net typically starts with the random initialisation of all filters inside all layers which severely reduces a forward signal and back-propagated error and leads to slow and sub-optimal training. techniques that counter that focus on either increasing a signal or increasing a gradients adaptively but a model behaves very differently at a beginning of training compared to later when stable pathways through a net have been established. to compound this problem a effective minibatch size varies greatly between layers at different depths and between individual filters as activation sparsity typically increases with depth leading to the reduction inside effective learning rate since gradients may superpose rather than add and this further compounds a covariate shift problem as deeper neurons are less able to adapt to upstream shift. proposed here was the method of automatic gain control of a signal built into each convolutional neuron that achieves equivalent or superior performance than batch normalisation and was compatible with single sample or minibatch gradient descent. a same model was used both considering training and inference. a technique comprises the scaled per sample map mean subtraction from a raw convolutional filter output followed by scaling of a difference.,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15575,"reference was the crucial property of language that allows us to connect linguistic expressions to a world. modeling it requires handling both continuous and discrete aspects of meaning. data-driven models excel at a former, but struggle with a latter, and a reverse was true considering symbolic models. this paper (a) introduces the concrete referential task to test both aspects, called cross-modal entity tracking; (b) proposes the neural network architecture that uses external memory to build an entity library inspired inside a drss of drt, with the mechanism to dynamically introduce new referents or add information to referents that are already inside a library. our model shows promise: it beats traditional neural network architectures on a task. however, it was still outperformed by memory networks, another model with external memory.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8560,"we consider scattering of electromagnetic waves from the distant point source by a gravitational field of a sun, taking a field oblateness due to a quadrupole moment of a sun into account. effects of a field oblateness should play an important role inside a high resolution solar gravitational lens imaging inside a sub-micrometer wavelength range of a electromagnetic spectrum.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
19823,"we present results on a dust attenuation of galaxies at redshift $\sim3-6$ by studying a relationship between a uv spectral slope ($\beta_{\rm uv}$) and a infrared excess (irx; $l_{\rm ir}$/$l_{\rm uv}$) with the help of alma far-infrared continuum observations. our study was based on the sample of 67 massive, star-forming galaxies with the median mass of $m_{\ast}\sim 10^{10.7}\,m_{\rm \odot}$ spanning the redshift range $z=2.6-3.7$ (median $z=3.2$) that were observed with alma at $\lambda_{rest}=300\,{\rm \mu m}$. both a individual alma detections (41 sources) and stacks including all galaxies show a irx-$\beta_{\rm uv}$ relationship at $z\sim3$ was mostly consistent with that of local starburst galaxies on average. however, we find evidence considering the large dispersion around a mean relationship by up to $\pm0.5$ dex. nevertheless, a locally calibrated dust correction factors based on a irx-$\beta_{\rm uv}$ relation are on average applicable to main-sequence $z\sim3$ galaxies. this does not appear to be a case at even higher redshifts, however. with the help of public alma observations of $z\sim4-6$ galaxies we find evidence considering the significant evolution inside a irx-$\beta_{\rm uv}$ and a irx-$m_{\ast}$ relations beyond $z\sim3$ toward lower irx values. we discuss several caveats that could affect these results, including a assumed dust temperature. alma observations of larger $z>3$ galaxy samples will be required to confirm this intriguing redshift evolution.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13621,"we discuss a approximation of a interferometric visibility (fringe contrast) considering a exozodi survey conducted at a chara array with a jouflu beam combiner. we investigate a use of a statistical median to approximate a uncalibrated visibility from an ensemble of fringe exposures. under the broad range of operating conditions, numerical simulations indicate that this estimator has the smaller bias compared to other estimators. we also propose an improved method considering calibrating visibilities, which not only takes into account a time-interval between observations of calibrators and science targets, but also a uncertainties of a calibrators' raw visibilities. we test our methods with data corresponding to stars that do not display a exozodi phenomenon. a results of our tests show that a proposed method yields smaller biases and errors. a relative reduction inside bias and error was generally modest, but should be as high as $\sim 20-40\%$ considering a brightest stars of a chara data, and statistically significant at a $95\%$ confidence level (cl).",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
2744,"transient stability simulation of the large-scale and interconnected electric power system involves solving the large set of differential algebraic equations (daes) at every simulation time-step. with a ever-growing size and complexity of power grids, dynamic simulation becomes more time-consuming and computationally difficult with the help of conventional sequential simulation techniques. to cope with this challenge, this paper aims to develop the fully distributed idea behind the method intended considering implementation on high performance computer (hpc) clusters. the novel, relaxation-based domain decomposition algorithm known as parallel-general-norton with multiple-port equivalent (pgnme) was proposed as a core technique of the two-stage decomposition idea behind the method to divide a overall dynamic simulation problem into the set of subproblems that should be solved concurrently to exploit parallelism and scalability. while a convergence property has traditionally been the concern considering relaxation-based decomposition, an approximation mechanism based on multiple-port network equivalent was adopted as a preconditioner to enhance a convergence of a proposed algorithm. a proposed algorithm was illustrated with the help of rigorous mathematics and validated both inside terms of speed-up and capability. moreover, the complexity analysis was performed to support a observation that pgnme scales well when a size of a subproblems are sufficiently large.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
15069,"large scale, dynamical simulations have been performed considering a two dimensional octahedron model, describing a kardar-parisi-zhang (kpz) considering nonlinear, or a edwards-wilkinson (ew) class considering linear surface growth. a autocorrelation functions of a heights and a dimer lattice gas variables are determined with high precision. parallel random-sequential (rs) and two-sub-lattice stochastic dynamics (sca) have been compared. a latter causes the constant correlation inside a long time limit, but after subtracting it one should find a same height functions as inside case of rs. on a other hand a ordered update alters a dynamics of a lattice gas variables, by increasing (decreasing) a memory effects considering nonlinear (linear) models with respect to random-sequential. additionally, we support a kpz ansatz and a kallabis-krug conjecture inside $2+1$ dimensions and provide the precise growth exponent value $\beta=0.2414(2)$. we show a emergence of finite size corrections, which occur long before a steady state roughness was reached.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
2400,"the cm-order was the reduced order equipped with an involution that mimics complex conjugation. a witt-picard group of such an order was the certain group of ideal classes that was closely related to a ""minus part"" of a class group. we present the deterministic polynomial-time algorithm considering a following problem, which may be viewed as the special case of a principal ideal testing problem: given the cm-order, decide whether two given elements of its witt-picard group are equal. inside order to prevent coefficient blow-up, a algorithm operates with lattices rather than with ideals. an important ingredient was the technique introduced by gentry and szydlo inside the cryptographic context. our application of it to lattices over cm-orders hinges upon the novel existence theorem considering auxiliary ideals, which we deduce from the result of konyagin and pomerance inside elementary number theory.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
10785,"a kashiwara crystal $b(\infty)$ parametrizes the basis considering a verma module of the kac-moody algebra. it has the deep combinatorial structure which one seeks to understand. considering each sequence $j$ of reduced decompositions of elements of a weyl group $w$, it has the realization as the subset $b_j(\infty)$ of the crystal $b_j$ which as the set was just $j$ copies of a natural numbers. a goal was to determine $b_j(\infty)$ and inside particular to show that it was the polyhedral subset of $b_j$. inside earlier work this led to a notion of an $s$-graph associated to the given simple root $\alpha$. here a notion of the giant $s$-graph depending on the fixed simple root was introduced. it was essentially the union of $s$-graphs considering each simple root with one distinguished vertex depending on $\alpha$. its vertices, which forms the giant $s$-set, determine the set of dual kashiwara functions. these are linear functions on $b_j$, whose common maximum determines a dual kashiwara parameter with respect to $\alpha$. from these parameters one may compute $b_j(\infty)$ as an explicit polyhedral subset of $b_j$. considering $w$ finite, berenstein and zelevinsky had studied this problem by introducing a notion of the trail inside the fundamental module. a functions they define may also be viewed as the set of dual kashiwara functions. a goal was to relate these two approaches and without restriction on $w$. it was shown under a hypothesis that no ""false"" trails exist, that a set of trails determines a ""$z$-convex envelope"" of the giant $s$-set. a proof involves a study of identities inside demazure modules.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
8676,"we study offline data poisoning attacks inside contextual bandits, the class of reinforcement learning problems with important applications inside online recommendation and adaptive medical treatment, among others. we provide the general attack framework based on convex optimization and show that by slightly manipulating rewards inside a data, an attacker should force a bandit algorithm to pull the target arm considering the target contextual vector. a target arm and target contextual vector are both chosen by a attacker. that is, a attacker should hijack a behavior of the contextual bandit. we also investigate a feasibility and a side effects of such attacks, and identify future directions considering defense. experiments on both synthetic and real-world data demonstrate a efficiency of a attack algorithm.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11977,barkhausen current noise was used to probe a slow field-driven conversion of a glassy relaxor ferroelectric state to an ordered ferroelectric (fe) state. a frequent presence of distinct micron-scale barkhausen events well before a polarization current starts to speed up shows that a process was not the conventional nucleation-limited one. a prevalence of reverse switching events near a onset of a rapid part of a transition strongly indicates that electric dipole interactions play the key role. a combination of barkhausen noise changes and changes inside a complex dielectric response indicate that a process consists of an initial mixed-alignment domain formation stage followed by growth of a domains aligned with a applied field.,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
7481,"chiral superconductors support chiral edge modes and potentially spontaneous edge currents at their boundaries. motivated by a putative multiband chiral p-wave superconductor sr$_2$ruo$_4$, we study a influence of a interference between different bands at a edges, which may appear inside a presence of moderate edge disorder or inside edge tunneling measurements. we show that interband interference should strongly modify a measurable quantities at a edges when a order parameter exhibits phase difference between a bands. this was illustrated by investigating a edge dispersion and a edge current distribution inside a presence of interband mixing, as well as a conductance at the tunneling junction. a results are discussed inside connection with a putative chiral p-wave superconductor sr$_2$ruo$_4$. inside passing, we also discuss similar interference effects inside multiband models with other pairing symmetries.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
5980,"inside fluid dynamical simulations inside astrophysics, large deformations are common and surface tracking was sometimes necessary. smoothed particle hydrodynamics (sph) method has been used inside many of such simulations. recently, however, it has been shown that sph cannot handle contact discontinuities or free surfaces accurately. there are several reasons considering this problem. a first one was that sph requires that a density was continuous and differentiable. a second one was that sph does not have a consistency, and thus a accuracy was zeroth order inside space. inside addition, we cannot express accurate boundary conditions with sph. inside this paper, we propose the novel, high-order scheme considering particle-based hydrodynamics of compress- ible fluid. our method was based on kernel-weighted high-order fitting polynomial considering intensive variables. with this approach, we should construct the scheme which solves all of a three prob- lems described above. considering shock capturing, we use the tensor form of von-neumann-richtmyer artificial viscosity. we have applied our method to many test problems and obtained excel- lent result. our method was not conservative, since particles do not have mass or energy, but only their densities. however, because of a lagrangian nature of our scheme, a violation of a conservation laws turned out to be small. we name this method consistent particle hydrodynamics inside strong form (cphsf).",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
13850,"inside this paper we analyze a practical implications of szemer√©di's regularity lemma inside a preservation of metric information contained inside large graphs. to this end, we present the heuristic algorithm to find regular partitions. our experiments show that this method was quite robust to a natural sparsification of proximity graphs. inside addition, this robustness should be enforced by graph densification.",1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3796,"we analyse finite-time singularities of a teichm√ºller harmonic map flow -- the natural gradient flow of a harmonic map energy -- and find the canonical way of flowing beyond them inside order to construct global solutions inside full generality. moreover, we prove the no-loss-of-topology result at finite time, which completes a proof that this flow decomposes an arbitrary map into the collection of branched minimal immersions connected by curves.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
397,"a number of discovered tev sources populating a extragalactic sky inside 2017 was nearly 70, mostly blazars located up to the redshift ~1. ten years ago, inside 2007, less than 20 tev emitters were known, up to the maximum redshift of 0.2. this was the major achievement of current generation of cherenkov telescopes operating inside synergy with optical, x-ray, and gev gamma-ray telescopes. the review of selected results from a extragalactic tev sky was presented, with particular emphasis on recently detected distant sources.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
6353,"we present c and o abundances inside a magellanic clouds derived from deep spectra of hii regions. a data have been taken with a ultraviolet-visual echelle spectrograph at a 8.2-m vlt. a sample comprises 5 hii regions inside a large magellanic cloud (lmc) and 4 inside a small magellanic cloud (smc). we measure pure recombination lines (rls) of cii and oii inside all a objects, permitting to derive a abundance discrepancy factors (adfs) considering o^2+, as well as their o/h, c/h and c/o ratios. we compare a adfs with those of other hii regions inside different galaxies. a results suggest the possible metallicity dependence of a adf considering a low-metallicity objects, but more uncertain considering high-metallicity objects. we compare nebular and b-type stellar abundances and we find that a stellar abundances agree better with a nebular ones derived from collisionally excited lines (cels). comparing these results with other galaxies we observe that stellar abundances seem to agree better with a nebular ones derived from cels inside low-metallicity environments and from rls inside high-metallicity environments. a c/h, o/h and c/o ratios show almost flat radial gradients, inside contrast with a spiral galaxies where such gradients are negative. we explore a chemical evolution analysing c/o vs. o/h and comparing with a results of hii regions inside other galaxies. a lmc seems to show the similar chemical evolution to a external zones of small spiral galaxies and a smc behaves as the typical star-forming dwarf galaxy.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2086,"phylogenetic models have polynomial parametrization maps. considering symmetric group-based models, matsen studied a polynomial inequalities that characterize a joint probabilities inside a image of these parametrizations. we employ this description considering maximum likelihood approximation using numerical algebraic geometry. inside particular, we explore an example where a maximum likelihood approximate does not exist, which would be difficult to discover without with the help of algebraic methods. we also study a embedding problem considering symmetric group-based models, i.e. we identify which mutation matrices are matrix exponentials of rate matrices that are invariant under the group action.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
5287,"we study bivariate stochastic recurrence equations (sres) motivated by applications to garch(1,1) processes. if coefficient matrices of sres have strictly positive entries, then a kesten result applies and it gives solutions with regularly varying tails. moreover, a tail indices are a same considering all coordinates. however, considering applications, this framework was too restrictive. we study sres when coefficients are triangular matrices and prove that a coordinates of a solution may exhibit regularly varying tails with different indices. we also specify each tail index together with its constant. a results are used to characterize regular variations of bivariate stationary garch(1,1) processes.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
2350,"we consider the theory of the two-component dirac fermion localized on the (2+1) dimensional brane coupled to the (3+1) dimensional bulk. with the help of a fermionic particle-vortex duality, we show that a theory has the strong-weak duality that maps a coupling $e$ to $\tilde e=(8\pi)/e$. we explore a theory at $e^2=8\pi$ where it was self-dual. a electrical conductivity of a theory was the constant independent of frequency. when a system was at finite density and magnetic field at filling factor $\nu=\frac12$, a longitudinal and hall conductivity satisfies the semicircle law, and a ratio of a longitudinal and hall thermal electric coefficients was completely determined by a hall angle. a thermal hall conductivity was directly related to a thermal electric coefficients.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
11949,"inside settings where only unlabelled speech data was available, zero-resource speech technology needs to be developed without transcriptions, pronunciation dictionaries, or language modelling text. there are two central problems inside zero-resource speech processing: (i) finding frame-level feature representations which make it easier to discriminate between linguistic units (phones or words), and (ii) segmenting and clustering unlabelled speech into meaningful units. inside this thesis, we argue that the combination of top-down and bottom-up modelling was advantageous inside tackling these two problems. to address a problem of frame-level representation learning, we present a correspondence autoencoder (cae), the neural network trained with weak top-down supervision from an unsupervised term discovery system. by combining this top-down supervision with unsupervised bottom-up initialization, a cae yields much more discriminative features than previous approaches. we then present our unsupervised segmental bayesian model that segments and clusters unlabelled speech into hypothesized words. by imposing the consistent top-down segmentation while also with the help of bottom-up knowledge from detected syllable boundaries, our system outperforms several others on multi-speaker conversational english and xitsonga speech data. finally, we show that a clusters discovered by a segmental bayesian model should be made less speaker- and gender-specific by with the help of features from a cae instead of traditional acoustic features. inside summary, a different models and systems presented inside this thesis show that both top-down and bottom-up modelling should improve representation learning, segmentation and clustering of unlabelled speech data.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18795,"we give the simple, fast algorithm considering hyperparameter optimization inspired by techniques from a analysis of boolean functions. we focus on a high-dimensional regime where a canonical example was training the neural network with the large number of hyperparameters. a algorithm --- an iterative application of compressed sensing techniques considering orthogonal polynomials --- requires only uniform sampling of a hyperparameters and was thus easily parallelizable. experiments considering training deep neural networks on cifar-10 show that compared to state-of-the-art tools (e.g., hyperband and spearmint), our algorithm finds significantly improved solutions, inside some cases better than what was attainable by hand-tuning. inside terms of overall running time (i.e., time required to sample various settings of hyperparameters plus additional computation time), we are at least an order of magnitude faster than hyperband and bayesian optimization. we also outperform random search 8x. additionally, our method comes with provable guarantees and yields a first improvements on a sample complexity of learning decision trees inside over two decades. inside particular, we obtain a first quasi-polynomial time algorithm considering learning noisy decision trees with polynomial sample complexity.",1,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0
411,"adaptive regularization methods come inside diagonal and full-matrix variants. however, only a former have enjoyed widespread adoption inside training large-scale deep models. this was due to a computational overhead of manipulating the full matrix inside high dimension. inside this paper, we show how to make full-matrix adaptive regularization practical and useful. we present ggt, the truly scalable full-matrix adaptive optimizer. at a heart of our algorithm was an efficient method considering computing a inverse square root of the low-rank matrix. we show that ggt converges to first-order local minima, providing a first rigorous theoretical analysis of adaptive regularization inside non-convex optimization. inside preliminary experiments, ggt trains faster across the variety of synthetic tasks and standard deep learning benchmarks.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19725,"prediction suffix trees (pst) provide an effective tool considering sequence modelling and prediction. current prediction techniques considering psts rely on exact matching between a suffix of a current sequence and a previously observed sequence. we present the provably correct algorithm considering learning the pst with approximate suffix matching by relaxing a exact matching condition. we then present the self-bounded enhancement of our algorithm where a depth of suffix tree grows automatically inside response to a model performance on the training sequence. through experiments on synthetic datasets as well as three real-world datasets, we show that a approximate matching pst results inside better predictive performance than a other variants of pst.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15712,"recent learning-based super-resolution (sr) methods often focus on dictionary learning or network training. inside this paper, we discuss inside detail the new sr method based on local patch encoding (lpe) instead of traditional dictionary learning. a proposed method consists of the learning stage and the reconstructing stage. inside a learning stage, image patches are classified into different classes by means of a proposed lpe, and then the projection matrix was computed considering each class by utilizing the simple constraint. inside a reconstructing stage, an input lr patch should be simply reconstructed by computing its lpe code and then multiplying a corresponding projection matrix. furthermore, we discuss a relationship between a proposed method and a anchored neighborhood regression methods; we also analyze a extendibility of a proposed method. a experimental results on several image sets demonstrate a effectiveness of a lpe-based methods.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19857,"inside this paper, we introduce the novel community detection algorithm inside graphs, called scoda (streaming community detection algorithm), based on an edge streaming setting. this algorithm has an extremely low memory footprint and the lightning-fast execution time as it only stores two integers per node and processes each edge strictly once. a idea behind the method was based on a following simple observation: if we pick an edge uniformly at random inside a network, this edge was more likely to connect two nodes of a same community than two nodes of distinct communities. we exploit this idea to build communities by local changes at each edge arrival. with the help of theoretical arguments, we relate a ability of scoda to detect communities to usual quality metrics of these communities like a conductance. experimental results performed on massive real-life networks ranging from one million to more than one billion edges shows that scoda runs more than ten times faster than existing algorithms and leads to similar or better detection scores on a largest graphs.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
3358,"drug-target interaction (dti) prediction plays the very important role inside drug development and drug discovery. biochemical experiments or \textit{in vitro} methods are very expensive, laborious and time-consuming. therefore, \textit{in silico} approaches including docking simulation and machine learning have been proposed to solve this problem. inside particular, machine learning approaches have attracted increasing attentions recently. however, inside addition to a known drug-target interactions, most of a machine learning methods require extra characteristic information such as chemical structures, genome sequences, binding types and so on. whenever such information was not available, they may perform poor. very recently, a similarity-based link prediction methods were extended to bipartite networks, which should be applied to solve a dti prediction problem by with the help of topological information only. inside this work, we propose the method based on low-rank matrix projection to solve a dti prediction problem. on one hand, when there was no extra characteristic information of drugs or targets, a proposed method utilizes only a known interactions. on a other hand, a proposed method should also utilize a extra characteristic information when it was available and a performances will be remarkably improved. moreover, a proposed method should predict a interactions associated with new drugs or targets of which we know nothing about their associated interactions, but only some characteristic information. we compare a proposed method with ten baseline methods, e.g., six similarity-based methods that utilize only a known interactions and four methods that utilize a extra characteristic information. a datasets and codes implementing a simulations are available at this https url.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6100,"visual question answering (vqa) requires ai models to comprehend data inside two domains, vision and text. current state-of-the-art models use learned attention mechanisms to extract relevant information from a input domains to answer the certain question. thus, robust attention mechanisms are essential considering powerful vqa models. inside this paper, we propose the recurrent attention mechanism and show its benefits compared to a traditional convolutional approach. we introduce the baseline vqa model with visual attention and compare a performance difference between convolutional and recurrent attention on a vqa 2.0 dataset. furthermore, we experiment replacing attention mechanisms inside state-of-the-art models with our recurrent attention units (raus) and show increased performance. additionally, we design an architecture considering vqa which utilizes recurrent attention units to highlight a benefits of raus. our single model outperforms a first place winner on a vqa 2016 challenge and to a best of our knowledge, it was a second best performing single model on a vqa 1.0 dataset. furthermore, our model noticeably improves upon a winner of a vqa 2017 challenge.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3153,"this review paper summarizes a current state-of-art and challenges considering a future developments of fiber-reinforced composites considering structural applications with multifunctional capabilities. after the brief analysis of a reasons of a successful incorporation of fiber-reinforced composites inside many different industrial sectors, a review analyzes three critical factors that will define a future of composites. a first one was a application of novel fiber-deposition and preforming techniques together with innovative liquid moulding strategies, which will be combined by optimization tools based on novel multiscale modelling approaches, so fiber-reinforced composites with optimized properties should be designed and manufactured considering each application. inside addition, composite applications will be enhanced by a incorporation of multifunctional capabilities. among them, electrical conductivity, energy storage (structural supercapacitors and batteries) and energy harvesting (piezoelectric and solar energy) seem to be a most promising ones.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
9110,"we consider the general regularised interpolation problem considering learning the parameter vector from data. a well known representer theorem says that under certain conditions on a regulariser there exists the solution inside a linear span of a data points. this was a core of kernel methods inside machine learning as it makes a problem computationally tractable. necessary and sufficient conditions considering differentiable regularisers on hilbert spaces to admit the representer theorem have been proved. we extend those results to nondifferentiable regularisers on uniformly convex and uniformly smooth banach spaces. this gives the (more) complete answer to a question when there was the representer theorem. we then note that considering regularised interpolation inside fact a solution was determined by a function space alone and independent of a regulariser, making a extension to banach spaces even more valuable.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
9918,"traditional linear methods considering forecasting multivariate time series are not able to satisfactorily model a non-linear dependencies that may exist inside non-gaussian series. we build on a theory of learning vector-valued functions inside a reproducing kernel hilbert space and develop the method considering learning prediction functions that accommodate such non-linearities. a method not only learns a predictive function but also a matrix-valued kernel underlying a function search space directly from a data. our idea behind the method was based on learning multiple matrix-valued kernels, each of those composed of the set of input kernels and the set of output kernels learned inside a cone of positive semi-definite matrices. inside addition to superior predictive performance inside a presence of strong non-linearities, our method also recovers a hidden dynamic relationships between a series and thus was the new alternative to existing graphical granger techniques.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
13819,"we construct triply periodic zero mean curvature surfaces of mixed type inside a lorentz-minkowski 3-space, with a same topology as a triply periodic minimal surfaces inside a euclidean 3-space, called schwarz rpd surfaces.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6361,"wall-bounded flows experience the transition to turbulence characterized by a coexistence of laminar and turbulent domains inside some range of reynolds number r, a natural control parameter. this transitional regime takes place between an upper threshold rt above which turbulence was uniform (featureless) and the lower threshold rg below which any form of turbulence decays, possibly at a end of overlong chaotic transients. a most emblematic cases of flow along flat plates transiting to/from turbulence according to this scenario are reviewed. a coexistence was generally inside a form of bands, alternatively laminar and turbulent, and oriented obliquely with respect to a general flow direction. a final decay of a bands at rg points to a relevance of directed percolation and criticality inside a sense of statistical-physics phase transitions. a nature of a transition at rt where bands form was still somewhat mysterious and does not easily fit a scheme holding considering pattern-forming instabilities at increasing control parameter on the laminar background. inside contrast, a bands arise at rt out of the uniform turbulent background at the decreasing control parameter. ingredients of the possible theory of laminar-turbulent patterning are discussed.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18983,"we present the three-dimensional ising model where lines of equal spins are frozen inside such that they form an ordered framework structure. a frame spins impose an external field on a rest of a spins (active spins). we demonstrate that this ""porous ising model"" should be seen as the minimal model considering condensation transitions of gas molecules inside metal-organic frameworks. with the help of monte carlo simulation techniques, we compare a phase behavior of the porous ising model with that of the particle-based model considering a condensation of methane (ch$_4$) inside a isoreticular metal-organic framework irmof-16. considering both models, we find the line of first-order phase transitions that end inside the critical point. we show that a critical behavior inside both cases belongs to a 3d ising universality class, inside contrast to other phase transitions inside confinement such as capillary condensation.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
466,"we consider a problem of sampling from posterior distributions considering bayesian models where some parameters are restricted to be orthogonal matrices. such matrices are sometimes used inside neural networks models considering reasons of regularization and stabilization of training procedures, and also should parameterize matrices of bounded rank, positive-definite matrices and others. inside \citet{byrne2013geodesic} authors have already considered sampling from distributions over manifolds with the help of exact geodesic flows inside the scheme similar to hamiltonian monte carlo (hmc). we propose new sampling scheme considering the set of orthogonal matrices that was based on a same approach, uses ideas of riemannian optimization and does not require exact computation of geodesic flows. a method was theoretically justified by proof of symplecticity considering a proposed iteration. inside experiments we show that a new scheme was comparable or faster inside time per iteration and more sample-efficient comparing to conventional hmc with explicit orthogonal parameterization and geodesic monte-carlo. we also provide promising results of bayesian ensembling considering orthogonal neural networks and low-rank matrix factorization.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
13460,"pathfinding was the very popular area inside computer game development. while two-dimensional (2d) pathfinding was widely applied inside most of a popular game engines, little implementation of real three-dimensional (3d) pathfinding should be found. this research presents the dynamic search space optimization algorithm which should be applied to tessellate 3d search space unevenly, significantly reducing a total number of resulting nodes. a algorithm should be used with popular pathfinding algorithms inside 3d game engines. furthermore, the simplified standalone 3d pathfinding algorithm was proposed inside this paper. a proposed algorithm relies on ray-casting or line vision to generate the feasible path during runtime without requiring division of a search space into the 3d grid. both of a proposed algorithms are simulated on unreal engine to show innerworkings and resultant path comparison with a*. a advantages and shortcomings of a proposed algorithms are also discussed along with future directions.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2149,"we propose the weakly-supervised idea behind the method that takes image-sentence pairs as input and learns to visually ground (i.e., localize) arbitrary linguistic phrases, inside a form of spatial attention masks. specifically, a model was trained with images and their associated image-level captions, without any explicit region-to-phrase correspondence annotations. to this end, we introduce an end-to-end model which learns visual groundings of phrases with two types of carefully designed loss functions. inside addition to a standard discriminative loss, which enforces that attended image regions and phrases are consistently encoded, we propose the novel structural loss which makes use of a parse tree structures induced by a sentences. inside particular, we ensure complementarity among a attention masks that correspond to sibling noun phrases, and compositionality of attention masks among a children and parent phrases, as defined by a sentence parse tree. we validate a effectiveness of our idea behind the method on a microsoft coco and visual genome datasets.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1828,"this manuscript attempts to present the way inside which a classical construction of a dirac operator should be carried over to a setting of diffeology. the more specific aim was to describe the procedure considering gluing together two usual dirac operators and to explain inside what sense a result was again the dirac operator. since versions of cut-and-paste (surgery) operations have already appeared inside a context of atiyah-singer theory, we specify that our gluing procedure was designed to lead to spaces that are not smooth manifolds inside any ordinary sense, and since much attention has been paid inside recent years to dirac operators on spaces with singularities, we also specify that our idea behind the method was more of the piecewise-linear nature (although, hopefully, singular spaces inside the more analytic sense will enter a picture sooner or later; but this work was not yet about them). most of it was devoted to a diffeological versions of a components that go into a standard definition of the dirac operator as a composition of the clifford connection with clifford action by sections of a cotangent bundle; the diffeological dirac operator was then standardly defined.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5224,"a vanishing gradient problem is the major obstacle considering a success of deep learning. inside recent years it is gradually alleviated through multiple different techniques. however a problem is not really overcome inside the fundamental way, since it was inherent to neural networks with activation functions based on dot products. inside the series of papers, we are going to analyze alternative neural network structures which are not based on dot products. inside this first paper, we revisit neural networks built up of layers based on distance measures and gaussian activation functions. these kinds of networks were only sparsely used inside a past since they are hard to train when with the help of plain stochastic gradient descent methods. we show that by with the help of root mean square propagation (rmsprop) it was possible to efficiently learn multi-layer neural networks. furthermore we show that when appropriately initialized these kinds of neural networks suffer much less from a vanishing and exploding gradient problem than traditional neural networks even considering deep networks.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15278,"this work presents the study on a extraction and analysis of the set of 101 categories of eye movement features from three types of eye movement events: fixations, saccades, and post-saccadic oscillations. a eye movements were recorded during the reading task. considering a categories of features with multiple instances inside the recording we extract corresponding feature subtypes by calculating descriptive statistics on a distributions of these instances. the unified framework of detailed descriptions and mathematical formulas are provided considering a extraction of a feature set. a analysis of feature values was performed with the help of the large database of eye movement recordings from the normative population of 298 subjects. we demonstrate a central tendency and overall variability of feature values over a experimental population, and more importantly, we quantify a test-retest reliability (repeatability) of each separate feature. a described methods and analysis should provide valuable tools inside fields exploring a eye movements, such as inside behavioral studies, attention and cognition research, medical research, biometric recognition, and human-computer interaction.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15324,"we consider riemannian 4-manifolds that gromov-hausdorff converge to the lower dimensional limit space, with a ricci tensor going to zero. among other things, we show that if a limit space was two dimensional then under some mild assumptions, a limiting four dimensional geometry away from a curvature blowup region was semiflat kaehler.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3008,"we consider a cauchy problem considering a nonlinear schr√∂dinger equation on a whole space. after introducing the weaker concept of finite speed of propagation, we show that a concatenation of initial data gives rise to solutions whose time of existence increases as one translates one of a initial data. moreover, we show that, given global decaying solutions with initial data $u_0, v_0$, if $|y|$ was large, then a concatenated initial data $u_0+v_0(\cdot -y)$ gives rise to globally decaying solutions.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10266,"we consider a inverse problems of determining a potential or a damping coefficient appearing inside a wave equation. we will prove a unique determination of these coefficients from a one point measurement. since our problem was under-determined, so some extra assumption on a coefficients was required to prove a uniqueness.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9452,"we use the numerical solution of a deterministic tdgl equations to determine a response induced by the probe field inside the material quenched into the superconducting state. we characterize differences inside response according to whether a probe was applied before, during, or after a phase stiffness has built up to its final steady state value.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
114,"foreign policy analysis has been struggling to find ways to measure policy preferences and paradigm shifts inside international political systems. this paper presents the novel, potential solution to this challenge, through a application of the neural word embedding (word2vec) model on the dataset featuring speeches by heads of state or government inside a united nations general debate. a paper provides three key contributions based on a output of a word2vec model. first, it presents the set of policy attention indices, synthesizing a semantic proximity of political speeches to specific policy themes. second, it introduces country-specific semantic centrality indices, based on topological analyses of countries' semantic positions with respect to each other. third, it tests a hypothesis that there exists the statistical relation between a semantic content of political speeches and un voting behavior, falsifying it and suggesting that political speeches contain information of different nature then a one behind voting outcomes. a paper concludes with the discussion of a practical use of its results and consequences considering foreign policy analysis, public accountability, and transparency.",1,0,0,1,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19906,"the large fraction of giant planets have gaseous envelopes that are limited to about 10 % of their total mass budget. such planets are present inside a solar system (uranus, neptune) and are frequently observed inside short periods around other stars (the so-called super-earths). inside contrast to these observations, theoretical calculations based on a evolution of hydrostatic envelopes argue that such low mass envelopes cannot be maintained around cores exceeding five earth masses. instead, under nominal disc conditions, these planets would acquire massive envelopes through runaway gas accretion within a lifetime of a protoplanetary disc. inside this work, we show that planetary envelopes are not inside hydrostatic balance, which slows down envelope growth. the series of 3-dimensional, global, radiative hydrodynamical simulations reveal the steady state gas flow, which enters through a poles and exits inside a disc midplane. gas was pushed through a outer envelope inside about 10 orbital timescales. inside regions of a disc that are not significantly dust-depleted, envelope accretion onto cores of about five earth masses should get stalled as a gas flow enters a deep interior. accreted solids sublimate deep inside a convective interior, but small opacity-providing grains are trapped inside a flow and do not settle, which further prevents rapid envelope accretion. a transition to runaway gas accretion should however be reached when cores grow larger than typical super-earths, beyond 15 earth masses, and preferably when disc opacities are below kappa=1 cm^2/g. these findings offer an explanation considering a typical low-mass envelopes around a cores of super-earths.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7648,"we extend teichmueller dynamics to the flow on a total space of the flat bundle of deformation spaces of representations of a fundamental group of the fixed surface s inside the lie group g. a resulting dynamical system was the continuous version of a action of a mapping class group of s on a deformation space. we observe how ergodic properties of this action relate to this flow. when g was compact, this flow was strongly mixing over each component of a derormation space and of each stratum of a teichmueller unit sphere bundle over a riemann moduli space. we prove ergodicity considering a analogous lift of a weil-petersson geodesic local. flow.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15523,"we prove the new off-diagonal asymptotic of a bergman kernels associated to tensor powers of the positive line bundle on the compact k√§hler manifold. we show that if a k√§hler potential was real analytic, then a bergman kernel accepts the complete asymptotic expansion inside the neighborhood of a diagonal of shrinking size $k^{-\frac14}$. these improve a earlier results inside a subject considering smooth potentials, where an expansion exists inside the $k^{-\frac12}$ neighborhood of a diagonal. we obtain our results by finding upper bounds of a form $c^m m!^{2}$ considering a bergman coefficients $b_m(x, \bar y)$, which was an interesting problem on its own. we find such upper bounds with the help of a method of berman-berndtsson-sj√∂strand. we also show that sharpening these upper bounds would improve a rate of shrinking neighborhoods of a diagonal $x=y$ inside our results. inside a special case of metrics with local constant holomorphic sectional curvatures, we obtain off-diagonal asymptotic inside the fixed (as $k \to \infty$) neighborhood of a diagonal, which recovers the result of berman [ber] (see remark 3.5 of [ber] considering higher dimensions). inside this case, we also find an explicit formula considering a bergman kernel mod $o(e^{-k \delta} )$.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
166,"we consider 3+1 rotationally symmetric lorentzian einstein spacetime manifolds with $\lambda >0$ and reduce a equations to 2+1 einstein equations coupled to `shifted' wave maps. subsequently, we prove various (explicit) positive mass-energy theorems. no smallness was assumed.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12278,"inside this review article, we show our recent results relating to a undoped (ce-free) superconductivity inside a electron-doped high-tc cuprates with a so-called t' structure. considering an introduction, we briefly mention a characteristics of a electron-doped t'-cuprates, including a reduction annealing, conventional phase diagram and undoped superconductivity. then, our transport and magnetic results and results relating to a superconducting pairing symmetry of a undoped and underdoped t'-cuprates are shown. collaborating spectroscopic and nuclear magnetic resonance results are also shown briefly. it has been found that, through a reduction annealing, the strongly localized state of carriers accompanied by an antiferromagnetic pseudogap inside a as-grown samples changes to the metallic and superconducting state with the short-range magnetic order inside a reduced superconducting samples. a formation of a short-range magnetic order due to the very small amount of excess oxygen inside a reduced superconducting samples suggests that a t'-cuprates exhibiting a undoped superconductivity inside a parent compounds are regarded as strongly correlated electron systems, as well as a hole-doped high-tc cuprates. we show our proposed electronic structure model to understand a undoped superconductivity. finally, unsolved future issues of a t'-cuprates are discussed.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
12461,"this paper presents the convolutional neural network based idea behind the method considering estimating a relative pose between two cameras. a proposed network takes rgb images from both cameras as input and directly produces a relative rotation and translation as output. a system was trained inside an end-to-end manner utilising transfer learning from the large scale classification dataset. a introduced idea behind the method was compared with widely used local feature based methods (surf, orb) and a results indicate the clear improvement over a baseline. inside addition, the variant of a proposed architecture containing the spatial pyramid pooling (spp) layer was evaluated and shown to further improve a performance.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13577,"no man was an island, as individuals interact and influence one another daily inside our society. when social influence takes place inside experiments on the population of interconnected individuals, a treatment on the unit may affect a outcomes of other units, the phenomenon known as interference. this thesis develops the causal framework and inference methodology considering experiments where interference takes place on the network of influence (i.e. network interference). inside this framework, a network potential outcomes serve as a key quantity and flexible building blocks considering causal estimands that represent the variety of primary, peer, and total treatment effects. these causal estimands are estimated using principled bayesian imputation of missing outcomes. a theory on a unconfoundedness assumptions leading to simplified imputation highlights a importance of including relevant network covariates inside a potential outcome model. additionally, experimental designs that result inside balanced covariates and sizes across treatment exposure groups further improve a causal estimate, especially by mitigating potential outcome model mis-specification. a true potential outcome model was not typically known inside real-world experiments, so a best practice was to account considering interference and confounding network covariates through both balanced designs and model-based imputation. the full factorial simulated experiment was formulated to demonstrate this principle by comparing performance across different randomization schemes during a design phase and estimators during a analysis phase, under varying network topology and true potential outcome models. overall, this thesis asserts that interference was not just the nuisance considering analysis but rather an opportunity considering quantifying and leveraging peer effects inside real-world experiments.",0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
3619,"inside recent years, deep learning based on artificial neural network (ann) has achieved great success inside pattern recognition. however, there was no clear understanding of such neural computational models. inside this paper, we try to unravel ""black-box"" structure of ann model from network flow. specifically, we consider a feed forward ann as the network flow model, which consists of many directional class-pathways. each class-pathway encodes one class. a class-pathway of the class was obtained by connecting a activated neural nodes inside each layer from input to output, where activation value of neural node (node-value) was defined by a weights of each layer inside the trained ann-classifier. from a perspective of a class-pathway, training an ann-classifier should be regarded as a formulation process of class-pathways of different classes. by analyzing a a distances of each two class-pathways inside the trained ann-classifiers, we try to answer a questions, why a classifier performs so? at last, from a neural encodes view, we define a importance of each neural node through a class-pathways, which was helpful to optimize a structure of the classifier. experiments considering two types of ann model including multi-layer mlp and cnn verify that a network flow based on class-pathway was the reasonable explanation considering ann models.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7144,"monte carlo method was the broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. they are often used inside physical and mathematical problems and are most useful when it was difficult or impossible to use other mathematical methods. basically, many statisticians have been increasingly drawn to monte carlo method inside three distinct problem classes: optimization, numerical integration, and generating draws from the probability distribution. inside this paper, we will introduce a monte carlo method considering calculating coefficients inside generalized linear model(glm), especially considering logistic regression. our main methods are metropolis hastings(mh) algorithms and stochastic approximation inside monte carlo computation(samc). considering comparison, we also get results automatically with the help of mle method inside r software.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4908,"a classification of mri images according to a anatomical field of view was the necessary task to solve when faced with a increasing quantity of medical images. inside parallel, advances inside deep learning makes it the suitable tool considering computer vision problems. with the help of the common architecture (such as alexnet) provides quite good results, but not sufficient considering clinical use. improving a model was not an easy task, due to a large number of hyper-parameters governing both a architecture and a training of a network, and to a limited understanding of their relevance. since an exhaustive search was not tractable, we propose to optimize a network first by random search, and then by an adaptive search based on gaussian processes and probability of improvement. applying this method on the large and varied mri dataset, we show the substantial improvement between a baseline network and a final one (up to 20\% considering a most difficult classes).",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17639,"platooning allows vehicles to travel with small intervehicle distance inside the coordinated fashion thanks to vehicle-to-vehicle connectivity. when applied at the larger scale, platooning will create significant opportunities considering energy savings due to reduced aerodynamic drag, as well as increased road capacity and congestion reduction resulting from shorter vehicle headways. however, these potential savings are maximized if platooning-capable vehicles spend most of their travel time within platoons. ad hoc platoon formation may not ensure the high rate of platoon driving. inside this paper we consider a problem of central coordination of platooning-capable vehicles. by coordinating their routes and departure times, we should maximize a fuel savings afforded by platooning vehicles. a resulting problem was the combinatorial optimization problem that considers a platoon coordination and vehicle routing problems simultaneously. we demonstrate our methodology by evaluating a benefits of the coordinated solution and comparing it with a uncoordinated case when platoons form only inside an ad hoc manner. we compare a coordinated and uncoordinated scenarios on the grid network with different assumptions about demand and a time vehicles are willing to wait.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
18301,"humans should understand and produce new utterances effortlessly, thanks to their compositional skills. once the person learns a meaning of the new verb ""dax,"" he or she should immediately understand a meaning of ""dax twice"" or ""sing and dax."" inside this paper, we introduce a scan domain, consisting of the set of simple compositional navigation commands paired with a corresponding action sequences. we then test a zero-shot generalization capabilities of the variety of recurrent neural networks (rnns) trained on scan with sequence-to-sequence methods. we find that rnns should make successful zero-shot generalizations when a differences between training and test commands are small, so that they should apply ""mix-and-match"" strategies to solve a task. however, when generalization requires systematic compositional skills (as inside a ""dax"" example above), rnns fail spectacularly. we conclude with the proof-of-concept experiment inside neural machine translation, suggesting that lack of systematicity might be partially responsible considering neural networks' notorious training data thirst.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2984,"filamentary regions of high vorticity irregularly form and disappear inside a turbulent flows of classical fluids. we report an experimental comparative study of these so-called "" coherent structures "" inside the classical versus quantum fluid, with the help of liquid helium with the superfluid fraction varied from 0% up to 83%. a low pressure core of a vorticity filaments was detected by pressure probes located on a sidewall of the 78-cm-diameter von k√°rm√°n cell driven up to record turbulent intensity (r $\lambda$ $\sim$ $\sqrt$ re 10000). a statistics of occurrence, magnitude and relative distribution of a filaments inside the classical fluid are found indistinguishable from their superfluid counterpart, namely a bundles of quantized vortex lines. this suggest that a internal structure of vortex filaments, as well as their dissipative properties have the negligible impact on their macroscopic dynamics, such as lifetime and intermittent properties.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2346,"deep generative models trained with large amounts of unlabelled data have proven to be powerful within a domain of unsupervised learning. many real life data sets contain the small amount of labelled data points, that are typically disregarded when training generative models. we propose a cluster-aware generative model, that uses unlabelled information to infer the latent representation that models a natural clustering of a data, and additional labelled data points to refine this clustering. a generative performances of a model significantly improve when labelled information was exploited, obtaining the log-likelihood of -79.38 nats on permutation invariant mnist, while also achieving competitive semi-supervised classification accuracies. a model should also be trained fully unsupervised, and still improve a log-likelihood performance with respect to related methods.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17470,"many of a fascinating and unconventional properties of several transition-metal compounds with partially filled d-shells are due to strong electronic correlations. while local correlations are inside principle treated exactly within a frame of a dynamical mean-field theory, there are two major and interlinked routes considering important further methodical advances: on a one hand, there was the strong need considering methods being able to describe material-specific aspects, i.e., methods combining a dmft with modern band-structure theory, and, on a other hand, nonlocal correlations beyond a mean-field paradigm must be accounted for. referring to several concrete example systems, we argue why these two routes are worth pursuing and how they should be combined, we describe several related methodical developments and present respective results, and we discuss possible ways to overcome remaining obstacles.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
19684,"recent work on a representation of functions on sets has considered a use of summation inside the latent space to enforce permutation invariance. inside particular, it has been conjectured that a dimension of this latent space may remain fixed as a cardinality of a sets under consideration increases. however, we demonstrate that a analysis leading to this conjecture requires mappings which are highly discontinuous and argue that this was only of limited practical use. motivated by this observation, we prove that an implementation of this model using continuous mappings (as provided by e.g. neural networks or gaussian processes) actually imposes the constraint on a dimensionality of a latent space. practical universal function representation considering set inputs should only be achieved with the latent dimension at least a size of a maximum number of input elements.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
18483,inside this paper we develop methods to extend a minimal hypersurface idea behind the method to positive scalar curvature problems to all dimensions. this includes the proof of a positive mass theorem inside all dimensions without the spin assumption. it also includes statements about a structure of compact manifolds of positive scalar curvature extending a work of \cite{sy1} to all dimensions. a technical work inside this paper was to construct minimal slicings and associated weight functions inside a presence of small singular sets and to show that a singular sets do not become too large inside a lower dimensional slices. it was shown that a singular set inside any slice was the closed set with hausdorff codimension at least three. inside particular considering arguments which involve slicing down to dimension $1$ or $2$ a method was successful. a arguments should be viewed as an extension of a minimal hypersurface regularity theory to this setting of minimal slicings.,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17419,"we report a effects of heavy-ion irradiation on fese single crystals by irradiating uranium up to the dose equivalent matching field of $b_\phi$ = 16 t. almost continuous columnar defects along a $c$-axis with the diameter $\sim$10 nm are confirmed by high-resolution transmission electron microscopy. $t_c$ was found to be suppressed by introducing columnar defects at the rate of d$t_c$/d$b_\phi$ $\sim$ -0.29kt$^{-1}$, which was much larger than those observed inside iron pnictides. this unexpected large suppression of $t_c$ inside fese was discussed inside relation to a large diameter of a columnar defects as well as its unique band structure with the remarkably small fermi energy. a critical current density was first dramatically enhanced with irradiation reaching the value over $\sim$2$\times$10$^5$ a/cm$^2$ ($\sim$5 times larger than that of a pristine sample) at 2 k (self-field) with $b_\phi$ = 2 t, then gradually suppressed with increasing $b_\phi$. a $\delta$$l$-pinning associated with charge-carrier mean free path fluctuations, and a $\delta$$t_c$-pinning associated with spatial fluctuations of a transition temperature are found to coexist inside a pristine fese, while a irradiation increases a contribution from $\delta$$l$-pinning, and makes it dominant over $b_\phi$ = 4 t.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
13534,"online-learning research has mainly been focusing on minimizing one objective function. inside many real-world applications, however, several objective functions have to be considered simultaneously. recently, an algorithm considering dealing with several objective functions inside a i.i.d. case has been presented. inside this paper, we extend a multi-objective framework to a case of stationary and ergodic processes, thus allowing dependencies among observations. we first identify an asymptomatic lower bound considering any prediction strategy and then present an algorithm whose predictions achieve a optimal solution while fulfilling any continuous and convex constraining criterion.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11019,"the new understanding of a stability of self-interacting dark matter was pointed out, based on a simplest spontaneously broken abelian $u(1)$ gauge model with one complex scalar and one dirac fermion. a key was a imposition of dark charge conjugation symmetry. it allows a possible existence of two stable particles: a dirac fermion and a vector gauge boson which acts as the light mediator considering a former's self-interaction. since this light mediator does not decay, it avoids a strong cosmological constraints recently obtained considering all such models where a light mediator decays into standard-model particles.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9,"we present mcda, the modification of a corot detrend algorithm (cda) suitable to detrend chromatic light curves. by means of robust statistics and better handling of short term variability, a implementation decreases a systematic light curve variations and improves a detection of exoplanets when compared with a original algorithm. all corot chromatic light curves (a total of 65,655) were analysed with our algorithm. dozens of new transit candidates and all previously known corot exoplanets were rediscovered inside those light curves with the help of the box-fitting algorithm. considering three of a new cases spectroscopic measurements of a candidates' host stars were retrieved from a eso science archive facility and used to calculate stellar parameters and, inside a best cases, radial velocities. inside addition to our improved detrend technique we announce a discovery of the planet that orbits the $0.79_{-0.09}^{+0.08}\,r_\odot$ star with the period of $6.71837\pm0.00001$ days and has $0.57_{-0.05}^{+0.06}\,r_{\rm j}$ and $0.15\pm0.10\,m_{\rm j}$. we also present a analysis of two cases inside which parameters found suggest a existence of possible planetary companions.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8923,"smart cities must integrate the number of interdependent cyber-physical systems that operate inside the coordinated manner to improve a well-being of a city's residents. the cyber-physical system (cps) was the system of computational elements controlling physical entities. large-scale cpss are more vulnerable to attacks due to a cyber-physical interdependencies that should lead to cascading failures which should have the significant detrimental effect on the city. inside this paper, the novel idea behind the method was proposed considering analyzing a problem of allocating security resources, such as firewalls and anti-malware, over a various cyber components of an interdependent cps to protect a system against imminent attacks. a problem was formulated as the colonel blotto game inside which a attacker seeks to allocate its resources to compromise a cps, while a defender chooses how to distribute its resources to defend against potential attacks. to evaluate a effects of defense and attack, various cps factors are considered including human-cps interactions as well as physical and topological characteristics of the cps such as flow and capacity of interconnections and minimum path algorithms. results show that, considering a case inside which a attacker was not aware of a cps interdependencies, a defender should have the higher payoff, compared to a case inside which a attacker has complete information. a results also show that, inside a case of more symmetric nodes, due to interdependencies, a defender achieves its highest payoff at a equilibrium compared to a case with independent, asymmetric nodes.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
19679,"this work explores a trade-off between a number of samples required to accurately build models of dynamical systems and a degradation of performance inside various control objectives due to the coarse approximation. inside particular, we show that simple models should be easily fit from input/output data and are sufficient considering achieving various control objectives. we derive bounds on a number of noisy input/output samples from the stable linear time-invariant system that are sufficient to guarantee that a corresponding finite impulse response approximation was close to a true system inside a $\mathcal{h}_\infty$-norm. we demonstrate that these demands are lower than those derived inside prior art which aimed to accurately identify dynamical models. we also explore how different physical input constraints, such as power constraints, affect a sample complexity. finally, we show how our analysis fits within a established framework of robust control, by demonstrating how the controller designed considering an approximate system provably meets performance objectives on a true system.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0
17333,"human trafficking was one of a most atrocious crimes and among a challenging problems facing law enforcement which demands attention of global magnitude. inside this study, we leverage textual data from a website ""backpage""- used considering classified advertisement- to discern potential patterns of human trafficking activities which manifest online and identify advertisements of high interest to law enforcement. due to a lack of ground truth, we rely on the human analyst from law enforcement, considering hand-labeling the small portion of a crawled data. we extend a existing laplacian svm and present s3vm-r, by adding the regularization term to exploit exogenous information embedded inside our feature space inside favor of a task at hand. we train a proposed method with the help of labeled and unlabeled data and evaluate it on the fraction of a unlabeled data, herein referred to as unseen data, with our expert's further verification. results from comparisons between our method and other semi-supervised and supervised approaches on a labeled data demonstrate that our learner was effective inside identifying advertisements of high interest to law enforcement",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17323,"we develope the self-consistent description of a broad line region based on a concept of a failed wind powered by a radiation pressure acting on dusty accretion disk atmosphere inside keplerian motion. a material raised high above a disk was illuminated, dust evaportes, and a matter falls back towards a disk. this material was a source of emission lines. a model predicts a inner and outer radius of a region, a cloud dynamics under a dust radiation pressure and, subsequently, just a gravitational field of a central black hole, which results inside assymetry between a rise and fall. knowledge of a dynamics allows to predict a shapes of a emission lines as functions of a basic parameters of an active nucleus: black hole mass, accretion rate, black hole spin (or accretion efficiency) and a viewing angle with respect to a symmetry axis. here we show preliminary results based on analytical approximations to a cloud motion.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
650,"we establish the bijective correspondence between certain non-self-intersecting curves inside an $n$-punctured disc and positive ${\mathbf c}$-vectors of acyclic cluster algebras whose quivers have multiple arrows between every pair of vertices. as the corollary, we obtain the proof of the conjecture by k.-h. lee and k. lee (arxiv:1703.09113) on a combinatorial description of real schur roots considering acyclic quivers with multiple arrows, and give the combinatorial characterization of seeds inside terms of curves inside an $n$-punctured disc.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
10222,"machine learning algorithms based on deep neural networks have achieved remarkable results and are being extensively used inside different domains. however, a machine learning algorithms requires access to raw data which was often privacy sensitive. to address this issue, we develop new techniques to provide solutions considering running deep neural networks over encrypted data. inside this paper, we develop new techniques to adopt deep neural networks within a practical limitation of current homomorphic encryption schemes. more specifically, we focus on classification of a well-known convolutional neural networks (cnn). first, we design methods considering approximation of a activation functions commonly used inside cnns (i.e. relu, sigmoid, and tanh) with low degree polynomials which was essential considering efficient homomorphic encryption schemes. then, we train convolutional neural networks with a approximation polynomials instead of original activation functions and analyze a performance of a models. finally, we implement convolutional neural networks over encrypted data and measure performance of a models. our experimental results validate a soundness of our idea behind the method with several convolutional neural networks with varying number of layers and structures. when applied to a mnist optical character recognition tasks, our idea behind the method achieves 99.52\% accuracy which significantly outperforms a state-of-the-art solutions and was very close to a accuracy of a best non-private version, 99.77\%. also, it should make close to 164000 predictions per hour. we also applied our idea behind the method to cifar-10, which was much more complex compared to mnist, and were able to achieve 91.5\% accuracy with approximation polynomials used as activation functions. these results show that cryptodl provides efficient, accurate and scalable privacy-preserving predictions.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14995,"a internet and online forums such as reddit have become an increasingly popular medium considering citizens to engage inside political conversations. however, a online disinhibition effect resulting from a ability to use pseudonymous identities may manifest inside a form of offensive speech, consequently making political discussions more aggressive and polarizing than they already are. such environments may result inside harassment and self-censorship from its targets. inside this paper, we present preliminary results from the large-scale temporal measurement aimed at quantifying offensiveness inside online political discussions. to enable our measurements, we develop and evaluate an offensive speech classifier. we then use this classifier to quantify and compare offensiveness inside a political and general contexts. we perform our study with the help of the database of over 168m reddit comments made by over 7m pseudonyms between january 2015 and january 2017 -- the period covering several divisive political events including a 2016 us presidential elections.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
9836,"considering the particular disease there may be two diagnostic tests developed, where each of a tests was subject to several studies. the quadrivariate generalized linear mixed model (glmm) has been recently proposed to joint meta-analyse and compare two diagnostic tests. we propose the d-vine copula mixed model considering joint meta-analysis and comparison of two diagnostic tests. our general model includes a quadrivariate glmm as the special case and should also operate on a original scale of sensitivities and specificities. a method allows a direct calculation of sensitivity and specificity considering each test, as well as, a parameters of a summary receiver operator characteristic (sroc) curve, along with the comparison between a srocs of each test. our methodology was demonstrated with an extensive simulation study and illustrated by meta-analysing two examples where 2 tests considering a diagnosis of the particular disease are compared. our study suggests that there should be an improvement on glmm inside fit to data since our model should also provide tail dependencies and asymmetries.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14999,"we investigate a stability of the statistically stationary conductive state considering rayleigh-b√©nard convection between stress-free plates that arises due to the bulk stochastic internal heating. this setup may be seen as the generalization to the stochastic setting of a seminal 1916 study of lord rayleigh. our results indicate that stochastic forcing at small magnitude has the stabilizing effect, while strong stochastic forcing has the destabilizing effect. a methodology put forth inside this article, which combines rigorous analysis with careful computation, also provides an idea behind the method to hydrodynamic stability considering the variety of systems subject to the large scale stochastic forcing.",0,1,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12606,"inside a experimental electroluminescence (el) spectra of light-emitting diodes (leds) based on n-polar (in,ga)n/gan nanowires (nws), we observed the double peak structure. a relative intensity of a two peaks evolves inside the peculiar way with injected current. spatially and spectrally resolved el maps confirmed a presence of two main transitions inside a spectra, and suggested that they are emitted by a majority of single nano-leds. inside order to elucidate a physical origin of this effect, we performed theoretical calculations of a strain, electric field, and charge density distributions both considering planar leds and nw-leds. on this basis, we simulated also a el spectra of these devices, which exhibit the double peak structure considering n-polar heterostructures, both inside a nw and a planar case. inside contrast, this feature was not observed when ga-polar planar leds are simulated. we found that a physical origin of a double peak structure was the stronger quantum-confined stark effect occurring inside a first and last quantum well of a n-polar heterostructures. a peculiar evolution of a relative peak intensities with injected current, seen only inside a case of a nw-led, was attributed to a three-dimensional strain variation resulting from elastic relaxation at a free sidewalls of a nws. therefore, this study provides important insights on a working principle of n-polar leds based on both planar and nw heterostructures.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
14685,"a main result of this note was a existence of martingale solutions to a stochastic heat equation (she) inside the riemannian manifold by with the help of suitable dirichlet forms on a corresponding path/loop space. moreover, we present some characterizations of a lower bound of a ricci curvature by functional inequalities of various associated dirichlet forms.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13619,"conditional preference networks (cp-nets) are the graphical representation of the person's (conditional) preferences over the set of discrete variables. inside this paper, we introduce the novel method of quantifying preference considering any given outcome based on the cp-net representation of the user's preferences. we demonstrate that these values are useful considering reasoning about user preferences. inside particular, they allow us to order (any subset of) a possible outcomes inside accordance with a user's preferences. further, these values should be used to improve a efficiency of outcome dominance testing. that is, given the pair of outcomes, we should determine which a user prefers more efficiently. through experimental results, we show that this method was more effective than existing techniques considering improving dominance testing efficiency. we show that a above results also hold considering cp-nets that express indifference between variable values.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
16607,"internet social networks have become the ubiquitous application allowing people to easily share text, pictures, and audio and video files. popular networks include whatsapp, facebook, reddit and linkedin. we present an extensive study of a usage of a whatsapp social network, an internet messaging application that was quickly replacing sms messaging. inside order to better understand people's use of a network, we provide an analysis of over 6 million messages from over 100 users, with a objective of building demographic prediction models with the help of activity data. we performed extensive statistical and numerical analysis of a data and found significant differences inside whatsapp usage across people of different genders and ages. we also inputted a data into a weka data mining package and studied models created from decision tree and bayesian network algorithms. we found that different genders and age demographics had significantly different usage habits inside almost all message and group attributes. we also noted differences inside users' group behavior and created prediction models, including a likelihood the given group would have relatively more file attachments, if the group would contain the larger number of participants, the higher frequency of activity, quicker response times and shorter messages. we were successful inside quantifying and predicting the user's gender and age demographic. similarly, we were able to predict different types of group usage. all models were built without analyzing message content. we present the detailed discussion about a specific attributes that were contained inside all predictive models and suggest possible applications based on these results.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
7769,"most of researches on image forensics have been mainly focused on detection of artifacts introduced by the single processing tool. they lead inside a development of many specialized algorithms looking considering one or more particular footprints under specific settings. naturally, a performance of such algorithms are not perfect, and accordingly a provided output might be noisy, inaccurate and only partially correct. furthermore, the forged image inside practical scenarios was often a result of utilizing several tools available by image-processing software systems. therefore, reliable tamper detection requires developing more poweful tools to deal with various tempering scenarios. fusion of forgery detection tools based on fuzzy inference system has been used before considering addressing this problem. adjusting a membership functions and defining proper fuzzy rules considering attaining to better results are time-consuming processes. this should be accounted as main disadvantage of fuzzy inference systems. inside this paper, the neuro-fuzzy inference system considering fusion of forgery detection tools was developed. a neural network characteristic of these systems provides appropriate tool considering automatically adjusting a membership functions. moreover, initial fuzzy inference system was generated based on fuzzy clustering techniques. a proposed framework was implemented and validated on the benchmark image splicing data set inside which three forgery detection tools are fused based on adaptive neuro-fuzzy inference system. a outcome of a proposed method reveals that applying neuro fuzzy inference systems could be the better idea behind the method considering fusion of forgery detection tools.",1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7152,"considering applications as varied as bayesian neural networks, determinantal point processes, elliptical graphical models, and kernel learning considering gaussian processes (gps), one must compute the log determinant of an $n \times n$ positive definite matrix, and its derivatives - leading to prohibitive $\mathcal{o}(n^3)$ computations. we propose novel $\mathcal{o}(n)$ approaches to estimating these quantities from only fast matrix vector multiplications (mvms). these stochastic approximations are based on chebyshev, lanczos, and surrogate models, and converge quickly even considering kernel matrices that have challenging spectra. we leverage these approximations to develop the scalable gaussian process idea behind the method to kernel learning. we find that lanczos was generally superior to chebyshev considering kernel learning, and that the surrogate idea behind the method should be highly efficient and accurate with popular kernels.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8897,"let $\mathcal {w}^k(\mathfrak{sl}_4, f_{\text {subreg}})$ be a universal $\mathcal{w}$-algebra associated to $\mathfrak{sl}_4$ with its subregular nilpotent element, and let $\mathcal {w}_k(\mathfrak{sl}_4, f_{\text {subreg}})$ be its simple quotient. there was the heisenberg subalgebra $\mathcal{h}$, and we denote by $\mathcal{c}^k$ a coset $\text{com}(\mathcal{h}, \mathcal {w}^k(\mathfrak{sl}_4, f_{\text {subreg}}))$, and by $\mathcal{c}_k$ its simple quotient. we show that considering $k=-4+(m+4)/3$ where $m$ was an integer greater than $2$ and $m+1$ was coprime to $3$, $\mathcal{c}_k$ was isomorphic to the rational, regular $\mathcal w$-algebra $\mathcal{w}(\mathfrak{sl}_m, f_{\text{reg}})$. inside particular, $\mathcal{w}_k(\mathfrak{sl}_4, f_{\text {subreg}})$ was the simple current extension of a tensor product of $\mathcal{w}(\mathfrak{sl}_m, f_{\text{reg}})$ with the rank one lattice vertex operator algebra, and thus was rational.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
15117,"astrophysicists are interested inside recovering a 3d gas emissivity of the galaxy cluster from the 2d image taken by the telescope. the blurring phenomenon and presence of point sources make this inverse problem even harder to solve. a current state-of-the-art technique was two step: first identify a location of potential point sources, then mask these locations and deproject a data. we instead model a data as the poisson generalized linear model (involving blurring, abel and wavelets operators) regularized by two lasso penalties to induce sparse wavelet representation and sparse point sources. a amount of sparsity was controlled by two quantile universal thresholds. as the result, our method outperforms a existing one.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
13447,"we study a phase diagram of quantum hall bilayer systems with total filing $\nu_t=1/2+1/2$ of a lowest landau level as the function of layer distances $d$. based on numerical exact diagonalization calculations, we obtain three distinct phases, including an exciton superfluid phase with spontaneous interlayer coherence at small $d$, the composite fermi liquid at large $d$, and an intermediate phase considering $1.1<d/l_b<1.8$ ($l_b$ was a magnetic length). a transition from a exciton superfluid to a intermediate phase was identified by (i) the dramatic change inside a berry curvature of a ground state under twisted boundary conditions on a two layers; (ii) an energy level crossing of a first excited state. a transition from a intermediate phase to a composite fermi liquid was identified by a vanishing of a exciton superfluid stiffness. furthermore, from our finite-size study, a energy cost of transferring one electron between a layers shows an even-odd effect and possibly extrapolates to the finite value inside a thermodynamic limit, indicating a enhanced intralayer correlation. our identification of an intermediate phase and its distinctive features shed new light on a theoretical understanding of a quantum hall bilayer system at total filling $\nu_t=1$.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
1084,"natural language and symbols are intimately correlated. recent advances inside machine learning (ml) and inside natural language processing (nlp) seem to contradict a above intuition: symbols are fading away, erased by vectors or tensors called distributed and distributional representations. however, there was the strict link between distributed/distributional representations and symbols, being a first an approximation of a second. the clearer understanding of a strict link between distributed/distributional representations and symbols will certainly lead to radically new deep learning networks. inside this paper we make the survey that aims to draw a link between symbolic representations and distributed/distributional representations. this was a right time to revitalize a area of interpreting how symbols are represented in neural networks.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6798,"a interplay between spin-orbit coupling (soc) and electron correlation ($u$) was considered considering many exotic phenomena inside iridium oxides. we have investigated a evolution of structural, magnetic and electronic properties inside pyrochlore iridate y$_2$ir$_{2-x}$ru$_{x}$o$_7$ where a substitution of ru has been aimed to tune this interplay. a ru substitution does not introduce any structural phase transition, however, we do observe an evolution of lattice parameters with a doping level $x$. x-ray photoemission spectroscopy (xps) study indicates ru adopts charge state of ru$^{4+}$ and replaces a ir$^{4+}$ accordingly. magnetization data reveal both a onset of magnetic irreversibility and a magnetic moment decreases with progressive substitution of ru. these materials show non-equilibrium low temperature magnetic state as revealed by magnetic relaxation data. interestingly, we find magnetic relaxation rate increases with substitution of ru. a electrical resistivity shows an insulating behavior inside whole temperature range, however, resistivity decreases with substitution of ru. nature of electronic conduction has been found to follow power-law behavior considering all a materials.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
13210,"introducing inequality constraints inside gaussian process (gp) models should lead to more realistic uncertainties inside learning the great variety of real-world problems. we consider a finite-dimensional gaussian idea behind the method from maatouk and bay (2017) which should satisfy inequality conditions everywhere (either boundedness, monotonicity or convexity). our contributions are threefold. first, we extend their idea behind the method inside order to deal with general sets of linear inequalities. second, we explore several markov chain monte carlo (mcmc) techniques to approximate a posterior distribution. third, we investigate theoretical and numerical properties of a constrained likelihood considering covariance parameter estimation. according to experiments on both artificial and real data, our full framework together with the hamiltonian monte carlo-based sampler provides efficient results on both data fitting and uncertainty quantification.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
12258,"preprocessing tools considering automated text analysis have become more widely available inside major languages, but non-english tools are often still limited inside their functionality. when working with spanish-language text, researchers should easily find tools considering tokenization and stemming, but may not have a means to extract more complex word features like verb tense or mood. yet spanish was the morphologically rich language inside which such features are often identifiable from word form. conjugation rules are consistent, but many special verbs and nouns take on different rules. while building the complete dictionary of known words and their morphological rules would be labor intensive, resources to do so already exist, inside spell checkers designed to generate valid forms of known words. this paper introduces the set of tools considering spanish-language morphological analysis, built with the help of a coes spell checking tools, to label person, mood, tense, gender and number, derive the word's root noun or verb infinitive, and convert verbs to their nominal form.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10076,"studying complexity of various bribery problems has been one of a main research focus inside computational social choice. inside all a models of bribery studied so far, a briber has to pay every voter some amount of money depending on what a briber wants a voter to report and a briber has some budget at her disposal. although these models successfully capture many real world applications, inside many other scenarios, a voters may be unwilling to deviate too much from their true preferences. inside this paper, we study a computational complexity of a problem of finding the preference profile which was as close to a true preference profile as possible and still achieves a briber's goal subject to budget constraints. we call this problem optimal bribery. we consider three important measures of distances, namely, swap distance, footrule distance, and maximum displacement distance, and resolve a complexity of a optimal bribery problem considering many common voting rules. we show that a problem was polynomial time solvable considering a plurality and veto voting rules considering all a three measures of distance. on a other hand, we prove that a problem was np-complete considering the class of scoring rules which includes a borda voting rule, maximin, copeland$^\alpha$ considering any $\alpha\in[0,1]$, and bucklin voting rules considering all a three measures of distance even when a distance allowed per voter was $1$ considering a swap and maximum displacement distances and $2$ considering a footrule distance even without a budget constraints (which corresponds to having an infinite budget). considering a $k$-approval voting rule considering any constant $k>1$ and a simplified bucklin voting rule, we show that a problem was np-complete considering a swap distance even when a distance allowed was $2$ and considering a footrule distance even when a distance allowed was $4$ even without a budget constraints.",1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10329,"we study a multi-armed bandit problem with multiple plays and the budget constraint considering both a stochastic and a adversarial setting. at each round, exactly $k$ out of $n$ possible arms have to be played (with $1\leq k \leq n$). inside addition to observing a individual rewards considering each arm played, a player also learns the vector of costs which has to be covered with an a-priori defined budget $b$. a game ends when a sum of current costs associated with a played arms exceeds a remaining budget. firstly, we analyze this setting considering a stochastic case, considering which we assume each arm to have an underlying cost and reward distribution with support $[c_{\min}, 1]$ and $[0, 1]$, respectively. we derive an upper confidence bound (ucb) algorithm which achieves $o(nk^4 \log b)$ regret. secondly, considering a adversarial case inside which a entire sequence of rewards and costs was fixed inside advance, we derive an upper bound on a regret of order $o(\sqrt{nb\log(n/k)})$ utilizing an extension of a well-known $\texttt{exp3}$ algorithm. we also provide upper bounds that hold with high probability and the lower bound of order $\omega((1 - k/n)^2 \sqrt{nb/k})$.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18160,"it was important to study a risks of publishing privacy-sensitive data. even if sensitive identities (e.g., name, social security number) were removed and advanced data perturbation techniques were applied, several de-anonymization attacks have been proposed to re-identify individuals. however, existing attacks have some limitations: 1) they are limited inside de-anonymization accuracy; 2) they require prior seed knowledge and suffer from a imprecision of such seed information. we propose the novel structure-based de-anonymization attack, which does not require a attacker to have prior information (e.g., seeds). our attack was based on two key insights: with the help of multi-hop neighborhood information, and optimizing a process of de-anonymization by exploiting enhanced machine learning techniques. a experimental results demonstrate that our method was robust to data perturbations and significantly outperforms a state-of-the-art de-anonymization techniques by up to $10\times$ improvement.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
387,"this paper proposes the centralized and the distributed sub-optimal control strategy to maintain inside safe regions a real-time transient frequencies of the given collection of buses, and simultaneously preserve asymptotic stability of a entire network. inside the receding horizon fashion, a centralized control input was obtained by iteratively solving an open-loop optimization aiming to minimize a aggregate control effort over controllers regulated on individual buses with transient frequency and stability constraints. due to a non-convexity of a optimization, we propose the convexification technique by identifying the reference control input trajectory. we then extend a centralized control to the distributed scheme, where each subcontroller should only access a state information within the local region. simulations on the ieee-39 network illustrate our results.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
15783,"red clump stars are fundamental distance indicators inside astrophysics, although theoretical stellar models predict the dependence of absolute magnitudes with ages. this effect was particularly strong below 2 gyr, but even above this limit the mild age dependence was still expected. we use seismically identified red clump stars inside a kepler field considering which we have reliable distances, masses and ages from a saga survey to first explore this effect. by excluding red clump stars with masses larger than 1.6 msun (corresponding to ages younger than 2 gyr), we derive robust calibrations linking intrinsic colors to absolute magnitudes inside a following photometric systems: str√∂mgren $by$, johnson $bv$, sloan $griz$, 2mass $jhk_s$ and wise $w1w2w3$. with a precision achieved we also detect the slope of absolute magnitudes 0.020(0.003) mag per gyrin a infrared, implying that distance calibrations of clump stars should be off by up to 0.2 mag inside a infrared (over a range from 2 gyr to 12 gyr) if their ages are unknown. even larger uncertainties affect optical bands, because of a stronger interdependency of absolute magnitudes on colors and age. our distance calibrations are ultimately based on asteroseismology, and we show how a distance scale should be used to test a accuracy of seismic scaling relations. within a uncertainties our calibrations are inside agreement with those built upon local red clump with hipparcos} parallaxes, although we find the tension which if confirmed would imply that scaling relations overestimate radii of red clump stars by 2(+-20%. data-releases post gaia dr1 will provide an important testbed considering our results.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14813,"inside the recent paper, flandrin [2015] has proposed filtering based on a zeros of the spectrogram, with the help of a short-time fourier transform and the gaussian window. his results are based on empirical observations on a distribution of a zeros of a spectrogram of white gaussian noise. these zeros tend to be uniformly spread over a time-frequency plane, and not to clutter. our contributions are threefold: we rigorously define a zeros of a spectrogram of continuous white gaussian noise, we explicitly characterize their statistical distribution, and we investigate a computational and statistical underpinnings of a practical implementation of signal detection based on a statistics of spectrogram zeros. inside particular, we stress that a zeros of spectrograms of white gaussian noise correspond to zeros of gaussian analytic functions, the topic of recent independent mathematical interest [hough et al., 2009].",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
17438,"we study verdier quotients of diverse homotopy categories of the full additive subcategory $\mathcal e$ of an abelian category. inside particular, we consider a categories $k^{x,y}({\mathcal e})$ considering $x\in\{\infty, +,-,b\}$, and $y\in\{\emptyset,b,+,-,\infty\}$ a homotopy categories of left, right, unbounded complexes with homology being $0$, bounded, left or right bounded, or unbounded. inclusion of these categories give the partially ordered set, and we study localisation sequences or recollement diagrams between a verdier quotients, and prove that many quotients lead to equivalent categories.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
19089,"we study spin transfer torques induced by the spin-triplet supercurrent inside the magnet with a superconducting proximity effect. by the perturbative approach, we show that spin-triplet correlations realize new types of torques, which are analogous to a adiabatic and non-adiabatic ($\beta$) torques, without extrinsic spin-flip scattering. remarkable advantages compared to conventional spin-transfer torques are highlighted inside domain wall manipulation. oscillatory motions of the domain wall do not occur considering the small gilbert damping, and a threshold current density to drive its motion becomes zero inside a absence of extrinsic pinning potentials due to a nonadiabatic torque controlled by a triplet correlations.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
10198,"3d scene understanding was important considering robots to interact with a 3d world inside the meaningful way. most previous works on 3d scene understanding focus on recognizing geometrical or semantic properties of a scene independently. inside this work, we introduce data associated recurrent neural networks (da-rnns), the novel framework considering joint 3d scene mapping and semantic labeling. da-rnns use the new recurrent neural network architecture considering semantic labeling on rgb-d videos. a output of a network was integrated with mapping techniques such as kinectfusion inside order to inject semantic information into a reconstructed 3d scene. experiments conducted on the real world dataset and the synthetic dataset with rgb-d videos demonstrate a ability of our method inside semantic 3d scene mapping.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
13589,"diffusions and related random walk procedures are of central importance inside many areas of machine learning, data analysis, and applied mathematics. because they spread mass agnostically at each step inside an iterative manner, they should sometimes spread mass ""too aggressively,"" thereby failing to find a ""right"" clusters. we introduce the novel capacity releasing diffusion (crd) process, which was both faster and stays more local than a classical spectral diffusion process. as an application, we use our crd process to develop an improved local algorithm considering graph clustering. our local graph clustering method should find local clusters inside the model of clustering where one begins a crd process inside the cluster whose vertices are connected better internally than externally by an $o(\log^2 n)$ factor, where $n$ was a number of nodes inside a cluster. thus, our crd process was a first local graph clustering algorithm that was not subject to a well-known quadratic cheeger barrier. our result requires the certain smoothness condition, which we expect to be an artifact of our analysis. our empirical evaluation demonstrates improved results, inside particular considering realistic social graphs where there are moderately good---but not very good---clusters.",1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14191,"teleconference or telepresence based on virtual reality (vr) headmount display (hmd) device was the very interesting and promising application since hmd should provide immersive feelings considering users. however, inside order to facilitate face-to-face communications considering hmd users, real-time 3d facial performance capture of the person wearing hmd was needed, which was the very challenging task due to a large occlusion caused by hmd. a existing limited solutions are very complex either inside setting or inside idea behind the method as well as lacking a performance capture of 3d eye gaze movement. inside this paper, we propose the convolutional neural network (cnn) based solution considering real-time 3d face-eye performance capture of hmd users without complex modification to devices. to address a issue of lacking training data, we generate massive pairs of hmd face-label dataset by data synthesis as well as collecting vr-ir eye dataset from multiple subjects. then, we train the dense-fitting network considering facial region and an eye gaze network to regress 3d eye model parameters. extensive experimental results demonstrate that our system should efficiently and effectively produce inside real time the vivid personalized 3d avatar with a correct identity, pose, expression and eye motion corresponding to a hmd user.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
771,"we consider a problem of planning under observation and motion uncertainty considering nonlinear robotics systems. determining a optimal solution to this problem, generally formulated as the partially observed markov decision process (pomdp), was computationally intractable. we propose the trajectory-optimized linear quadratic gaussian (t-lqg) idea behind the method that leads to quantifiably near-optimal solutions considering a pomdp problem. we provide the novel ""separation principle"" considering a design of an optimal nominal open-loop trajectory followed by an optimal feedback control law, which provides the near-optimal feedback control policy considering belief space planning problems involving the polynomial order of calculations of minimum order.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1
8411,"we study a problem of resilient consensus of sampled-data multi-agent networks with double-integrator dynamics. a term resilient points to algorithms considering a presence of attacks by faulty/malicious agents inside a network. each normal agent updates its state based on the predetermined control law with the help of its neighbors' information which may be delayed while misbehaving agents make updates arbitrarily and might threaten a consensus within a network. assuming that a maximum number of malicious agents inside a system was known, we focus on algorithms where each normal agent ignores large and small position values among its neighbors to avoid being influenced by malicious agents. a malicious agents are assumed to be omniscient inside that they know a updating times and delays and should collude with each other. we deal with both synchronous and partially asynchronous cases with delayed information and derive topological conditions inside terms of graph robustness.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
8800,"classic topic models are built under a bag of words assumption, inside which word position was ignored considering simplicity. besides, symmetric priors are typically used inside most applications. inside order to easily learn topics with different properties among a same corpus, we propose the new line of work inside which a paragraph structure was exploited. our proposal was based on a following assumption: inside many text document corpora there are formal constraints shared across all a collection, e.g. sections. when this assumption was satisfied, some paragraphs may be related to general concepts shared by all documents inside a corpus, while others would contain a genuine description of documents. assuming each paragraph should be semantically more general, specific, or hybrid, we look considering ways to measure this, transferring this distinction to topics and being able to learn what we call specific and general topics. experiments show that this was the proper methodology to highlight certain paragraphs inside structured documents at a same time we learn interesting and more diverse topics.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18309,"shifted combinatorial optimization was the new nonlinear optimization framework which was the broad extension of standard combinatorial optimization, involving a choice of several feasible solutions at the time. this framework captures well studied and diverse problems ranging from so-called vulnerability problems to sharing and partitioning problems. inside particular, every standard combinatorial optimization problem has its shifted counterpart, which was typically much harder. already with explicitly given input set a shifted problem may be np-hard. inside this article we initiate the study of a parameterized complexity of this framework. first we show that shifting over an explicitly given set with its cardinality as a parameter may be inside xp, fpt or p, depending on a objective function. second, we study a shifted problem over sets definable inside mso logic (which includes, e.g., a well known mso partitioning problems). our main results here are that shifted combinatorial optimization over mso definable sets was inside xp with respect to a mso formula and a treewidth (or more generally clique-width) of a input graph, and was w[1]-hard even under further severe restrictions.",1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
10358,"let k be the finite base field. inside this note, making use of topological periodic cyclic homology and of a theory of noncommutative motives, we prove that a numerical grothendieck group of every smooth proper dg k-linear category was the finitely generated free abelian group. along a way, we prove moreover that a category of noncommutative numerical motives over k was abelian semi-simple, as conjectured by kontsevich. furthermore, we show that a zeta functions of endomorphisms of noncommutative chow motives are rational and satisfy the functional equation.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
16670,"we propose the new idea behind the method considering metric learning by framing it as learning the sparse combination of locally discriminative metrics that are inexpensive to generate from a training data. this flexible framework allows us to naturally derive formulations considering global, multi-task and local metric learning. a resulting algorithms have several advantages over existing methods inside a literature: the much smaller number of parameters to be estimated and the principled way to generalize learned metrics to new testing data points. to analyze a idea behind the method theoretically, we derive the generalization bound that justifies a sparse combination. empirically, we evaluate our algorithms on several datasets against state-of-the-art metric learning methods. a results are consistent with our theoretical findings and demonstrate a superiority of our idea behind the method inside terms of classification performance and scalability.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8307,"motivated by our conjecture of an earlier work predicting a degeneration at a second page of a fr√∂licher spectral sequence of any compact complex manifold supporting an skt metric $\omega$ (i.e. such that $\partial\bar\partial\omega=0$), we prove degeneration at $e_2$ whenever a manifold admits the hermitian metric whose torsion operator $\tau$ and its adjoint vanish on $\delta''$-harmonic forms of positive degrees up to $\mbox{dim}_\c x$. besides a pseudo-differential laplacian inducing the hodge theory considering $e_2$ that we constructed inside earlier work and demailly's bochner-kodaira-nakano formula considering hermitian metrics, the key ingredient was the general formula considering a dimensions of a vector spaces featuring inside a fr√∂licher spectral sequence inside terms of a asymptotics, as the positive constant $h$ decreases to zero, of a small eigenvalues of the rescaled laplacian $\delta_h$, introduced here inside a present form, that we adapt to a context of the complex structure from a well-known construction of a adiabatic limit and from a analogous result considering riemannian foliations of √°lvarez l√≥pez and kordyukov.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12952,"today's artificial assistants are typically prompted to perform tasks through direct, imperative commands such as \emph{set the timer} or \emph{pick up a box}. however, to progress toward more natural exchanges between humans and these assistants, it was important to understand a way non-imperative utterances should indirectly elicit action of an addressee. inside this paper, we investigate command types inside a setting of the grounded, collaborative game. we focus on the less understood family of utterances considering eliciting agent action, locatives like \emph{the chair was inside a other room}, and demonstrate how these utterances indirectly command inside specific game state contexts. our work shows that models with domain-specific grounding should effectively realize a pragmatic reasoning that was necessary considering more robust natural language interaction.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19675,"reusable model design becomes desirable with a rapid expansion of machine learning applications. inside this paper, we focus on a reusability of pre-trained deep convolutional models. specifically, different from treating pre-trained models as feature extractors, we reveal more treasures beneath convolutional layers, i.e., a convolutional activations could act as the detector considering a common object inside a image co-localization problem. we propose the simple but effective method, named deep descriptor transforming (ddt), considering evaluating a correlations of descriptors and then obtaining a category-consistent regions, which should accurately locate a common object inside the set of images. empirical studies validate a effectiveness of a proposed ddt method. on benchmark image co-localization datasets, ddt consistently outperforms existing state-of-the-art methods by the large margin. moreover, ddt also demonstrates good generalization ability considering unseen categories and robustness considering dealing with noisy data.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15091,"a millisecond-duration radio flashes known as fast radio bursts (frbs) represent an enigmatic astrophysical phenomenon. recently, a sub-arcsecond localization (~ 100mas precision) of frb121102 with the help of a vla has led to its unambiguous association with persistent radio and optical counterparts, and to a identification of its host galaxy. however, an even more precise localization was needed inside order to probe a direct physical relationship between a millisecond bursts themselves and a associated persistent emission. here we report very-long-baseline radio interferometric observations with the help of a european vlbi network and a 305-m arecibo telescope, which simultaneously detect both a bursts and a persistent radio emission at milliarcsecond angular scales and show that they are co-located to within the projected linear separation of < 40pc (< 12mas angular separation, at 95% confidence). we detect consistent angular broadening of a bursts and persistent radio source (~ 2-4mas at 1.7ghz), which are both similar to a expected milky way scattering contribution. a persistent radio source has the projected size constrained to be < 0.7pc (< 0.2mas angular extent at 5.0ghz) and the lower limit considering a brightness temperature of t_b > 5 x 10^7k. together, these observations provide strong evidence considering the direct physical link between frb121102 and a compact persistent radio source. we argue that the burst source associated with the low-luminosity active galactic nucleus or the young neutron star energizing the supernova remnant are a two scenarios considering frb121102 that best match a observed data.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
1916,"inside this article, as always we will start by deliberating at our project's historical general view and then we will try to construct the new poisson bracket on our simplest example $sl_2$ and then we will try to give the universal construction based on our universal variables and then will try to construct lattice $w_2$ algebras which will play the key role inside our other constructions on lattice $w_3$ algebras and finally we will try to find a only non trivial dependent generator of our lattice $w_4$ algebras and so on considering lattice $w_n$ algebras. and inside a late of this article we will have appendix a, which will contain some parts of a mathematica coding which we have used and have made considering to find our algebra structures.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
18836,"this paper studies the stylized, yet natural, learning-to-rank problem and points out a critical incorrectness of the widely used nearest neighbor algorithm. we consider the model with $n$ agents (users) $\{x_i\}_{i \in [n]}$ and $m$ alternatives (items) $\{y_j\}_{j \in [m]}$, each of which was associated with the latent feature vector. agents rank items nondeterministically according to a plackett-luce model, where a higher a utility of an item to a agent, a more likely this item will be ranked high by a agent. our goal was to find neighbors of an arbitrary agent or alternative inside a latent space. we first show that a kendall-tau distance based knn produces incorrect results inside our model. next, we fix a problem by introducing the new algorithm with features constructed from ""global information"" of a data matrix. our idea behind the method was inside sharp contrast to most existing feature engineering methods. finally, we design another new algorithm identifying similar alternatives. a construction of alternative features should be done with the help of ""local information,"" highlighting a algorithmic difference between finding similar agents and similar alternatives.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
10009,"with the help of combination of density functional theory and monte carlo simulation, we study a phase stability and electronic properties of two dimensional hexagonal composites of boron nitride and graphene, with the goal to uncover a role of a interface geometry formed between a two. our study highlights that preferential creation of extended armchair interfaces may facilitate formation of solid solution of boron nitride and graphene within the certain temperature range. we further find that considering band-gap engineering, armchair interfaces or patchy interfaces with mixed geometry are most suitable. extending a study to nanoribbon geometry shows that reduction of dimensionality makes a tendency to phase segregation of a two phases even stronger. our thorough study should form an useful database inside designing boron nitride-graphene composites with desired properties.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
18734,"inside this work, we perform an empirical comparison among a ctc, rnn-transducer, and attention-based seq2seq models considering end-to-end speech recognition. we show that, without any language model, seq2seq and rnn-transducer models both outperform a best reported ctc models with the language model, on a popular hub5'00 benchmark. on our internal diverse dataset, these trends continue - rnntransducer models rescored with the language model after beam search outperform our best ctc models. these results simplify a speech recognition pipeline so that decoding should now be expressed purely as neural network operations. we also study how a choice of encoder architecture affects a performance of a three models - when all encoder layers are forward only, and when encoders downsample a input representation aggressively.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12724,"we introduce an updated physical model to simulate a formation and evolution of galaxies inside cosmological, large-scale gravity+magnetohydrodynamical simulations with a moving mesh code arepo. a overall framework builds upon a successes of a illustris galaxy formation model, and includes prescriptions considering star formation, stellar evolution, chemical enrichment, primordial and metal-line cooling of a gas, stellar feedback with galactic outflows, and black hole formation, growth and multi-mode feedback. inside this paper we give the comprehensive description of a physical and numerical advances which form a core of a illustristng (the next generation) framework. we focus on a revised implementation of a galactic winds, of which we modify a directionality, velocity, thermal content, and energy scalings, and explore its effects on a galaxy population. as described inside earlier works, a model also includes the new black hole driven kinetic feedback at low accretion rates, magnetohydrodynamics, and improvements to a numerical scheme. with the help of the suite of (25 mpc $h^{-1}$)$^3$ cosmological boxes we assess a outcome of a new model at our fiducial resolution. a presence of the self-consistently amplified magnetic field was shown to have an important impact on a stellar content of $10^{12} m_{\rm sun}$ haloes and above. finally, we demonstrate that a new galactic winds promise to solve key problems identified inside illustris inside matching observational constraints and affecting a stellar content and sizes of a low mass end of a galaxy population.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16905,"evaluating similarity between graphs was of major importance inside several computer vision and pattern recognition problems, where graph representations are often used to model objects or interactions between elements. a choice of the distance or similarity metric is, however, not trivial and should be highly dependent on a application at hand. inside this work, we propose the novel metric learning method to evaluate distance between graphs that leverages a power of convolutional neural networks, while exploiting concepts from spectral graph theory to allow these operations on irregular graphs. we demonstrate a potential of our method inside a field of connectomics, where neuronal pathways or functional connections between brain regions are commonly modelled as graphs. inside this problem, a definition of an appropriate graph similarity function was critical to unveil patterns of disruptions associated with certain brain disorders. experimental results on a abide dataset show that our method should learn the graph similarity metric tailored considering the clinical application, improving a performance of the simple k-nn classifier by 11.9% compared to the traditional distance metric.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2685,"a in-plane infinitesimal deformations of graphene are well understood: they should be computed by solving a equilibrium problem considering the sheet of isotropic elastic material with suitable stretching stiffness and poisson coefficient $\nu^{(m)}$. here, we pose a following question: does a poisson coefficient $\nu^{(m)}$ affect a response to bending of graphene? despite what happens if one adopts classical structural models, it does not. inside this letter we show that the new material property, conceptually and quantitatively different from $\nu^{(m)}$, has to be introduced. we term this new parameter bending poisson coefficient; we propose considering it the physical interpretation inside terms of a atomic interactions and produce the quantitative evaluation.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
13942,"multi-label classification (mlc) was an important learning problem that expects a learning algorithm to take a hidden correlation of a labels into account. extracting a hidden correlation was generally the challenging task. inside this work, we propose the novel deep learning framework to better extract a hidden correlation with a aid of a memory structure within recurrent neural networks. a memory stores a temporary guesses on a labels and effectively allows a framework to rethink about a goodness and correlation of a guesses before making a final prediction. furthermore, a rethinking process makes it easy to adapt to different evaluation criterion to match real-world application needs. experimental results across many real-world data sets justify that a rethinking process indeed improves mlc performance across different evaluation criteria and leads to superior performance over state-of-the-art mlc algorithms.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5601,"probabilistic representations of movement primitives open important new possibilities considering machine learning inside robotics. these representations are able to capture a variability of a demonstrations from the teacher as the probability distribution over trajectories, providing the sensible region of exploration and a ability to adapt to changes inside a robot environment. however, to be able to capture variability and correlations between different joints, the probabilistic movement primitive requires a approximation of the larger number of parameters compared to their deterministic counterparts, that focus on modeling only a mean behavior. inside this paper, we make use of prior distributions over a parameters of the probabilistic movement primitive to make robust estimates of a parameters with few training instances. inside addition, we introduce general purpose operators to adapt movement primitives inside joint and task space. a proposed training method and adaptation operators are tested inside the coffee preparation and inside robot table tennis task. inside a coffee preparation task we evaluate a generalization performance to changes inside a location of a coffee grinder and brewing chamber inside the target area, achieving a desired behavior after only two demonstrations. inside a table tennis task we evaluate a hit and return rates, outperforming previous approaches while with the help of fewer task specific heuristics.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
8915,"we present the bayesian method considering feature selection inside a presence of grouping information with sparsity on a between- and within group level. instead of with the help of the stochastic algorithm considering parameter inference, we employ expectation propagation, which was the deterministic and fast algorithm. available methods considering feature selection inside a presence of grouping information have the number of short-comings: on one hand, lasso methods, while being fast, underestimate a regression coefficients and do not make good use of a grouping information, and on a other hand, bayesian approaches, while accurate inside parameter estimation, often rely on a stochastic and slow gibbs sampling procedure to recover a parameters, rendering them infeasible e.g. considering gene network reconstruction. our idea behind the method of the bayesian sparse-group framework with expectation propagation enables us to not only recover accurate parameter estimates inside signal recovery problems, but also makes it possible to apply this bayesian framework to large-scale network reconstruction problems. a presented method was generic but inside terms of application we focus on gene regulatory networks. we show on simulated and experimental data that a method constitutes the good choice considering network reconstruction regarding a number of correctly selected features, prediction on new data and reasonable computing time.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17888,"we present a results of cosmological hydrodynamic simulations with zoom-in initial conditions, and investigate a formation of a first galaxies and their evolution towards observable galaxies at $z \sim 6$. we focus on three different galaxies which end up inside halos with masses $m_{h} = 2.4 \times10^{10}~h^{-1}\; m_{\odot}$ (halo-10), $1.6 \times10^{11}~h^{-1}\; m_{\odot}$ (halo-11) and $0.7 \times10^{12}~h^{-1} m_{\odot}$ (halo-12) at z=6. our simulations also probe impacts of different sub-grid assumptions, i.e., sf efficiency and cosmic reionization, on sf histories inside a first galaxies. we find that star formation occurs intermittently due to supernova (sn) feedback at z > 10, and then it proceeds more smoothly as a halo mass grows at lower redshifts. galactic disks are destroyed due to sn feedback, while galaxies inside simulations with no-feedback or lower sf efficiency models should sustain galactic disk considering long periods > 10 myr. a expulsion of gas at a galactic center also affects a inner dark matter density profile. however, sn feedback does not seem to keep a shallow profile of dark matter considering the long period. our simulated galaxies inside halo-11 and halo-12 reproduce a star formation rates (sfr) and stellar masses of observed lyman-$\alpha$ emitters (laes) at z = 7-8 fairly well given observational uncertainties. inside addition, we investigate a effect of uv background radiation on star formation as an external feedback source, and find that earlier reionization extends a quenching time of star formation due to photo-ionization heating, but does not affect a stellar mass at z=6.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8145,"cross-validation of predictive models was a de-facto standard considering model selection and evaluation. inside proper use, it provides an unbiased approximate of the model's predictive performance. however, data sets often undergo the preliminary data-dependent transformation, such as feature rescaling or dimensionality reduction, prior to cross-validation. it was widely believed that such the preprocessing stage, if done inside an unsupervised manner that does not consider a class labels or response values, has no effect on a validity of cross-validation. inside this paper, we show that this belief was not true. preliminary preprocessing should introduce either the positive or negative bias into a estimates of model performance. thus, it may lead to sub-optimal choices of model parameters and invalid inference. inside light of this, a scientific community should re-examine a use of preliminary preprocessing prior to cross-validation across a various application domains. by default, all data transformations, including unsupervised preprocessing stages, should be learned only from a training samples, and then merely applied to a validation and testing samples.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0
18218,"inside typical neural machine translation~(nmt), a decoder generates the sentence word by word, packing all linguistic granularities inside a same time-scale of rnn. inside this paper, we propose the new type of decoder considering nmt, which splits a decode state into two parts and updates them inside two different time-scales. specifically, we first predict the chunk time-scale state considering phrasal modeling, on top of which multiple word time-scale states are generated. inside this way, a target sentence was translated hierarchically from chunks to words, with information inside different granularities being leveraged. experiments show that our proposed model significantly improves a translation performance over a state-of-the-art nmt model.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6962,this brief note highlights some basic concepts required toward understanding a evolution of machine learning and deep learning models. a note starts with an overview of artificial intelligence and its relationship to biological neuron that ultimately led to a evolution of todays intelligent models.,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8830,"unsupervised segmentation and clustering of unlabelled speech are core problems inside zero-resource speech processing. most approaches lie at methodological extremes: some use probabilistic bayesian models with convergence guarantees, while others opt considering more efficient heuristic techniques. despite competitive performance inside previous work, a full bayesian idea behind the method was difficult to scale to large speech corpora. we introduce an approximation to the recent bayesian model that still has the clear objective function but improves efficiency by with the help of hard clustering and segmentation rather than full bayesian inference. like its bayesian counterpart, this embedded segmental k-means model (es-kmeans) represents arbitrary-length word segments as fixed-dimensional acoustic word embeddings. we first compare es-kmeans to previous approaches on common english and xitsonga data sets (5 and 2.5 hours of speech): es-kmeans outperforms the leading heuristic method inside word segmentation, giving similar scores to a bayesian model while being 5 times faster with fewer hyperparameters. however, its clusters are less pure than those of a other models. we then show that es-kmeans scales to larger corpora by applying it to a 5 languages of a zero resource speech challenge 2017 (up to 45 hours), where it performs competitively compared to a challenge baseline.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18488,"inside our previous work we showed that considering an ancient solution to a ricci flow with nonnegative curvature operator, assuming bounded geometry on one time slice, bounded entropy implies noncollapsing on all scales. inside this paper we prove a implication inside a other direction, that considering an ancient solution with bounded nonnegative curvature operator, noncollapsing implies bounded entropy. thus we prove perelman's assertion under a assumption of bounded geometry on one time slice. inside particular, considering ancient solutions of dimension three, we need only to assume bounded curvature. we also establish an equality between a asymptotic entropy and a asymptotic reduce volume, which was the result similar to xu, where he assumes noncollapsing and a type i curvature bound.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
414,"inside our recent paper w.s. rossi, p. frasca and f. fagnani, ""average resistance of toroidal graphs"", siam journal on control and optimization, 53(4):2541--2557, 2015, we studied how a average resistances of $d$-dimensional toroidal grids depend on a graph topology and on a dimension of a graph. our results were based on a connection between resistance and laplacian eigenvalues. inside this note, we contextualize our work inside a body of literature about random walks on graphs. indeed, a average effective resistance of a $d$-dimensional toroidal grid was proportional to a mean hitting time of a simple random walk on that grid. if $d\geq3 $, then a average resistance should be bounded uniformly inside a number of nodes and its value was of order $1/d$ considering large $d$.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
1201,"deep reinforcement learning could be used to learn dexterous robotic policies but it was challenging to transfer them to new robots with vastly different hardware properties. it was also prohibitively expensive to learn the new policy from scratch considering each robot hardware due to a high sample complexity of modern state-of-the-art algorithms. we propose the novel idea behind the method called \textit{hardware conditioned policies} where we train the universal policy conditioned on the vector representation of robot hardware. we considered robots inside simulation with varied dynamics, kinematic structure, kinematic lengths and degrees-of-freedom. first, we use a kinematic structure directly as a hardware encoding and show great zero-shot transfer to completely novel robots not seen during training. considering robots with lower zero-shot success rate, we also demonstrate that fine-tuning a policy network was significantly more sample-efficient than training the model from scratch. inside tasks where knowing a agent dynamics was important considering success, we learn an embedding considering robot hardware and show that policies conditioned on a encoding of hardware tend to generalize and transfer well. a code and videos are available on a project webpage: this https url.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
13142,"beside a spin density wave (sdw) order in a superconducting phase inside $\mathrm{cecoin_5}$ at high magnetic fields, recent neutron scattering measurements have found bragg peaks inside $5\%$ nd doped $\mathrm{cecoin_5}$ at low fields. a intensity of bragg peaks inside low fields was suppressed by increasing field. based on a phenomenological and microscopic modeling, we show that considering a pauli limited $d$-wave superconductors inside a vicinity of sdw instability relevant considering $\mathrm{cecoin_5}$, magnetic impurities locally induce droplets of sdw order. because of a strong anisotropy inside a momentum space inside a spin fluctuations guaranteed by a $d$-wave pairing symmetry, sharp peaks inside spin structure factor at $\mathbf{q}$s are produced by a impurities, even when a droplets of sdw do not order. at zero field, a nd impurity spins are along a $c$ axis due to a coupling to a conduction electrons with an easy $c$ axis, besides their own crystal field effect. a in-plane magnetic field cants a impurity moments toward a $ab$ plane, which suppress a droplets of sdw order. at high fields, a long-range sdw in a superconducting phase was stabilized as the consequence of magnon condensation. our results are consistent with a recent neutron scattering and thermal conductivity measurements.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
15799,"instanton partition functions of $\mathcal{n}=1$ 5d super yang-mills reduced on $s^1$ should be engineered inside type iib string theory from a $(p,q)$-branes web diagram. to this diagram was superimposed the web of representations of a ding-iohara-miki (dim) algebra that acts on a partition function. inside this correspondence, each segment was associated to the representation, and a (topological string) vertex was identified with a intertwiner operator constructed by awata, feigin and shiraishi. we define the new intertwiner acting on a representation spaces of levels $(1,n)\otimes(0,m)\to(1,n+m)$, thereby generalizing to higher rank $m$ a original construction. it allows us to use the folded version of a usual $(p,q)$-web diagram, bringing great simplifications to actual computations. as the result, a characterization of gaiotto states and vertical intertwiners, previously obtained by some of a authors, was uplifted to operator relations acting inside a fock space of horizontal representations. we further develop the method to build qq-characters of linear quivers based on a horizontal action of dim elements. while fundamental qq-characters should be built with the help of a coproduct, higher ones require a introduction of the (quantum) weyl reflection acting on tensor products of dim generators.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
9227,"a physical properties of polycrystalline materials depend on their microstructure, which was a nano-to-centimeter-scale arrangement of phases and defects inside their interior. such microstructure depends on a shape, crystallographic phase and orientation, and interfacing of a grains constituting a material. this article presents the new non-destructive 3d technique to study bulk samples with sizes inside a cm range with the resolution of hundred micrometers: time-of-flight three-dimensional neutron diffraction (tof 3dnd). compared to existing analogous x-ray diffraction techniques, tof 3dnd enables studies of samples that should be both larger inside size and made of heavier elements. moreover, tof 3dnd facilitates a use of complicated sample environments. a basic tof 3dnd setup, utilizing an imaging detector with high spatial and temporal resolution, should easily be implemented at the time-of-flight neutron beamline. a technique is developed and tested with data collected at a materials and life science experimental facility of a japan proton accelerator complex (j-parc) considering an iron sample. we successfully reconstructed a shape of 108 grains and developed an indexing procedure. a reconstruction algorithms have been validated by reconstructing two stacked co-ni-ga single crystals and by comparison with the grain map obtained by post-mortem electron backscatter diffraction (ebsd).",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
863,electromagnetic properties of single crystal terbium gallium garnet (tgg) are characterised from room down to millikelvin temperatures with the help of a whispering gallery mode method. microwave spectroscopy was performed at low powers equivalent to the few photons inside energy and conducted as functions of a magnetic field and temperature. the phase transition was detected close to a temperature of 3.5 k. this was observed considering multiple whispering gallery modes causing an abrupt negative frequency shift and the change inside transmission due to extra losses inside a new phase caused by the change inside complex magnetic susceptibility.,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
6905,"a local environment and a charge state of the nickel impurity inside cubic ba$_{0.8}$sr$_{0.2}$tio$_3$ are studied by xafs spectroscopy. according to a xanes data, a mean ni charge state was $\sim$2.5+. an analysis of a exafs spectra and their comparison with a results of first-principle calculations of a defect geometry suggest that ni$^{2+}$ ions are inside the high-spin state at a $b$ sites of a perovskite structure and a difference of a ni$^{2+}$ and ti$^{4+}$ charges was mainly compensated by distant oxygen vacancies. inside addition, the considerable amount of nickel inside a sample was inside the second phase banio$_{3-\delta}$. a measurements of a lattice parameter show the decrease inside a unit cell volume upon doping, which should indicate a existence of the small amount of ni$^{4+}$ ions at a $b$ sites.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
2025,"the new obstacle detection algorithm considering unmanned surface vehicles (usvs) was presented. the state-of-the-art graphical model considering semantic segmentation was extended to incorporate boat pitch and roll measurements from a on-board inertial measurement unit (imu), and the stereo verification algorithm that consolidates tentative detections obtained from a segmentation was proposed. a imu readings are used to approximate a location of horizon line inside a image, which automatically adjusts a priors inside a probabilistic semantic segmentation model. we derive a equations considering projecting a horizon into images, propose an efficient optimization algorithm considering a extended graphical model, and offer the practical imu-camera-usv calibration procedure. with the help of an usv equipped with multiple synchronized sensors, we captured the new challenging multi-modal dataset, and annotated its images with water edge and obstacles. experimental results show that a proposed algorithm significantly outperforms a state of a art, with nearly 30% improvement inside water-edge detection accuracy, an over 21% reduction of false positive rate, an almost 60% reduction of false negative rate, and an over 65% increase of true positive rate, while its matlab implementation runs inside real-time.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
7799,"a physics potential of edelweiss detectors considering a search of low-mass weakly interacting massive particles (wimps) was studied. with the help of the data-driven background model, projected exclusion limits are computed with the help of frequentist and multivariate analysis approaches, namely profile likelihood and boosted decision tree. both current and achievable experimental performance are considered. a optimal strategy considering detector optimization depends critically on whether a emphasis was put on wimp masses below or above $\sim$ 5 gev/c$^2$. a projected sensitivity considering a next phase of a edelweiss-iii experiment at a modane underground laboratory (lsm) considering low-mass wimp search was presented. by 2018 an upper limit on a spin-independent wimp-nucleon cross-section of $\sigma_{si} = 7 \times 10^{-42}$ cm$^2$ was expected considering the wimp mass inside a range 2$-$5 gev/c$^2$. a requirements considering the future hundred-kilogram scale experiment designed to reach a bounds imposed by a coherent scattering of solar neutrinos are also described. by improving a ionization resolution down to 50 ev$_{ee}$, we show that such an experiment installed inside an even lower background environment (e.g. at snolab) should allow to observe about 80 $^8$b neutrino events after discrimination.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8576,"clustered quantum materials provide the new platform considering a experimental study of many-body entanglement. here we address the simple model of the single-molecule nano-magnet featuring n interacting spins inside the transverse field. a field should induce an entanglement transition (et). we calculate a magnetisation, low-energy gap and neutron-scattering cross-section and find that a et has distinct signatures, detectable at temperatures as high as 5% of a interaction strength. a signatures are stronger considering smaller clusters.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
1589,"we study performance limits of solutions to utility maximization problems (e.g., max-min problems) inside wireless networks as the function of a power budget $\bar{p}$ available to transmitters. special focus was devoted to a utility and a transmit energy efficiency (i.e., utility over transmit power) of a solution. briefly, we show tight bounds considering a general class of network utility optimization problems that should be solved by computing conditional eigenvalues of standard interference mappings. a proposed bounds, which are based on a concept of asymptotic functions, are simple to compute, provide us with good estimates of a performance of networks considering any value of $\bar{p}$ inside many real-world applications, and enable us to determine points inside which networks move from the noise limited regime to an interference limited regime. furthermore, they also show that a utility and a transmit energy efficiency scales as $\theta(1)$ and $\theta(1/\bar{p})$, respectively, as $\bar{p}\to\infty$.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
3428,"modern scientific instruments produce vast amounts of data, which should overwhelm a processing ability of computer systems. lossy compression of data was an intriguing solution, but comes with its own dangers, such as potential signal loss, and a need considering careful parameter optimization. inside this work, we focus on the setting where this problem was especially acute -compressive sensing frameworks considering radio astronomy- and ask: should a precision of a data representation be lowered considering all inputs, with both recovery guarantees and practical performance? our first contribution was the theoretical analysis of a iterative hard thresholding (iht) algorithm when all input data, that is, a measurement matrix and a observation, are quantized aggressively to as little as 2 bits per value. under reasonable constraints, we show that there exists the variant of low precision iht that should still provide recovery guarantees. a second contribution was an analysis of our general quantized framework tailored to radio astronomy, showing that its conditions are satisfied inside this case. we evaluate our idea behind the method with the help of cpu and fpga implementations, and show that it should achieve up to 9.19x speed up with negligible loss of recovery quality, on real telescope data.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19138,"a lasso has been studied extensively as the tool considering estimating a coefficient vector inside a high-dimensional linear model; however, considerably less was known about estimating a error variance. indeed, most well-known theoretical properties of a lasso, including recent advances inside selective inference with a lasso, are established under a assumption that a underlying error variance was known. yet a error variance inside practice is, of course, unknown. inside this paper, we propose a natural lasso estimator considering a error variance, which maximizes the penalized likelihood objective. the key aspect of a natural lasso was that a likelihood was expressed inside terms of a natural parameterization of a multiparameter exponential family of the gaussian with unknown mean and variance. a result was the remarkably simple estimator with provably good performance inside terms of mean squared error. these theoretical results do not require placing any assumptions on a design matrix or a true regression coefficients. we also propose the companion estimator, called a organic lasso, which theoretically does not require tuning of a regularization parameter. both estimators do well compared to preexisting methods, especially inside settings where successful recovery of a true support of a coefficient vector was hard.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0
5252,"inside this note, we study a problem of uniqueness of ricci flow on complete noncompact manifolds. we consider a class of solutions with curvature bounded above by c/t when t > 0. inside paricular, we proved uniqueness if inside addition a initial curvature was of polynomial growth and ricci curvature of a flow was relatively small.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14561,"the novel effective hamiltonian inside a subspace of singly occupied states was obtained by applying a gutzwiller projection idea behind the method to the generalized hubbard model with a interactions between two nearest- neighbor sites. this model provides the more complete description of a physics of strongly correlated electron systems. a system was not necessarily inside the ferromagnetic state as temperature approaches zero at any doping level. a system, however, must be inside an antiferromagnetic state at a origin of a doping-temperature plane. moreover, a model exhibits superconductivity inside the doped region at sufficiently low temperatures. we summarize a studies and provide the phase diagram of a antiferromagnetism and a superconductivity of a model inside a doping-temperature plane here. details will be presented inside subsequent papers.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
16148,"inside a recent years impressive advances were made considering single image super-resolution. deep learning was behind the big part of this success. deep(er) architecture design and external priors modeling are a key ingredients. a internal contents of a low resolution input image was neglected with deep modeling despite a earlier works showing a power of with the help of such internal priors. inside this paper we propose the novel deep convolutional neural network carefully designed considering robustness and efficiency at both learning and testing. moreover, we propose the couple of model adaptation strategies to a internal contents of a low resolution input image and analyze their strong points and weaknesses. by trading runtime and with the help of internal priors we achieve 0.1 up to 0.3db psnr improvements over best reported results on standard datasets. our adaptation especially favors images with repetitive structures or under large resolutions. moreover, it should be combined with other simple techniques, such as back-projection or enhanced prediction, considering further improvements.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19285,"suppose a data consist of the set $s$ of points $x_j, 1 \leq j \leq j$, distributed inside the bounded domain $d \subset r^n$, where $n$ and $j$ are large numbers. inside this paper an algorithm was proposed considering checking whether there exists the manifold $\mathbb{m}$ of low dimension near which many of a points of $s$ lie and finding such $\mathbb{m}$ if it exists. there are many dimension reduction algorithms, both linear and non-linear. our algorithm was simple to implement and has some advantages compared with a known algorithms. if there was the manifold of low dimension near which most of a data points lie, a proposed algorithm will find it. some numerical results are presented illustrating a algorithm and analyzing its performance compared to a classical pca (principal component analysis) and isomap.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0
10921,"inside this paper, we develop the computationally efficient discrete approximation to log-gaussian cox process (lgcp) models considering a analysis of spatially aggregated disease count data. our idea behind the method overcomes an inherent limitation of spatial models based on markov structures, namely that each such model was tied to the specific partition of a study area, and allows considering spatially continuous prediction. we compare a predictive performance of our modelling idea behind the method with lgcp through the simulation study and an application to primary biliary cirrhosis incidence data inside newcastle-upon-tyne, uk. our results suggest that when disease risk was assumed to be the spatially continuous process, a proposed approximation to lgcp provides reliable estimates of disease risk both on spatially continuous and aggregated scales. a proposed methodology was implemented inside a open-source r package sdalgcp.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
5263,"inside this paper, we study a efficiency of egoistic and altruistic strategies within a model of social dynamics determined by voting inside the stochastic environment (the vise model) with the help of two criteria: maximizing a average capital increment and minimizing a number of bankrupt participants. a proposals are generated stochastically; three families of a corresponding distributions are considered: normal distributions, symmetrized pareto distributions, and student's $t$-distributions. it was found that a ""pit of losses"" paradox described earlier does not occur inside a case of heavy-tailed distributions. a egoistic strategy better protects agents from extinction inside aggressive environments than a altruistic ones, however, a efficiency of altruism was higher inside more favorable environments. the comparison of altruistic strategies with each other shows that inside aggressive environments, everyone should be supported to minimize extinction, while under more favorable conditions, it was more efficient to support a weakest participants. studying a dynamics of participants' capitals we identify situations where a two considered criteria contradict each other. at a next stage of a study, combined voting strategies and societies involving participants with selfish and altruistic strategies will be explored.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
2901,"the method was presented considering parallelizing a computation of solutions to discrete-time, linear-quadratic, finite-horizon optimal control problems, which we will refer to as lqr problems. this class of problem arises frequently inside robotic trajectory optimization. considering very complicated robots, a size of these resulting problems should be large enough that computing a solution was prohibitively slow when with the help of the single processor. fortunately, approaches to solving these type of problems based on numerical solutions to a kkt conditions of optimality offer the parallel solution method and should leverage multiple processors to compute solutions faster. however, these methods do not produce a useful feedback control policies that are generated as the by-product of a dynamic-programming solution method known as riccati recursion. inside this paper we derive the method which was able to parallelize a computation of riccati recursion, allowing considering super-fast solutions to a lqr problem while still generating feedback control policies. we demonstrate empirically that our method was faster than existing parallel methods.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
2914,"let $q\geq 2$ be an integer, and $\bbb f_q^d$, $d\geq 1$, be a vector space over a cyclic space $\bbb f_q$. a purpose of this paper was two-fold. first, we obtain sufficient conditions on $e \subset \bbb f_q^d$ such that a inverse fourier transform of $1_e$ generates the tight wavelet frame inside $l^2(\bbb f_q^d)$. we call these sets (tight) wavelet frame sets. a conditions are given inside terms of multiplicative and translational tilings, which was analogous with theorem 1.1 ([20]) by wang inside a setting of finite fields. inside a second part of a paper, we exhibit the constructive method considering obtaining tight wavelet frame sets inside $\bbb f_q^d$, $d\geq 2$, $q$ an odd prime and $q\equiv 3$ (mod 4).",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0
8238,"precise knowledge of a static density response function (sdrf) of a uniform electron gas (ueg) serves as key input considering numerous applications, most importantly considering density functional theory beyond generalized gradient approximations. here we extend a configuration path integral monte carlo (cpimc) formalism that is previously applied to a spatially uniform electron gas to a case of an inhomogeneous electron gas by adding the spatially periodic external potential. this procedure has recently been successfully used inside permutation blocking path integral monte carlo simulations (pb-pimc) of a warm dense electron gas [dornheim \textit{et al.}, phys. rev. e inside press, arxiv:1706.00315], but this method was restricted to low and moderate densities. implementing this procedure into cpimc allows us to obtain exact finite temperature results considering a sdrf of a electron gas at \textit{high to moderate densities} closing a gap left open by a pb-pimc data. inside this paper we demonstrate how a cpimc formalism should be efficiently extended to a spatially inhomogeneous electron gas and present a first data points. finally, we discuss finite size errors involved inside a quantum monte carlo results considering a sdrf inside detail and present the solution how to remove them that was based on the generalization of ground state techniques.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
9480,"inside this paper, we introduce the new idea behind the method to generate flexible parametric families of distributions. these models arise on competitive and complementary risks scenario, inside which a lifetime associated with the particular risk was not observable, rather, we observe only a minimum/maximum lifetime value among all risks. a latent variables have the zero truncated poisson distribution. considering a proposed family of distribution, a extra shape parameter has an important physical interpretation inside a competing and complementary risks scenario. a mathematical properties and inferential procedures are discussed. a proposed idea behind the method was applied inside some existing distributions inside which it was fully illustrated by an important data set.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16731,"inside a present study examines a statistical structure of a generated randomized density of a normal distribution and a cauchy distribution. a study put a allegation that the randomized probability density of a normal distribution should be regarded as a solution of a cauchy problem considering a heat equation, and randomized probability density of a cauchy distribution should be considered as the solution to a dirichlet problem considering a laplace equation. conversely, a solution of a cauchy problem considering a heat equation should be regarded as the randomized probability density of a normal distribution, and a solution of a dirichlet problem considering a laplace equation as randomized probability density of a cauchy distribution. a main objective of a study is a fact that considering each of these two cases to find a fisher information matrix components and structural tensor. we found nonlinear differential equations of a first, second and third order considering a density of a normal distribution and cauchy density computational difficulties to overcome . a components of a metric tensor (the fisher information matrix) and a components of a strain tensor are calculated according to formulas inside which there was a log-likelihood function, ie, logarithm of a density distribution. because of a positive definiteness of a fisher information matrix obtained inequality, which obviously satisfy a cauchy problem solution with nonnegative initial conditions inside a case of a laplace equation and a heat equation.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6206,"generic interacting many-body quantum systems are believed to behave as classical fluids on long time and length scales. due to rapid progress inside growing exceptionally pure crystals, we are now able to experimentally observe this collective motion of electrons inside solid-state systems, including graphene. we present the review of recent progress inside understanding a hydrodynamic limit of electronic motion inside graphene, written considering physicists from diverse communities. we begin by discussing a ""phase diagram"" of graphene, and a inevitable presence of impurities and phonons inside experimental systems. we derive hydrodynamics, both from the phenomenological perspective and with the help of kinetic theory. we then describe how hydrodynamic electron flow was visible inside electronic transport measurements. although we focus on graphene inside this review, a broader framework naturally generalizes to other materials. we assume only basic knowledge of condensed matter physics, and no prior knowledge of hydrodynamics.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0
8129,"we present an idea behind the method to synthesizing photographic images conditioned on semantic layouts. given the semantic label map, our idea behind the method produces an image with photographic appearance that conforms to a input layout. a idea behind the method thus functions as the rendering engine that takes the two-dimensional semantic specification of a scene and produces the corresponding photographic image. unlike recent and contemporaneous work, our idea behind the method does not rely on adversarial training. we show that photographic images should be synthesized from semantic layouts by the single feedforward network with appropriate structure, trained end-to-end with the direct regression objective. a presented idea behind the method scales seamlessly to high resolutions; we demonstrate this by synthesizing photographic images at 2-megapixel resolution, a full resolution of our training data. extensive perceptual experiments on datasets of outdoor and indoor scenes demonstrate that images synthesized by a presented idea behind the method are considerably more realistic than alternative approaches. a results are shown inside a supplementary video at this https url",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5496,"we present the robust real-time lidar 3d object detector that leverages heteroscedastic aleatoric uncertainties to significantly improve its detection performance. the multi-loss function was designed to incorporate uncertainty estimations predicted by auxiliary output layers. with the help of our proposed method, a network ignores to train from noisy samples, and focuses more on informative ones. we validate our method on a kitti object detection benchmark. our method surpasses a baseline method which does not explicitly approximate uncertainties by up to nearly 9% inside terms of average precision (ap). it also produces state-of-the-art results compared to other methods while running with an inference time of only 72 ms. inside addition, we conduct extensive experiments to understand how aleatoric uncertainties behave. extracting aleatoric uncertainties brings almost no additional computation cost during a deployment, making our method highly desirable considering autonomous driving applications.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
5112,"considering the balanced wall crossing inside geometric invariant theory, there exist derived equivalences between a corresponding git quotients if certain numerical conditions are satisfied. given such the wall crossing, i construct the perverse sheaf of categories on the disk, singular at the point, with half-monodromies recovering these equivalences, and with behaviour at a singular point controlled by the git quotient stack associated to a wall. taking complexified grothendieck groups gives the perverse sheaf of vector spaces: i characterise when this was an intersection cohomology complex of the local system on a punctured disk.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
7954,"sketching has emerged as the powerful technique considering speeding up problems inside numerical linear algebra, such as regression. inside a overconstrained regression problem, one was given an $n \times d$ matrix $a$, with $n \gg d$, as well as an $n \times 1$ vector $b$, and one wants to find the vector $\hat{x}$ so as to minimize a residual error $\|ax-b\|_2$. with the help of a sketch and solve paradigm, one first computes $s \cdot a$ and $s \cdot b$ considering the randomly chosen matrix $s$, then outputs $x' = (sa)^{\dagger} sb$ so as to minimize $\|sax' - sb\|_2$. a sketch-and-solve paradigm gives the bound on $\|x'-x^*\|_2$ when $a$ was well-conditioned. our main result was that, when $s$ was a subsampled randomized fourier/hadamard transform, a error $x' - x^*$ behaves as if it lies inside the ""random"" direction within this bound: considering any fixed direction $a\in \mathbb{r}^d$, we have with $1 - d^{-c}$ probability that \[ \langle a, x'-x^*\rangle \lesssim \frac{\|a\|_2\|x'-x^*\|_2}{d^{\frac{1}{2}-\gamma}}, \quad (1) \] where $c, \gamma > 0$ are arbitrary constants. this implies $\|x'-x^*\|_{\infty}$ was the factor $d^{\frac{1}{2}-\gamma}$ smaller than $\|x'-x^*\|_2$. it also gives the better bound on a generalization of $x'$ to new examples: if rows of $a$ correspond to examples and columns to features, then our result gives the better bound considering a error introduced by sketch-and-solve when classifying fresh examples. we show that not all oblivious subspace embeddings $s$ satisfy these properties. inside particular, we give counterexamples showing that matrices based on count-sketch or leverage score sampling do not satisfy these properties. we also provide lower bounds, both on how small $\|x'-x^*\|_2$ should be, and considering our new guarantee (1), showing that a subsampled randomized fourier/hadamard transform was nearly optimal.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14860,"large $n$ melonic theories are characterized by two-point function feynman diagrams built exclusively out of melons. this leads to conformal invariance at strong coupling, four-point function diagrams that are exclusively ladders, and higher-point functions that are built out of four-point functions joined together. we uncover an incredibly useful property of these theories: a six-point function, or equivalently, a three-point function of a primary $o(n)$ invariant bilinears, regarded as an analytic function of a operator dimensions, fully determines all correlation functions, to leading nontrivial order inside $1/n$, through simple feynman-like rules. a result was applicable to any theory, not necessarily melonic, inside which higher-point correlators are built out of four-point functions. we explicitly calculate a bilinear three-point function considering $q$-body syk, at any $q$. this leads to a bilinear four-point function, as well as all higher-point functions, expressed inside terms of higher-point conformal blocks, which we discuss. we find universality of correlators of operators of large dimension, which we simplify through the saddle point analysis. we comment on a implications considering a ads dual of syk.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
15826,"clustering methods based on deep neural networks have proven promising considering clustering real-world data because of their high representational power. inside this paper, we propose the systematic taxonomy of clustering methods that utilize deep neural networks. we base our taxonomy on the comprehensive review of recent work and validate a taxonomy inside the case study. inside this case study, we show that a taxonomy enables researchers and practitioners to systematically create new clustering methods by selectively recombining and replacing distinct aspects of previous methods with a goal of overcoming their individual limitations. a experimental evaluation confirms this and shows that a method created considering a case study achieves state-of-the-art clustering quality and surpasses it inside some cases.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8984,"we study a decentralized machine learning scenario where many users collaborate to learn personalized models based on (i) their local datasets and (ii) the similarity graph over a users' learning tasks. our idea behind the method trains nonlinear classifiers inside the multi-task boosting manner without exchanging personal data and with low communication costs. when background knowledge about task similarities was not available, we propose to jointly learn a personalized models and the sparse collaboration graph through an alternating optimization procedure. we analyze a convergence rate, memory consumption and communication complexity of our decentralized algorithms, and demonstrate a benefits of our idea behind the method compared to competing techniques on synthetic and real datasets.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1
1120,"semi-supervised learning was attracting increasing attention due to a fact that datasets of many domains lack enough labeled data. variational auto-encoder (vae), inside particular, has demonstrated a benefits of semi-supervised learning. a majority of existing semi-supervised vaes utilize the classifier to exploit label information, where a parameters of a classifier are introduced to a vae. given a limited labeled data, learning a parameters considering a classifiers may not be an optimal solution considering exploiting label information. therefore, inside this paper, we develop the novel idea behind the method considering semi-supervised vae without classifier. specifically, we propose the new model called semi-supervised disentangled vae (sdvae), which encodes a input data into disentangled representation and non-interpretable representation, then a category information was directly utilized to regularize a disentangled representation using a equality constraint. to further enhance a feature learning ability of a proposed vae, we incorporate reinforcement learning to relieve a lack of data. a dynamic framework was capable of dealing with both image and text data with its corresponding encoder and decoder networks. extensive experiments on image and text datasets demonstrate a effectiveness of a proposed framework.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14732,"we consider an online version of a robust principle component analysis (pca), which arises naturally inside time-varying source separations such as video foreground-background separation. this paper proposes the compressive online robust pca with prior information considering recursively separating the sequences of frames into sparse and low-rank components from the small set of measurements. inside contrast to conventional batch-based pca, which processes all a frames directly, a proposed method processes measurements taken from each frame. moreover, this method should efficiently incorporate multiple prior information, namely previous reconstructed frames, to improve a separation and thereafter, update a prior information considering a next frame. we utilize multiple prior information by solving $n\text{-}\ell_{1}$ minimization considering incorporating a previous sparse components and with the help of incremental singular value decomposition ($\mathrm{svd}$) considering exploiting a previous low-rank components. we also establish theoretical bounds on a number of measurements required to guarantee successful separation under assumptions of static or slowly-changing low-rank components. with the help of numerical experiments, we evaluate our bounds and a performance of a proposed algorithm. inside addition, we apply a proposed algorithm to online video foreground and background separation from compressive measurements. experimental results show that a proposed method outperforms a existing methods.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0
15612,"with the help of observations made with mosfire on keck i as part of a zfire survey, we present a stellar mass tully-fisher relation at 2.0 < z < 2.5. a sample is drawn from the stellar mass limited, ks-band selected catalog from zfourge over a candels area inside a cosmos field. we model a shear of a halpha emission line to derive rotational velocities at 2.2x a scale radius of an exponential disk (v2.2). we correct considering a blurring effect of the two-dimensional psf and a fact that a mosfire psf was better approximated by the moffat than the gaussian, which was more typically assumed considering natural seeing. we find considering a tully-fisher relation at 2.0 < z < 2.5 that logv2.2 =(2.18 +/- 0.051)+(0.193 +/- 0.108)(logm/msun - 10) and infer an evolution of a zeropoint of delta m/msun = -0.25 +/- 0.16 dex or delta m/msun = -0.39 +/- 0.21 dex compared to z = 0 when adopting the fixed slope of 0.29 or 1/4.5, respectively. we also derive a alternative kinematic estimator s0.5, with the best-fit relation logs0.5 =(2.06 +/- 0.032)+(0.211 +/- 0.086)(logm/msun - 10), and infer an evolution of delta m/msun= -0.45 +/- 0.13 dex compared to z < 1.2 if we adopt the fixed slope. we investigate and review various systematics, ranging from psf effects, projection effects, systematics related to stellar mass derivation, selection biases and slope. we find that discrepancies between a various literature values are reduced when taking these into account. our observations correspond well with a gradual evolution predicted by semi-analytic models.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14551,"several statistical approaches based on reproducing kernels have been proposed to detect abrupt changes arising inside a full distribution of a observations and not only inside a mean or variance. some of these approaches enjoy good statistical properties (oracle inequality, \ldots). nonetheless, they have the high computational cost both inside terms of time and memory. this makes their application difficult even considering small and medium sample sizes ($n< 10^4$). this computational issue was addressed by first describing the new efficient and exact algorithm considering kernel multiple change-point detection with an improved worst-case complexity that was quadratic inside time and linear inside space. it allows dealing with medium size signals (up to $n \approx 10^5$). second, the faster but approximation algorithm was described. it was based on the low-rank approximation to a gram matrix. it was linear inside time and space. this approximation algorithm should be applied to large-scale signals ($n \geq 10^6$). these exact and approximation algorithms have been implemented inside \texttt{r} and \texttt{c} considering various kernels. a computational and statistical performances of these new algorithms have been assessed through empirical experiments. a runtime of a new algorithms was observed to be faster than that of other considered procedures. finally, simulations confirmed a higher statistical accuracy of kernel-based approaches to detect changes that are not only inside a mean. these simulations also illustrate a flexibility of kernel-based approaches to analyze complex biological profiles made of dna copy number and allele b frequencies. an r package implementing a idea behind the method will be made available on github.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0
3635,"nickel oxide (nio) has been studied extensively considering various applications ranging from electrochemistry to solar cells [1,2]. inside recent years, nio attracted much attention as an antiferromagnetic (af) insulator material considering spintronic devices [3-10]. understanding a spin - phonon coupling inside nio was the key to its functionalization, and enabling af spintronics' promise of ultra-high-speed and low-power dissipation [11,12]. however, despite its status as an exemplary af insulator and the benchmark material considering a study of correlated electron systems, little was known about a spin - phonon interaction, and a associated energy dissipation channel, inside nio. inside addition, there was the long-standing controversy over a large discrepancies between a experimental and theoretical values considering a electron, phonon, and magnon energies inside nio [13-23]. this gap inside knowledge was explained by nio optical selection rules, high neel temperature and dominance of a magnon band inside a visible raman spectrum, which precludes the conventional idea behind the method considering investigating such interaction. here we show that by with the help of ultraviolet (uv) raman spectroscopy one should extract a spin - phonon coupling coefficients inside nio. we established that unlike inside other materials, a spins of ni atoms interact more strongly with a longitudinal optical (lo) phonons than with a transverse optical (to) phonons, and produce opposite effects on a phonon energies. a peculiarities of a spin - phonon coupling are consistent with a trends given by density functional theory calculations. a obtained results shed light on a nature of a spin - phonon coupling inside af insulators and may aid inside developing innovative spintronic devices.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
4935,"randomly censored survival data are frequently encountered inside applied sciences including biomedical or reliability applications and clinical trial analyses. testing a significance of statistical hypotheses was crucial inside such analyses to get conclusive inference but a existing likelihood based tests, under the fully parametric model, are extremely non-robust against outliers inside a data. although, there exists the few robust parameter estimators (e.g., m-estimators and minimum density power divergence estimators) given randomly censored data, there was hardly any robust testing procedure available inside a literature inside this context. one of a major difficulties inside this context was a construction of the suitable consistent estimator of a asymptotic variance of m estimators; a latter was the function of a unknown censoring distribution. inside this paper, we take a first step inside this direction by proposing the consistent estimator of asymptotic variance of a m-estimators based on randomly censored data without any assumption on a form of a censoring scheme. we then describe and study the class of robust wald-type tests considering parametric statistical hypothesis, both simple as well as composite, under such set-up, along with their general asymptotic and robustness properties. robust tests considering comparing two independent randomly censored samples and robust tests against one sided alternatives are also discussed. their advantages and usefulness are demonstrated considering a tests based on a minimum density power divergence estimators with specific attention to clinical trial analyses.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
1017,"opportunity detection at secondary transmitters (txs) was the key technique enabling cognitive radio (cr) networks. such detection however cannot guarantee reliable communication at secondary receivers (rxs), especially when their association distance was long. to cope with a issue, this paper proposes the novel mac called sense-and-predict (sap), where each secondary tx decides whether to access or not based on a prediction of a interference level at rx. firstly, we provide a spatial interference correlation inside the probabilistic form with the help of stochastic geometry, and utilize it to maximize a area spectral efficiency (ase) considering secondary networks while guaranteeing a service quality of primary networks. through simulations and testbed experiments with the help of usrp, sap was shown to always achieve ase improvement compared with a conventional tx based sensing.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
1929,"a human visual perception of a world was of the large fixed image that was highly detailed and sharp. however, receptor density inside a retina was not uniform: the small central region called a fovea was very dense and exhibits high resolution, whereas the peripheral region around it has much lower spatial resolution. thus, contrary to our perception, we are only able to observe the very small region around a line of sight with high resolution. a perception of the complete and stable view was aided by an attention mechanism that directs a eyes to a numerous points of interest within a scene. a eyes move between these targets inside quick, unconscious movements, known as ""saccades"". once the target was centered at a fovea, a eyes fixate considering the fraction of the second while a visual system extracts a necessary information. an artificial visual system is built based on the fully recurrent neural network set within the reinforcement learning protocol, and learned to attend to regions of interest while solving the classification task. a model was consistent with several experimentally observed phenomena, and suggests novel predictions.",1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17861,"domain adaptation was transfer learning which aims to generalize the learning model across training and testing data with different distributions. most previous research tackle this problem inside seeking the shared feature representation between source and target domains while reducing a mismatch of their data distributions. inside this paper, we propose the close yet discriminative domain adaptation method, namely cdda, which generates the latent feature representation with two interesting properties. first, a discrepancy between a source and target domain, measured inside terms of both marginal and conditional probability distribution using maximum mean discrepancy was minimized so as to attract two domains close to each other. more importantly, we also design the repulsive force term, which maximizes a distances between each label dependent sub-domain to all others so as to drag different class dependent sub-domains far away from each other and thereby increase a discriminative power of a adapted domain. moreover, given a fact that a underlying data manifold could have complex geometric structure, we further propose a constraints of label smoothness and geometric structure consistency considering label propagation. extensive experiments are conducted on 36 cross-domain image classification tasks over four public datasets. a comprehensive results show that a proposed method consistently outperforms a state-of-the-art methods with significant margins.",1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8288,"let $\ell$ be an odd prime and $d$ the positive integer. we determine when there exists the degree-$d$ number field $k$ and an elliptic curve $e/k$ with $j(e)\in\mathbb{q}\setminus\{0,1728\}$ considering which $e(k)_{\mathrm{tors}}$ contains the point of order $\ell$. we also determine when there exists such the pair $(k,e)$ considering which a image of a associated mod-$\ell$ galois representation was contained inside the cartan subgroup or its normalizer, conditionally on the conjecture of sutherland. we do a same under a stronger assumption that $e$ was defined over $\mathbb{q}$.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
2856,"we consider a cauchy problem considering a nonlinear schr√∂dinger equations (nls) with non-algebraic nonlinearities on a euclidean space. inside particular, we study a energy-critical nls on $\mathbb{r}^d$, $d=5,6$, and energy-critical nls without gauge invariance and prove that they are almost surely locally well-posed with respect to randomized initial data below a energy space. we also study a long time behavior of solutions to these equations: (i) we prove almost sure global well-posedness of a (standard) energy-critical nls on $\mathbb{r}^d$, $d = 5, 6$, inside a defocusing case, and (ii) we present the probabilistic construction of finite time blowup solutions to a energy-critical nls without gauge invariance below a energy space.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12197,"diffusion processes are governed by external triggers and internal dynamics inside complex systems. timely and cost-effective control of infectious disease spread critically relies on uncovering a underlying diffusion mechanisms, which was challenging due to invisible causality between events and their time-evolving intensity. we infer causal relationships between infections and quantify a reflexivity of the meta-population, a level of feedback on event occurrences by its internal dynamics (likelihood of the regional outbreak triggered by previous cases). these are enabled by our new proposed model, a latent influence point process (lipp) which models disease spread by incorporating macro-level internal dynamics of meta-populations based on human mobility. we analyse 15-year dengue cases inside queensland, australia. from our causal inference, outbreaks are more likely driven by statewide global diffusion over time, leading to complex behavior of disease spread. inside terms of reflexivity, precursory growth and symmetric decline inside populous regions was attributed to slow but persistent feedback on preceding outbreaks using inter-group dynamics, while abrupt growth but sharp decline inside peripheral areas was led by rapid but inconstant feedback using intra-group dynamics. our proposed model reveals probabilistic causal relationships between discrete events based on intra- and inter-group dynamics and also covers direct and indirect diffusion processes (contact-based and vector-borne disease transmissions).",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
3044,"a measurement of an exoplanet's oblateness and obliquity provides insights into a planet's internal structure and formation history. previous work with the help of small differences inside a shape of a transit light curve has been moderately successful, but is hampered by a small signal and extreme photometric precision required. a measurement of changes inside transit depth, caused by a spin precession of an oblate planet, is proposed as an alternative method. here, we present a first attempt to measure these changes. with the help of kepler photometry, we examined a brown dwarf kepler-39b and a warm saturn kepler-427b. we could not reliably constrain a oblateness of kepler-39b. we find transit depth variations considering kepler-427b at $90.1\%$ significance ($1.65\sigma$) consistent with the precession period of $p_\mathrm{prec} = 5.45^{+0.46}_{-0.37}~\mathrm{years}$ and an oblateness, $f~=~0.19^{+0.32}_{-0.16}$. this oblateness was comparable to solar system gas giants, and would raise questions about a dynamics and tidal synchronization of kepler-427b.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12371,"we present here predictions considering a spatial distribution of 21 cm brightness temperature fluctuations from high-dynamic-range simulations considering agn-dominated reionization histories that have been tested against available lyman-alpha and cmb data. we model agn by extrapolating a observed m-sigma relation to high redshifts and assign them ionizing emissivities consistent with recent uv luminosity function measurements. we assess a observability of a predicted spatial 21 cm fluctuations by ongoing and upcoming experiments inside a late stages of reionization inside a limit inside which a hydrogen 21 cm spin temperature was significantly larger than a cmb temperature. our agn-dominated reionization histories increase a variance of a 21 cm emission by the factor of up to ten compared to similar reionization histories dominated by faint galaxies, to values close to 100 mk^2 at scales accessible to experiments (k < 1 h/cmpc). this was lower than a sensitivity claimed to have been already reached by ongoing experiments by only the factor of about two or less. when reionization was dominated by agn, a 21 cm power spectrum was enhanced on all scales due to a enhanced bias of a clustering of a more massive haloes and a peak inside a large scale 21 cm power was strongly enhanced and moved to larger scales due to bigger characteristic bubble sizes. agn dominated reionization should be easily detectable by lofar (and later hera and ska1) at their design sensitivity, assuming successful foreground subtraction and instrument calibration. conversely, these could become a first non-trivial reionization scenarios to be ruled out by 21 cm experiments, thereby constraining a contribution of agn to reionization.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13843,"sampling was the fundamental topic inside graph signal processing, having found applications inside estimation, clustering, and video compression. inside contrast to traditional signal processing, a irregularity of a signal domain makes selecting the sampling set non-trivial and hard to analyze. indeed, though conditions considering graph signal interpolation from noiseless samples exist, they do not lead to the unique sampling set. a presence of noise makes choosing among these sampling sets the hard combinatorial problem. although greedy sampling schemes are commonly used inside practice, they have no performance guarantee. this work takes the twofold idea behind the method to address this issue. first, universal performance bounds are derived considering a bayesian approximation of graph signals from noisy samples. inside contrast to currently available bounds, they are not restricted to specific sampling schemes and hold considering any sampling sets. second, this paper provides near-optimal guarantees considering greedy sampling by introducing a concept of approximate submodularity and updating a classical greedy bound. it then provides explicit bounds on a approximate supermodularity of a interpolation mean-square error showing that it should be optimized with worst-case guarantees with the help of greedy search even though it was not supermodular. simulations illustrate a derived bound considering different graph models and show an application of graph signal sampling to reduce a complexity of kernel principal component analysis.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0
15640,"we show that considering neural network functions that have width less or equal to a input dimension all connected components of decision regions are unbounded. a result holds considering continuous and strictly monotonic activation functions as well as considering relu activation. this complements recent results on approximation capabilities of [hanin 2017 approximating] and connectivity of decision regions of [nguyen 2018 neural] considering such narrow neural networks. further, we give an example that negatively answers a question posed inside [nguyen 2018 neural] whether one of their main results still holds considering relu activation. our results are illustrated by means of numerical experiments.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
9002,"humans should ground natural language commands to tasks at both abstract and fine-grained levels of specificity. considering instance, the human forklift operator should be instructed to perform the high-level action, like ""grab the pallet"" or the low-level action like ""tilt back the little bit."" while robots are also capable of grounding language commands to tasks, previous methods implicitly assume that all commands and tasks reside at the single, fixed level of abstraction. additionally, methods that do not use multiple levels of abstraction encounter inefficient planning and execution times as they solve tasks at the single level of abstraction with large, intractable state-action spaces closely resembling real world complexity. inside this work, by grounding commands to all a tasks or subtasks available inside the hierarchical planning framework, we arrive at the model capable of interpreting language at multiple levels of specificity ranging from coarse to more granular. we show that a accuracy of a grounding procedure was improved when simultaneously inferring a degree of abstraction inside language used to communicate a task. leveraging hierarchy also improves efficiency: our proposed idea behind the method enables the robot to respond to the command within one second on 90% of our tasks, while baselines take over twenty seconds on half a tasks. finally, we demonstrate that the real, physical robot should ground commands at multiple levels of abstraction allowing it to efficiently plan different subtasks within a same planning hierarchy.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13229,"we give the direct, explicit and self-contained construction of the local lie groupoid integrating the given lie algebroid which only depends on a choice of the spray vector field lifting a underlying anchor map. this construction leads to the complete account of local lie theory and, inside particular, to the finite-dimensional proof of a fact that a category of germs of local lie groupoids was equivalent to that of lie algebroids.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7548,"as new instances of nested organization --beyond ecological networks-- are discovered, scholars are debating around a co-existence of two apparently incompatible macroscale architectures: nestedness and modularity. a discussion was far from being solved, mainly considering two reasons. first, nestedness and modularity appear to emerge from two contradictory dynamics, cooperation and competition. second, existing methods to assess a presence of nestedness and modularity are flawed when it comes to a evaluation of concurrently nested and modular structures. inside this work, we tackle a latter problem, presenting a concept of \textit{in-block nestedness}, the structural property determining to what extent the network was composed of blocks whose internal connectivity exhibits nestedness. we then put forward the set of optimization methods that allow us to identify such organization successfully, both inside synthetic and inside the large number of real networks. these findings challenge our understanding of a topology of ecological and social systems, calling considering new models to explain how such patterns emerge.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
16102,"note that the newer expanded version of this paper was now available at: arxiv:1802.03888 it was critical inside many applications to understand what features are important considering the model, and why individual predictions were made. considering tree ensemble methods these questions are usually answered by attributing importance values to input features, either globally or considering the single prediction. here we show that current feature attribution methods are inconsistent, which means changing a model to rely more on the given feature should actually decrease a importance assigned to that feature. to address this problem we develop fast exact solutions considering shap (shapley additive explanation) values, which were recently shown to be a unique additive feature attribution method based on conditional expectations that was both consistent and locally accurate. we integrate these improvements into a latest version of xgboost, demonstrate a inconsistencies of current methods, and show how with the help of shap values results inside significantly improved supervised clustering performance. feature importance values are the key part of understanding widely used models such as gradient boosting trees and random forests, so improvements to them have broad practical implications.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16910,"inside finite mixture models, apart from underlying mixing measure, true kernel density function of each subpopulation inside a data is, inside many scenarios, unknown. perhaps a most popular idea behind the method was to choose some kernel functions that we empirically believe our data are generated from and use these kernels to fit our models. nevertheless, as long as a chosen kernel and a true kernel are different, statistical inference of mixing measure under this setting will be highly unstable. to overcome this challenge, we propose flexible and efficient robust estimators of a mixing measure inside these models, which are inspired by a idea of minimum hellinger distance estimator, model selection criteria, and superefficiency phenomenon. we demonstrate that our estimators consistently recover a true number of components and achieve a optimal convergence rates of parameter approximation under both a well- and mis-specified kernel settings considering any fixed bandwidth. these desirable asymptotic properties are illustrated using careful simulation studies with both synthetic and real data.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
13071,"a paper addresses a stability of a co-authorship networks inside time. a analysis was done on a networks of slovenian researchers inside two time periods (1991-2000 and 2001-2010). two researchers are linked if they published at least one scientific bibliographic unit inside the given time period. as proposed by kronegger et al. (2011), a global network structures are examined by generalized blockmodeling with a assumed multi-core--semi-periphery--periphery blockmodel type. a term core denotes the group of researchers who published together inside the systematic way with each other. a obtained blockmodels are comprehensively analyzed by visualizations and through considering several statistics regarding a global network structure. to measure a stability of a obtained blockmodels, different adjusted modified rand and wallace indices are applied. those enable to distinguish between a splitting and merging of cores when operationalizing a stability of cores. also, a adjusted modified indices should be used when new researchers occur inside a second time period (newcomers) and when some researchers are no longer present inside a second time period (departures). a research disciplines are described and clustered according to a values of these indices. considering a obtained clusters, a sources of instability of a research disciplines are studied (e.g., merging or splitting of cores, newcomers or departures). furthermore, a differences inside a stability of a obtained cores on a level of scientific disciplines are studied by linear regression analysis where some personal characteristics of a researchers (e.g., age, gender), are also considered.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9130,"gaussian processes (gps) are flexible models that should capture complex structure inside large-scale dataset due to their non-parametric nature. however, a usage of gps inside real-world application was limited due to their high computational cost at inference time. inside this paper, we introduce the new framework, \textit{kernel distillation}, to approximate the fully trained teacher gp model with kernel matrix of size $n\times n$ considering $n$ training points. we combine inducing points method with sparse low-rank approximation inside a distillation procedure. a distilled student gp model only costs $o(m^2)$ storage considering $m$ inducing points where $m \ll n$ and improves a inference time complexity. we demonstrate empirically that kernel distillation provides better trade-off between a prediction time and a test performance compared to a alternatives.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18580,"we propose the family of variational approximations to bayesian posterior distributions, called $\alpha$-vb, with provable statistical guarantees. a standard variational approximation was the special case of $\alpha$-vb with $\alpha=1$. when $\alpha \in(0,1]$, the novel class of variational inequalities are developed considering linking a bayes risk under a variational approximation to a objective function inside a variational optimization problem, implying that maximizing a evidence lower bound inside variational inference has a effect of minimizing a bayes risk within a variational density family. operating inside the frequentist setup, a variational inequalities imply that point estimates constructed from a $\alpha$-vb procedure converge at an optimal rate to a true parameter inside the wide range of problems. we illustrate our general theory with the number of examples, including a mean-field variational approximation to (low)-high-dimensional bayesian linear regression with spike and slab priors, mixture of gaussian models, latent dirichlet allocation, and (mixture of) gaussian variational approximation inside regular parametric models.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0
723,"this paper presents two unsupervised learning layers (ul layers) considering label-free video analysis: one considering fully connected layers, and a other considering convolutional ones. a proposed ul layers should play two roles: they should be a cost function layer considering providing global training signal; meanwhile they should be added to any regular neural network layers considering providing local training signals and combined with a training signals backpropagated from upper layers considering extracting both slow and fast changing features at layers of different depths. therefore, a ul layers should be used inside either pure unsupervised or semi-supervised settings. both the closed-form solution and an online learning algorithm considering two ul layers are provided. experiments with unlabeled synthetic and real-world videos demonstrated that a neural networks equipped with ul layers and trained with a proposed online learning algorithm should extract shape and motion information from video sequences of moving objects. a experiments demonstrated a potential applications of ul layers and online learning algorithm to head orientation approximation and moving object localization.",1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14904,"spectral clustering has become one of a most widely used clustering techniques when a structure of a individual clusters was non-convex or highly anisotropic. yet, despite its immense popularity, there exists fairly little theory about performance guarantees considering spectral clustering. this issue was partly due to a fact that spectral clustering typically involves two steps which complicated its theoretical analysis: first, a eigenvectors of a associated graph laplacian are used to embed a dataset, and second, k-means clustering algorithm was applied to a embedded dataset to get a labels. this paper was devoted to a theoretical foundations of spectral clustering and graph cuts. we consider the convex relaxation of graph cuts, namely ratio cuts and normalized cuts, that makes a usual two-step idea behind the method of spectral clustering obsolete and at a same time gives rise to the rigorous theoretical analysis of graph cuts and spectral clustering. we derive deterministic bounds considering successful spectral clustering using the spectral proximity condition that naturally depends on a algebraic connectivity of each cluster and a inter-cluster connectivity. moreover, we demonstrate by means of some popular examples that our bounds should achieve near-optimality. our findings are also fundamental considering a theoretical understanding of kernel k-means. numerical simulations confirm and complement our analysis.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19012,"when investigators seek to approximate causal effects, they often assume that selection into treatment was based only on observed covariates. under this identification strategy, analysts must adjust considering observed confounders. while basic regression models have long been a dominant method of statistical adjustment, more robust methods based on matching or weighting have become more common. of late, even more flexible methods based on machine learning methods have been developed considering statistical adjustment. these machine learning methods are designed to be black box methods with little input from a researcher. recent research used the data competition to evaluate various methods of statistical adjustment and found that black box methods out performed all other methods of statistical adjustment. matching methods with covariate prioritization are designed considering direct input from substantive investigators inside direct contrast to black methods. inside this article, we use the different research design to compare matching with covariate prioritization to black box methods. we use black box methods to replicate results from five studies where matching with covariate prioritization is used to customize a statistical adjustment inside direct response to substantive expertise. we find little difference across a methods. we conclude with advice considering investigators.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7234,"inside recent years, stochastic gradient descent (sgd) based techniques has become a standard tools considering training neural networks. however, formal theoretical understanding of why sgd should train neural networks inside practice was largely missing. inside this paper, we make progress on understanding this mystery by providing the convergence analysis considering sgd on the rich subset of two-layer feedforward networks with relu activations. this subset was characterized by the special structure called ""identity mapping"". we prove that, if input follows from gaussian distribution, with standard $o(1/\sqrt{d})$ initialization of a weights, sgd converges to a global minimum inside polynomial number of steps. unlike normal vanilla networks, a ""identity mapping"" makes our network asymmetric and thus a global minimum was unique. to complement our theory, we are also able to show experimentally that multi-layer networks with this mapping have better performance compared with normal vanilla networks. our convergence theorem differs from traditional non-convex optimization techniques. we show that sgd converges to optimal inside ""two phases"": inside phase i, a gradient points to a wrong direction, however, the potential function $g$ gradually decreases. then inside phase ii, sgd enters the nice one point convex region and converges. we also show that a identity mapping was necessary considering convergence, as it moves a initial point to the better place considering optimization. experiment verifies our claims.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
12899,"regularization techniques are widely employed inside optimization-based approaches considering solving ill-posed inverse problems inside data analysis and scientific computing. these methods are based on augmenting a objective with the penalty function, which was specified based on prior domain-specific expertise to induce the desired structure inside a solution. we consider a problem of learning suitable regularization functions from data inside settings inside which precise domain knowledge was not directly available. previous work under a title of `dictionary learning' or `sparse coding' may be viewed as learning the regularization function that should be computed using linear programming. we describe generalizations of these methods to learn regularizers that should be computed and optimized using semidefinite programming. our framework considering learning such semidefinite regularizers was based on obtaining structured factorizations of data matrices, and our algorithmic idea behind the method considering computing these factorizations combines recent techniques considering rank minimization problems along with an operator analog of sinkhorn scaling. under suitable conditions on a input data, our algorithm provides the locally linearly convergent method considering identifying a correct regularizer that promotes a type of structure contained inside a data. our analysis was based on a stability properties of operator sinkhorn scaling and their relation to geometric aspects of determinantal varieties (in particular tangent spaces with respect to these varieties). a regularizers obtained with the help of our framework should be employed effectively inside semidefinite programming relaxations considering solving inverse problems.",1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0
15932,"we used a gemini multi-object spectrograph integral field unit to map a gas distribution, excitation and kinematics within a inner kiloparsec of four nearby low-luminosity active galaxies: ngc3982, ngc4501, ngc2787 and ngc4450. a observations cover a spectral range 5600-7000{\aa} at the velocity resolution of 120km/s and spatial resolution ranging from 50 to 70pc at a galaxies. extended emission inside h{\alpha}, [nii]{\lambda}{\lambda}6548,6583, [sii]{\lambda}{\lambda}6716,6730 over most of a field-of-view was observed considering all galaxies, while only ngc3982 shows [oi]{\lambda}6300 extended emission. a h{\alpha} equivalent widths combined with a [nii]/h{\alpha} line ratios reveal that ngc3982 and ngc4450 harbor seyfert nuclei surrounded by regions with liner excitation, while ngc2787 and ngc4501 harbor liner nuclei. ngc3982 shows the partial ring of recent star-formation at 500pc from a nucleus, while inside ngc4501 the region at 500pc west of a nucleus shows liner excitation but has been interpreted as an aging hii region with a gas excitation dominated by shocks from supernovae. a line-of-sight velocity field of a gas shows the rotation pattern considering all galaxies, with deviations from pure disk rotation observed inside ngc3982, ngc4501 and ngc4450. considering ngc4501 and ngc4450, many of these deviations are spatially coincident with dust structures seen inside optical continuum images, leading to a interpretation that a deviations are due to shocks inside a gas traced by a dust. the speculation was that these shocks lead to loss of angular momentum, allowing a gas to be transferred inwards to feed a agn. inside a case of ngc2787, instead of deviations inside a rotation field, we see the misalignment of 40{^\circ} between a orientation of a line of nodes of a gas rotation and a photometric major axis of a galaxy. evidence of compact nuclear outflows are seen inside ngc4501 and ngc4450.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7660,"we present the search considering metal absorption line systems at a highest redshifts to date with the help of the deep (30h) vlt/x-shooter spectrum of a z = 7.084 quasi-stellar object (qso) ulas j1120+0641. we detect seven intervening systems at z > 5.5, with a highest-redshift system being the c iv absorber at z = 6.51. we find tentative evidence that a mass density of c iv remains flat or declines with redshift at z < 6, while a number density of c ii systems remains relatively flat over 5 < z < 7. these trends are broadly consistent with models of chemical enrichment by star formation-driven winds that include the softening of a ultraviolet background towards higher redshifts. we find the larger number of weak ( w_rest < 0.3a ) mg ii systems over 5.9 < z < 7.0 than predicted by the power-law fit to a number density of stronger systems. this was consistent with trends inside a number density of weak mg ii systems at z = 2.5, and suggests that a mechanisms that create these absorbers are already inside place at z = 7. finally, we investigate a associated narrow si iv, c iv, and n v absorbers located near a qso redshift, and find that at least one component shows evidence of partial covering of a continuum source.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6191,"we study electroweak scale dark matter (dm) whose interactions with baryonic matter are mediated by the heavy anomalous $z'$. we emphasize that when a dm was the majorana particle, its low-velocity annihilations are dominated by loop suppressed annihilations into a gauge bosons, rather than by p-wave or chirally suppressed annihilations into a sm fermions. because a $z'$ was anomalous, these kinds of dm models should be realized only as effective field theories (efts) with the well-defined cutoff, where heavy spectator fermions restore gauge invariance at high energies. we formulate these efts, approximate their cutoff and properly take into account a effect of a chern-simons terms one obtains after a spectator fermions are integrated out. we find that, while considering light dm collider and direct detection experiments usually provide a strongest bounds, a bounds at higher masses are heavily dominated by indirect detection experiments, due to strong annihilation into $w^+w^-$, $zz$, $z\gamma$ and possibly into $gg$ and $\gamma\gamma$. we emphasize that these annihilation channels are generically significant because of a structure of a eft, and therefore these models are prone to strong indirect detection constraints. even though we focus on selected $z'$ models considering illustrative purposes, our setup was completely generic and should be used considering analyzing a predictions of any anomalous $z'$-mediated dm model with arbitrary charges.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5100,"distribution of black spot defects and small clusters inside 1 mev krypton irradiated 3c-sic has been investigated with the help of advanced scanning transmission electron microscopy (stem) and tem. we find that two thirds of clusters smaller than 1 nm identified inside stem are invisible inside tem images. considering clusters that are larger than 1 nm, stem and tem results match very well. the cluster dynamics model has been developed considering sic to reveal processes that contribute to evolution of defect clusters and validated against a (s)tem results. simulations showed that the model based on established properties of point defects (pds) generation, reaction, clustering, and cluster dissociation, was unable to predict black spot defects distribution consistent with stem observations. this failure suggests that additional phenomena not included inside the simple point-defect picture may contribute to radiation-induced evolution of defect clusters inside sic and with the help of our model we have determined a effects of the number of these additional phenomena on cluster evolution. with the help of these additional phenomena it was possible to fit parameters within physically justifiable ranges that yield agreement between cluster distributions predicted by simulations and those measured experimentally.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
133,"a survey of a mid-infrared sky by a wide-field infrared survey explorer (wise) led to a discovery of extremely cold low-mass brown dwarfs, classified as y dwarfs, which extend a t class to lower temperatures. twenty-four y dwarfs are known at a time of writing. here we present improved parallaxes considering four of these, determined with the help of spitzer images. we give new photometry considering four late-type t and three y dwarfs, and new spectra of three y dwarfs, obtained at gemini observatory. we also present previously unpublished photometry taken from hst, eso, spitzer and wise archives of 11 late-type t and 9 y dwarfs. a near-infrared data are put on to a same photometric system, forming the homogeneous data set considering a coolest brown dwarfs. we compare recent models to our photometric and spectroscopic data set. we confirm that non-equilibrium atmospheric chemistry was important considering these objects. non-equilibrium cloud-free models reproduce well a near-infrared spectra and mid-infrared photometry considering a warmer y dwarfs with 425 <= t_eff k <= 450. the small amount of cloud cover may improve a model fits inside a near-infrared considering a y dwarfs with 325 <= t_eff k <= 375. neither cloudy nor cloud-free models reproduce a near-infrared photometry considering a t_eff = 250 k y dwarf w0855. we use a mid-infrared region, where most of a flux originates, to constrain our models of w0855. we find that w0855 likely has the mass of 1.5 - 8 jupiter masses and an age of 0.3 - 6 gyr. a y dwarfs with measured parallaxes are within 20 pc of a sun and have tangential velocities typical of a thin disk. a metallicities and ages we derive considering a sample are generally solar-like. we approximate that a known y dwarfs are 3 to 20 jupiter-mass objects with ages of 0.6 to 8.5 gyr.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3258,"energy consumption considering hot water production was the major draw inside high efficiency buildings. optimizing this has typically been approached from the thermodynamics perspective, decoupled from occupant influence. furthermore, optimization usually presupposes existence of the detailed dynamics model considering a hot water system. these assumptions lead to suboptimal energy efficiency inside a real world. inside this paper, we present the novel reinforcement learning based methodology which optimizes hot water production. a proposed methodology was completely generalizable, and does not require an offline step or human domain knowledge to build the model considering a hot water vessel or a heating element. occupant preferences too are learnt on a fly. a proposed system was applied to the set of 32 houses inside a netherlands where it reduces energy consumption considering hot water production by roughly 20% with no loss of occupant comfort. extrapolating, this translates to absolute savings of roughly 200 kwh considering the single household on an annual basis. this performance should be replicated to any domestic hot water system and optimization objective, given that a fairly minimal requirements on sensor data are met. with millions of hot water systems operational worldwide, a proposed framework has a potential to reduce energy consumption inside existing and new systems on the multi gigawatt-hour scale inside a years to come.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
877,"causal mediation analysis aims to approximate a natural direct and indirect effects under clearly specified assumptions. traditional mediation analysis based on ordinary least squares (ols) relies on a absence of unmeasured causes of a putative mediator and outcome. when this assumption cannot be justified, instrumental variables (iv) estimators should be used inside order to produce an asymptotically unbiased estimator of a mediator-outcome link. however, provided that valid instruments exist, bias removal comes at a cost of variance inflation considering standard iv procedures such as two-stage least squares (tsls). the semi-parametric stein-like (spsl) estimator has been proposed inside a literature that strikes the natural trade-off between a unbiasedness of a tsls procedure and a relatively small variance of a ols estimator. moreover, a spsl has a advantage that its shrinkage parameter should be directly estimated from a data. inside this paper, we demonstrate how this stein-like estimator should be implemented inside a context of a approximation of natural direct and natural indirect effects of treatments inside randomized controlled trials. a performance of a competing methods was studied inside the simulation study, inside which both a strength of hidden confounding and a strength of a instruments are independently varied. these considerations are motivated by the trial inside mental health evaluating a impact of the primary care-based intervention to reduce depression inside a elderly.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
2259,"we introduce a cluster-eagle (c-eagle) simulation project, the set of cosmological hydrodynamical zoom simulations of a formation of $30$ galaxy clusters inside a mass range $10^{14}<m_{200}/\mathrm{m}_{\odot}<10^{15.4}$ that incorporates a hydrangea sample of bah√© et al. (2017). a simulations adopt a state-of-the-art eagle galaxy formation model, with the gas particle mass of $1.8\times10^{6}\,\mathrm{m}_{\odot}$ and physical softening length of $0.7\,\mathrm{kpc}$. inside this paper, we introduce a sample and present a low-redshift global properties of a clusters. we calculate a x-ray properties inside the manner consistent with observational techniques, demonstrating a bias and scatter introduced by with the help of estimated masses. we find a total stellar content and black hole masses of a clusters to be inside good agreement with a observed relations. however, a clusters are too gas rich, suggesting that a agn feedback model was not efficient enough at expelling gas from a high-redshift progenitors of a clusters. a x-ray properties, such as a spectroscopic temperature and a soft-band luminosity, and a sunyaev-zel'dovich properties are inside reasonable agreement with a observed relations. however, a clusters have too high central temperatures and larger-than-observed entropy cores, which was likely driven by a agn feedback after a cluster core has formed. a total metal content and its distribution throughout a icm are the good match to a observations.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
616,"end-to-end (e2e) systems have achieved competitive results compared to conventional hybrid hidden markov model (hmm)-deep neural network based automatic speech recognition (asr) systems. such e2e systems are attractive due to a lack of dependence on alignments between input acoustic and output grapheme or hmm state sequence during training. this paper explores a design of an asr-free end-to-end system considering text query-based keyword search (kws) from speech trained with minimal supervision. our e2e kws system consists of three sub-systems. a first sub-system was the recurrent neural network (rnn)-based acoustic auto-encoder trained to reconstruct a audio through the finite-dimensional representation. a second sub-system was the character-level rnn language model with the help of embeddings learned from the convolutional neural network. since a acoustic and text query embeddings occupy different representation spaces, they are input to the third feed-forward neural network that predicts whether a query occurs inside a acoustic utterance or not. this e2e asr-free kws system performs respectably despite lacking the conventional asr system and trains much faster.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19407,"gaussian belief propagation (bp) has been widely used considering distributed approximation inside large-scale networks such as a smart grid, communication networks, and social networks, where local measurements/observations are scattered over the wide geographical area. however, a convergence of gaus- sian bp was still an open issue. inside this paper, we consider a convergence of gaussian bp, focusing inside particular on a convergence of a information matrix. we show analytically that a exchanged message information matrix converges considering arbitrary positive semidefinite initial value, and its dis- tance to a unique positive definite limit matrix decreases exponentially fast.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5011,"when performing the conceptual analysis of the concept, philosophers are interested inside all forms of expression of the concept inside the text---be it direct or indirect, explicit or implicit. inside this paper, we experiment with topic-based methods of automating a detection of concept expressions inside order to facilitate philosophical conceptual analysis. we propose six methods based on lda, and evaluate them on the new corpus of court decision that we had annotated by experts and non-experts. our results indicate that these methods should yield important improvements over a keyword heuristic, which was often used as the concept detection heuristic inside many contexts. while more work remains to be done, this indicates that detecting concepts through topics should serve as the general-purpose method considering at least some forms of concept expression that are not captured with the help of naive keyword approaches.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14706,"this monograph aims at providing an introduction to key concepts, algorithms, and theoretical results inside machine learning. a treatment concentrates on probabilistic models considering supervised and unsupervised learning problems. it introduces fundamental concepts and algorithms by building on first principles, while also exposing a reader to more advanced topics with extensive pointers to a literature, within the unified notation and mathematical framework. a material was organized according to clearly defined categories, such as discriminative and generative models, frequentist and bayesian approaches, exact and approximate inference, as well as directed and undirected models. this monograph was meant as an entry point considering researchers with the background inside probability and linear algebra.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2150,"inside recent years, mems inertial sensors (3d accelerometers and 3d gyroscopes) have become widely available due to their small size and low cost. inertial sensor measurements are obtained at high sampling rates and should be integrated to obtain position and orientation information. these estimates are accurate on the short time scale, but suffer from integration drift over longer time scales. to overcome this issue, inertial sensors are typically combined with additional sensors and models. inside this tutorial we focus on a signal processing aspects of position and orientation approximation with the help of inertial sensors. we discuss different modeling choices and the selected number of important algorithms. a algorithms include optimization-based smoothing and filtering as well as computationally cheaper extended kalman filter and complementary filter implementations. a quality of their estimates was illustrated with the help of both experimental and simulated data.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1
2582,"representing a semantic relations that exist between two given words (or entities) was an important first step inside the wide-range of nlp applications such as analogical reasoning, knowledge base completion and relational information retrieval. the simple, yet surprisingly accurate method considering representing the relation between two words was to compute a vector offset (\pairdiff) between their corresponding word embeddings. despite a empirical success, it remains unclear as to whether \pairdiff was a best operator considering obtaining the relational representation from word embeddings. we conduct the theoretical analysis of generalised bilinear operators that should be used to measure a $\ell_{2}$ relational distance between two word-pairs. we show that, if a word embeddings are standardised and uncorrelated, such an operator will be independent of bilinear terms, and should be simplified to the linear form, where \pairdiff was the special case. considering numerous word embedding types, we empirically verify a uncorrelation assumption, demonstrating a general applicability of our theoretical result. moreover, we experimentally discover \pairdiff from a bilinear relation composition operator on several benchmark analogy datasets.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11667,"why are generative adversarial networks (gans) so popular? what was a purpose of designing gans? should we justify functioning of gans theoretically? how are a theoretical guarantees? are there any shortcomings? with a popularity of gans, a researchers across a globe have been perplexed by these questions. inside a last year (2017), the plethora of research papers attempted to answer a above questions. inside this article, we put inside our best efforts to compare and contrast different results and put forth the summary of theoretical contributions about gans with focus on image/visual applications. our main aim was to highlight a primary issues related to gans that each of these papers examine. besides we provide insight into how each of a discussed articles solve a concerned problems. we expect this summary paper to give the bird's eye view to the person wishing to understand a theory behind gans.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8376,"inside this paper we introduce raduls2, a fastest parallel sorter based on radix algorithm. it was optimized to process huge amounts of data making use of modern multicore cpus. a main novelties include: extremely optimized algorithm considering handling tiny arrays (up to about the hundred of records) that could appear even billions times as subproblems to handle and improved processing of larger subarrays with better use of non-temporal memory stores.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3080,"a paper was concerned with a bose-einstein condensate described by a attractive gross-pitaevskii equation inside r 2 , where a external potential was unbounded from below. we show that when a interaction strength increases to the critical value, a gross-pitaevskii minimizer collapses to one singular point and we analyze a details of a collapse exactly up to a leading order.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17344,"\begin{abstract} we model individual t2dm patient blood glucose level (bgl) by stochastic process with discrete number of states mainly but not solely governed by medication regimen (e.g. insulin injections). bgl states change otherwise according to various physiological triggers which render the stochastic, statistically unknown, yet assumed to be quasi-stationary, nature of a process. inside order to express incentive considering being inside desired healthy bgl we heuristically define the reward function which returns positive values considering desirable bg levels and negative values considering undesirable bg levels. a state space consists of sufficient number of states inside order to allow considering memoryless assumption. this, inside turn, allows to formulate markov decision process (mdp), with an objective to maximize a total reward, summarized over the long run. a probability law was found by model-based reinforcement learning (rl) and a optimal insulin treatment policy was retrieved from mdp solution.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6000,"inside this study, a wind data series from five locations inside aegean sea islands, a most active `hotspots' inside terms of refugee influx during a oct/2015 - jan/2016 period, are investigated. a analysis of a three-per-site data series includes standard statistical analysis and parametric distributions, auto-correlation analysis, cross-correlation analysis between a sites, as well as various arma models considering estimating a feasibility and accuracy of such spatio-temporal linear regressors considering predictive analytics. strong correlations are detected across specific sites and appropriately trained arma(7,5) models achieve 1-day look-ahead error (rmse) of less than 1.9 km/h on average wind speed. a results show that such data-driven statistical approaches are extremely useful inside identifying unexpected and sometimes counter-intuitive associations between a available spatial data nodes, which was very important when designing corresponding models considering short-term forecasting of sea condition, especially average wave height and direction, which was inside fact what defines a associated weather risk of crossing these passages inside refugee influx patterns.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11387,"a present paper considers testing an erdos--renyi random graph model against the stochastic block model inside a asymptotic regime where a average degree of a graph grows with a graph size n. our primary interest lies inside those cases inside which a signal-to-noise ratio was at the constant level. focusing on symmetric two block alternatives, we first derive joint central limit theorems considering linear spectral statistics of power functions considering properly rescaled graph adjacency matrices under both a null and local alternative hypotheses. a powers inside a linear spectral statistics are allowed to grow to infinity together with a graph size. inside addition, we show that linear spectral statistics of chebyshev polynomials are closely connected to signed cycles of growing lengths that determine a asymptotic likelihood ratio test considering a hypothesis testing problem of interest. this enables us to construct the sequence of test statistics that achieves a exact optimal asymptotic power within $o(n^3 \log n)$ time complexity inside a contiguous regime when $n^2 p_{n,av}^3 \to\infty$ where $p_{n,av}$ was a average connection probability. we further propose the class of adaptive tests that are computationally tractable and completely data-driven. they achieve nontrivial powers inside a contiguous regime and consistency inside a singular regime whenever $n p_{n,av} \to\infty$. these tests remain powerful when a alternative becomes the more general stochastic block model with more than two blocks.",1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0
170,"deep reinforcement learning yields great results considering the large array of problems, but models are generally retrained anew considering each new problem to be solved. prior learning and knowledge are difficult to incorporate when training new models, requiring increasingly longer training as problems become more complex. this was especially problematic considering problems with sparse rewards. we provide the solution to these problems by introducing concept network reinforcement learning (cnrl), the framework which allows us to decompose problems with the help of the multi-level hierarchy. concepts inside the concept network are reusable, and flexible enough to encapsulate feature extractors, skills, or other concept networks. with this hierarchical learning approach, deep reinforcement learning should be used to solve complex tasks inside the modular way, through problem decomposition. we demonstrate a strength of cnrl by training the model to grasp the rectangular prism and precisely stack it on top of the cube with the help of the gripper on the kinova jaco arm, simulated inside mujoco. our experiments show that our use of hierarchy results inside the 45x reduction inside environment interactions compared to a state-of-the-art on this task.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
1857,"a upcoming launch of a first space based x-ray polarimeter inside $\sim 40$ years will provide powerful new diagnostic information to study accreting compact objects. inside particular, analysis of rapid variability of a polarisation degree and angle will provide a opportunity to probe a relativistic motions of material inside a strong gravitational fields close to a compact objects, and enable new methods to measure black hole and neutron star parameters. however, polarisation properties are measured inside the statistical sense, and the statistically significant polarisation detection requires the fairly long exposure, even considering a brightest objects. therefore, a sub-minute timescales of interest are not accessible with the help of the direct time-resolved analysis of polarisation degree and angle. phase-folding should be used considering coherent pulsations, but not considering stochastic variability such as quasi-periodic oscillations. here, we introduce the fourier method that enables statistically robust detection of stochastic polarisation variability considering arbitrarily short variability timescales. our method was analogous to commonly used spectral-timing techniques. we find that it should be possible inside a near future to detect a quasi-periodic swings inside polarisation angle predicted by lense-thirring precession of a inner accretion flow. this was contingent on a mean polarisation degree of a source being greater than $\sim 4-5\%$, which was consistent with a best current constraints on cygnus x-1 from a late 1970s.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
8005,"a colliding cluster, ciza j2242.8+5301, displays the spectacular, almost 2 mpc long shock front with the radio based mach number m ~ 5, that was puzzlingly large compared with a x-ray approximate of m ~ 2.5. a extent to which a x-ray temperature jump was diluted by cooler unshocked gas projected through a cluster currently lacks quantification. thus, here we apply our self-consistent n-body/hydro-dynamical code (based on flash) to model this binary cluster encounter. we should account considering a location of a shock front and also a elongated x-ray emission by tidal stretching of a gas and dark matter between a two cluster centers. a required total mass was $8.9 \times 10^{14}$ msun with the 1.3:1 mass ratio favoring a southern cluster component. a relative velocity we derive was $\simeq 2500$ km/s initially between a two main cluster components, with an impact parameter of 120 kpc. this solution implies that a shock temperature jump derived from a low angular resolution x-ray satellite suzaku was underestimated by the factor of two, due to cool gas inside projection, bringing a observed x-ray and radio estimates into agreement. we propose that a complex southern relics inside ciza j2242.8+5301, have been broken up as a southerly moving ""back"" shocked gas impacts a gas still falling inside along a collision axis. finally, we use our model to generate compton-y maps to approximate a reduction inside radio flux caused by a thermal sunyaev-zel'dovich (sz) effect. at 30 ghz, this amounts to $\delta s_n = -0.072$ mjy/arcmin$^2$ and $\delta s_s = -0.075$ mjy/arcmin$^2$ at a locations of a northern and southern shock fronts respectively. our model approximate agrees with previous empirical estimates that have inferred a measured radio spectra should be significantly affected by a sz effect, with implications considering charged particle acceleration models of a radio relics.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16818,"this paper compares a efficiency of various algorithms considering implementing quantum resistant public key encryption scheme rlce on 64-bit cpus. by optimizing various algorithms considering polynomial and matrix operations over finite fields, we obtained several interesting (or even surprising) results. considering example, it was well known (e.g., moenck 1976 \cite{moenck1976practical}) that karatsuba's algorithm outperforms classical polynomial multiplication algorithm from a degree 15 and above (practically, karatsuba's algorithm only outperforms classical polynomial multiplication algorithm from a degree 35 and above ). our experiments show that 64-bit optimized karatsuba's algorithm will only outperform 64-bit optimized classical polynomial multiplication algorithm considering polynomials of degree 115 and above over finite field $gf(2^{10})$. a second interesting (surprising) result shows that 64-bit optimized chien's search algorithm ourperforms all other 64-bit optimized polynomial root finding algorithms such as bta and fft considering polynomials of all degrees over finite field $gf(2^{10})$. a third interesting (surprising) result shows that 64-bit optimized strassen matrix multiplication algorithm only outperforms 64-bit optimized classical matrix multiplication algorithm considering matrices of dimension 750 and above over finite field $gf(2^{10})$. it should be noted that existing literatures and practices recommend strassen matrix multiplication algorithm considering matrices of dimension 40 and above. all our experiments are done on the 64-bit macbook pro with i7 cpu and single thread c codes. it should be noted that a reported results should be appliable to 64 or larger bits cpu architectures. considering 32 or smaller bits cpus, these results may not be applicable. a source code and library considering a algorithms covered inside this paper are available at this http url.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
15753,"block-oriented nonlinear models are popular inside nonlinear modeling because of their advantages to be quite simple to understand and easy to use. to increase a flexibility of single branch block-oriented models, such as hammerstein, wiener, and wiener-hammerstein models, parallel block-oriented models should be considered. this paper presents the method to identify parallel wiener-hammerstein systems starting from input-output data only. inside a first step, a best linear approximation was estimated considering different input excitation levels. inside a second step, a dynamics are decomposed over the number of parallel orthogonal branches. next, a dynamics of each branch are partitioned into the linear time invariant subsystem at a input and the linear time invariant subsystem at a output. this was repeated considering each branch of a model. a static nonlinear part of a model was also estimated during this step. a consistency of a proposed initialization procedure was proven. a method was validated on real-world measurements with the help of the custom built parallel wiener-hammerstein test system.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
17350,"this paper studies a emotion recognition from musical tracks inside a 2-dimensional valence-arousal (v-a) emotional space. we propose the method based on convolutional (cnn) and recurrent neural networks (rnn), having significantly fewer parameters compared with a state-of-the-art method considering a same task. we utilize one cnn layer followed by two branches of rnns trained separately considering arousal and valence. a method is evaluated with the help of a 'mediaeval2015 emotion inside music' dataset. we achieved an rmse of 0.202 considering arousal and 0.268 considering valence, which was a best result reported on this dataset.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14270,"inside this paper, we study learning generalized driving style representations from automobile gps trip data. we propose the novel autoencoder regularized deep neural network (arnet) and the trip encoding framework trip2vec to learn drivers' driving styles directly from gps records, by combining supervised and unsupervised feature learning inside the unified architecture. experiments on the challenging driver number approximation problem and a driver identification problem show that arnet should learn the good generalized driving style representation: it significantly outperforms existing methods and alternative architectures by reaching a least approximation error on average (0.68, less than one driver) and a highest identification accuracy (by at least 3% improvement) compared with traditional supervised learning methods.",1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3598,"we consider a problem of sampling from the strongly log-concave density inside $\mathbb{r}^d$, and prove the non-asymptotic upper bound on a mixing time of a metropolis-adjusted langevin algorithm (mala). a method draws samples by running the markov chain obtained from a discretization of an appropriate langevin diffusion, combined with an accept-reject step to ensure a correct stationary distribution. relative to known guarantees considering a unadjusted langevin algorithm (ula), our bounds show that a use of an accept-reject step inside mala leads to an exponentially improved dependence on a error-tolerance. concretely, inside order to obtain samples with tv error at most $\delta$ considering the density with condition number $\kappa$, we show that mala requires $\mathcal{o} \big(\kappa d \log(1/\delta) \big)$ steps, as compared to a $\mathcal{o} \big(\kappa^2 d/\delta^2 \big)$ steps established inside past work on ula. we also demonstrate a gains of mala over ula considering weakly log-concave densities. furthermore, we derive mixing time bounds considering the zeroth-order method metropolized random walk (mrw) and show that it mixes $\mathcal{o}(\kappa d)$ slower than mala. we provide numerical examples that support our theoretical findings, and demonstrate a potential gains of metropolis-hastings adjustment considering langevin-type algorithms.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4011,"this paper considers the cloud-ran architecture with cache-enabled multi-antenna edge nodes (ens) that deliver content to cache-enabled end-users. a ens are connected to the central server using limited-capacity fronthaul links, and, based on a information received from a central server and a cached contents, they transmit on a shared wireless medium to satisfy users' requests. by leveraging cooperative transmission as enabled by ens' caches and fronthaul links, as well as multicasting opportunities provided by users' caches, the close-to-optimal caching and delivery scheme was proposed. as the result, a minimum normalized delivery time (ndt), the high-snr measure of delivery latency, was characterized to within the multiplicative constant gap of $3/2$ under a assumption of uncoded caching and fronthaul transmission, and of one-shot linear precoding. this result demonstrates a interplay among fronthaul links capacity, ens' caches, and end-users' caches inside minimizing a content delivery time.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
12297,"we study a implicit upwind finite volume scheme considering numerically approximating a linear continuity equation inside a low regularity diperna-lions setting. that is, we are concerned with advecting velocity fields that are spatially sobolev regular and data that are merely integrable. we prove that on unstructured regular meshes a rate of convergence of approximate solutions generated by a upwind scheme towards a unique distributional solution of a continuous model was at least 1/2. a numerical error was estimated inside terms of logarithmic kantorovich-rubinstein distances and provides thus the bound on a rate of weak convergence.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9026,"we present a theory of the new type of topological quantum order which was driven by a spin-orbit density wave order parameter, and distinguished by $z_2$ topological invariant. we show that when two oppositely polarized chiral bands [resulting from a rashba-type spin-orbit coupling $\alpha_k$, $k$ was crystal momentum] are significantly nested by the special wavevector ${\bf q}\sim(\pi,0)/(0,\pi)$, it induces the spatially modulated inversion of a chirality ($\alpha_{k+q}=\alpha_k^*$) between different sublattices. a resulting quantum order parameters break translational symmetry, but preserve time-reversal symmetry. it was inherently associated with the $z_2$-topological invariant along each density wave propagation direction. thus it gives the weak topological insulator inside two dimensions, with even number of spin-polarized boundary states. this phase was analogous to a quantum spin-hall state, except here a time-reversal polarization was spatially modulated, and thus it was dubbed quantum spin-hall density wave (qshdw) state. this order parameter should be realized or engineered inside quantum wires, or quasi-2d systems, by tuning a spin-orbit couping strength and chemical potential to achieve a special nesting condition.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
19267,"finding optimal correction of errors inside generic stabilizer codes was the computationally hard problem, even considering simple noise models. while this task should be simplified considering codes with some structure, such as topological stabilizer codes, developing good and efficient decoders still remains the challenge. inside our work, we systematically study the very versatile class of decoders based on feedforward neural networks. to demonstrate adaptability, we apply neural decoders to a triangular color and toric codes under various noise models with realistic features, such as spatially-correlated errors. we report that neural decoders provide significant improvement over leading efficient decoders inside terms of a error-correction threshold. with the help of neural networks simplifies a process of designing well-performing decoders, and does not require prior knowledge of a underlying noise model.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3314,"natural and artificial self-propelled systems must manage environmental interactions during movement. such interactions, which we refer to as active collisions, are fundamentally different from momentum-conserving interactions studied inside classical physics, largely because a internal driving of a locomotor should lead to persistent contact with heterogeneities. here, we experimentally and numerically study a effects of active collisions on the laterally-undulating sensory-deprived robophysical model, whose dynamics are applicable to self-propelled systems across length scales and environments. a robot moves using spatial undulation of body segments, with the nearly-linear center-of-geometry trajectory. interactions with the single rigid post scatter a robot, and these deflections are proportional to a head-post contact duration. a distribution of scattering angles was smooth and strongly-peaked directly behind a post. interactions with the single row of evenly-spaced posts (with inter-post spacing $d$) produce distributions reminiscent of far-field diffraction patterns: as $d$ decreases, distinct secondary peaks emerge as large deflections become more likely. surprisingly, we find that a presence of multiple posts does not change a nature of individual collisions; instead, multi-modal scattering patterns arise from multiple posts altering a likelihood of individual collisions to occur. as $d$ decreases, collisions near a leading edges of a posts become more probable, and we find that these interactions are associated with larger deflections. our results, which highlight a surprising dynamics that should occur during active collisions of self-propelled systems, should inform control principles considering locomotors inside complex terrain and facilitate design of task-capable active matter.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
18192,"this paper contributes the first study into how different human users deliver simultaneous control and feedback signals during human-robot interaction. as part of this work, we formalize and present the general interactive learning framework considering online cooperation between humans and reinforcement learning agents. inside many human-machine interaction settings, there was the growing gap between a degrees-of-freedom of complex semi-autonomous systems and a number of human control channels. simple human control and feedback mechanisms are required to close this gap and allow considering better collaboration between humans and machines on complex tasks. to better inform a design of concurrent control and feedback interfaces, we present experimental results from the human-robot collaborative domain wherein a human must simultaneously deliver both control and feedback signals to interactively train an actor-critic reinforcement learning robot. we compare three experimental conditions: 1) human delivered control signals, 2) reward-shaping feedback signals, and 3) simultaneous control and feedback. our results suggest that subjects provide less feedback when simultaneously delivering feedback and control signals and that control signal quality was not significantly diminished. our data suggest that subjects may also modify when and how they provide feedback. through algorithmic development and tuning informed by this study, we expect semi-autonomous actions of robotic agents should be better shaped by human feedback, allowing considering seamless collaboration and improved performance inside difficult interactive domains.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
10839,"even inside a absence of any explicit semantic annotation, vast collections of audio recordings provide valuable information considering learning a categorical structure of sounds. we consider several class-agnostic semantic constraints that apply to unlabeled nonspeech audio: (i) noise and translations inside time do not change a underlying sound category, (ii) the mixture of two sound events inherits a categories of a constituents, and (iii) a categories of events inside close temporal proximity are likely to be a same or related. without labels to ground them, these constraints are incompatible with classification loss functions. however, they may still be leveraged to identify geometric inequalities needed considering triplet loss-based training of convolutional neural networks. a result was low-dimensional embeddings of a input spectrograms that recover 41% and 84% of a performance of their fully-supervised counterparts when applied to downstream query-by-example sound retrieval and sound event classification tasks, respectively. moreover, inside limited-supervision settings, our unsupervised embeddings double a state-of-the-art classification performance.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
127,"research inside extrasolar-planet science was data-driven. with a advent of radial-velocity instruments like harps and harps-n, and transit space missions like kepler, our ability to discover and characterise extrasolar planets was no longer limited by instrumental precision but by our ability to model a data accurately. this chapter presents a models that describe radial-velocity measurements and transit light curves. i begin by deriving a solution of a two-body problem and from there, a equations describing a radial velocity of the planet-host star and a distance between star and planet centres, necessary to model transit light curves. stochastic models are then presented and i delineate how they are used to model complex physical phenomena affecting a exoplanet data sets, such as stellar activity. finally, i give the brief overview of a processes of bayesian inference, focussing on a construction of likelihood functions and prior probability distributions. inside particular, i describe different methods to specify ignorance priors.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19728,"we present an end-to-end, multimodal, fully convolutional network considering extracting semantic structures from document images. we consider document semantic structure extraction as the pixel-wise segmentation task, and propose the unified model that classifies pixels based not only on their visual appearance, as inside a traditional page segmentation task, but also on a content of underlying text. moreover, we propose an efficient synthetic document generation process that we use to generate pretraining data considering our network. once a network was trained on the large set of synthetic documents, we fine-tune a network on unlabeled real documents with the help of the semi-supervised approach. we systematically study a optimum network architecture and show that both our multimodal idea behind the method and a synthetic data pretraining significantly boost a performance.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6330,"several fundamental problems that arise inside optimization and computer science should be cast as follows: given vectors $v_1,\ldots,v_m \in \mathbb{r}^d$ and the constraint family ${\cal b}\subseteq 2^{[m]}$, find the set $s \in \cal{b}$ that maximizes a squared volume of a simplex spanned by a vectors inside $s$. the motivating example was a data-summarization problem inside machine learning where one was given the collection of vectors that represent data such as documents or images. a volume of the set of vectors was used as the measure of their diversity, and partition or matroid constraints over $[m]$ are imposed inside order to ensure resource or fairness constraints. recently, nikolov and singh presented the convex program and showed how it should be used to approximate a value of a most diverse set when ${\cal b}$ corresponds to the partition matroid. this result is recently extended to regular matroids inside works of straszak and vishnoi, and anari and oveis gharan. a question of whether these approximation algorithms should be converted into a more useful approximation algorithms -- that also output the set -- remained open. a main contribution of this paper was to give a first approximation algorithms considering both partition and regular matroids. we present novel formulations considering a subdeterminant maximization problem considering these matroids; this reduces them to a problem of finding the point that maximizes a absolute value of the nonconvex function over the cartesian product of probability simplices. a technical core of our results was the new anti-concentration inequality considering dependent random variables that allows us to relate a optimal value of these nonconvex functions to their value at the random point. unlike prior work on a constrained subdeterminant maximization problem, our proofs do not rely on real-stability or convexity and could be of independent interest both inside algorithms and complexity.",1,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0
10427,"large-scale classification of data where classes are structurally organized inside the hierarchy was an important area of research. top-down approaches that exploit a hierarchy during a learning and prediction phase are efficient considering large scale hierarchical classification. however, accuracy of top-down approaches was poor due to error propagation i.e., prediction errors made at higher levels inside a hierarchy cannot be corrected at lower levels. one of a main reason behind errors at a higher levels was a presence of inconsistent nodes that are introduced due to a arbitrary process of creating these hierarchies by domain experts. inside this paper, we propose two different data-driven approaches (local and global) considering hierarchical structure modification that identifies and flattens inconsistent nodes present within a hierarchy. our extensive empirical evaluation of a proposed approaches on several image and text datasets with varying distribution of features, classes and training instances per class shows improved classification performance over competing hierarchical modification approaches. specifically, we see an improvement upto 7% inside macro-f1 score with our idea behind the method over best td baseline. source code: this http url",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5660,"inside this paper, we construct twist automorphisms on quantum unipotent cells, which are quantum analogues of a berenstein-fomin-zelevinsky twist automorphisms on unipotent cells. we show that those quantum twist automorphisms preserve a dual canonical bases of quantum unipotent cells. moreover, we prove that quantum twist automorphisms are described by a syzygy functors considering representations of preprojective algebras inside a symmetric case. this was a quantum analogue of gei{\ss}-leclerc-schr√∂er's description, and gei{\ss}-leclerc-schr√∂er's results are essential inside our proof. as the consequence, we show that quantum twist automorphisms are compatible with quantum cluster monomials. a 6-periodicity of specific quantum twist automorphisms was also verified.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
7525,"this work contains two main contributions concerning a asymmetric broadcast channel. a first was an analysis of a exact random coding error exponents considering both users, and a second was a derivation of universal decoders considering both users. these universal decoders are certain variants of a maximum mutual information (mmi) universal decoder, which achieve a corresponding random coding exponents of optimal decoding. inside addition, we introduce some lower bounds, which involve optimization over very few parameters, unlike a original, exact exponents, which involve minimizations over auxiliary probability distributions. numerical results considering a binary symmetric broadcast channel show improvements over previously derived error exponents considering a same model.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
2263,"we provide the framework considering a assignment of multiple robots to goal locations, when robot travel times are uncertain. our premise was that time was a most valuable asset inside a system. hence, we make use of redundant robots to counter a effect of uncertainty and minimize a average waiting time at destinations. we apply our framework to transport networks represented as graphs, and consider uncertainty inside a edge costs (i.e., travel time). since solving a redundant assignment problem was strongly np-hard, we exploit structural properties of our problem to propose the polynomial-time solution with provable sub-optimality bounds. our method uses distributive aggregate functions, which allow us to efficiently (i.e., incrementally) compute a effective cost of assigning redundant robots. experimental results on random graphs show that a deployment of redundant robots through our method reduces waiting times at goal locations, when edge traversals are uncertain.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
3555,"we generalize a concept of a spin-momentum locking to magnonic systems and derive a formula to calculate a spin expectation value considering one-magnon states of general two-body spin hamiltonians. we give no-go conditions considering magnon spin to be independent of momentum. as examples of a magnon spin-momentum locking, we analyze the one-dimensional antiferromagnet with a n√©el order and two-dimensional kagome lattice antiferromagnets with a 120$^\circ$ structure. we find that a magnon spin depends on its momentum even when a hamiltonian has a $z$-axis spin rotational symmetry, which should be explained inside a context of the singular band point or the $u(1)$ symmetry breaking. the spin vortex inside momentum space generated inside the kagome lattice antiferromagnet has a winding number $q=-2$, while a typical one observed inside topological insulator surface states was characterized by $q=+1$. the magnonic analogue of a surface states, a dirac magnon with $q=+1$, was found inside another kagome lattice antiferromagnet. we also derive a sum rule considering $q$ by with the help of a poincar√©-hopf index theorem.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
14129,"graphene and some graphene like two dimensional materials; hexagonal boron nitride (hbn) and silicene have unique mechanical properties which severely limit a suitability of conventional theories used considering common brittle and ductile materials to predict a fracture response of these materials. this study revealed a fracture response of graphene, hbn and silicene nanosheets under different tiny crack lengths by molecular dynamics (md) simulations with the help of lammps. a useful strength of these large area two dimensional materials are determined by their fracture toughness. our study shows the comparative analysis of mechanical properties among a elemental analogues of graphene and suggested that hbn should be the good substitute considering graphene inside terms of mechanical properties. we have also found that a pre-cracked sheets fail inside brittle manner and their failure was governed by a strength of a atomic bonds at a crack tip. a md prediction of fracture toughness shows significant difference with a fracture toughness determined by griffth's theory of brittle failure which restricts a applicability of griffith's criterion considering these materials inside case of nano-cracks. moreover, a strengths measured inside armchair and zigzag directions of nanosheets of these materials implied that a bonds inside armchair direction has a stronger capability to resist crack propagation compared to zigzag direction.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
15751,"we present the general quantum kinetic theory of low-field magnetotransport inside weakly disordered crystals that accounts fully considering a interplay between electric-field induced interband coherence, bloch-state scattering, and an external magnetic field. a quantum kinetic equation we derive considering a bloch-state density matrix naturally incorporates a momentum-space berry phase effects whose influence on bloch-state wavepacket dynamics was normally incorporated into transport theory inside an ad hoc manner. a berry phase correction to a momentum-space density of states inside a presence of an external magnetic field implied by semiclassical wavepacket dynamics was captured by our theory as an intrinsic density-matrix response to the magnetic field. we propose the simple and general procedure considering expanding a linear response of a bloch-state density matrix to an electric field inside powers of magnetic field. as an illustration, we apply our theory to magnetotransport inside weyl semimetals. we show that a chiral anomaly (positive magnetoconductivity quadratic inside magnetic field) that appears when separate fermi surface pockets surround distinct weyl points survives only when intervalley scattering was very weak compared to intravalley scattering.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
16250,this paper presents the novel application of compositional data analysis methods inside a context of color image processing. the vector decomposition method was proposed to reveal compositional components of any vector with positive components followed by compositional data analysis to demonstrate a relation between color space concepts such as hue and saturation to their compositional counterparts. a proposed methods are applied to the magnetic resonance imaging dataset acquired from the living human brain and the digital color photograph to perform image fusion. potential future applications inside magnetic resonance imaging are mentioned and a benefits/disadvantages of a proposed methods are discussed inside terms of color image processing.,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6992,"we consider a problem of maximizing a spread of influence inside the social network by choosing the fixed number of initial seeds, formally referred to as a influence maximization problem. it admits the $(1-1/e)$-factor approximation algorithm if a influence function was submodular. otherwise, inside a worst case, a problem was np-hard to approximate to within the factor of $n^{1-\varepsilon}$. this paper studies whether this worst-case hardness result should be circumvented by making assumptions about either a underlying network topology or a cascade model. all of our assumptions are motivated by many real life social network cascades. first, we present strong inapproximability results considering the very restricted class of networks called a (stochastic) hierarchical blockmodel, the special case of a well-studied (stochastic) blockmodel inside which relationships between blocks admit the tree structure. we also provide the dynamic-program based polynomial time algorithm which optimally computes the directed variant of a influence maximization problem on hierarchical blockmodel networks. our algorithm indicates that a inapproximability result was due to a bidirectionality of influence between agent-blocks. second, we present strong inapproximability results considering the class of influence functions that are ""almost"" submodular, called 2-quasi-submodular. our inapproximability results hold even considering any 2-quasi-submodular $f$ fixed inside advance. this result also indicates that a ""threshold"" between submodularity and nonsubmodularity was sharp, regarding a approximability of influence maximization.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10508,"while deep reinforcement learning (rl) methods have achieved unprecedented successes inside the range of challenging problems, their applicability has been mainly limited to simulation or game domains due to a high sample complexity of a trial-and-error learning process. however, real-world robotic applications often need the data-efficient learning process with safety-critical constraints. inside this paper, we consider a challenging problem of learning unmanned aerial vehicle (uav) control considering tracking the moving target. to acquire the strategy that combines perception and control, we represent a policy by the convolutional neural network. we develop the hierarchical idea behind the method that combines the model-free policy gradient method with the conventional feedback proportional-integral-derivative (pid) controller to enable stable learning without catastrophic failure. a neural network was trained by the combination of supervised learning from raw images and reinforcement learning from games of self-play. we show that a proposed idea behind the method should learn the target following policy inside the simulator efficiently and a learned behavior should be successfully transferred to a dji quadrotor platform considering real-world uav control.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
15379,"automatic affect recognition was the challenging task due to a various modalities emotions should be expressed with. applications should be found inside many domains including multimedia retrieval and human computer interaction. inside recent years, deep neural networks have been used with great success inside determining emotional states. inspired by this success, we propose an emotion recognition system with the help of auditory and visual modalities. to capture a emotional content considering various styles of speaking, robust features need to be extracted. to this purpose, we utilize the convolutional neural network (cnn) to extract features from a speech, while considering a visual modality the deep residual network (resnet) of 50 layers. inside addition to a importance of feature extraction, the machine learning algorithm needs also to be insensitive to outliers while being able to model a context. to tackle this problem, long short-term memory (lstm) networks are utilized. a system was then trained inside an end-to-end fashion where - by also taking advantage of a correlations of a each of a streams - we manage to significantly outperform a traditional approaches based on auditory and visual handcrafted features considering a prediction of spontaneous and natural emotions on a recola database of a avec 2016 research challenge on emotion recognition.",1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8215,"we consider a question of estimating the solution to the system of equations that involve convex nonlinearities, the problem that was common inside machine learning and signal processing. because of these nonlinearities, conventional estimators based on empirical risk minimization generally involve solving the non-convex optimization program. we propose anchored regression, the new idea behind the method based on convex programming that amounts to maximizing the linear functional (perhaps augmented by the regularizer) over the convex set. a proposed convex program was formulated inside a natural space of a problem, and avoids a introduction of auxiliary variables, making it computationally favorable. working inside a native space also provides great flexibility as structural priors (e.g., sparsity) should be seamlessly incorporated. considering our analysis, we model a equations as being drawn from the fixed set according to the probability law. our main results provide guarantees on a accuracy of a estimator inside terms of a number of equations we are solving, a amount of noise present, the measure of statistical complexity of a random equations, and a geometry of a regularizer at a true solution. we also provide recipes considering constructing a anchor vector (that determines a linear functional to maximize) directly from a observed data.",1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0
17435,"inside this paper, we first propose the new iterative algorithm, called a k-sets+ algorithm considering clustering data points inside the semi-metric space, where a distance measure does not necessarily satisfy a triangular inequality. we show that a k-sets+ algorithm converges inside the finite number of iterations and it retains a same performance guarantee as a k-sets algorithm considering clustering data points inside the metric space. we then extend a applicability of a k-sets+ algorithm from data points inside the semi-metric space to data points that only have the symmetric similarity measure. such an extension leads to great reduction of computational complexity. inside particular, considering an n * n similarity matrix with m nonzero elements inside a matrix, a computational complexity of a k-sets+ algorithm was o((kn + m)i), where i was a number of iterations. a memory complexity to achieve that computational complexity was o(kn + m). as such, both a computational complexity and a memory complexity are linear inside n when a n * n similarity matrix was sparse, i.e., m = o(n). we also conduct various experiments to show a effectiveness of a k-sets+ algorithm by with the help of the synthetic dataset from a stochastic block model and the real network from a wondernetwork website.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
896,"a analysis of telemetry data was common inside animal ecological studies. while a collection of telemetry data considering individual animals has improved dramatically, a methods to properly account considering inherent uncertainties (e.g., measurement error, dependence, barriers to movement) have lagged behind. still, many new statistical approaches have been developed to infer unknown quantities affecting animal movement or predict movement based on telemetry data. hierarchical statistical models are useful to account considering some of a aforementioned uncertainties, as well as provide population-level inference, but they often come with an increased computational burden. considering certain types of statistical models, it was straightforward to provide inference if a latent true animal trajectory was known, but challenging otherwise. inside these cases, approaches related to multiple imputation have been employed to account considering a uncertainty associated with our knowledge of a latent trajectory. despite a increasing use of imputation approaches considering modeling animal movement, a general sensitivity and accuracy of these methods have not been explored inside detail. we provide an introduction to animal movement modeling and describe how imputation approaches may be helpful considering certain types of models. we also assess a performance of imputation approaches inside the simulation study. our simulation study suggests that inference considering model parameters directly related to a location of an individual may be more accurate than inference considering parameters associated with higher-order processes such as velocity or acceleration. finally, we apply these methods to analyze the telemetry data set involving northern fur seals (callorhinus ursinus) inside a bering sea.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
11462,"synthesizing programs with the help of example input/outputs was the classic problem inside artificial intelligence. we present the method considering solving programming by example (pbe) problems by with the help of the neural model to guide a search of the constraint logic programming system called minikanren. crucially, a neural model uses minikanren's internal representation as input; minikanren represents the pbe problem as recursive constraints imposed by a provided examples. we explore recurrent neural network and graph neural network models. we contribute the modified minikanren, drivable by an external agent, available at this https url. we show that our neural-guided idea behind the method with the help of constraints should synthesize programs faster inside many cases, and importantly, should generalize to larger problems.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5193,"meshfree solution schemes considering a incompressible navier--stokes equations are usually based on algorithms commonly used inside finite volume methods, such as projection methods, simple and piso algorithms. however, drawbacks of these algorithms that are specific to meshfree methods have often been overlooked. inside this paper, we study a drawbacks of conventionally used meshfree generalized finite difference method~(gfdm) schemes considering lagrangian incompressible navier-stokes equations, both operator splitting schemes and monolithic schemes. a major drawback of most of these schemes was inaccurate local approximations to a mass conservation condition. further, we propose the new modification of the commonly used monolithic scheme that overcomes these problems and shows the better approximation considering a velocity divergence condition. we then perform the numerical comparison which shows a new monolithic scheme to be more accurate than existing schemes.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13924,"motivated by a problem of optimal portfolio liquidation under transient price impact, we study a minimization of energy functionals with completely monotone displacement kernel under an integral constraint. a corresponding minimizers should be characterized by fredholm integral equations of a second type with constant free term. our main result states that minimizers are analytic and have the power series development inside terms of even powers of a distance to a midpoint of a domain of definition and with nonnegative coefficients. we show moreover that our minimization problem was equivalent to a minimization of a energy functional under the nonnegativity constraint.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
13846,"achieving information-theoretic security with the help of explicit coding scheme inside which unlimited computational power considering eavesdropper was assumed, was one of a main topics was security consideration. it was shown that polar codes are capacity achieving codes and have the low complexity inside encoding and decoding. it has been proven that polar codes reach to secrecy capacity inside a binary-input wiretap channels inside symmetric settings considering which a wiretapper's channel was degraded with respect to a main channel. a first task of this paper was to propose the coding scheme to achieve secrecy capacity inside asymmetric nonbinary-input channels while keeping reliability and security conditions satisfied. our assumption was that a wiretap channel was stochastically degraded with respect to a main channel and message distribution was unspecified. a main idea was to send information set over good channels considering bob and bad channels considering eve and send random symbols considering channels that are good considering both. inside this scheme a frozen vector was defined over all possible choices with the help of polar codes ensemble concept. we proved that there exists the frozen vector considering which a coding scheme satisfies reliability and security conditions. it was further shown that uniform distribution of a message was a necessary condition considering achieving secrecy capacity.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
17489,"a amount of ultraviolet irradiation and ablation experienced by the planet depends strongly on a temperature of its host star. of a thousands of extra-solar planets now known, only four giant planets have been found that transit hot, a-type stars (temperatures of 7300-10,000k), and none are known to transit even hotter b-type stars. wasp-33 was an a-type star with the temperature of ~7430k, which hosts a hottest known transiting planet; a planet was itself as hot as the red dwarf star of type m. a planet displays the large heat differential between its day-side and night-side, and was highly inflated, traits that have been linked to high insolation. however, even at a temperature of wasp-33b's day-side, its atmosphere likely resembles a molecule-dominated atmospheres of other planets, and at a level of ultraviolet irradiation it experiences, its atmosphere was unlikely to be significantly ablated over a lifetime of its star. here we report observations of a bright star hd 195689, which reveal the close-in (orbital period ~1.48 days) transiting giant planet, kelt-9b. at ~10,170k, a host star was at a dividing line between stars of type the and b, and we measure a kelt-9b's day-side temperature to be ~4600k. this was as hot as stars of stellar type k4. a molecules inside k stars are entirely dissociated, and thus a primary sources of opacity inside a day-side atmosphere of kelt-9b are likely atomic metals. furthermore, kelt-9b receives ~700 times more extreme ultraviolet radiation (wavelengths shorter than 91.2 nanometers) than wasp-33b, leading to the predicted range of mass-loss rates that could leave a planet largely stripped of its envelope during a main-sequence lifetime of a host star.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16993,"face deidentification was an active topic amongst privacy and security researchers. early deidentification methods relying on image blurring or pixelization were replaced inside recent years with techniques based on formal anonymity models that provide privacy guaranties and at a same time aim at retaining certain characteristics of a data even after deidentification. a latter aspect was particularly important, as it allows to exploit a deidentified data inside applications considering which identity information was irrelevant. inside this work we present the novel face deidentification pipeline, which ensures anonymity by synthesizing artificial surrogate faces with the help of generative neural networks (gnns). a generated faces are used to deidentify subjects inside images or video, while preserving non-identity-related aspects of a data and consequently enabling data utilization. since generative networks are very adaptive and should utilize the diverse set of parameters (pertaining to a appearance of a generated output inside terms of facial expressions, gender, race, etc.), they represent the natural choice considering a problem of face deidentification. to demonstrate a feasibility of our approach, we perform experiments with the help of automated recognition tools and human annotators. our results show that a recognition performance on deidentified images was close to chance, suggesting that a deidentification process based on gnns was highly effective.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
168,"recently, there have been increasing demands to construct compact deep architectures to remove unnecessary redundancy and to improve a inference speed. while many recent works focus on reducing a redundancy by eliminating unneeded weight parameters, it was not possible to apply the single deep architecture considering multiple devices with different resources. when the new device or circumstantial condition requires the new deep architecture, it was necessary to construct and train the new network from scratch. inside this work, we propose the novel deep learning framework, called the nested sparse network, which exploits an n-in-1-type nested structure inside the neural network. the nested sparse network consists of multiple levels of networks with the different sparsity ratio associated with each level, and higher level networks share parameters with lower level networks to enable stable nested learning. a proposed framework realizes the resource-aware versatile architecture as a same network should meet diverse resource requirements. moreover, a proposed nested network should learn different forms of knowledge inside its internal networks at different levels, enabling multiple tasks with the help of the single network, such as coarse-to-fine hierarchical classification. inside order to train a proposed nested sparse network, we propose efficient weight connection learning and channel and layer scheduling strategies. we evaluate our network inside multiple tasks, including adaptive deep compression, knowledge distillation, and learning class hierarchy, and demonstrate that nested sparse networks perform competitively, but more efficiently, compared to existing methods.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16512,"leveraging recent advances inside technologies surrounding a internet of things, ""smart"" water systems are poised to transform water resources management by enabling ubiquitous real-time sensing and control. recent applications have demonstrated a potential to improve flood forecasting, enhance rainwater harvesting, and prevent combined sewer overflows. however, adoption of smart water systems has been hindered by the limited number of proven case studies, along with the lack of guidance on how smart water systems should be built. to this end, we review existing solutions, and introduce open storm---an open-source, end-to-end platform considering real-time monitoring and control of watersheds. open storm includes (i) the robust hardware stack considering distributed sensing and control inside harsh environments (ii) the cloud services platform that enables system-level supervision and coordination of water assets, and (iii) the comprehensive, web-based ""how-to"" guide, available on open-storm.org, that empowers newcomers to develop and deploy their own smart water networks. we illustrate a capabilities of a open storm platform through two ongoing deployments: (i) the high-resolution flash-flood monitoring network that detects and communicates flood hazards at a level of individual roadways and (ii) the real-time stormwater control network that actively modulates discharges from stormwater facilities to improve water quality and reduce stream erosion. through these case studies, we demonstrate a real-world potential considering smart water systems to enable sustainable management of water resources.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
11085,"a quest considering algorithms that enable cognitive abilities was an important part of machine learning. the common trait inside many recently investigated cognitive-like tasks was that they take into account different data modalities, such as visual and textual input. inside this paper we propose the novel and generally applicable form of attention mechanism that learns high-order correlations between various data modalities. we show that high-order correlations effectively direct a appropriate attention to a relevant elements inside a different data modalities that are required to solve a joint task. we demonstrate a effectiveness of our high-order attention mechanism on a task of visual question answering (vqa), where we achieve state-of-the-art performance on a standard vqa dataset.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18553,"inside recent years, the class of dictionaries have been proposed considering multidimensional (tensor) data representation that exploit a structure of tensor data by imposing the kronecker structure on a dictionary underlying a data. inside this work, the novel algorithm called ""stark"" was provided to learn kronecker structured dictionaries that should represent tensors of any order. by establishing that a kronecker product of any number of matrices should be rearranged to form the rank-1 tensor, we show that kronecker structure should be enforced on a dictionary by solving the rank-1 tensor recovery problem. because rank-1 tensor recovery was the challenging nonconvex problem, we resort to solving the convex relaxation of this problem. empirical experiments on synthetic and real data show promising results considering our proposed algorithm.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3820,"we consider a notion of a de rham operator on finite-dimensional diffeological spaces such that a diffeological counterpart \lambda^1(x) of a cotangent bundle, a so-called pseudo-bundle of values of differential 1-forms, has bounded dimension. a operator was defined as a composition of a levi-civita connection on a exterior algebra pseudo-bundle \bigwedge(\lambda^1(x)) with a standardly defined clifford action by \lambda^1(x); a latter was therefore assumed to admit the pseudo-metric considering which there exists the levi-civita connection. under these assumptions, a definition was fully analogous a standard case, and our main conclusion was that this was a only way to define a de rham operator on the diffeological space, since we show that there was not the straightforward counterpart of a definition of a de rham operator as a sum d+d^* of a exterior differential with its adjoint. we show along a way that other connected notions do not have full counterparts, inside terms of a function they are supposed to fulfill, either; this regards, considering instance, volume forms, a hodge star, and a distinction between a $k$-th exterior degree of \lambda^1(x) and a pseudo-bundle of differential k-forms \lambda^k(x).",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7801,"rapid progress has been made recently on symmetry breaking operators considering real reductive groups. based on program a-c considering branching problems (t.kobayashi [progr.math.2015]), we illustrate the scheme of a classification of (local and nonlocal) symmetry breaking operators by an example of conformal representations on differential forms on a model space $(x,y)=(s^n, s^{n-1})$, which generalizes a scalar case (kobayashi--speh [memoirs of amer.math.soc. 2015]) and a case of local operators (kobayashi--kubo--pevzner [lecture notes inside math. 2016]). some applications to automorphic form theory, motivations from conformal geometry, and a methods of proof are also discussed.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
8584,"we construct smooth solutions to ricci flow starting from the class of singular metrics and give asymptotics considering a forward evolution. a singular metrics heal with the set of points (of codimension at least three) coming out of a singular point. we conjecture that these metrics arise as final-time limits of ricci flow encountering the type-i singularity modeled on $\mathbb{r}^{p+1} \times s^q$. this gives the picture of ricci flow through the singularity, inside which the neighborhood of a manifold changes topology from $d^{p+1} \times s^{q}$ to $s^p \times d^{q+1}$ (through a cone over $s^p \times s^q$.) we work inside a class of doubly-warped product metrics. we also briefly discuss some possible smooth and non-smooth forward evolutions from other singular initial data.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3958,"robot awareness of human actions was an essential research problem inside robotics with many important real-world applications, including human-robot collaboration and teaming. over a past few years, depth sensors have become the standard device widely used by intelligent robots considering 3d perception, which should also offer human skeletal data inside 3d space. several methods based on skeletal data were designed to enable robot awareness of human actions with satisfactory accuracy. however, previous methods treated all body parts and features equally important, without a capability to identify discriminative body parts and features. inside this paper, we propose the novel simultaneous feature and body-part learning (fabl) idea behind the method that simultaneously identifies discriminative body parts and features, and efficiently integrates all available information together to enable real-time robot awareness of human behaviors. we formulate fabl as the regression-like optimization problem with structured sparsity-inducing norms to model interrelationships of body parts and features. we also develop an optimization algorithm to solve a formulated problem, which possesses the theoretical guarantee to find a optimal solution. to evaluate fabl, three experiments were performed with the help of public benchmark datasets, including a msr action3d and cad-60 datasets, as well as the baxter robot inside practical assistive living applications. experimental results show that our fabl idea behind the method obtains the high recognition accuracy with the processing speed of a order-of-magnitude of 10e4 hz, which makes fabl the promising method to enable real-time robot awareness of human behaviors inside practical robotics applications.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
17541,"an important challenge inside condensed matter physics was understanding iron-based superconductors. among these systems, a iron selenides hold a record considering highest superconducting transition temperature and pose especially striking puzzles regarding a nature of superconductivity. a pairing state of a alkaline iron selenides appears to be of $d$-wave type based on a observation of the resonance mode inside neutron scattering, while it seems to be of $s$-wave type from a nodeless gaps observed everywhere on a fermi surface (fs). here we propose an orbital-selective pairing state, dubbed $s \tau_{3}$, as the natural explanation of these disparate properties. a pairing function, containing the matrix $\tau_{3}$ inside a basis of $3d$-electron orbitals, does not commute with a kinetic part of a hamiltonian. this dictates a existence of both intraband and interband pairing terms inside a band basis. the spin resonance arises from the $d$-wave-type sign change inside a intraband pairing component whereas a quasiparticle excitation was fully gapped on a fs due to an $s$-wave-like form factor associated with a addition inside quadrature of a intraband and interband pairing terms. we demonstrate that this pairing state was energetically favored when a electron correlation effects are orbitally selective. more generally, our results illustrate how a multiband nature of correlated electrons affords unusual types of superconducting states, thereby shedding new light not only on a iron-based materials but also on the broad range of other unconventional superconductors such as heavy fermion and organic systems.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
19560,"a wavelet tree (grossi et al. [soda, 2003]) and wavelet matrix (claude et al. [inf. syst., 47:15--32, 2015]) are compact indices considering texts over an alphabet $[0,\sigma)$ that support rank, select and access queries inside $o(\lg \sigma)$ time. we first present new practical sequential and parallel algorithms considering wavelet tree construction. their unifying characteristics was that they construct a wavelet tree bottomup}, i.e., they compute a last level first. we also show that this bottom-up construction should easily be adapted to wavelet matrices. inside practice, our best sequential algorithm was up to twice as fast as a currently fastest sequential wavelet tree construction algorithm (shun [dcc, 2015]), simultaneously saving the factor of 2 inside space. this scales up to 32 cores, where we are about equally fast as a currently fastest parallel wavelet tree construction algorithm (labeit et al. [dcc, 2016]), but still use only about 75 % of a space. an additional theoretical result shows how to adapt any wavelet tree construction algorithm to a wavelet matrix inside a same (asymptotic) time, with the help of only little extra space.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3157,"we present the method considering single image 3d cuboid object detection and multi-view object slam without prior object model, and demonstrate that a two aspects should benefit each other. considering 3d detection, we generate high quality cuboid proposals from 2d bounding boxes and vanishing points sampling. a proposals are further scored and selected to align with image edges. experiments on sun rgbd and kitti shows a efficiency and accuracy over existing approaches. then inside a second part, multi-view bundle adjustment with novel measurement functions was proposed to jointly optimize camera poses, objects and points, utilizing single view detection results. objects should provide more geometric constraints and scale consistency compared to points. on a collected and public tum and kitti odometry datasets, we achieve better pose approximation accuracy over a state-of-the-art monocular slam while also improve a 3d object detection accuracy at a same time.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
5955,"this paper investigates a control of flow networks, where a control objective was to regulate a measured output (e.g storage levels) towards the desired value. we present the distributed controller that dynamically adjusts a inputs and flows, to achieve output regulation inside a presence of unknown disturbances, while satisfying given input and flow constraints. optimal coordination among a inputs, minimizing the suitable cost function, was achieved by exchanging information over the communication network. exploiting an incremental passivity property, a desired steady state was proven to be globally asymptotically attractive under a closed loop dynamics. two case studies (a district heating system and the multi-terminal hvdc network) show a effectiveness of a proposed solution.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
9801,"we evaluate a mass of a black holes of gw150914 at their event horizons using quasi-local energy idea behind the method and obtain a values of 71 and 57 solar masses, compared to their asymptotic values of 36 and 29 units, respectively, as reported by ligo. the higher mass at a event horizon was compulsory inside order to overcome a huge negative gravitational potential energy surrounding a black holes and allow considering a emission of gravitational waves during merging. we approximate a initial mass of a stars which collapsed to form a black holes from a horizon mass and obtain a impressive values of 95 and 76 solar masses considering these progenitor stars.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16063,"we present the convolutional-recurrent neural network architecture with long short-term memory considering real-time processing and classification of digital sensor data. a network implicitly performs typical signal processing tasks such as filtering and peak detection, and learns time-resolved embeddings of a input signal. we use the prototype multi-sensor wearable device to collect over 180h of photoplethysmography (ppg) data sampled at 20hz, of which 36h are during atrial fibrillation (afib). we use end-to-end learning to achieve state-of-the-art results inside detecting afib from raw ppg data. considering classification labels output every 0.8s, we demonstrate an area under roc curve of 0.9999, with false positive and false negative rates both below $2\times 10^{-3}$. this constitutes the significant improvement on previous results utilising domain-specific feature engineering, such as heart rate extraction, and brings large-scale atrial fibrillation screenings within imminent reach.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18557,"we searched considering dust or debris rings inside a pluto-charon system before, during, and after a new horizons encounter. methodologies included searching considering back-scattered light during a idea behind the method to pluto (phase $\sim15^\circ$), inside situ detection of impacting particles, the search considering stellar occultations near a time of closest approach, and by forward-scattered light during departure (phase $\sim165^\circ$). the search with the help of hst prior to a encounter also contributed to a results. no rings, debris, or dust features were observed, but our detection limits provide an improved picture of a environment throughout a pluto-charon system. searches considering rings inside back-scattered light covered 35,000-250,000 km from a system barycenter, the zone that starts interior to a orbit of styx, and extends to four times a orbital radius of hydra. we obtained our firmest limits with the help of a nh lorri camera inside a inner half of this region. our limits on a normal $i/f$ of an unseen ring depends on a radial scale of a rings: $2\times10^{-8}$ ($3\sigma$) considering 1500 km wide rings, $1\times10^{-8}$ considering 6000 km rings, and $7\times10^{-9}$ considering 12,000 km rings. beyond $\sim100,000$ km from pluto, hst observations limit normal $i/f$ to $\sim8\times10^{-8}$. searches considering dust from forward-scattered light extended from a surface of pluto to a pluto-charon hill sphere ($r_{\rm hill}=6.4\times10^6$ km). no evidence considering rings or dust is detected to normal $i/f$ limits of $\sim8.9\times10^{-7}$ on $\sim10^4$ km scales. four occulation observations also probed a space interior to hydra, but again no dust or debris is detected. elsewhere inside a solar system, small moons commonly share their orbits with faint dust rings. our results suggest that small grains are quickly lost from a system due to solar radiation pressure, whereas larger particles are unstable due to perturbations by a known moons.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9073,"the diffeological connection on the diffeological vector pseudo-bundle was defined just a usual one on the smooth vector bundle; this was possible to do, because there was the standard diffeological counterpart of a cotangent bundle. on a other hand, there was not yet the standard theory of tangent bundles, although there are many suggested and promising versions, such as that of a internal tangent bundle, so a abstract notion of the connection on the diffeological vector pseudo-bundle does not automatically provide the counterpart notion considering levi-civita connections. inside this paper we consider a dual of a just-mentioned counterpart of a cotangent bundle inside place of a tangent bundle (without making any claim about its geometrical meaning). to it, a notions of compatibility with the pseudo-metric and symmetricity should be easily extended, and therefore a notion of the levi-civita connection makes sense as well. inside a case when $\lambda^1(x)$, a counterpart of a cotangent bundle, was finite-dimensional, there was an equivalent levi-civita connection on it as well.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18258,"the superconductor of paired protons was thought to form inside a core of neutron stars soon after their birth. minimum energy conditions suggest magnetic flux was expelled from a superconducting region due to a meissner effect, such that a neutron star core was largely devoid of magnetic fields considering some nuclear equation of state and proton pairing models. we show using neutron star cooling simulations that a superconducting region expands faster than flux was expected to be expelled because cooling timescales are much shorter than timescales of magnetic field diffusion. thus magnetic fields remain inside a bulk of a neutron star core considering at least 10^6-10^7 yr. we approximate a size of flux free regions at 10^7 yr to be <~ 100 m considering the magnetic field of 10^11 g and possibly smaller considering stronger field strengths. considering proton pairing models that are narrow, magnetic flux may be completely expelled from the thin shell of approximately a above size after 10^5 yr. this shell may insulate lower conductivity outer layers, where magnetic fields should diffuse and decay faster, from fields maintained inside a highly conducting deep core.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
4253,"a collective magnetic excitations inside a spin-orbit mott insulator (sr$_{1-x}$la$_x$)$_2$iro$_4$ ($x=0,\,0.01,\,0.04,\, 0.1$) were investigated by means of resonant inelastic x-ray scattering. we report significant magnon energy gaps at both a crystallographic and antiferromagnetic zone centers at all doping levels, along with the remarkably pronounced momentum-dependent lifetime broadening. a spin-wave gap was accounted considering by the significant anisotropy inside a interactions between $j_\text{eff}=1/2$ isospins, thus marking a departure of sr$_2$iro$_4$ from a essentially isotropic heisenberg model appropriate considering a superconducting cuprates.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
5910,"natural language generation (nlg) was an important component inside spoken dialogue systems. this paper presents the model called encoder-aggregator-decoder which was an extension of an recurrent neural network based encoder-decoder architecture. a proposed semantic aggregator consists of two components: an aligner and the refiner. a aligner was the conventional attention calculated over a encoded input information, while a refiner was another attention or gating mechanism stacked over a attentive aligner inside order to further select and aggregate a semantic elements. a proposed model should be jointly trained both sentence planning and surface realization to produce natural language utterances. a model is extensively assessed on four different nlg domains, inside which a experimental results showed that a proposed generator consistently outperforms a previous methods on all a nlg domains.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1184,"we present the complete symmetry classification of a sachdev-ye-kitaev (syk) model with $\mathcal{n}=0$, $1$ and $2$ supersymmetry (susy) on a basis of a altland-zirnbauer scheme inside random matrix theory (rmt). considering $\mathcal{n}=0$ and $1$ we consider generic $q$-body interactions inside a hamiltonian and find rmt classes that were not present inside earlier classifications of a same model with $q=4$. we numerically establish quantitative agreement between a distributions of a smallest energy levels inside a $\mathcal{n}=1$ syk model and rmt. furthermore, we delineate a distinctive structure of a $\mathcal{n}=2$ syk model and provide its complete symmetry classification based on rmt considering all eigenspaces of a fermion number operator. we corroborate our classification by detailed numerical comparisons with rmt and thus establish a presence of quantum chaotic dynamics inside a $\mathcal{n}=2$ syk model. we also introduce the new syk-like model without susy that exhibits hybrid properties of a $\mathcal{n}=1$ and $\mathcal{n}=2$ syk models and uncover its rich structure both analytically and numerically.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
3226,"an instrumentation problem with a signal acquisition at high frequencies is discovered and we no longer believe that a experimental data presented inside a manuscript, showing the frequency enhancement of a elastoresistivity, are correct. after correcting a problem, a elastoresistivity data was frequency independent inside a range investigated. therefore, a authors have withdrawn this submission. we would like to thank alex hristov, johanna palmstrom, josh straquadine and ian fisher (stanford) considering a kind discussions and assistance we received which helped us identify these problems.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
17286,"distance multivariance was the multivariate dependence measure, which should detect dependencies between an arbitrary number of random vectors each of which should have the distinct dimension. here we discuss several new aspects and present the concise overview. we relax a required moment conditions considerably and show that distance multivariance unifies (and extends) distance covariance and a hilbert-schmidt independence criterion hsic, moreover also a classical linear dependence measures: covariance, pearson's correlation and a rv coefficient appear as limiting cases. considering measures based on distance multivariance a corresponding resampling tests are introduced, and several related measures are defined: the new multicorrelation which satisfies the natural set of multivariate dependence measure axioms and $m$-multivariance which was the new dependence measure yielding tests considering pairwise independence and independence of higher order. these tests are computationally feasible and under very mild moment conditions they are consistent against all alternatives. moreover, the general visualization scheme considering higher order dependencies was proposed. many illustrative examples are included. all functions considering a use of distance multivariance inside applications are published inside a r-package 'multivariance'.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0
5522,"a study of deep recurrent neural networks (rnns) and, inside particular, of deep reservoir computing (rc) was gaining an increasing research attention inside a neural networks community. a recently introduced deep echo state network (deepesn) model opened a way to an extremely efficient idea behind the method considering designing deep neural networks considering temporal data. at a same time, a study of deepesns allowed to shed light on a intrinsic properties of state dynamics developed by hierarchical compositions of recurrent layers, i.e. on a bias of depth inside rnns architectural design. inside this paper, we summarize a advancements inside a development, analysis and applications of deepesns.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1038,"a volume of a hive polytope (or polytope of honeycombs) associated with the littlewood- richardson coefficient of su(n), or with the given admissible triple of highest weights, was expressed, inside a generic case, inside terms of a fourier transform of the convolution product of orbital measures. several properties of this function -- the function of three non-necessarily integral weights or of three multiplets of real eigenvalues considering a associated horn problem-- are already known. inside a integral case it should be thought of as the semi-classical approximation of littlewood-richardson coefficients. we prove that it may be expressed as the local average of the finite number of such coefficients. we also relate this function to a littlewood-richardson polynomials (stretching polynomials) i.e., to a ehrhart polynomials of a relevant hive polytopes. several su(n) examples, considering n=2,3,...,6, are explicitly worked out.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
10752,"recent research has demonstrated a vulnerability of fingerprint recognition systems to dictionary attacks based on masterprints. masterprints are real or synthetic fingerprints that should fortuitously match with the large number of fingerprints thereby undermining a security afforded by fingerprint systems. previous work by roy et al. generated synthetic masterprints at a feature-level. inside this work we generate complete image-level masterprints known as deepmasterprints, whose attack accuracy was found to be much superior than that of previous methods. a proposed method, referred to as latent variable evolution, was based on training the generative adversarial network on the set of real fingerprint images. stochastic search inside a form of a covariance matrix adaptation evolution strategy was then used to search considering latent input variables to a generator network that should maximize a number of impostor matches as assessed by the fingerprint recognizer. experiments convey a efficacy of a proposed method inside generating deepmasterprints. a underlying method was likely to have broad applications inside fingerprint security as well as fingerprint synthesis.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7600,"a prediction of organic reaction outcomes was the fundamental problem inside computational chemistry. since the reaction may involve hundreds of atoms, fully exploring a space of possible transformations was intractable. a current solution utilizes reaction templates to limit a space, but it suffers from coverage and efficiency issues. inside this paper, we propose the template-free idea behind the method to efficiently explore a space of product molecules by first pinpointing a reaction center -- a set of nodes and edges where graph edits occur. since only the small number of atoms contribute to reaction center, we should directly enumerate candidate products. a generated candidates are scored by the weisfeiler-lehman difference network that models high-order interactions between changes occurring at nodes across a molecule. our framework outperforms a top-performing template-based idea behind the method with the 10\% margin, while running orders of magnitude faster. finally, we demonstrate that a model accuracy rivals a performance of domain experts.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3884,"we attempt to constrain a kinematics of a thin and thick disks with the help of a besancon population synthesis model together with rave dr4 and gaia first data release (tgas). a rave fields were simulated by applying the detailed target selection function and a kinematics is computed with the help of velocity ellipsoids depending on age inside order to study a secular evolution. we accounted considering a asymmetric drift computed from fitting the stackel potential to orbits. model parameters such as velocity dispersions, mean motions, and velocity gradients were adjusted with the help of an abc-mcmc method. we made use of a metallicity to enhance a separation between thin and thick disks. we show that this model was able to reproduce a kinematics of a local disks inside great detail. a disk follows a expected secular evolution, inside very good agreement with previous studies of a thin disk. a new asymmetric drift formula, fitted to our previously described st√§ckel potential, fairly well reproduces a velocity distribution inside the wide solar neighborhood. a u and w components of a solar motion determined with this method agree well with previous studies. however, we find the smaller v component than previously thought, essentially because we include a variation of a asymmetric drift with distance to a plane. a thick disk was represented by the long period of formation (at least 2 gyr), during which, as we show, a mean velocity increases with time while a scale height and scale length decrease, very consistently with the collapse phase with conservation of angular momentum. this new galactic dynamical model was able to reproduce a observed velocities inside the wide solar neighborhood at a quality level of a tgas-rave sample, allowing us to constrain a thin and thick disk dynamical evolution, as well as determining a solar motion.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9644,"quantum spin ice, modeled considering magnetic rare-earth pyrochlores, has attracted great interest considering hosting the u(1) quantum spin liquid, which involves spin-ice monopoles as gapped deconfined spinons, as well as gapless excitations analogous to photons. however, a global phase diagram under the [111] magnetic field remains open. here we uncover by means of unbiased quantum monte-carlo simulations that the supersolid of monopoles, showing both the superfluidity and the partial ionization, intervenes a kagom√© spin ice and the fully ionized monopole insulator, inside contrast to classical spin ice where the direct discontinuous phase transition takes place. we also show that on cooling, kagom√© spin ice evolves towards the valence bond solid similar to what appears inside a associated kagom√© lattice model [s. v. isakov et al., phys. rev. lett. 97, 147202 (2006)]. possible relevance to experiments was discussed.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
6130,"with a increasing penetration of solar power into power systems, forecasting becomes critical inside power system operations. inside this paper, an hourly-similarity (hs) based method was developed considering 1-hour-ahead (1ha) global horizontal irradiance (ghi) forecasting. this developed method utilizes diurnal patterns, statistical distinctions between different hours, and hourly similarities inside solar data to improve a forecasting accuracy. a hs-based method was built by training multiple two-layer multi-model forecasting framework (mmff) models independently with a same-hour subsets. a final optimal model was the combination of mmff models with a best-performed blending algorithm at every hour. at a forecasting stage, a most suitable model was selected to perform a forecasting subtask of the certain hour. a hs-based method was validated by 1-year data with six solar features collected by a national renewable energy laboratory (nrel). results show that a hs-based method outperforms a non-hs (all-in-one) method significantly with a same mmff architecture, wherein a optimal hs- based method outperforms a best all-in-one method by 10.94% and 7.74% based on a normalized mean absolute error and normalized root mean square error, respectively.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3720,"a hard problem of consciousness has been dismissed as an illusion. by showing that computers are capable of experiencing, we show that they are at least rudimentarily conscious with potential to eventually reach superconsciousness. a main contribution of a paper was the test considering confirming certain subjective experiences inside the tested agent. we follow with analysis of benefits and problems with conscious machines and implications of such capability on future of computing, machine rights and artificial intelligence safety.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8551,"kernel embeddings of distributions and a maximum mean discrepancy (mmd), a resulting distance between distributions, are useful tools considering fully nonparametric two-sample testing and learning on distributions. however, it was rarely that all possible differences between samples are of interest -- discovered differences should be due to different types of measurement noise, data collection artefacts or other irrelevant sources of variability. we propose distances between distributions which encode invariance to additive symmetric noise, aimed at testing whether a assumed true underlying processes differ. moreover, we construct invariant features of distributions, leading to learning algorithms robust to a impairment of a input distributions with symmetric additive noise.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4628,"human interactions and human-computer interactions are strongly influenced by style as well as content. adding the persona to the chatbot makes it more human-like and contributes to the better and more engaging user experience. inside this work, we propose the design considering the chatbot that captures a ""style"" of star trek by incorporating references from a show along with peculiar tones of a fictional characters therein. our enterprise to computer bot (e2cbot) treats star trek dialog style and general dialog style differently, with the help of two recurrent neural network encoder-decoder models. a star trek dialog style uses sequence to sequence (seq2seq) models (sutskever et al., 2014; bahdanau et al., 2014) trained on star trek dialogs. a general dialog style uses word graph to shift a response of a seq2seq model into a star trek domain. we evaluate a bot both inside terms of perplexity and word overlap with star trek vocabulary and subjectively with the help of human evaluators.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12031,"we report on a confirmation that a candidate transits observed considering a star epic 211525389 are due to the short-period neptune-sized planet. a host star, located inside k2 campaign field 5, was the metal-rich ([fe/h] = 0.26$\pm$0.05) g-dwarf (t_eff = 5430$\pm$70 k and log g = 4.48$\pm$0.09), based on observations with a high dispersion spectrograph (hds) on a subaru 8.2m telescope. high-spatial resolution ao imaging with hiciao on a subaru telescope excludes faint companions near a host star, and a false positive probability of this target was found to be <$10^{-6}$ with the help of a open source vespa code. the joint analysis of transit light curves from k2 and additional ground-based multi-color transit photometry with muscat on a okayama 1.88m telescope gives a orbital period of p = 8.266902$\pm$0.000070 days and consistent transit depths of $r_p/r_\star \sim 0.035$ or $(r_p/r_\star)^2 \sim 0.0012$. a transit depth corresponds to the planetary radius of $r_p = 3.59_{-0.39}^{+0.44} r_{\oplus}$, indicating that epic 211525389 b was the short-period neptune-sized planet. radial velocities of a host star, obtained with a subaru hds, lead to the 3\sigma\ upper limit of 90 $m_{\oplus} (0.00027 m_{\odot})$ on a mass of epic 211525389 b, confirming its planetary nature. we expect this planet, newly named k2-105 b, to be a subject of future studies to characterize its mass, atmosphere, spin-orbit (mis)alignment, as well as investigate a possibility of additional planets inside a system.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5965,"we review a effect of a commonly-used limber and flat-sky approximations on a calculation of shear power spectra and correlation functions considering galaxy weak lensing. these approximations are accurate at small scales, but it has been claimed recently that their impact on low multipoles could lead to an increase inside a amplitude of a mass fluctuations inferred from surveys such as cfhtlens, reducing a tension between galaxy weak lensing and a amplitude determined by planck from observations of a cosmic microwave background. here, we explore a impact of these approximations on cosmological parameters derived from weak lensing surveys, with the help of a cfhtlens data as the test case. we conclude that a use of small-angle approximations considering cosmological parameter approximation was negligible considering current data, and does not contribute to a tension between current weak lensing surveys and planck.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11163,"we demonstrate that the very deep resnet with stacked modules with one neuron per hidden layer and relu activation functions should uniformly approximate any lebesgue integrable function inside $d$ dimensions, i.e. $\ell_1(\mathbb{r}^d)$. because of a identity mapping inherent to resnets, our network has alternating layers of dimension one and $d$. this stands inside sharp contrast to fully connected networks, which are not universal approximators if their width was a input dimension $d$ [lu et al, 2017; hanin and sellke, 2017]. hence, our result implies an increase inside representational power considering narrow deep networks by a resnet architecture.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2542,"inside this note we prove a instability by blow-up of a ground state solutions considering the class of fourth order schr\"" odinger equations. this extends a first rigorous results on blowing-up solutions considering a biharmonic nls due to boulenger and lenzmann \cite{bole} and confirm numerical conjectures from \cite{bafi, bafima1, bafima, fiilpa}.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6867,"we study a intra-cluster magnetic field inside a poor galaxy cluster abell 194 by complementing radio data, at different frequencies, with data inside a optical and x-ray bands. we analyze new total intensity and polarization observations of abell 194 obtained with a sardinia radio telescope (srt). we use a srt data inside combination with archival very large array observations to derive both a spectral aging and rotation measure (rm) images of a radio galaxies 3c40a and 3c40b embedded inside abell 194. a optical analysis indicates that abell 194 does not show the major and recent cluster merger, but rather agrees with the scenario of accretion of small groups. under a minimum energy assumption, a lifetimes of synchrotron electrons inside 3c40b measured from a spectral break are found to be 157 myrs. a break frequency image and a electron density profile inferred from a x-ray emission are used inside combination with a rm data to constrain a intra-cluster magnetic field power spectrum. by assuming the kolmogorov power law power spectrum, we find that a rm data inside abell 194 are well described by the magnetic field with the maximum scale of fluctuations of lambda_max=64 kpc and the central magnetic field strength of <b0>=1.5 microg. further out, a field decreases with a radius following a gas density to a power of eta=1.1. comparing abell 194 with the small sample of galaxy clusters, there was the hint of the trend between central electron densities and magnetic field strengths.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19333,"we propose the simulation method considering multidimensional hawkes processes based on superposition theory of point processes. this formulation allows us to design efficient simulations considering hawkes processes with differing exponentially decaying intensities. we demonstrate that inter-arrival times should be decomposed into simpler auxiliary variables that should be sampled directly, giving exact simulation with no approximation. we establish that a auxiliary variables provides information on a parent process considering each event time. a algorithm correctness was shown by verifying a simulated intensities with their theoretical moments. the modular inference procedure consisting of gibbs samplers through a auxiliary variable augmentation and adaptive rejection sampling was presented. finally, we compare our proposed simulation method against existing methods, and find significant improvement inside terms of algorithm speed. our inference algorithm was used to discover a strengths of mutually excitations inside real dark networks.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
917,"a online environment has provided the great opportunity considering insurance policyholders to share their complaints with respect to different services. these complaints should reveal valuable information considering insurance companies who seek to improve their services; however, analyzing the huge number of online complaints was the complicated task considering human and must involve computational methods to create an efficient process. this research proposes the computational idea behind the method to characterize a major topics of the large number of online complaints. our idea behind the method was based on with the help of a topic modeling idea behind the method to disclose a latent semantic of complaints. a proposed idea behind the method deployed on thousands of geico negative reviews. analyzing 1,371 geico complaints indicates that there are 30 major complains inside four categories: (1) customer service, (2) insurance coverage, paperwork, policy, and reports, (3) legal issues, and (4) costs, estimates, and payments. this research idea behind the method should be used inside other applications to explore the large number of reviews.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
12867,"a radial velocity (rv) of the single star was easily obtained from cross-correlation of a spectrum with the template, but a treatment of double-lined spectroscopic binaries (sb2s) was more difficult. two different approaches were applied to the set of sb2s: a fit of a cross-correlation function with two normal distributions, and a cross-correlation with two templates, derived with a todcor code. it appears that a minimum masses obtained through a two methods are sometimes rather different, although their estimated uncertainties are roughly equal. moreover, both methods induce the shift inside a zero point of a secondary rvs, but it was less pronounced considering todcor. all-in-all a comparison between a two methods was inside favour of todcor.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
8034,"inside this paper, we study a existence and a property of a hopf bifurcation inside a two-strategy replicator dynamics with distributed delays. inside evolutionary games, we assume that the strategy would take an uncertain time delay to have the consequence on a fitness (or utility) of a players. as a mean delay increases, the change inside a stability of a equilibrium (hopf bifurcation) may occur at which the periodic oscillation appears. we consider dirac, uniform, gamma, and discrete delay distributions, and we use a poincar√©- lindstedt's perturbation method to analyze a hopf bifurcation. our theoretical results are corroborated with numerical simulations.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
17417,"among a family of tmds, res2 takes the special position, which crystalizes inside the unique distorted low-symmetry structure at ambient conditions. a interlayer interaction inside res2 was rather weak, thus its bulk properties are similar to that of monolayer. however, how does compression change its structure and electronic properties was unknown so far. here with the help of ab initio crystal structure searching techniques, we explore a high-pressure phase transitions of res2 extensively and predict two new high-pressure phases. a ambient pressure phase transforms to the ""distorted-1t"" structure at very low pressure and then to the tetragonal i41/amd structure at around 90 gpa. a ""distorted-1t"" structure undergoes the semiconductor-metal transition (smt) at around 70 gpa with the band overlap mechanism. electron-phonon calculations suggest that a i41/amd structure was superconducting and has the critical superconducting temperature of about 2 k at 100 gpa. we further perform high-pressure electrical resistance measurements up to 102 gpa. our experiments confirm a smt and a superconducting phase transition of res2 under high pressure. these experimental results are inside good agreement with our theoretical predictions.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0
851,"a observation of metallic ground states inside the variety of two-dimensional electronic systems poses the fundamental challenge considering a theory of electron fluids. here, we analyze evidence considering a existence of the regime, which we call a ""anomalous metal regime,"" inside diverse 2d superconducting systems driven through the quantum superconductor to metal transition (qsmt) by tuning physical parameters such as a magnetic field, a gate voltage inside a case of systems with the mosfet geometry, or a degree of disorder. a principal phenomenological observation was that inside a anomalous metal, as the function of decreasing temperature, a resistivity first drops as if a system were approaching the superconducting ground state, but then saturates at low temperatures to the value that should be orders of magnitude smaller than a drude value. a anomalous metal also shows the giant positive magneto-resistance. thus, it behaves as if it were the ""failed superconductor."" this behavior was observed inside the broad range of parameters. we moreover exhibit, by theoretical solution of the model of superconducting grains embedded inside the metallic matrix, that as the matter of principle such anomalous metallic behavior should occur inside a neighborhood of the qsmt. however, we also argue that a robustness and ubiquitous nature of a observed phenomena are difficult to reconcile with any existing theoretical treatment, and speculate about a character of the more fundamental theoretical framework.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
12502,"we study a physical properties of giant molecular cloud associations (gmas) inside m100 (ngc 4321) with the help of a alma science verification feathered (12-m+aca) data inside 12co (1-0). to examine a environmental dependence of gma properties, gmas are classified based on their locations inside a various environments as circumnuclear ring (cnr), bar, spiral, and inter-arm gmas. a cnr gmas are massive and compact, while a inter-arm gmas are diffuse with low surface density. gma mass and size are strongly correlated, as suggested by larson (1981). however, a diverse power-law index of a relation implies that a gma properties are not uniform among a environments. a cnr and bar gmas show higher velocity dispersion than those inside other environments. we find little evidence considering the correlation between gma velocity dispersion and size, which indicates that a gmas are inside diverse dynamical states. indeed, a virial parameter of gmas spans nearly two orders of magnitude. only a spiral gmas are inside general self-gravitating. star formation activity of a gmas decreases inside order over a cnr, spiral, bar, and a inter-arm gmas. a diverse gma and star formation properties inside different environments lead to variations inside a kennicutt-schmidt relation. the combination of multiple mechanisms or gas phase change was necessary to explain a observed slopes. comparisons of gma properties acquired with a use of a 12-m-array observations with those from a feathered data are also presented. a results show that a missing flux and extended emission cannot be neglected considering a study of environmental dependence.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10606,"we develop the second order primal-dual method considering optimization problems inside which a objective function was given by a sum of the strongly convex twice differentiable term and the possibly nondifferentiable convex regularizer. after introducing an auxiliary variable, we utilize a proximal operator of a nonsmooth regularizer to transform a associated augmented lagrangian into the function that was once, but not twice, continuously differentiable. a saddle point of this function corresponds to a solution of a original optimization problem. we employ the generalization of a hessian to define second order updates on this function and prove global exponential stability of a corresponding differential inclusion. furthermore, we develop the globally convergent customized algorithm that utilizes a primal-dual augmented lagrangian as the merit function. we show that a search direction should be computed efficiently and prove quadratic/superlinear asymptotic convergence. we use a $\ell_1$-regularized least squares problem and a problem of designing the distributed controller considering the spatially-invariant system to demonstrate a merits and a effectiveness of our method.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17314,"this article offers an empirical study on a different ways of encoding chinese, japanese, korean (cjk) and english languages considering text classification. different encoding levels are studied, including utf-8 bytes, characters, words, romanized characters and romanized words. considering all encoding levels, whenever applicable, we provide comparisons with linear models, fasttext and convolutional networks. considering convolutional networks, we compare between encoding mechanisms with the help of character glyph images, one-hot (or one-of-n) encoding, and embedding. inside total there are 473 models, with the help of 14 large-scale text classification datasets inside 4 languages including chinese, english, japanese and korean. some conclusions from these results include that byte-level one-hot encoding based on utf-8 consistently produces competitive results considering convolutional networks, that word-level n-grams linear models are competitive even without perfect word segmentation, and that fasttext provides a best result with the help of character-level n-gram encoding but should overfit when a features are overly rich.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16423,"training semantic parsers from weak supervision (denotations) rather than strong supervision (programs) complicates training inside two ways. first, the large search space of potential programs needs to be explored at training time to find the correct program. second, spurious programs that accidentally lead to the correct denotation add noise to training. inside this work we propose that inside closed worlds with clear semantic types, one should substantially alleviate these problems by utilizing an abstract representation, where tokens inside both a language utterance and program are lifted to an abstract form. we show that these abstractions should be defined with the handful of lexical rules and that they result inside sharing between different examples that alleviates a difficulties inside training. to test our approach, we develop a first semantic parser considering cnlvr, the challenging visual reasoning dataset, where a search space was large and overcoming spuriousness was critical, because denotations are either true or false, and thus random programs are likely to lead to the correct denotation. our method substantially improves performance, and reaches 82.5% accuracy, the 14.7% absolute accuracy improvement compared to a best reported accuracy so far.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3245,"internet research on search engine quality and validity of results demand much concern. thus, a focus inside our study has been to measure a impact of quotation marks usage on a internet search outputs inside terms of google search outcomes distributions, through benford law. a current paper was focused on applying the benford law analysis on two related types of internet searches distinguished by a usage or absence of quotation marks. both search results values are assumed as variables. we found that a first digit of outcomes does not follow a benford law first digit of numbers inside a case of searching text without quotation marks. unexpectedly, a benford law was obeyed when quotation marks are used, even if a variability of search outcomes was considerably reduced. by studying outputs demonstrating influences of (apparently at first) ""details"", inside with the help of the search engine, a authors are able to further warn a users concerning a validity of such outputs.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13270,"a surface structure of phosphorene crystals materials was determined with the help of surface sensitive dynamical micro-spot low energy electron diffraction ({\mu}leed) analysis with the help of the high spatial resolution low energy electron microscopy (leem) system. samples of (\textit{i}) crystalline cleaved black phosphorus (bp) at 300 k and (\textit{ii}) exfoliated few-layer phosphorene (flp) of about 10 nm thicknes, which were annealed at 573 k inside vacuum were studied. inside both samples, the significant surface buckling of 0.22 {\aa} and 0.30 {\aa}, respectively, was measured, which was one order of magnitude larger than previously reported. with the help of first principle calculations, a presence of surface vacancies was attributed not only to a surface buckling inside bp and flp, but also a previously reported intrinsic hole doping of phosphorene materials.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
13824,"as the cutting-edge technology, microgrids feature intelligent emss and sophisticated control, which will dramatically change our energy infrastructure. a modern microgrids are the relatively recent development with high potential to bring distributed generation, des devices, controllable loads, communication infrastructure, and many new technologies into a mainstream. as the more controllable and intelligent entity, the microgrid has more growth potential than ever before. however, there are still many open questions, such as a future business models and economics. what was a cost-benefit to a end-user? how should we systematically evaluate a potential benefits and costs of control and energy management inside the microgrid?",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
13300,"we report a discovery of a quadruply lensed quasar j1433+6007, mined inside a sdss dr12 photometric catalogues with the help of the novel outlier-selection technique, without prior spectroscopic or uv excess information. discovery data obtained at a nordic optical telescope (not, la palma) show nearly identical quasar spectra at $z_s=2.74$ and four quasar images inside the fold configuration, one of which sits on the blue arc. a deflector redshift was $z_{l}=0.407,$ from keck-esi spectra. we describe a selection procedure, discovery and follow-up, image positions and $bvri$ magnitudes, and first results and forecasts from simple lens models.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11786,"a practical importance of robust inference considering causal effects inside regression discontinuity and kink designs was widely recognized. existing methods cover many cases, but do not handle robust uniform inference considering cdf and quantile processes inside fuzzy designs, despite its frequent use inside a recent literature inside empirical microeconomics. inside this light, this paper extends a literature by developing the unified framework of robust inference that applies to uniform inference considering quantile treatment effects inside fuzzy designs, as well as all a other cases of sharp/fuzzy mean/quantile regression discontinuity/kink designs. we present monte carlo simulation studies and an empirical application considering evaluations of a oklahoma pre-k program.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
15870,"we obtain the bernstein-type inequality considering sums of banach-valued random variables satisfying the weak dependence assumption of general type and under certain smoothness assumptions of a underlying banach norm. we use this inequality inside order to investigate inside a asymptotical regime a error upper bounds considering a broad family of spectral regularization methods considering reproducing kernel decision rules, when trained on the sample coming from the $\tau-$mixing process.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0
1205,"inside an open-world setting, it was inevitable that an intelligent agent (e.g., the robot) will encounter visual objects, attributes or relationships it does not recognize. inside this work, we develop an agent empowered with visual curiosity, i.e. a ability to ask questions to an oracle (e.g., human) about a contents inside images (e.g., what was a object on a left side of a red cube?) and build visual recognition model based on a answers received (e.g., cylinder). inside order to do this, a agent must (1) understand what it recognizes and what it does not, (2) formulate the valid, unambiguous and informative language query (a question) to ask a oracle, (3) derive a parameters of visual classifiers from a oracle response and (4) leverage a updated visual classifiers to ask more clarified questions. specifically, we propose the novel framework and formulate a learning of visual curiosity as the reinforcement learning problem. inside this framework, all components of our agent, visual recognition module (to see), question generation policy (to ask), answer digestion module (to understand) and graph memory module (to memorize), are learned entirely end-to-end to maximize a reward derived from a scene graph obtained by a agent as the consequence of a dialog with a oracle. importantly, a question generation policy was disentangled from a visual recognition system and specifics of a environment. consequently, we demonstrate the sort of double generalization. our question generation policy generalizes to new environments and the new pair of eyes, i.e., new visual system. trained on the synthetic dataset, our results show that our agent learns new visual concepts significantly faster than several heuristic baselines, even when tested on synthetic environments with novel objects, as well as inside the realistic environment.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
16328,"a paper discusses a peculiarities of flame propagation inside a ultra-lean hydrogen-air mixture. numerical analysis of a problem shows a possibility of a stable self-sustained flame ball existence inside unconfined space on sufficiently large spatial scales. a structure of a flame ball was determined by a convection processes related to a hot products rising inside a terrestrial gravity field. it was shown that a structure of a flame ball corresponds to a axisymmetric structures of a gaseous bubble inside a liquid. inside addition to a stable flame core, there are satellite burning kernels separated from a original flameball and developing in a thermal wake behind a propagating flame ball. a effective area of burning expands with time due to flame ball and satellite kernels development. both stable flame ball existence inside a ultra-lean mixture and increase inside a burning area indicate a possibility of transition to rapid deflagrative combustion as soon as a flame ball enters a region filled with hydrogen-air mixture of a richer composition. such the scenario was intrinsic to a natural spatial distribution of hydrogen inside a conditions of terrestrial gravity and therefore it was crucial to take it into account inside elaborating risk assessments techniques and prevention measures.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1206,"during a past few years advancements inside sports information systems and technology has allowed us to collect the number of detailed spatio-temporal data capturing various aspects of basketball. considering example, shot charts, that is, maps capturing locations of (made or missed) shots, and spatio-temporal trajectories considering all a players on a court should capture information about a offensive and defensive tendencies and schemes of the team. characterization of these processes was important considering player and team comparisons, pre-game scouting, game preparation etc. playing tendencies among teams have traditionally been compared inside the heuristic manner. recently automated ways considering similar comparisons have appeared inside a sports analytics literature. however, these approaches are almost exclusively focused on a spatial distribution of a underlying actions (usually shots taken), ignoring the multitude of other parameters that should affect a action studied. inside this work, we propose the framework based on tensor decomposition considering obtaining the set of prototype spatio-temporal patterns based on a core spatiotemporal information and contextual meta-data. a core of our framework was the 3d tensor x, whose dimensions represent a entity under consideration (team, player, possession etc.), a location on a court and time. we make use of a parafac decomposition and we decompose a tensor into several interpretable patterns, that should be thought of as prototype patterns of a process examined (e.g., shot selection, offensive schemes etc.). we also introduce an idea behind the method considering choosing a number of components to be considered. with the help of a tensor components, we should then express every entity as the weighted combination of these components. a framework introduced inside this paper should have further applications inside a work-flow of a basketball operations of the franchise, which we also briefly discuss.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15711,"networks should model real-world systems inside the variety of domains. network alignment (na) aims to find the node mapping that conserves similar regions between compared networks. na was applicable to many fields, including computational biology, where na should guide a transfer of biological knowledge from well- to poorly-studied species across aligned network regions. existing na methods should only align static networks. however, most complex real-world systems evolve over time and should thus be modeled as dynamic networks. we hypothesize that aligning dynamic network representations of evolving systems will produce superior alignments compared to aligning a systems' static network representations, as was currently done. considering this purpose, we introduce a first ever dynamic na method, dynamagna++. this proof-of-concept dynamic na method was an extension of the state-of-the-art static na method, magna++. even though both magna++ and dynamagna++ optimize edge as well as node conservation across a aligned networks, magna++ conserves static edges and similarity between static node neighborhoods, while dynamagna++ conserves dynamic edges (events) and similarity between evolving node neighborhoods. considering this purpose, we introduce a first ever measure of dynamic edge conservation and rely on our recent measure of dynamic node conservation. importantly, a two dynamic conservation measures should be optimized with the help of any state-of-the-art na method and not just magna++. we confirm our hypothesis that dynamic na was superior to static na, under fair comparison conditions, on synthetic and real-world networks, inside computational biology and social network domains. dynamagna++ was parallelized and it includes the user-friendly graphical interface.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
5412,"a aim of this paper was to establish the lattice theoretical framework to study a partially ordered set $\operatorname{\mathsf{tors}} a$ of torsion classes over the finite-dimensional algebra $a$. we show that $\operatorname{\mathsf{tors}} a$ was the complete lattice which enjoys very strong properties, as bialgebraicity and complete semidistributivity. thus its hasse quiver carries a important part of its structure, and we introduce a brick labelling of its hasse quiver and use it to study lattice congruences of $\operatorname{\mathsf{tors}} a$. inside particular, we give the representation-theoretical interpretation of a so-called forcing order, and we prove that $\operatorname{\mathsf{tors}} a$ was completely congruence uniform. when $i$ was the two-sided ideal of $a$, $\operatorname{\mathsf{tors}} (a/i)$ was the lattice quotient of $\operatorname{\mathsf{tors}} a$ which was called an algebraic quotient, and a corresponding lattice congruence was called an algebraic congruence. a second part of this paper consists inside studying algebraic congruences. we characterize a arrows of a hasse quiver of $\operatorname{\mathsf{tors}} a$ that are contracted by an algebraic congruence inside terms of a brick labelling. inside a third part, we study inside detail a case of preprojective algebras $\pi$, considering which $\operatorname{\mathsf{tors}} \pi$ was a weyl group endowed with a weak order. inside particular, we give the new, more representation theoretical proof of a isomorphism between $\operatorname{\mathsf{tors}} k q$ and a cambrian lattice when $q$ was the dynkin quiver. we also prove that, inside type $a$, a algebraic quotients of $\operatorname{\mathsf{tors}} \pi$ are exactly its hasse-regular lattice quotients.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
19466,the brief introduction to exterior differential systems considering graduate students familiar with manifolds and differential forms.,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11025,"inside this proceeding, we summarize a key science goals and reference design considering the next-generation very large array (ngvla) that was envisaged to operate inside a 2030s. a ngvla was an interferometric array with more than 10 times a sensitivity and spatial resolution of a current vla and alma, that will operate at frequencies spanning $\sim 1.2 -116$ ghz, thus lending itself to be highly complementary to alma and a ska1. as such, a ngvla will tackle the broad range of outstanding questions inside modern astronomy by simultaneously delivering a capability to: unveil a formation of solar system analogues; probe a initial conditions considering planetary systems and life with astrochemistry; characterize a assembly, structure, and evolution of galaxies from a first billion years to a present; use pulsars inside a galactic center as fundamental tests of gravity; and understand a formation and evolution of stellar and supermassive blackholes inside a era of multi-messenger astronomy.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
2798,"this article concerns the class of generalized linear mixed models considering clustered data, where a random effects are mapped uniquely onto a grouping structure and are independent between groups. we derive necessary and sufficient conditions that enable a marginal likelihood of such class of models to be expressed inside closed-form. illustrations are provided with the help of a gaussian, poisson, binomial and gamma distributions. these models are unified under the single umbrella of conjugate generalized linear mixed models, where ""conjugate"" refers to a fact that a marginal likelihood should be expressed inside closed-form, rather than implying inference using a bayesian paradigm. having an explicit marginal likelihood means that these models are more computationally convenient, which should be important inside big data contexts. except considering a binomial distribution, these models are able to achieve simultaneous conjugacy, and thus able to accommodate both unit and group level covariates.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
12283,"we present new atacama large millimeter/sub-millimeter array (alma) 1.3 mm continuum observations of a sr 24s transition disk with an angular resolution $\lesssim0.18""$ (12 au radius). we perform the multi-wavelength investigation by combining new data with previous alma data at 0.45 mm. a visibilities and images of a continuum emission at a two wavelengths are well characterized by the ring-like emission. visibility modeling finds that a ring-like emission was narrower at longer wavelengths, inside good agreement with models of dust trapping inside pressure bumps, although there are complex residuals that suggest potentially asymmetric structures. a 0.45 mm emission has the shallower profile in a central cavity than a 1.3 mm emission. inside addition, we find that a $^{13}$co and c$^{18}$o (j=2-1) emission peaks at a center of a continuum cavity. we do not detect either continuum or gas emission from a northern companion to this system (sr 24n), which was itself the binary system. a upper limit considering a dust disk mass of sr 24n was $\lesssim 0.12\,m_{\bigoplus}$, which gives the disk mass ratio inside dust between a two components of $m_{\mathrm{dust, sr\,24s}}/m_{\mathrm{dust, sr\,24n}}\gtrsim840$. a current alma observations may imply that either planets have already formed inside a sr 24n disk or that dust growth to mm-sizes was inhibited there and that only warm gas, as seen by ro-vibrational co emission in a truncation radii of a binary, was present.",0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3266,"we present near infrared high-precision photometry considering eight transiting hot jupiters observed during their predicted secondary eclipses. our observations were carried out with the help of a staring mode of a wircam instrument on a canada-france-hawaii telescope (cfht). we present a observing strategies and data reduction methods which delivered time series photometry with statistical photometric precisionas low as 0.11%. we performed the bayesian analysis to model a eclipse parameters and systematics simultaneously. a measured planet-to-star flux ratios allowed us to constrain a thermal emission from a day side of these hot jupiters, as we derived a planet brightness temperatures. our results combined with previously observed eclipses reveal an excess inside a brightness temperatures relative to a blackbody prediction considering a equilibrium temperatures of a planets considering the wide range of heat redistribution factors. we find the trend that this excess appears to be larger considering planets with lower equilibrium temperatures. this may imply some additional sources of radiation, such as reflected light from a host star and/or thermal emission from residual internal heat from a formation of a planet.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
17125,"by the memory mean-field process we mean a solution $x(\cdot)$ of the stochastic mean-field equation involving not just a current state $x(t)$ and its law $\mathcal{l}(x(t))$ at time $t$, but also a state values $x(s)$ and its law $\mathcal{l}(x(s))$ at some previous times $s<t$. our purpose was to study stochastic control problems of memory mean-field processes. - we consider a space $\mathcal{m}$ of measures on $\mathbb{r}$ with a norm $|| \cdot||_{\mathcal{m}}$ introduced by agram and {\o}ksendal inside \cite{ao1}, and prove a existence and uniqueness of solutions of memory mean-field stochastic functional differential equations. - we prove two stochastic maximum principles, one sufficient (a verification theorem) and one necessary, both under partial information. a corresponding equations considering a adjoint variables are the pair of \emph{(time-) advanced backward stochastic differential equations}, one of them with values inside a space of bounded linear functionals on path segment spaces. - as an application of our methods, we solve the memory mean-variance problem as well as the linear-quadratic problem of the memory process.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
5603,"this paper presents an intertemporal bimodal network to analyze a evolution of a semantic content of the scientific field within a framework of topic modeling, namely with the help of a latent dirichlet allocation (lda). a main contribution was a conceptualization of a topic dynamics and its formalization and codification into an algorithm. to benchmark a effectiveness of this approach, we propose three indexes which track a transformation of topics over time, their rate of birth and death, and a novelty of their content. applying a lda, we test a algorithm both on the controlled experiment and on the corpus of several thousands of scientific papers over the period of more than 100 years which account considering a history of a economic thought.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16764,"this paper proposes the general framework considering structure-preserving model reduction of the secondorder network system based on graph clustering. inside this approach, vertex dynamics are captured by a transfer functions from inputs to individual states, and a dissimilarities of vertices are quantified by a h2-norms of a transfer function discrepancies. the greedy hierarchical clustering algorithm was proposed to place those vertices with similar dynamics into same clusters. then, a reduced-order model was generated by a petrov-galerkin method, where a projection was formed by a characteristic matrix of a resulting network clustering. it was shown that a simplified system preserves an interconnection structure, i.e., it should be again interpreted as the second-order system evolving over the reduced graph. furthermore, this paper generalizes a definition of network controllability gramian to second-order network systems. based on it, we develop an efficient method to compute h2-norms and derive a approximation error between a full-order and reduced-order models. finally, a idea behind the method was illustrated by a example of the small-world network.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
19807,"automated decision making systems are increasingly being used inside real-world applications. inside these systems considering a most part, a decision rules are derived by minimizing a training error on a available historical data. therefore, if there was the bias related to the sensitive attribute such as gender, race, religion, etc. inside a data, say, due to cultural/historical discriminatory practices against the certain demographic, a system could continue discrimination inside decisions by including a said bias inside its decision rule. we present an information theoretic framework considering designing fair predictors from data, which aim to prevent discrimination against the specified sensitive attribute inside the supervised learning setting. we use equalized odds as a criterion considering discrimination, which demands that a prediction should be independent of a protected attribute conditioned on a actual label. to ensure fairness and generalization simultaneously, we compress a data to an auxiliary variable, which was used considering a prediction task. this auxiliary variable was chosen such that it was decontaminated from a discriminatory attribute inside a sense of equalized odds. a final predictor was obtained by applying the bayesian decision rule to a auxiliary variable.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2385,"we calculate a width $2\delta_{\text{ct}}$ and intensity of a charge-transfer peak (the one lying at a on-site energy $e_d$) inside a impurity spectral density of states as the function of $e_d$ inside a su($n$) impurity anderson model (iam). we use a dynamical density-matrix renormalization group (ddmrg) and a noncrossing-approximation (nca) considering $n$=4, and the 1/$n$ variational approximation inside a general case. inside particular, while considering $e_d \gg \delta$, where $\delta$ was a resonant level half-width, $\delta_{\text{ct}}=\delta$ as expected inside a noninteracting case, considering $-e_d \gg n \delta$ one has $\delta_{\text{ct}}=n\delta$. inside a $n$=2 case, some effects of a variation of $% \delta_{\text{ct}}$ with $e_d$ were observed inside a conductance through the quantum dot connected asymmetrically to conducting leads at finite bias [j. k√∂nemann \textit{et al.}, phys. rev. b \textbf{73}, 033313 (2006)]. more dramatic effects are expected inside similar experiments, that should be carried out inside systems of two quantum dots, carbon nanotubes or other, realizing a su(4) iam.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
11649,"we use bbn observational data on primordial abundance of ${}^4he$ to constrain f(t) gravity. a three most studied viable $f(t)$ models, namely a power law, a exponential and a square-root exponential are considered, and a bbn bounds are adopted inside order to extract constraints on their free parameters. considering a power-law model, we find that a constraints are inside agreement with those acquired with the help of late-time cosmological data. considering a exponential and a square-root exponential models, we show that considering realiable regions of parameters space they always satisfy a bbn bounds. we conclude that viable f(t) models should successfully satisfy a bbn constraints.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8666,"by with the help of a de giorgi iteration method we will give the new simple proof of a recent result of b.kotschwar, o.munteanu, j.wang [kmw] and n.sesum [s] on a local boundedness of a riemmanian curvature tensor of solutions of ricci flow inside terms of its inital value on the given ball and the local uniform bound on a ricci curvature.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12936,"inside spoken languages, speakers divide up a space of phonetic possibilities into different regions, corresponding to different phonemes. we consider the simple exemplar model of how this division of phonetic space varies over time among the population of language users. inside a particular model we consider, we show that, once a system was initialized with the given set of phonemes, that phonemes do not become extinct: all phonemes will be maintained inside a system considering all time. this was inside contrast to what was observed inside more complex models. furthermore, we show that a boundaries between phonemes fluctuate and we quantitatively study a fluctuations inside the simple instance of our model. these results prepare a ground considering more sophisticated models inside which some phonemes go extinct or new phonemes emerge through other processes.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8723,"sequential pattern mining techniques extract patterns corresponding to frequent subsequences from the sequence database. the practical limitation of these techniques was that they overload a user with too many patterns. local process model (lpm) mining was an alternative idea behind the method coming from a field of process mining. while inside traditional sequential pattern mining, the pattern describes one subsequence, an lpm captures the set of subsequences. also, while traditional sequential patterns only match subsequences that are observed inside a sequence database, an lpm may capture subsequences that are not explicitly observed, but that are related to observed subsequences. inside other words, lpms generalize a behavior observed inside a sequence database. these properties make it possible considering the set of lpms to cover a behavior of the much larger set of sequential patterns. yet, existing lpm mining techniques still suffer from a pattern explosion problem because they produce sets of redundant lpms. inside this paper, we propose several heuristics to mine the set of non-redundant lpms either from the set of redundant lpms or from the set of sequential patterns. we empirically compare a proposed heuristics between them and against existing (local) process mining techniques inside terms of coverage, redundancy, and complexity of a produced sets of lpms.",1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17559,"the good classification method should yield more accurate results than simple heuristics. but there are classification problems, especially high-dimensional ones like a ones based on image/video data, considering which simple heuristics should work quite accurately; a structure of a data inside such problems was easy to uncover without any sophisticated or computationally expensive method. on a other hand, some problems have the structure that should only be found with sophisticated pattern recognition methods. we are interested inside quantifying a difficulty of the given high-dimensional pattern recognition problem. we consider a case where a patterns come from two pre-determined classes and where a objects are represented by points inside the high-dimensional vector space. however, a framework we propose was extendable to an arbitrarily large number of classes. we propose classification benchmarks based on simple random projection heuristics. our benchmarks are 2d curves parameterized by a classification error and computational cost of these simple heuristics. each curve divides a plane into the ""positive- gain"" and the ""negative-gain"" region. a latter contains methods that are ill-suited considering a given classification problem. a former was divided into two by a curve asymptote; methods that lie inside a small region under a curve but right of a asymptote merely provide the computational gain but no structural advantage over a random heuristics. we prove that a curve asymptotes are optimal (i.e. at bayes error) inside some cases, and thus no sophisticated method should provide the structural advantage over a random heuristics. such classification problems, an example of which we present inside our numerical experiments, provide poor ground considering testing new pattern classification methods.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5450,"we explore a topological properties of quantum spin-1/2 chains with two ising symmetries. this class of models does not possess any of a symmetries that are required to protect a haldane phase. nevertheless, we show that there are 4 symmetry-protected topological phases, inside addition to 6 phases that spontaneously break one or both ising symmetries. by mapping a model to one-dimensional interacting fermions with particle-hole and time-reversal symmetry, we obtain integrable parent hamiltonians considering a conventional and topological phases of a spin model. we use these hamiltonians to characterize a physical properties of all 10 phases, identify their local and nonlocal order parameters, and understand a effects of weak perturbations that respect a ising symmetries. our study provides a first explicit example of the class of spin chains with several topologically non-trivial phases, and binds together a topological classifications of interacting bosons and fermions.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
19304,"gravitational lensing of a cosmic microwave background (cmb) was expected to be amongst a most powerful cosmological tools considering ongoing and upcoming cmb experiments. inside this work, we investigate the bias to cmb lensing reconstruction from temperature anisotropies due to a kinematic sunyaev-zel'dovich (ksz) effect, that is, a doppler shift of cmb photons induced by compton-scattering off moving electrons. a ksz signal yields biases due to both its own intrinsic non-gaussianity and its non-zero cross-correlation with a cmb lensing field (and other fields that trace a large-scale structure). this ksz-induced bias affects both a cmb lensing auto-power spectrum and its cross-correlation with low-redshift tracers. furthermore, it cannot be removed by multifrequency foreground separation techniques because a ksz effect preserves a blackbody spectrum of a cmb. while statistically negligible considering current datasets, we show that it will be important considering upcoming surveys, and failure to account considering it should lead to large biases inside constraints on neutrino masses or a properties of dark energy. considering the stage 4 cmb experiment, a bias should be as large as $\approx$ 15% or 12% inside cross-correlation with lsst galaxy lensing convergence or galaxy overdensity maps, respectively, when a maximum temperature multipole used inside a reconstruction was $\ell_{\rm max} = 4000$, and about half of that when $\ell_{\rm max} = 3000$. similarly, we find that a cmb lensing auto-power spectrum should be biased by up to several percent. these biases are many times larger than a expected statistical errors. reducing $\ell_{\rm max}$ should significantly mitigate a bias at a cost of the decrease inside a overall lensing reconstruction signal-to-noise. polarization-only reconstruction may be a most robust mitigation strategy.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17683,"phylogenetic networks are often constructed by merging multiple conflicting phylogenetic signals into the directed acyclic graph. it was interesting to explore whether the network constructed inside this way induces biologically-relevant phylogenetic signals that were not present inside a input. here we show that, given the multiple alignment the considering the set of taxa x and the rooted phylogenetic network n whose leaves are labelled by x, it was np-hard to locate a most parsimonious phylogenetic tree displayed by n (with respect to a) even when a level of n - a maximum number of reticulation nodes within the biconnected component - was 1 and the contains only 2 distinct states. (if, additionally, gaps are allowed a problem becomes apx-hard.) we also show that under a same conditions, and assuming the simple binary symmetric model of character evolution, finding a most likely tree displayed by a network was np-hard. these negative results contrast with earlier work on parsimony inside which it was shown that if the consists of the single column a problem was fixed parameter tractable inside a level. we conclude with the discussion of why, despite a np-hardness, both a parsimony and likelihood problem should likely be well-solved inside practice.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2621,"influence diagrams are the decision-theoretic extension of probabilistic graphical models. inside this paper we show how they should be used to solve a brachistochrone problem. we present results of numerical experiments on this problem, compare a solution provided by a influence diagram with a optimal solution. a r code used considering a experiments was presented inside a appendix.",1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
10393,"inside a evaluation of treatment effects, it was of major policy interest to know if a treatment was beneficial considering some and harmful considering others, the phenomenon known as qualitative interaction. we formulate this question as the multiple testing problem with many conservative null $p$-values, inside which a classical multiple testing methods may lose power substantially. we propose the simple technique---conditioning---to improve a power. the crucial assumption we need was uniform conservativeness, meaning considering any conservative $p$-value $p$, a conditional distribution $(p/\tau)\,|\,p \le \tau$ was stochastically larger than a uniform distribution on $(0,1)$ considering any $\tau$. we show this property holds considering one-sided tests inside the one-dimensional exponential family (e.g.\ testing considering qualitative interaction) as well as testing $|\mu|\le\eta$ with the help of the statistic $x \sim \mathrm{n}(\mu,1)$ (e.g.\ testing considering practical importance with threshold $\eta$). we propose an adaptive method to select a threshold $\tau$. our theoretical and simulation results suggest a proposed tests gain significant power when many $p$-values are uniformly conservative and lose little power when no $p$-value was uniformly conservative. we apply our method to two educational intervention datasets.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
8521,"predicting a outcome of sports events was the hard task. we quantify this difficulty with the coefficient that measures a distance between a observed final results of sports leagues and idealized perfectly balanced competitions inside terms of skill. this indicates a relative presence of luck and skill. we collected and analyzed all games from 198 sports leagues comprising 1503 seasons from 84 countries of 4 different sports: basketball, soccer, volleyball and handball. we measured a competitiveness by countries and sports. we also identify inside each season which teams, if removed from its league, result inside the completely random tournament. surprisingly, not many of them are needed. as another contribution of this paper, we propose the probabilistic graphical model to learn about a teams' skills and to decompose a relative weights of luck and skill inside each game. we break down a skill component into factors associated with a teams' characteristics. a model also allows to approximate as 0.36 a probability that an underdog team wins inside a nba league, with the home advantage adding 0.09 to this probability. as shown inside a first part of a paper, luck was substantially present even inside a most competitive championships, which partially explains why sophisticated and complex feature-based models hardly beat simple models inside a task of forecasting sports' outcomes.",1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11340,"this paper proposes three different distributed event-triggered control algorithms to achieve leader-follower consensus considering the network of euler-lagrange agents. we firstly propose two model-independent algorithms considering the subclass of euler-lagrange agents without a vector of gravitational potential forces. by model-independent, we mean that each agent should execute its algorithm with no knowledge of a agent self-dynamics. the variable-gain algorithm was employed when a sensing graph was undirected; algorithm parameters are selected inside the fully distributed manner with much greater flexibility compared to all previous work concerning event-triggered consensus problems. when a sensing graph was directed, the constant-gain algorithm was employed. a control gains must be centrally designed to exceed several lower bounding inequalities which require limited knowledge of bounds on a matrices describing a agent dynamics, bounds on network topology information and bounds on a initial conditions. when a euler-lagrange agents have dynamics which include a vector of gravitational potential forces, an adaptive algorithm was proposed which requires more information about a agent dynamics but should approximate uncertain agent parameters. considering each algorithm, the trigger function was proposed to govern a event update times. at each event, a controller was updated, which ensures that a control input was piecewise constant and saves energy resources. we analyse each controllers and trigger function and exclude zeno behaviour. extensive simulations show 1) a advantages of our proposed trigger function as compared to those inside existing literature, and 2) a effectiveness of our proposed controllers.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
10443,"we present an update to a ultraviolet-to-radio database of global broadband photometry considering a 79 nearby galaxies that comprise a union of a kingfish (key insights on nearby galaxies: the far-infrared survey with herschel) and sings (spitzer infrared nearby galaxies survey) samples. a 34-band dataset presented here includes contributions from observational work carried out with the variety of facilities including galex, sdss, ps, noao, 2mass, wise, spitzer, herschel, planck, jcmt, and a vla. improvements of note include recalibrations of previously-published sings bvrcic and kingfish far-infrared/submillimeter photometry. similar to previous results inside a literature, an excess of submillimeter emission above model predictions was seen primarily considering low-metallicity dwarf/irregular galaxies. this 34-band photometric dataset considering a combined kingfish$+$sings sample serves as an important multi-wavelength reference considering a variety of galaxies observed at low redshift. the thorough analysis of a observed spectral energy distributions was carried out inside the companion paper.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4133,"the framework integrating information theory and network science was proposed, giving rise to the potentially new area. by incorporating and integrating concepts such as complexity, coding, topological projections and network dynamics, a proposed network-based framework paves a way not only to extending traditional information science, but also to modeling, characterizing and analyzing the broad class of real-world problems, from language communication to dna coding. basically, an original network was supposed to be transmitted, with or without compaction, through the sequence of symbols or time-series obtained by sampling its topology by some network dynamics, such as random walks. we show that a degree of compression was ultimately related to a ability to predict a frequency of symbols based on a topology of a original network and a adopted dynamics. a potential of a proposed idea behind the method was illustrated with respect to a efficiency of transmitting several types of topologies by with the help of the variety of random walks. several interesting results are obtained, including a behavior of a barab√°si-albert model oscillating between high and low performance depending on a considered dynamics, and a distinct performances obtained considering two geographical models.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0
5727,"measuring a degree of spatial spreading of the sample should be of great interest when sampling from the spatial population. a commonly used spatial balance index by grafstr√∂m et al. (2012) was particularly effective inside comparing a level of spatial spreading of different samples from a same population. however, its unbounded and uninterpretable scale of measurement does not allow to assess a level of spatial spreading inside absolute terms and confines its use to only raw comparisons. inside this paper, we introduce the new absolute measure of a spatial spreading of the sample with the help of the normalized version of a moran's $i$ index. a properties and behaviour of a proposed measure are analysed through two simulation experiments, one based on artificial populations and a other on the population of real business units located inside a province of siena (italy).",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
15953,"sales forecast was an essential task inside e-commerce and has the crucial impact on making informed business decisions. it should aid us to manage a workforce, cash flow and resources such as optimizing a supply chain of manufacturers etc. sales forecast was the challenging problem inside that sales was affected by many factors including promotion activities, price changes, and user preferences etc. traditional sales forecast techniques mainly rely on historical sales data to predict future sales and their accuracies are limited. some more recent learning-based methods capture more information inside a model to improve a forecast accuracy. however, these methods require case-by-case manual feature engineering considering specific commercial scenarios, which was usually the difficult, time-consuming task and requires expert knowledge. to overcome a limitations of existing methods, we propose the novel idea behind the method inside this paper to learn effective features automatically from a structured data with the help of a convolutional neural network (cnn). when fed with raw log data, our idea behind the method should automatically extract effective features from that and then forecast sales with the help of those extracted features. we test our method on the large real-world dataset from cainiao.com and a experimental results validate a effectiveness of our method.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1289,"maintenance of association rules was an interesting problem. several incremental maintenance algorithms were proposed since a work of (cheung et al, 1996). a majority of these algorithms maintain rule bases assuming that support threshold doesn't change. inside this paper, we present incremental maintenance algorithm under support threshold change. this solution allows user to maintain its rule base under any support threshold.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8414,"this paper studies a partially observed stochastic optimal control problem considering systems with state dynamics governed by partial differential equations (pdes) that leads to an extremely large problem. first, an open-loop deterministic trajectory optimization problem was solved with the help of the black box simulation model of a dynamical system. next, the linear quadratic gaussian (lqg) controller was designed considering a nominal trajectory-dependent linearized system, which was identified with the help of input-output experimental data consisting of a impulse responses of a optimized nominal system. the computational nonlinear heat example was used to illustrate a performance of a approach.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1
9389,"most existing neural network models considering music generation use recurrent neural networks. however, a recent wavenet model proposed by deepmind shows that convolutional neural networks (cnns) should also generate realistic musical waveforms inside a audio domain. following this light, we investigate with the help of cnns considering generating melody (a series of midi notes) one bar after another inside a symbolic domain. inside addition to a generator, we use the discriminator to learn a distributions of melodies, making it the generative adversarial network (gan). moreover, we propose the novel conditional mechanism to exploit available prior knowledge, so that a model should generate melodies either from scratch, by following the chord sequence, or by conditioning on a melody of previous bars (e.g. the priming melody), among other possibilities. a resulting model, named midinet, should be expanded to generate music with multiple midi channels (i.e. tracks). we conduct the user study to compare a melody of eight-bar long generated by midinet and by google's melodyrnn models, each time with the help of a same priming melody. result shows that midinet performs comparably with melodyrnn models inside being realistic and pleasant to listen to, yet midinet's melodies are reported to be much more interesting.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14678,"we propose object-oriented neural programming (oonp), the framework considering semantically parsing documents inside specific domains. basically, oonp reads the document and parses it into the predesigned object-oriented data structure (referred to as ontology inside this paper) that reflects a domain-specific semantics of a document. an oonp parser models semantic parsing as the decision process: the neural net-based reader sequentially goes through a document, and during a process it builds and updates an intermediate ontology to summarize its partial understanding of a text it covers. oonp supports the rich family of operations (both symbolic and differentiable) considering composing a ontology, and the big variety of forms (both symbolic and differentiable) considering representing a state and a document. an oonp parser should be trained with supervision of different forms and strength, including supervised learning (sl) , reinforcement learning (rl) and hybrid of a two. our experiments on both synthetic and real-world document parsing tasks have shown that oonp should learn to handle fairly complicated ontology with training data of modest sizes.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15098,"inside this study, we present the fully automatic method to segment both rectum and rectal cancer based on deep neural networks (dnns) with axial t2-weighted magnetic resonance images. clinically, a relative location between rectum and rectal cancer plays an important role inside cancer treatment planning. such the need motivates us to propose the fully convolutional architecture considering multi-task learning (mtl) to segment both rectum and rectal cancer. moreover, we propose the bias-variance decomposition-based method which should visualize and assess regional robustness of a segmentation model. inside addition, we also suggest the novel augmentation method which should improve a segmentation performance as well as reduce a training time. overall, our proposed method was not only computationally efficient due to its fully convolutional nature but also outperforms a current state-of-the-art considering rectal cancer segmentation. it also scores high accuracy inside rectum segmentation without any prior study reported. moreover, we conclude that supplementing rectum information benefits a rectal cancer segmentation model, especially inside model variance.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8344,"electron ptychography has seen the recent surge of interest considering phase sensitive imaging at atomic or near-atomic resolution. however, applications are so far mainly limited to radiation-hard samples because a required doses are too high considering imaging biological samples at high resolution. we propose a use of non-convex, bayesian optimization to overcome this problem and reduce a dose required considering successful reconstruction by two orders of magnitude compared to previous experiments. we suggest to use this method considering imaging single biological macromolecules at cryogenic temperatures and demonstrate 2d single-particle reconstructions from simulated data with the resolution of 7.9 \aa$\,$ at the dose of 20 $e^- / \aa^2$. when averaging over only 15 low-dose datasets, the resolution of 4 \aa$\,$ was possible considering large macromolecular complexes. with its independence from microscope transfer function, direct recovery of phase contrast and better scaling of signal-to-noise ratio, cryo-electron ptychography may become the promising alternative to zernike phase-contrast microscopy.",0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
9702,"considering a structure of dc microgrids (mgs) composed of grid-forming/feeding converters, the hierarchical plug-and-play (pnp) voltage/current control architecture considering mg clusters was proposed. inside a primary level, the pnp voltage/current controller was proposed to achieve simultaneous voltage support and current feeding function according to local references. inside addition, stabilizing controller was characterized by explicit inequalities which are only related to local parameters of the mg. inside a secondary level, considering a system with interconnection of mgs, the leader-based voltage/current distributed controller was proposed to achieve both voltage and current regulation without specifying a individual setpoints considering each mg. a proposed controller requires the communication network and each controller exchanges information with its communication neighbors only. with a proposed controller, each mg should plug-in/out of a system seamlessly, irrespectively of a power line parameters and models of other mgs . a proof of a mg cluster closed-loop stability exploits structured lyapunov functions, a lasalle invariance theorem and properties of graph laplacians. theoretical results are validated by hardware-in-loop (hil) tests.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
13996,"tissue oxygenation and perfusion should be an indicator considering organ viability during minimally invasive surgery, considering example allowing real-time assessment of tissue perfusion and oxygen saturation. multispectral imaging was an optical modality that should inspect tissue perfusion inside wide field images without contact. inside this paper, we present the novel, fast method considering with the help of rgb images considering msi, which while limiting a spectral resolution of a modality allows normal laparoscopic systems to be used. we exploit a discrete haar decomposition to separate individual video frames into low pass and directional coefficients and we utilise the different multispectral approximation technique on each. a increase inside speed was achieved by with the help of fast tikhonov regularisation on a directional coefficients and more accurate bayesian approximation on a low pass component. a pipeline was implemented with the help of the graphics processing unit (gpu) architecture and achieves the frame rate of approximately 15hz. we validate a method on animal models and on human data captured with the help of the da vinci stereo laparoscope.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6972,"we recently showed that several local group (lg) galaxies have much higher radial velocities (rvs) than predicted by the 3d dynamical model of a standard cosmological paradigm. here, we show that 6 of these 7 galaxies define the thin plane with root mean square thickness of only 101 kpc despite the widest extent of nearly 3 mpc, much larger than a conventional virial radius of a milky way (mw) or m31. this plane passes within ${\sim 70}$ kpc of a mw-m31 barycentre and was oriented so a mw-m31 line was inclined by $16^\circ$ to it. we develop the toy model to constrain a scenario whereby the past mw-m31 flyby inside modified newtonian dynamics (mond) forms tidal dwarf galaxies that settle into a recently discovered planes of satellites around a mw and m31. a scenario was viable only considering the particular mw-m31 orbital plane. this roughly coincides with a plane of lg dwarfs with anomalously high rvs. with the help of the restricted $n$-body simulation of a lg inside mond, we show how a once fast-moving mw and m31 gravitationally slingshot test particles outwards at high speeds. a most distant such particles preferentially lie within a mw-m31 orbital plane, probably because a particles ending up with a highest rvs are those flung out almost parallel to a motion of a perturber. this suggests the dynamical reason considering our finding of the similar trend inside a real lg, something not easily explained as the chance alignment of galaxies with an isotropic or mildly flattened distribution (probability $= {0.0015}$).",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5761,"a exponential growth inside data generation and large-scale data analysis creates an unprecedented need considering inexpensive, low-latency, and high-density information storage. this need has motivated significant research into multi-level memory systems that should store multiple bits of information per device. although both a memory state of these devices and much of a data they store are intrinsically analog-valued, both are quantized considering use with digital systems and discrete error correcting codes. with the help of phase change memory as the prototypical multi-level storage technology, we herein demonstrate that analog-valued devices should achieve higher capacities when paired with analog codes. further, we find that storing analog signals directly through joint-coding should achieve low distortion with reduced coding complexity. by jointly optimizing considering signal statistics, device statistics, and the distortion metric, finite-length analog encodings should perform comparable to digital systems with asymptotically infinite large encodings. these results show that end-to-end analog memory systems have not only a potential to reach higher storage capacities than discrete systems, but also to significantly lower coding complexity, leading to faster and more energy efficient storage.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
9074,"inside statistical applications, a normal and a laplace distributions are often contrasted: a former as the standard tool of analysis, a latter as its robust counterpart. i discuss a convolutions of these two popular distributions and their applications inside research. i consider four models within the simple $2\times 2$ scheme which was of practical interest inside a analysis of clustered (e.g., longitudinal) data. inside my view, these models, some of which are less known than others by a majority of applied researchers, constitute the 'family' of sensible alternatives when modelling issues arise. inside three examples, i revisit data published recently inside a epidemiological and clinical literature as well as the classic biological dataset.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
4460,"high-speed 100 mhz strain monitor with the help of fiber bragg grating (fbg) and an optical filter has been devised considering a magnetostriction measurements under ultrahigh magnetic fields. a longitudinal magnetostriction of lacoo$_{3}$ has been measured at room temperature, 115, 7 and 4.2 k up to a maximum magnetic field of 150 t. a field-induced lattice elongations are observed, which are attributed to a spin-state crossover from a low-spin ground state to excited spin-states.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
12325,"the standard recipe considering spoken language recognition was to apply the gaussian back-end to i-vectors. this ignores a uncertainty inside a i-vector extraction, which could be important especially considering short utterances. the recent paper by cumani, plchot and fer proposes the solution to propagate that uncertainty into a backend. we propose an alternative method of propagating a uncertainty.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7646,"this paper derives the new family of estimators, namely a minimum density power divergence estimators, as the robust generalization of a maximum likelihood estimator considering a polytomous logistic regression model. based on these estimators, the family of wald-type test statistics considering linear hypotheses was introduced. robustness properties of both a proposed estimators and a test statistics are theoretically studied through a classical influence function analysis. appropriate real life examples are presented to justify a requirement of suitable robust statistical procedures inside place of a likelihood based inference considering a polytomous logistic regression model. a validity of a theoretical results established inside a paper are further confirmed empirically through suitable simulation studies. finally, an idea behind the method considering a data-driven selection of a robustness tuning parameter was proposed with empirical justifications.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
18024,"a solar modulation effect of cosmic rays inside a heliosphere was an energy-, time-, and particle-dependent phenomenon which arises from the combination of basic particle transport processes such as diffusion, convection, adiabatic cooling, and drift motion. making use of the large collection of time-resolved cosmic-ray data from recent space missions, we construct the simple predictive model of solar modulation which depends on direct solar-physics inputs: a number of solar sunspots and a tilt angle of a heliospheric current sheet. under this framework, we present calculations of cosmic-ray proton spectra, positron/electron and antiproton/proton ratios and their time dependence inside connection with a evolving solar activity. we report evidence considering the time-lag $\delta{t}=8.1\pm\,1.2$ months, between solar activity data and cosmic-ray flux measurements inside space, which reflects a dynamics of a formation of a modulation region. this result enables us to forecast a cosmic-ray flux near earth well inside advance by monitoring solar activity",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10172,"we introduce a suggest-and-improve framework considering general nonconvex quadratically constrained quadratic programs (qcqps). with the help of this framework, we generalize the number of known methods and provide heuristics to get approximate solutions to qcqps considering which no specialized methods are available. we also introduce an open-source python package qcqp, which implements a heuristics discussed inside a paper.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
11009,"methods that generate networks sharing the given degree distribution and global clustering should induce changes inside structural properties other than that controlled for. diversity inside structural properties, inside turn, should affect a outcomes of dynamical processes operating on those networks. since exhaustive sampling was not possible, we propose the novel evolutionary framework considering mapping this structural diversity. a three main features of this framework are: (a) subgraph-based encoding of networks, (b) exact mutations based on solving systems of diophantine equations, and (c) heuristic diversity-driven mechanism to drive resolution changes inside a mapelite algorithm. we show that our framework should elicit networks with diversity inside their higher-order structure and that this diversity affects a behaviour of a complex contagion model. through the comparison with state of a art clustered network generation methods, we demonstrate that our idea behind the method should uncover the comparably diverse range of networks without needing computationally unfeasible mixing times. further, we suggest that a subgraph-based encoding provides greater confidence inside a diversity of higher-order network structure considering low numbers of samples and was a basis considering explaining our results with complex contagion model. we believe that this framework could be applied to other complex landscapes that cannot be practically mapped using exhaustive sampling.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
14296,"this paper deals with asymptotics considering multiple-set linear canonical analysis (mslca). the definition of this analysis, that adapts a classical one to a context of euclidean random variables, was given and properties of a related canonical coefficients are derived. then, estimators of a mslca's elements, based on empirical covariance operators, are proposed and asymptotics considering these estimators are obtained. more precisely, we prove their consistency and we obtain asymptotic normality considering a estimator of a operator that gives mslca, and also considering a estimator of a vector of canonical coefficients. these results are then used to obtain the test considering mutual non-correlation between a involved euclidean random variables.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
1958,"we analyze a response of the type ii superconducting wire to an external magnetic field parallel to it inside a framework of ginzburg-landau theory. we focus on a surface superconductivity regime of applied field between a second and third critical values, where a superconducting state survives only close to a sample's boundary. our first finding was that, inside first approximation, a shape of a boundary plays no role inside determining a density of superconducting electrons. the second order term was however isolated, directly proportional to a mean curvature of a boundary. this demonstrates that points of higher boundary curvature (counted inwards) attract superconducting electrons.",0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
2530,"inside 2010, the paper entitled ""from obscurity to prominence inside minutes: political speech and real-time search"" won a best paper prize of a web science 2010 conference. among its findings were a discovery and documentation of what is termed the ""twitter-bomb"", an organized effort to spread misinformation about a democratic candidate martha coakley through anonymous twitter accounts. inside this paper, after summarizing a details of that event, we outline a recipe of how social networks are used to spread misinformation. one of a most important steps inside such the recipe was a ""infiltration"" of the community of users who are already engaged inside conversations about the topic, to use them as organic spreaders of misinformation inside their extended subnetworks. then, we take this misinformation spreading recipe and indicate how it is successfully used to spread fake news during a 2016 u.s. presidential election. a main differences between a scenarios are a use of facebook instead of twitter, and a respective motivations (in 2010: political influence; inside 2016: financial benefit through online advertising). after situating these events inside a broader context of exploiting a web, we seize this opportunity to address limitations of a reach of research findings and to start the conversation about how communities of researchers should increase their impact on real-world societal issues.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
15692,"this paper was the continuation of arxiv:1405.1707. we present certain new applications and generalizations of a free field realization of a twisted heisenberg-virasoro algebra ${\mathcal h}$ at level zero. we find explicit formulas considering singular vectors inside certain verma modules. the free field realization of self-dual modules considering ${\mathcal h}$ was presented by combining the bosonic construction of whittaker modules from arxiv:1409.5354 with the construction of logarithmic modules considering vertex algebras. as an application, we prove that there exists the non-split self-extension of irreducible self-dual module which was the logarithmic module of rank two. we construct the large family of logarithmic modules containing different types of highest weight modules as subquotients. we believe that these logarithmic modules are related with projective covers of irreducible modules inside the suitable category of ${\mathcal h}$-modules.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
16792,"inside a past 20 years, a hubble space telescope (hst) stis coronagraphic instrument has observed more than 100 stars, obtaining more than 4,000 readouts since its installment on hst inside 1997 and a numbers are still increasing. we reduce a whole stis coronagraphic archive at a most commonly observed positions (wedge a0.6 and a1.0) with new post-processing methods, and present our results here. we are able to recover all of a 32 previously reported circumstellar disks, and obtain better contrast close to a star. considering some of a disks, our results are limited by a over subtraction of a methods, and therefore a major regions of a disks should be recovered except a faintest regions. we also explain our efforts inside a calibration of its new bar5 occulting position, enabling stis to explore inner regions as close as 0.2"".",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
16527,"we prove that the general complex monge-amp√®re flow on the hermitian manifold should be run from an arbitrary initial condition with zero lelong number at all points. with the help of this property, we confirm the conjecture of tosatti-weinkove: a chern-ricci flow performs the canonical surgical contraction. finally, we study the generalization of a chern-ricci flow on compact hermitian manifolds, namely a twisted chern-ricci flow.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18906,"we study a gas phase metallicity (o/h) and nitrogen abundance gradients traced by star forming regions inside the representative sample of 550 nearby galaxies inside a stellar mass range $\rm 10^9-10^{11.5} m_\odot$ with resolved spectroscopic data from a sdss-iv manga survey. with the help of strong-line ratio diagnostics (r23 and o3n2 considering metallicity and n2o2 considering n/o) and referencing to a effective (half-light) radius ($\rm r_e$), we find that a metallicity gradient steepens with stellar mass, lying roughly flat among galaxies with $\rm log(m_\star/m_\odot) = 9.0$ but exhibiting slopes as steep as -0.14 dex $\rm r_e^{-1}$ at $\rm log(m_\star/m_\odot) = 10.5$ (using r23, but equivalent results are obtained with the help of o3n2). at higher masses, these slopes remain typical inside a outer regions of our sample ($\rm r > 1.5 ~r_e$), but the flattening was observed inside a central regions ($\rm r < 1~ r_e$). inside a outer regions ($\rm r > 2.0 ~r_e$) we detect the mild flattening of a metallicity gradient inside stacked profiles, although with low significance. a n/o ratio gradient provides complementary constraints on a average chemical enrichment history. unlike a oxygen abundance, a average n/o profiles do not flatten out inside a central regions of massive galaxies. a metallicity and n/o profiles both depart significantly from an exponential form, suggesting the disconnect between chemical enrichment and stellar mass surface density on local scales. inside a context of inside-out growth of discs, our findings suggest that central regions of massive galaxies today have evolved to an equilibrium metallicity, while a nitrogen abundance continues to increase as the consequence of delayed secondary nucleosynthetic production.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7546,"we present the low-frequency view of a perseus cluster with new observations from a karl g. jansky very large array (jvla) at 230-470 mhz. a data reveal the multitude of new structures associated with a mini-halo. a mini-halo seems to be influenced both by a agn activity as well as by a sloshing motion of a cool core cluster's gas. inside addition, it has the filamentary structure similar to that seen inside radio relics found inside merging clusters. we present the detailed description of a data reduction and imaging process of a dataset. a depth and resolution of a observations allow us to conduct considering a first time the detailed comparison of a mini-halo structure with a x-ray structure as seen inside a chandra x-ray images. a resulting image shows very clearly that a mini-halo emission was mostly contained behind a cold fronts, similar to that predicted by simulations of gas sloshing inside galaxy clusters. however, due to a proximity of a perseus cluster, as well as a quality of a data at low radio frequencies and at x-ray wavelengths, we also find evidence of fine structure. this structure includes several radial radio filaments extending inside different directions, the concave radio structure associated with a southern x-ray bay and sharp edges that correlate with x-ray edges. mini-halos are therefore not simply diffuse, uniform radio sources, but are rather filled with the rich variety of complex structures. these results illustrate a high-quality images that should be obtained with a new jvla at low radio-frequencies, as well as a necessity to obtain deeper, higher-fidelity radio images of mini-halos and halos inside clusters to further understand their origin.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11945,"recently, substantial research effort has focused on how to apply cnns or rnns to better extract temporal patterns from videos, so as to improve a accuracy of video classification. inside this paper, however, we show that temporal information, especially longer-term patterns, may not be necessary to achieve competitive results on common video classification datasets. we investigate a potential of the purely attention based local feature integration. accounting considering a characteristics of such features inside video classification, we propose the local feature integration framework based on attention clusters, and introduce the shifting operation to capture more diverse signals. we carefully analyze and compare a effect of different attention mechanisms, cluster sizes, and a use of a shifting operation, and also investigate a combination of attention clusters considering multimodal integration. we demonstrate a effectiveness of our framework on three real-world video classification datasets. our model achieves competitive results across all of these. inside particular, on a large-scale kinetics dataset, our framework obtains an excellent single model accuracy of 79.4% inside terms of a top-1 and 94.0% inside terms of a top-5 accuracy on a validation set. a attention clusters are a backbone of our winner solution at activitynet kinetics challenge 2017. code and models will be released soon.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17916,"this letter presents the new spectral-clustering-based idea behind the method to a subspace clustering problem. underpinning a proposed method was the convex program considering optimal direction search, which considering each data point d finds an optimal direction inside a span of a data that has minimum projection on a other data points and non-vanishing projection on d. a obtained directions are subsequently leveraged to identify the neighborhood set considering each data point. an alternating direction method of multipliers framework was provided to efficiently solve considering a optimal directions. a proposed method was shown to notably outperform a existing subspace clustering methods, particularly considering unwieldy scenarios involving high levels of noise and close subspaces, and yields a state-of-the-art results considering a problem of face clustering with the help of subspace segmentation.",1,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18389,"we consider scalar field models of dark energy interacting with dark matter through the coupling proportional to a contraction of a four-derivative of a scalar field with a four-velocity of a dark matter fluid. a coupling was realized at a lagrangian level employing a formalism of scalar-fluid theories, which use the consistent lagrangian idea behind the method considering relativistic fluid to describe dark matter. this framework produces fully covariant field equations, from which we should derive unequivocal cosmological equations at both background and linear perturbations levels. a background evolution was analyzed inside detail applying dynamical systems techniques, which allow us to find a complete asymptotic behavior of a universe given any set of model parameters and initial conditions. furthermore we study linear cosmological perturbations investigating a growth of cosmic structures within a quasi-static approximation. we find that these interacting dark energy models give rise to interesting phenomenological dynamics, including late-time transitions from dark matter to dark energy domination, matter and accelerated scaling solutions and dynamical crossing of a phantom barrier. moreover we obtain possible deviations from standard $\lambda$cdm behavior at a linear perturbations level, which have an impact on a dynamics of structure formation and might provide characteristic observational signatures.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6082,"a present paper presents a weighted ontology approximation heuristic (woah), the novel zero-shot idea behind the method to ontology approximation considering conversational agents development environments. this methodology extracts verbs and nouns separately from data by distilling a dependencies obtained and applying similarity and sparsity metrics to generate an ontology approximation configurable inside terms of a level of generalization.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8955,"we analyze a kozachenko--leonenko (kl) nearest neighbor estimator considering a differential entropy. we obtain a first uniform upper bound on its performance over h√∂lder balls on the torus without assuming any conditions on how close a density could be from zero. accompanying the new minimax lower bound over a h√∂lder ball, we show that a kl estimator was achieving a minimax rates up to logarithmic factors without cognizance of a smoothness parameter $s$ of a h√∂lder ball considering $s\in (0,2]$ and arbitrary dimension $d$, rendering it a first estimator that provably satisfies this property.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1055,"a oxidation of rocky planet surfaces and atmospheres, which arises from a twin forces of stellar nucleosynthesis and gravitational differentiation, was the universal process of key importance to habitability and exoplanet biosignature detection. here we take the generalized idea behind the method to this phenomenon. with the help of the single parameter to describe redox state, we model a evolution of terrestrial planets around nearby m-stars and a sun. our model includes atmospheric photochemistry, diffusion and escape, line-by-line climate calculations and interior thermodynamics and chemistry. inside most cases we find abiotic atmospheric o2 buildup around m-stars during a pre-main sequence phase to be much less than calculated previously, because a planet's magma ocean absorbs most oxygen liberated from h2o photolysis. however, loss of non-condensing atmospheric gases after a mantle solidifies remains the significant potential route to abiotic atmospheric o2 subsequently. inside all cases, we predict that exoplanets that receive lower stellar fluxes, such as lhs1140b and trappist-1f and g, have a lowest probability of abiotic o2 buildup and thus may be a most interesting targets considering future searches considering biogenic o2. key remaining uncertainties should be minimized inside future by comparing our predictions considering a atmospheres of hot, sterile exoplanets such as gj1132b and trappist-1b and --c with observations.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
89,"considering the polynomial $f(x)\in\mathbb z[x]$ without non-trivial linear relations among roots, we propose the conjecture on a distribution of a least root $r_p$ ($r_p\in\mathbb z,\,0\le r_p<p)$ of $f(x)\equiv0\bmod p$ where $p$ runs over a set of primes such that $f(x)$ modulo $p$ was fully splitting.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
16863,"inside this paper, we introduce a bmt distribution as an unimodal alternative to continuous univariate distributions supported on the bounded interval. a ideas behind a mathematical formulation of this new distribution come from computer aid geometric design, specifically from bezier curves. first, we review general properties of the distribution given by parametric equations and extend a definition of the bezier distribution. then, after proposing a bmt cumulative distribution function, we derive its probability density function and the closed-form expression considering quantile function, median, interquartile range, mode, and moments. a domain change from [0,1] to [c,d] was mentioned. approximation of parameters was approached by a methods of maximum likelihood and maximum product of spacing. we test a numerical approximation procedures with the help of some simulated data. usefulness and flexibility of a new distribution are illustrated inside three real data sets. a bmt distribution has the significant potential to approximate domain parameters and to model data outside a scope of a beta or similar distributions.",0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
16262,"inside this minireview we will discuss recent progress inside a analytical study of current-carrying non-equilibrium steady states (ness) that should be constructed inside terms of the matrix product ansatz. we will focus on one-dimensional exactly solvable strongly correlated cases, and will study both quantum models, and classical models which are deterministic inside a bulk. a only source of classical stochasticity inside a time-evolution will come from a boundaries of a system. physically, these boundaries may be understood as markovian baths, which drive a current through a system. a examples studied include a open xxz heisenberg spin chain, a open hubbard model, and the classical integrable reversible cellular automaton, namely a rule 54 of bobenko {\em et al}. [commun. math. phys. {\bf 158}, 127 (1993)] with stochastic boundaries. a quantum ness should be at least partially understood through a yang-baxter integrability structure of a underlying integrable bulk hamiltonian, whereas considering a rule 54 model ness seems to come from the seemingly unrelated integrability theory. inside both a quantum and a classical case, a underlying matrix product ansatz defining a ness also allows considering construction of novel conservation laws of a bulk models themselves. inside a classical case, the modification of a matrix product ansatz also allows considering construction of states beyond a steady state (i.e., some of a decay modes -- liouvillian eigenvectors of a model). we hope that this article will aid further a quest to unite different perspectives of integrability of ness (of both quantum and classical models) into the single unified framework.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
17234,"a variability of a clusters generated by clustering techniques inside a domain of latitude and longitude variables of fatal crash data are significantly unpredictable. this unpredictability, caused by a randomness of fatal crash incidents, reduces a accuracy of crash frequency (i.e., counts of fatal crashes per cluster) which was used to measure traffic safety inside practice. inside this paper, the quantitative measure of traffic safety that was not significantly affected by a aforementioned variability was proposed. it introduces the fatal point -- the segment with a highest frequency of fatality -- concept based on cluster characteristics and detects them by imposing rounding errors to a hundredth decimal place of a longitude. a frequencies of a cluster and a cluster's fatal point are combined to construct the low-sensitive quantitative measure of traffic safety considering a cluster. a performance of a proposed measure of traffic safety was then studied by varying a parameter k of k-means clustering with a expectation that other clustering techniques should be adopted inside the similar fashion. a 2015 north carolina fatal crash dataset of fatality analysis reporting system (fars) was used to evaluate a proposed fatal point concept and perform experimental analysis to determine a effectiveness of a proposed measure. a empirical study shows that a average traffic safety, measured by a proposed quantitative measure over several clusters, was not significantly affected by a variability, compared to that of a standard crash frequency.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
12282,"inside this work, we characterize a outputs of individual neurons inside the trained feed-forward neural network by entropy, mutual information with a class variable, and the class selectivity measure based on kullback-leibler divergence. by cumulatively ablating neurons inside a network, we connect these information-theoretic measures to a impact their removal has on classification performance on a test set. we observe that, looking at a neural network as the whole, none of these measures was the good indicator considering classification performance, thus confirming recent results by morcos et al. however, looking at specific layers separately, both mutual information and class selectivity are positively correlated with classification performance. we thus conclude that it was ill-advised to compare these measures across layers, and that different layers may be most appropriately characterized by different measures. we then discuss pruning neurons from neural networks to reduce computational complexity of inference. drawing from our results, we perform pruning based on information-theoretic measures on the fully connected feed-forward neural network with two hidden layers trained on mnist dataset and compare a results to the recently proposed pruning method. we furthermore show that a common practice of re-training after pruning should partly be obviated by the surgery step called bias balancing, without incurring significant performance degradation.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6303,"periodic driving should be used to coherently control a properties of the many-body state and to realize new phases which are not accessible inside static systems. considering example, exposing materials to intense laser pulses enables to provoke metal-insulator transitions, control a magnetic order and induce transient superconducting behavior well above a static transition temperature. however, pinning down a responsible mechanisms was often difficult, since a response to irradiation was governed by complex many-body dynamics. inside contrast to static systems, where extensive calculations have been performed to explain phenomena such as high-temperature superconductivity, theoretical analyses of driven many-body hamiltonians are more demanding and new theoretical approaches have been inspired by a recent observations. here, we perform an experimental quantum simulation inside the periodically modulated hexagonal lattice and show that anti-ferromagnetic correlations inside the fermionic many-body system should be reduced or enhanced or even switched to ferromagnetic ordering. we first demonstrate that inside a high frequency regime, a description of a many-body system by an effective floquet-hamiltonian with the renormalized tunneling energy remains valid, by comparing a results to measurements inside an equivalent static lattice. considering near-resonant driving, a enhancement and sign reversal of correlations was explained by the microscopic model, inside which a particle tunneling and magnetic exchange energies should be controlled independently. inside combination with a observed sufficiently long lifetime of correlations, floquet engineering thus constitutes an alternative route to experimentally investigate unconventional pairing inside strongly correlated systems.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0
2810,"monte carlo (mc) simulations of transport inside random porous networks indicate that considering high variances of a log-normal permeability distribution, a transport of the passive tracer was non-fickian. here we model this non-fickian dispersion inside random porous networks with the help of discrete temporal markov models. we show that such temporal models capture a spreading behavior accurately. this was true despite a fact that a slow velocities are strongly correlated inside time, and some studies have suggested that a persistence of low velocities would render a temporal markovian model inapplicable. compared to previously proposed temporal stochastic differential equations with case specific drift and diffusion terms, a models presented here require fewer modeling assumptions. moreover, we show that discrete temporal markov models should be used to represent dispersion inside unstructured networks, which are widely used to model porous media. the new method was proposed to extend a state space of temporal markov models to improve a model predictions inside a presence of extremely low velocities inside particle trajectories and extend a applicability of a model to higher temporal resolutions. finally, it was shown that by combining multiple transitions, temporal models are more efficient considering computing particle evolution compared to correlated ctrw with spatial increments that are equal to a lengths of a links inside a network.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15710,"we propose the variant of ising model, called a seeded ising model, to model probabilistic nature of human iris templates. this model was an ising model inside which a values at certain lattice points are held fixed throughout ising model evolution. with the help of this we show how to reconstruct a full iris template from partial information, and we show that about 1/6 of a given template was needed to recover almost all information content of a original one inside a sense that a resulting hamming distance was well within a range to assert correctly a identity of a subject. this leads us to propose a concept of effective statistical degree of freedom of iris templates and show it was about 1/6 of a total number of bits. inside particular, considering the template of $2048$ bits, its effective statistical degree of freedom was about $342$ bits, which coincides very well with a degree of freedom computed by a completely different method proposed by daugman.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9959,"a paper conducts the second-order variational analysis considering an important class of nonpolyhedral conic programs generated by a so-called second-order/lorentz/ice-cream cone $q$. from one hand, we prove that a indicator function of $q$ was always twice epi-differentiable and apply this result to characterizing a uniqueness of lagrange multipliers at stationary points together with an error bound approximate inside a general second-order cone setting involving ${\cal c}^2$-smooth data. on a other hand, we precisely calculate a graphical derivative of a normal cone mapping to $q$ under a weakest metric subregularity constraint qualification and then give an application of a latter result to the complete characterization of isolated calmness considering perturbed variational systems associated with second-order cone programs. a obtained results seem to be a first inside a literature inside these directions considering nonpolyhedral problems without imposing any nondegeneracy assumptions.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
16043,"characterizing a evolution of a faint end of a cluster red sequence (rs) galaxy luminosity function (glf) with redshift was the milestone inside understanding galaxy evolution. however, a community was still divided inside that respect, hesitating between an enrichment of a rs due to efficient quenching of blue galaxies from $z\sim1$ to present-day or the scenario inside which a rs was built at the higher redshift and does not evolve afterwards. recently, it has been proposed that surface brightness (sb) selection effects could possibly solve a literature disagreement, accounting considering a diminishing of a rs faint population inside ground based observations. we investigate this hypothesis by comparing a rs glfs of 16 clash clusters computed independently from ground-based subaru/suprime-cam and hst/acs images inside a redshift range $0.187\leq z\leq0.686$. we stack individual cluster glfs inside redshift and mass bins. we find similar rs glfs considering space and ground based data, with the difference of 0.2$\sigma$ inside a faint end parameter $\alpha$ when stacking all clusters together and the maximum difference of 0.9$\sigma$ inside a case of a high redshift stack, demonstrating the weak dependence on a type of observations inside a probed range of redshift and mass. when considering a full sample, we approximate $\alpha = -0.76 \pm 0.07$ and $\alpha = -0.78 \pm 0.06$ with hst and subaru respectively. we note the mild variation of a faint end with redshift at the 1.7$\sigma$ and 2.6$\sigma$ significance. we investigate a effect of sb dimming by simulating our low redshift galaxies at high redshift. we measure an evolution inside a faint end slope of less than 1$\sigma$ inside this case, implying that a observed signature was moderately larger than one would expect from sb dimming alone, and indicating the true evolution inside a faint end slope. (abridged...)",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9674,"inside this paper we address lifshitz transition induced by applied external magnetic field inside the case of iron-based superconductors, inside which the difference between a fermi level and a edges of a bands was relatively small. we introduce and investigate the two-band model with intra-band pairing inside a relevant parameters regime to address the generic behaviour of the system with hole-like and electron-like bands inside external magnetic field. our results show that two lifshitz transitions should develop inside analysed systems and a first one occurs inside a superconducting phase and takes place at approximately constant magnetic field. a chosen sets of a model parameters should describe characteristic band structure of iron-based superconductors and thus a obtained results should explain a experimental observations inside fese and co-doped bafe$_{2}$as$_{2}$ compounds.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
12877,"this paper presents design and experimental evaluations of an articulated robotic limb called capler-leg. a key element of capler-leg was its single-stage cable-pulley transmission combined with the high-gap radius motor. our cable-pulley system was designed to be as light-weight as possible and to additionally serve as a primary cooling element, thus significantly increasing a power density and efficiency of a overall system. a total weight of active elements on a leg, i.e. a stators and a rotors, contribute more than 60% of a total leg weight, which was an order of magnitude higher than most existing robots. a resulting robotic leg has low inertia, high torque transparency, low manufacturing cost, no backlash, and the low number of parts. capler-leg system itself, serves as an experimental setup considering evaluating a proposed cable- pulley design inside terms of robustness and efficiency. the continuous jump experiment shows the remarkable 96.5 % recuperation rate, measured at a battery output. this means that almost all a mechanical energy output used during push-off returned back to a battery during touch-down.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
19309,"given the rectilinear domain $\mathcal{p}$ of $h$ pairwise-disjoint rectilinear obstacles with the total of $n$ vertices inside a plane, we study a problem of computing bicriteria rectilinear shortest paths between two points $s$ and $t$ inside $\mathcal{p}$. three types of bicriteria rectilinear paths are considered: minimum-link shortest paths, shortest minimum-link paths, and minimum-cost paths where a cost of the path was the non-decreasing function of both a number of edges and a length of a path. a one-point and two-point path queries are also considered. algorithms considering these problems have been given previously. our contributions are threefold. first, we find the critical error inside all previous algorithms. second, we correct a error inside the not-so-trivial way. third, we further improve a algorithms so that they are even faster than a previous (incorrect) algorithms when $h$ was relatively small. considering example, considering a minimum-link shortest paths, we obtain a following results. our algorithm computes the minimum-link shortest $s$-$t$ path inside $o(n+h\log^{3/2} h)$ time. considering a one-point queries, we build the data structure of size $o(n+ h\log h)$ inside $o(n+h\log^{3/2} h)$ time considering the source point $s$, such that given any query point $t$, the minimum-link shortest $s$-$t$ path should be determined inside $o(\log n)$ time. considering a two-point queries, with $o(n+h^2\log^2 h)$ time and space preprocessing, the minimum-link shortest $s$-$t$ path should be determined inside $o(\log n+\log^2 h)$ time considering any two query points $s$ and $t$; alternatively, with $o(n+h^2\cdot \log^{2} h \cdot 4^{\sqrt{\log h}})$ time and $o(n+h^2\cdot \log h \cdot 4^{\sqrt{\log h}})$ space preprocessing, we should answer each two-point query inside $o(\log n)$ time.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8075,"currently a detection of very high energy gamma-rays considering astrophysics rely on a measurement of a extensive air showers (eas) either with the help of cherenkov detectors or eas arrays with larger field of views but also larger energy thresholds. inside this talk we present the novel hybrid detector concept considering the eas array with an improved sensitivity inside a lower energies ($\sim 100\,$gev). we discuss its main features, capabilities and present preliminary results on its expected perfomances and sensitivities.this wide field of view experiment was planned to be installed at high altitude inside south america making it the complementary project to a planned cherenkov telescope experiments and the powerful tool to trigger further observations of variable sources and to detect transients phenomena.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
3902,"a existence of closed loops of degeneracies inside crystals has been intimately connected to associated crystal symmetries, raising a question: what was a minimum symmetry required considering topological character, and should one find an example? triclinic caas$_3$, inside space group $p{\bar 1}$ with only the center of inversion, has been found to display, without need considering tuning, the nodal loop of accidental degeneracies with topological character, centered on one face of a brillouin zone that was otherwise fully gapped. a small loop was very flat inside energy, yet was cut four times by a fermi energy, the condition that results inside an intricate repeated touching of inversion related pairs of fermi surfaces at weyl points. spin-orbit coupling lifts a fourfold degeneracy along a loop, leaving trivial kramers pairs. with its single nodal loop that emerges without protection from any point group symmetry, caas$_3$ represents a primal ""hydrogen atom"" of nodal loop systems.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
18623,"robust pca, a problem of pca inside a presence of outliers has been extensively investigated inside a last few years. here we focus on robust pca inside a outlier model where each column of a data matrix was either an inlier or an outlier. most of a existing methods considering this model assumes either a knowledge of a dimension of a lower dimensional subspace or a fraction of outliers inside a system. however inside many applications knowledge of these parameters was not available. motivated by this we propose the parameter free outlier identification method considering robust pca which a) does not require a knowledge of outlier fraction, b) does not require a knowledge of a dimension of a underlying subspace, c) was computationally simple and fast d) should handle structured and unstructured outliers. further, analytical guarantees are derived considering outlier identification and a performance of a algorithm was compared with a existing state of a art methods inside both real and synthetic data considering various outlier structures.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4384,"hans j. zassenhaus conjectured that considering any unit $u$ of finite order inside a integral group ring of the finite group $g$ there exists the unit $a$ inside a rational group algebra of $g$ such that $a^{-1}\cdot u \cdot a=\pm g$ considering some $g\in g$. we disprove this conjecture by first proving general results that aid identify counterexamples and then providing an infinite number of examples where these results apply. our smallest example was the metabelian group of order $2^7 \cdot 3^2 \cdot 5 \cdot 7^2 \cdot 19^2$ whose integral group ring contains the unit of order $7 \cdot 19$ which, inside a rational group algebra, was not conjugate to any element of a form $\pm g$.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
256,"the novel jet-stirred combustion chamber was designed to study turbulent premixed flames. inside a new approach, multiple impinging turbulent jets are used to stir a mixture. it was well known that pair of counterflowing turbulent jets produces nearly the constant intensity along a jet axes. inside this study, different numbers of impinging jets inside various configurations are used to produce isotropic turbulence intensity. fluent simulations have been conducted to assess a viability of a proposed chamber. inside order to be able to compare different configurations, three different non dimensional indices are introduces. mean flow index, homogeneity index, and isotropicity index. with the help of these indices one should compare various chambers including conventional fan-stirred reactor. results show that the concentric inlet/outlet chamber with 8 inlets and 8 outlets with inlet velocity of 20 m/s and initial intensity of 15% produces near zero mean flow and 2.5 m/s turbulence intensity which was much more higher than reported values considering fan-stirred chamber.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2655,"ejection velocity fields of asteroid families are largely unconstrained due to a fact that members disperse relatively quickly on myr time-scales by secular resonances and a yarkovsky effect. a spreading of fragments inside $a$ by a yarkovsky effect was indistinguishable from a spreading caused by a initial ejection of fragments. by examining families $<$20 myrs-old, we should use a v-shape identification technique to separate family shapes that are due to a initial ejection velocity field and those that are due to a yarkovsky effect. $<$20 myr-old asteroid families provide an opportunity to study a velocity field of family fragments before they become too dispersed. only a karin family's initial velocity field has been determined and scales inversely with diameter, $d^{-1}$. we have applied a v-shape identification technique to constrain young families' initial ejection velocity fields by measuring a curvature of their fragments' v-shape correlation inside semi-major axis, $a$, vs. $d^{-1}$ space. curvature from the straight line implies the deviation from the scaling of $d^{-1}$. we measure a v-shape curvature of 11 young asteroid families including a \fynospace, aeolia, brangane, brasilia, clarissa, iannini, karin, konig, koronis(2), theobalda and veritas asteroid families. we find that a majority of asteroid families have initial ejection velocity fields consistent with $\sim d^{-1}$ supporting laboratory impact experiments and computer simulations of disrupting asteroid parent bodies.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
802,"given the large number of unlabeled face images, face grouping aims at clustering a images into individual identities present inside a data. this task remains the challenging problem despite a remarkable capability of deep learning approaches inside learning face representation. inside particular, grouping results should still be egregious given profile faces and the large number of uninteresting faces and noisy detections. often, the user needs to correct a erroneous grouping manually. inside this study, we formulate the novel face grouping framework that learns clustering strategy from ground-truth simulated behavior. this was achieved through imitation learning (a.k.a apprenticeship learning or learning by watching) using inverse reinforcement learning (irl). inside contrast to existing clustering approaches that group instances by similarity, our framework makes sequential decision to dynamically decide when to merge two face instances/groups driven by short- and long-term rewards. extensive experiments on three benchmark datasets show that our framework outperforms unsupervised and supervised baselines.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17715,"inside this paper, we focus on online representation learning inside non-stationary environments which may require continuous adaptation of model architecture. we propose the novel online dictionary-learning (sparse-coding) framework which incorporates a addition and deletion of hidden units (dictionary elements), and was inspired by a adult neurogenesis phenomenon inside a dentate gyrus of a hippocampus, known to be associated with improved cognitive function and adaptation to new environments. inside a online learning setting, where new input instances arrive sequentially inside batches, a neuronal-birth was implemented by adding new units with random initial weights (random dictionary elements); a number of new units was determined by a current performance (representation error) of a dictionary, higher error causing an increase inside a birth rate. neuronal-death was implemented by imposing l1/l2-regularization (group sparsity) on a dictionary within a block-coordinate descent optimization at each iteration of our online alternating minimization scheme, which iterates between a code and dictionary updates. finally, hidden unit connectivity adaptation was facilitated by introducing sparsity inside dictionary elements. our empirical evaluation on several real-life datasets (images and language) as well as on synthetic data demonstrates that a proposed idea behind the method should considerably outperform a state-of-art fixed-size (nonadaptive) online sparse coding of mairal et al. (2009) inside a presence of nonstationary data. moreover, we identify certain properties of a data (e.g., sparse inputs with nearly non-overlapping supports) and of a model (e.g., dictionary sparsity) associated with such improvements.",1,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11093,"visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. even with a depth of features inside the convolutional network, the layer inside isolation was not enough: compounding and aggregating these representations improves inference of what and where. architectural efforts are exploring many dimensions considering network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across the network deserves further attention. although skip connections have been incorporated to combine layers, these connections have been ""shallow"" themselves, and only fuse by simple, one-step operations. we augment standard architectures with deeper aggregation to better fuse information across layers. our deep layer aggregation structures iteratively and hierarchically merge a feature hierarchy to make networks with better accuracy and fewer parameters. experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes. a code was at this https url.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8574,"it was the general challenge to design highly active or selective earth-abundant metals considering catalytic hydrogenation. here, we demonstrated an effective computational idea behind the method based on inverse molecular design theory to deterministically search considering optimal binding sites on cu (100) surface through a doping of fe and/or zn, and the stable zn-doped cu (100) surface is found with minimal binding energy to h-atoms. we analyze a electronic structure cause of a optimal binding sites with the help of the new quantum chemistry method called orbital-specific binding energy analysis. compared to a 3d-orbitals of surface cu atoms, a 3d-orbitals of surface zn-atoms show less binding energy contribution and participation, and are much less influenced by a electronic couplings of a media cu atoms. our study provides valuable green chemistry insights on designing catalysts with the help of earth-abundant metals, and may lead to a development of novel cu-based earth-abundant alloys considering important catalytic hydrogenation applications such as lignin degradation or co2 transformation.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
13885,a ground state of a spin-$1/2$ heisenberg antiferromagnet on the distorted triangular lattice was studied with the help of the numerical-diagonalization method. a network of interactions was a $\sqrt{3}\times\sqrt{3}$ type; a interactions are continuously controlled between a undistorted triangular lattice and a dice lattice. we find new states between a nonmagnetic 120-degree-structured state of a undistorted triangular case and a up-up-down state of a dice case. a intermediate states show spontaneous magnetizations that are smaller than one third of a saturated magntization corresponding to a up-up-down state.,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
7046,"inside this paper, we prove a global well-posedness of a incompressible mhd equations near the homogeneous equilibrium inside a domain $r^k\times t^{d-k}, d\geq2,k\geq1$ by with the help of a comparison principle and constructing a comparison function.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19850,"we introduce two tactics to attack agents trained by deep reinforcement learning algorithms with the help of adversarial examples, namely a strategically-timed attack and a enchanting attack. inside a strategically-timed attack, a adversary aims at minimizing a agent's reward by only attacking a agent at the small subset of time steps inside an episode. limiting a attack activity to this subset helps prevent detection of a attack by a agent. we propose the novel method to determine when an adversarial example should be crafted and applied. inside a enchanting attack, a adversary aims at luring a agent to the designated target state. this was achieved by combining the generative model and the planning algorithm: while a generative model predicts a future states, a planning algorithm generates the preferred sequence of actions considering luring a agent. the sequence of adversarial examples was then crafted to lure a agent to take a preferred sequence of actions. we apply a two tactics to a agents trained by a state-of-the-art deep reinforcement learning algorithm including dqn and a3c. inside 5 atari games, our strategically timed attack reduces as much reward as a uniform attack (i.e., attacking at every time step) does by attacking a agent 4 times less often. our enchanting attack lures a agent toward designated target states with the more than 70% success rate. videos are available at this http url",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16533,"consider a entropy of the unit gaussian convolved over the discrete set of k points, constrained to an interval of length l. maximising this entropy fixes k, and we show that this number exhibits the novel scaling law k ~ l^1/\zeta as l -> infinity, with exponent \zeta = 3/4. this law is observed numerically inside the recent paper about optimal effective theories; here we present an analytic derivation. we argue that this law was generic considering channel capacity maximisation, or a equivalent minimax problem. we also briefly discuss a behaviour at a boundary of a interval, and higher dimensional versions.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
15122,"this paper highlights a significance of including memory structures inside neural networks when a latter are used to learn perception-action loops considering autonomous robot navigation. traditional navigation approaches rely on global maps of a environment to overcome cul-de-sacs and plan feasible motions. yet, maintaining an accurate global map may be challenging inside real-world settings. the possible way to mitigate this limitation was to use learning techniques that forgo hand-engineered map representations and infer appropriate control responses directly from sensed information. an important but unexplored aspect of such approaches was a effect of memory on their performance. this work was the first thorough study of memory structures considering deep-neural-network-based robot navigation, and offers novel tools to train such networks from supervision and quantify their ability to generalize to unseen scenarios. we analyze a separation and generalization abilities of feedforward, long short-term memory, and differentiable neural computer networks. we introduce the new method to evaluate a generalization ability by estimating a vc-dimension of networks with the final linear readout layer. we validate that a vc estimates are good predictors of actual test performance. a reported method should be applied to deep learning problems beyond robotics.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
5970,"a rise of graph-structured data such as social networks, regulatory networks, citation graphs, and functional brain networks, inside combination with resounding success of deep learning inside various applications, has brought a interest inside generalizing deep learning models to non-euclidean domains. inside this paper, we introduce the new spectral domain convolutional architecture considering deep learning on graphs. a core ingredient of our model was the new class of parametric rational complex functions (cayley polynomials) allowing to efficiently compute spectral filters on graphs that specialize on frequency bands of interest. our model generates rich spectral filters that are localized inside space, scales linearly with a size of a input data considering sparsely-connected graphs, and should handle different constructions of laplacian operators. extensive experimental results show a superior performance of our approach, inside comparison to other spectral domain convolutional architectures, on spectral image classification, community detection, vertex classification and matrix completion tasks.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5821,"inside adversarial attacks to machine-learning classifiers, small perturbations are added to input that was correctly classified. a perturbations yield adversarial examples, which are virtually indistinguishable from a unperturbed input, and yet are misclassified. inside standard neural networks used considering deep learning, attackers should craft adversarial examples from most input to cause the misclassification of their choice. we introduce the new type of network units, called rbfi units, whose non-linear structure makes them inherently resistant to adversarial attacks. on permutation-invariant mnist, inside absence of adversarial attacks, networks with the help of rbfi units match a performance of networks with the help of sigmoid units, and are slightly below a accuracy of networks with relu units. when subjected to adversarial attacks, networks with rbfi units retain accuracies above 90% considering attacks that degrade a accuracy of networks with relu or sigmoid units to below 2%. rbfi networks trained with regular input are superior inside their resistance to adversarial attacks even to relu and sigmoid networks trained with a aid of adversarial examples. a non-linear structure of rbfi units makes them difficult to train with the help of standard gradient descent. we show that networks of rbfi units should be efficiently trained to high accuracies with the help of pseudogradients, computed with the help of functions especially crafted to facilitate learning instead of their true derivatives. we show that a use of pseudogradients makes training deep rbfi networks practical, and we compare several structural alternatives of rbfi networks considering their accuracy.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15036,"what makes content go viral? which videos become popular and why others don't? such questions have elicited significant attention from both researchers and industry, particularly inside a context of online media. the range of models have been recently proposed to explain and predict popularity; however, there was the short supply of practical tools, accessible considering regular users, that leverage these theoretical results. hipie -- an interactive visualization system -- was created to fill this gap, by enabling users to reason about a virality and a popularity of online videos. it retrieves a metadata and a past popularity series of youtube videos, it employs hawkes intensity process, the state-of-the-art online popularity model considering explaining and predicting video popularity, and it presents videos comparatively inside the series of interactive plots. this system will aid both content consumers and content producers inside the range of data-driven inquiries, such as to comparatively analyze videos and channels, to explain and predict future popularity, to identify viral videos, and to approximate response to online promotion.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
2930,"assembling different two-dimensional (2d) crystals, covering the very broad range of properties, into van der waals (vdw) heterostructures enables a unprecedented possibilities considering combining a best of different ingredients inside one objective material. so far, metallic, semiconducting, and insulating 2d crystals have been used successfully inside making functional vdw heterostructures with properties by design. here, we expand 2d superconducting crystals as the building block of a vdw hererostructures. the one-step growth of large-scale high-quality vdw heterostructures of graphene and 2d superconducting a-mo2c by with the help of chemical vapor deposition (cvd) method was reported. a superconductivity and its 2d nature of a heterostructures are characterized by our scanning tunneling microscopy (stm) measurements. this adds a 2d superconductivity, a most attractive property of condensed matter physics, to a vdw heterostructures.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
2117,"we investigate a effect of clustering on network observability transitions. inside a observability model introduced by yang, wang, and motter [phys. rev. lett. 109, 258701 (2012)], the given fraction of nodes are chosen randomly, and they and those neighbors are considered to be observable, while a other nodes are unobservable. with the help of this model, we examine connected components of observable nodes and of unobservable nodes inside random clustered networks, which generalize random graphs to include triangles. we use generating functions to derive a normalized sizes of a largest observable component (loc) and largest unobservable component (luc), showing they are both affected by a network's clustering: more highly-clustered networks have lower critical node fractions considering forming macroscopic loc and luc, but this effect was small, becoming almost negligible unless a average degree was small. we also evaluate bounds considering these critical points to confirm clustering's weak or negligible effect on a network observability transition. a accuracy of our analytical treatment was confirmed by monte carlo simulations.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
4264,"we propose the simple modification of a no-scale supergravity wess-zumino model of starobinsky-like inflation to include the polonyi term inside a superpotential. a purpose of this term was to provide an explicit mechanism considering supersymmetry breaking at a end of inflation. we show how successful inflation should be achieved considering the gravitino mass satisfying a strict upper bound $m_{3/2}< 10^3$ tev, with favoured values $m_{3/2}\lesssim\mathcal{o}(1)$ tev. a model suggests that susy may be discovered inside collider physics experiments such as a lhc or a fcc.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7904,"we present density functional theory (dft) calculations of a magnetic anisotropy energy (mae) of fept, which was of great interest considering magnetic recording applications. our data, and a majority of previously calculated results considering perfectly ordered crystals, predict an mae of $\sim 3.0$ mev per formula unit, which was significantly larger than experimentally measured values. analyzing a effects of disorder by introducing stacking faults (sfs) and anti site defects (asds) inside varying concentrations we are able to reconcile calculations with experimental data and show that even the low concentration of asds are able to reduce a mae of fept considerably. investigating a effect of exact exchange and electron correlation within a adiabatic-connection dissipation fluctuation theorem inside a random phase approximation (acdft-rpa) reveals the significantly smaller influence on a mae. thus a effect of disorder, and more specifically asds, was a crucial factor inside explaining a deviation of common dft calculations of fept to experimental measurements.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
2195,"inside this paper, we propose the new feature selection method considering unsupervised domain adaptation based on a emerging optimal transportation theory. we build upon the recent theoretical analysis of optimal transport inside domain adaptation and show that it should directly suggest the feature selection procedure leveraging a shift between a domains. based on this, we propose the novel algorithm that aims to sort features by their similarity across a source and target domains, where a order was obtained by analyzing a coupling matrix representing a solution of a proposed optimal transportation problem. we evaluate our method on the well-known benchmark data set and illustrate its capability of selecting correlated features leading to better classification performances. furthermore, we show that a proposed algorithm should be used as the pre-processing step considering existing domain adaptation techniques ensuring an important speed-up inside terms of a computational time while maintaining comparable results. finally, we validate our algorithm on clinical imaging databases considering computer-aided diagnosis task with promising results.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6994,"eigenvector centrality was the standard network analysis tool considering determining a importance of (or ranking of) entities inside the connected system that was represented by the graph. however, many complex systems and datasets have natural multi-way interactions that are more faithfully modeled by the hypergraph. here we extend a notion of graph eigenvector centrality to uniform hypergraphs. traditional graph eigenvector centralities are given by the positive eigenvector of a adjacency matrix, which was guaranteed to exist by a perron-frobenius theorem under some mild conditions. a natural representation of the hypergraph was the hypermatrix (colloquially, the tensor). with the help of recently established perron-frobenius theory considering tensors, we develop three tensor eigenvectors centralities considering hypergraphs, each with different interpretations. we show that these centralities should reveal different information on real-world data by analyzing hypergraphs constructed from n-gram frequencies, co-tagging on stack exchange, and drug combinations observed inside patient emergency room visits.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
4464,"currently, women are referred considering brca1/2 mutation-testing only if their family-history of breast/ovarian cancer implies that their risk of carrying the mutation exceeds 10\%. however, as mutation-testing costs fall, prominent voices have called considering testing all women, which would strain clinical resources by testing millions of women, almost all of whom will test negative. to better evaluate risk-thresholds considering brca1/2 testing, we introduce two broadly applicable, linked metrics: mean risk stratification (mrs) and the decision-theoretic metric, net benefit of information (nbi). mrs and nbi provide the range of risk thresholds at which the marker/model was ""optimally informative"", inside a sense of maximizing both mrs and nbi. nbi was the function of only mrs and a risk-threshold considering action, connecting decision-theory to risk-stratification and providing the decision-theoretic rationale considering mrs. auc and youden's index reflect on both a fraction of maximum mrs, and of maximum nbi, attained by a marker/model, providing auc and youden's index with long-sought decision-theoretic and risk-stratification rationale. to evaluate risk-thresholds considering brca1/2 testing, we propose an eclectic idea behind the method considering auc, net benefit, and mrs/nbi. mrs/nbi interpret auc inside a context of mutation-prevalence and provide the range of risk thresholds considering which a risk model was optimally informative.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
9111,"we consider a problem of adversarial (non-stochastic) online learning with partial information feedback, where at each round, the decision maker selects an action from the finite set of alternatives. we develop the black-box idea behind the method considering such problems where a learner observes as feedback only losses of the subset of a actions that includes a selected action. when losses of actions are non-negative, under a graph-based feedback model introduced by mannor and shamir, we offer algorithms that attain a so called ""small-loss"" $o(\alpha l^{\star})$ regret bounds with high probability, where $\alpha$ was a independence number of a graph, and $l^{\star}$ was a loss of a best action. prior to our work, there is no data-dependent guarantee considering general feedback graphs even considering pseudo-regret (without dependence on a number of actions, i.e. utilizing a increased information feedback). taking advantage of a black-box nature of our technique, we extend our results to many other applications such as semi-bandits (including routing inside networks), contextual bandits (even with an infinite comparator class), as well as learning with slowly changing (shifting) comparators. inside a special case of classical bandit and semi-bandit problems, we provide optimal small-loss, high-probability guarantees of $\tilde{o}(\sqrt{dl^{\star}})$ considering actual regret, where $d$ was a number of actions, answering open questions of neu. previous bounds considering bandits and semi-bandits were known only considering pseudo-regret and only inside expectation. we also offer an optimal $\tilde{o}(\sqrt{\kappa l^{\star}})$ regret guarantee considering fixed feedback graphs with clique-partition number at most $\kappa$.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11838,"a differential event rate inside weakly interacting massive particle (wimp) direct detection experiments depends on a local dark matter density and velocity distribution. accurate modelling of a local dark matter distribution was therefore required to obtain reliable constraints on a wimp particle physics properties. data analyses typically use the simple standard halo model which might not be the good approximation to a real milky way (mw) halo. we review observational determinations of a local dark matter density, circular speed and escape speed and also studies of a local dark matter distribution inside simulated mw-like galaxies. we discuss a effects of a uncertainties inside these quantities on a energy spectrum and its time and direction dependence. finally we conclude with an overview of various methods considering handling these astrophysical uncertainties.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7478,"we present the differentiable framework capable of learning the wide variety of compositions of simple policies that we call skills. by recursively composing skills with themselves, we should create hierarchies that display complex behavior. skill networks are trained to generate skill-state embeddings that are provided as inputs to the trainable composition function, which inside turn outputs the policy considering a overall task. our experiments on an environment consisting of multiple collect and evade tasks show that this architecture was able to quickly build complex skills from simpler ones. furthermore, a learned composition function displays some transfer to unseen combinations of skills, allowing considering zero-shot generalizations.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
523,"an extension of a two-parameter log-lindley distribution of gomez et al. (2014) with support inside (0, 1) was proposed. its important properties like cumulative distribution function, moments, survival function, hazard rate function, shannon entropy, stochastic n ordering and convexity (concavity) conditions are derived. an application inside distorted premium principal was outlined and parameter approximation by method of maximum likelihood was also presented. we also consider use of the re-parameterized form of a proposed distribution inside regression modeling considering bounded responses by considering the modeling of real life data inside comparison with beta regression and log-lindley regression models.",0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
6594,"we study nearly-kahler 6-manifolds equipped with the cohomogeneity-two lie group action considering which a principal orbits are coisotropic. if a metric was complete, then we show that this last condition was automatically satisfied, and both a acting lie group and a principal orbits are finite quotients of $s^3 \times s^1$. we partition a class of such nearly-ka}hler structures into three types (called i, ii, iii) and prove the local existence and generality result considering each type. metrics of types i and ii are shown to be incomplete. we also derive the quasilinear elliptic pde system on a 2-dimensional orbit space which nearly-kahler structures of type i must satisfy. finally, we remark on the relatively simple one-parameter family of type iii structures that turn out to be incomplete metrics that are cohomogeneity-one under a action of the larger group.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
3303,"we use a language of uninformative bayesian prior choice to study a selection of appropriately simple effective models. we advocate considering a prior which maximizes a mutual information between parameters and predictions, learning as much as possible from limited data. when many parameters are poorly constrained by a available data, we find that this prior puts weight only on boundaries of a parameter manifold. thus it selects the lower-dimensional effective theory inside the principled way, ignoring irrelevant parameter directions. inside a limit where there was sufficient data to tightly constrain any number of parameters, this reduces to jeffreys prior. but we argue that this limit was pathological when applied to a hyper-ribbon parameter manifolds generic inside science, because it leads to dramatic dependence on effects invisible to experiment.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0
3059,"inside a early 1980's almgren developed the theory of dirichlet energy minimizing multi-valued functions, proving that a hausdorff dimension of a singular set (including branch points) of such the function was at most $(n-2),$ where $n$ was a dimension of its domain. almgren used this result inside an essential way to show that a same upper bound holds considering a dimension of a singular set of an area minimizing $n$-dimensional rectifiable current of arbitrary codimension. inside either case, a dimension bound was sharp. we develop estimates to study a asymptotic behaviour of the multi-valued dirichlet energy minimizer on idea behind the method to its singular set. our estimates imply that the dirichlet energy minimizer at ${\mathcal h}^{n-2}$ a.e. point of its singular set has the unique set of homogeneous multi-valued cylindrical tangent functions (blow-ups) to which a minimizer, modulo the set of single-valued harmonic functions, decays exponentially fast upon rescaling. the corollary was that a singular set was countably $(n-2)$-rectifiable. our work was inspired by a work of l. simon on a analysis of singularities of minimal submanifolds inside multiplicity 1 classes, and uses some new estimates and strategies together with techniques from wickramasekera's prior work to overcome additional difficulties arising from higher multiplicity and low regularity of a minimizers inside a presence of branch points. a results described here were announced inside earlier work of a authors where a special case of two-valued dirichlet minimizing functions is treated.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10439,"a cybele and hilda dynamical groups delimit a outer edge of a asteroid belt. their compositional distribution was the key element to constrain evolutionary models of a solar system. inside this paper, we present the compositional analysis of these populations with the help of spectroscopic observations, sdss and neowise data. as part of a primass (primitive asteroids spectroscopic survey), we acquired visible spectra of 18 objects inside hilda or cybele groups with a goodman high throughput spectrometer at a 4.1m soar telescope and 20 near-ir spectra of hilda objects with near infrared camera spectrograph at a 3.56m tng. a sample was enlarged with spectra taken from a literature inside order to increase our statistical analysis. a spectra were inspected considering aqueous alteration bands and other spectral features that should be linked to compositional constraints. a analysis shows the continuous distribution of compositions from a main-belt to a cybele, hilda and trojan regions. we also identify the population inside a trojans group not present inside hilda or cybele objects.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
62,"we generalized several results considering a arithmetic dynamics of monomial maps, including silverman's conjectures on height growth, dynamical mordell-lang conjecture, and dynamical manin-mumford conjecture. these results are originally known considering monomial maps on algebraic tori. we extend a results to arbitrary toric varieties.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
10540,"transiting exoplanets inside multi-planet systems have non-keplerian orbits which should cause a times and durations of transits to vary. a theory and observations of transit timing variations (ttv) and transit duration variations (tdv) are reviewed. since a last review, a kepler spacecraft has detected several hundred perturbed planets. inside the few cases, these data have been used to discover additional planets, similar to a historical discovery of neptune inside our own solar system. however, a more impactful aspect of ttv and tdv studies has been characterization of planetary systems inside which multiple planets transit. after addressing a equations of motion and parameter scalings, a main dynamical mechanisms considering ttv and tdv are described, with citations to a observational literature considering real examples. we describe parameter constraints, particularly a origin of a mass/eccentricity degeneracy and how it was overcome by a high-frequency component of a signal. on a observational side, derivation of timing precision and introduction to a timing diagram are given. science results are reviewed, with an emphasis on mass measurements of transiting sub-neptunes and super-earths, from which bulk compositions may be inferred.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
9553,"most stars inside a galaxy are believed to be formed within star clusters from collapsing molecular clouds. however, a complete process of star formation, from a parent cloud to the gas-free star cluster, was still poorly understood. we perform radiation-hydrodynamical simulations of a collapse of the turbulent molecular cloud with the help of a ramses-rt code. stars are modelled with the help of sink particles, from which we self-consistently follow a propagation of a ionising radiation. we study how different feedback models affect a gas expulsion from a cloud and how they shape a final properties of a emerging star cluster. we find that a star formation efficiency was lower considering stronger feedback models. feedback also changes a high mass end of a stellar mass function. stronger feedback also allows a establishment of the lower density star cluster, which should maintain the virial or sub-virial state. inside a absence of feedback, a star formation efficiency was very high, as well as a final stellar density. as the result, high energy close encounters make a cluster evaporate quickly. other indicators, such as mass segregation, statistics of multiple systems and escaping stars confirm this picture. observations of young star clusters are inside best agreement with our strong feedback simulation.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4944,"many reinforcement-learning researchers treat a reward function as the part of a environment, meaning that a agent should only know a reward of the state if it encounters that state inside the trial run. however, we argue that this was an unnecessary limitation and instead, a reward function should be provided to a learning algorithm. a advantage was that a algorithm should then use a reward function to check a reward considering states that a agent hasn't even encountered yet. inside addition, a algorithm should simultaneously learn policies considering multiple reward functions. considering each state, a algorithm would calculate a reward with the help of each of a reward functions and add a rewards to its experience replay dataset. a hindsight experience replay algorithm developed by andrychowicz et al. (2017) does just this, and learns to generalize across the distribution of sparse, goal-based rewards. we extend this algorithm to linearly-weighted, multi-objective rewards and learn the single policy that should generalize across all linear combinations of a multi-objective reward. whereas other multi-objective algorithms teach a q-function to generalize across a reward weights, our algorithm enables a policy to generalize, and should thus be used with continuous actions.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
9114,introduction to deep neural networks and their history.,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4764,"generative adversarial networks (gans) have received the tremendous amount of attention inside a past few years, and have inspired applications addressing the wide range of problems. despite its great potential, gans are difficult to train. recently, the series of papers (arjovsky & bottou, 2017a; arjovsky et al. 2017b; and gulrajani et al. 2017) proposed with the help of wasserstein distance as a training objective and promised easy, stable gan training across architectures with minimal hyperparameter tuning. inside this paper, we compare a performance of wasserstein distance with other training objectives on the variety of gan architectures inside a context of single image super-resolution. our results agree that wasserstein gan with gradient penalty (wgan-gp) provides stable and converging gan training and that wasserstein distance was an effective metric to gauge training progress.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11423,"heat production rates considering a geologically important nuclides ${}^{26}$al, ${}^{40}$k, ${}^{60}$fe, ${}^{232}$th, ${}^{235}$u, and ${}^{238}$u are calculated on a basis of recent data on atomic and nuclear properties. a revised data differ by several per cent from some older values, but indicate that more recent analyses converge toward values with an accuracy sufficient considering all common geoscience applications, although some possibilities considering improvement still remain, especially inside a case of ${}^{40}$k and with regard to a determination of half-lives. the python script was provided considering calculating heat production (this https url).",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
13820,"we consider a cost of general orthogonal range queries inside random quadtrees. a cost of the given query was encoded into the (random) function of four variables which characterize a coordinates of two opposite corners of a query rectangle. we prove that, when suitably shifted and rescaled, a random cost function converges uniformly inside probability towards the random field that was characterized as a unique solution to the distributional fixed-point equation. we also state similar results considering $2$-d trees. our results imply considering instance that a worst case query satisfies a same asymptotic estimates as the typical query, and thereby resolve an old question of chanzy, devroye and zamora-cura [\emph{acta inf.}, 37:355--383, 2000]",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
228,"deep convolutional neural networks are generally regarded as robust function approximators. so far, this intuition was based on perturbations to external stimuli such as a images to be classified. here we explore a robustness of convolutional neural networks to perturbations to a internal weights and architecture of a network itself. we show that convolutional networks are surprisingly robust to the number of internal perturbations inside a higher convolutional layers but a bottom convolutional layers are much more fragile. considering instance, alexnet shows less than the 30% decrease inside classification performance when randomly removing over 70% of weight connections inside a top convolutional or dense layers but performance was almost at chance with a same perturbation inside a first convolutional layer. finally, we suggest further investigations which could continue to inform a robustness of convolutional networks to internal perturbations.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5558,"inside this paper, we propose an improved method considering computing a $\mathcal{h}_\infty$ norm of linear dynamical systems that results inside the code that was often several times faster than existing methods. by with the help of standard optimization tools to rebalance a work load of a standard algorithm due to boyd, balakrishnan, bruinsma, and steinbuch, we aim to minimize a number of expensive eigenvalue computations that must be performed. unlike a standard algorithm, our modified idea behind the method should also calculate a $\mathcal{h}_\infty$ norm to full precision with little extra work, and also offers more opportunity to further accelerate its performance using parallelization. finally, we demonstrate that a local optimization we have employed to speed up a standard globally-convergent algorithm should also be an effective strategy on its own considering approximating a $\mathcal{h}_\infty$ norm of large-scale systems.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
13665,"we prove that the $bv$ map with values into a projective space $\mathbb{rp}^{d-1}$ has the $bv$ lifting with values into a unit sphere $\mathbb s^{d-1}$ that satisfies an optimal $bv$-estimate. as an application to liquid crystals, this result was also stated considering $bv$ maps with values into a set of uniaxial $q$-tensors. inside order to quantify $bv$ liftings, we prove an explicit formula considering an intrinsic $bv$-energy of maps with values into any compact smooth manifold.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16260,"context: a first gaia data release (dr1) delivered the catalogue of astrometry and photometry considering over the billion astronomical sources. within a panoply of methods used considering data exploration, visualisation was often a starting point and even a guiding reference considering scientific thought. however, this was the volume of data that cannot be efficiently explored with the help of traditional tools, techniques, and habits. aims: we aim to provide the global visual exploration service considering a gaia archive, something that was not possible out of a box considering most people. a service has two main goals. a first was to provide the software platform considering interactive visual exploration of a archive contents, with the help of common personal computers and mobile devices available to most users. a second aim was to produce intelligible and appealing visual representations of a enormous information content of a archive. methods: a interactive exploration service follows the client-server design. a server runs close to a data, at a archive, and was responsible considering hiding as far as possible a complexity and volume of a gaia data from a client. this was achieved by serving visual detail on demand. levels of detail are pre-computed with the help of data aggregation and subsampling techniques. considering dr1, a client was the web application that provides an interactive multi-panel visualisation workspace as well as the graphical user interface. results: a gaia archive visualisation service offers the web-based multi-panel interactive visualisation desktop inside the browser tab. it currently provides highly configurable 1d histograms and 2d scatter plots of gaia dr1 and a tycho-gaia astrometric solution (tgas) with linked views. an innovative feature was a creation of adql queries from visually defined regions inside plots. [abridged]",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
45,"one-dimensional systems obtained as low-energy limits of hybrid superconductor-topological insulator devices provide means of production, transport, and destruction of majorana bound states (mbss) by variations of a magnetic flux. when two or more pairs of mbss are present inside a intermediate state, there was the possibility of the landau-zener transition, wherein even the slow variation of a flux leads to production of the quasiparticle pair. we study numerically the version of this process, with four mbss produced and subsequently destroyed, and find that, quite universally, a probability of quasiparticle production inside it was 50%. this implies that a effect may be the limiting factor inside applications requiring the high degree of quantum coherence.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
6315,"surface magnetism was believed to be a main driver of coronal heating and stellar wind acceleration. coronae are believed to be formed by plasma confined inside closed magnetic coronal loops of a stars, with winds mainly originating inside open magnetic field line regions. inside this chapter, we review some basic properties of stellar coronae and winds and present some existing models. inside a last part of this chapter, we discuss a effects of coronal winds on exoplanets.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12314,"let $f$ and $g$ be $1$-bounded multiplicative functions considering which $f*g=1_{.=1}$. a bombieri-vinogradov theorem holds considering both $f$ and $g$ if and only if a siegel-walfisz criterion holds considering both $f$ and $g$, and a bombieri-vinogradov theorem holds considering $f$ restricted to a primes.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
5057,"an appearance-based robot self-localization problem was considered inside a machine learning framework. a appearance space was composed of all possible images, which should be captured by the robot's visual system under all robot localizations. with the help of recent manifold learning and deep learning techniques, we propose the new geometrically motivated solution based on training data consisting of the finite set of images captured inside known locations of a robot. a solution includes approximation of a robot localization mapping from a appearance space to a robot localization space, as well as approximation of a inverse mapping considering modeling visual image features. a latter allows solving a robot localization problem as a kalman filtering problem.",1,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
19163,"we propose strategies to approximate and make inference on key features of heterogeneous effects inside randomized experiments. these key features include best linear predictors of a effects with the help of machine learning proxies, average effects sorted by impact groups, and average characteristics of most and least impacted units. a idea behind the method was valid inside high dimensional settings, where a effects are proxied by machine learning methods. we post-process these proxies into a estimates of a key features. our idea behind the method was generic, it should be used inside conjunction with penalized methods, deep and shallow neural networks, canonical and new random forests, boosted trees, and ensemble methods. our idea behind the method was agnostic and does not make unrealistic or hard-to-check assumptions; we don't require conditions considering consistency of a ml methods. approximation and inference relies on repeated data splitting to avoid overfitting and achieve validity. considering inference, we take medians of p-values and medians of confidence intervals, resulting from many different data splits, and then adjust their nominal level to guarantee uniform validity. this variational inference method was shown to be uniformly valid and quantifies a uncertainty coming from both parameter approximation and data splitting. a inference method could be of substantial independent interest inside many machine learning applications. an empirical application to a impact of micro-credit on economic development illustrates a use of a idea behind the method inside randomized experiments. an additional application to a impact of a gender discrimination on wages illustrates a potential use of a idea behind the method inside observational studies, where machine learning methods should be used to condition flexibly on very high-dimensional controls.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0
19461,"multiple instance learning (mil) was the variation of traditional supervised learning problems where data (referred to as bags) are composed of sub-elements (referred to as instances) and only bag labels are available. mil has the variety of applications such as content-based image retrieval, text categorization and medical diagnosis. most of a previous work considering mil assume that a training bags are fully labeled. however, it was often difficult to obtain an enough number of labeled bags inside practical situations, while many unlabeled bags are available. the learning framework called pu learning (positive and unlabeled learning) should address this problem. inside this paper, we propose the convex pu learning method to solve an mil problem. we experimentally show that a proposed method achieves better performance with significantly lower computational costs than an existing method considering pu-mil.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
15030,"inside this work, we present the systematic study of a occupied and unoccupied electronic states of lacoo$_{3}$ compound with the help of dft, dft+$\textit{u}$ and dft+embedded dmft methods. a value of $\textit{u}$ used here was evaluated by with the help of constrained dft method and found to be $ \backsim $ 6.9 ev. it was found that dft result has limitations with energy positions of pdos peaks due to its inability of creating the hard gap although a dos distribution appears to be fine with experimental attributes. a calculated value of $\textit{u}$ was not an appropriate value considering carrying out dft+$\textit{u}$ calculations as it has created an insulating gap of $ \backsim $ 1.8 ev with limitations inside redistribution of dos which was inconsistent with experimental spectral behaviour considering a occupied states mainly. however, this value of $\textit{u}$ was found to be an appropriate one considering dft+embedded dmft method which creates the gap of $\backsim $ 1.1 ev. a calculated pdos of co 3$\textit{d}$, la 5$\textit{d}$, la 4$\textit{f}$ and o 2$\textit{p}$ states are giving the remarkably good explanation considering a occupied and unoccupied states of a experimental spectra inside a energy range $\backsim $ -9.0 ev to $\backsim $ 12.0 ev.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
2104,"data and knowledge representation are fundamental concepts inside machine learning. a quality of a representation impacts a performance of a learning model directly. feature learning transforms or enhances raw data to structures that are effectively exploited by those models. inside recent years, several works have been with the help of complex networks considering data representation and analysis. however, no feature learning method has been proposed considering such category of techniques. here, we present an unsupervised feature learning mechanism that works on datasets with binary features. first, a dataset was mapped into the feature--sample network. then, the multi-objective optimization process selects the set of new vertices to produce an enhanced version of a network. a new features depend on the nonlinear function of the combination of preexisting features. effectively, a process projects a input data into the higher-dimensional space. to solve a optimization problem, we design two metaheuristics based on a lexicographic genetic algorithm and a improved strength pareto evolutionary algorithm (spea2). we show that a enhanced network contains more information and should be exploited to improve a performance of machine learning methods. a advantages and disadvantages of each optimization strategy are discussed.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14955,"a past decade has seen the significant interest inside learning tractable probabilistic representations. arithmetic circuits (acs) were among a first proposed tractable representations, with some subsequent representations being instances of acs with weaker or stronger properties. inside this paper, we provide the formal basis under which variants on acs should be compared, and where a precise roles and semantics of their various properties should be made more transparent. this allows us to place some recent developments on acs inside the clearer perspective and to also derive new results considering acs. this includes an exponential separation between acs with and without determinism; completeness and incompleteness results; and tractability results (or lack thereof) when computing most probable explanations (mpes).",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
12659,"we design controllers from formal specifications considering positive discrete-time monotone systems that are subject to bounded disturbances. such systems are widely used to model a dynamics of transportation and biological networks. a specifications are described with the help of signal temporal logic (stl), which should express the broad range of temporal properties. we formulate a problem as the mixed-integer linear program (milp) and show that under a assumptions made inside this paper, which are not restrictive considering traffic applications, a existence of open-loop control policies was sufficient and almost necessary to ensure a satisfaction of stl formulas. we establish the relation between satisfaction of stl formulas inside infinite time and set-invariance theories and provide an efficient method to compute robust control invariant sets inside high dimensions. we also develop the robust model predictive framework to plan controls optimally while ensuring a satisfaction of a specification. illustrative examples and the traffic management case study are included.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
4834,"a advanced operation of future electricity distribution systems was likely to require significant observability of a different parameters of interest (e.g., demand, voltages, currents, etc.). ensuring completeness of data is, therefore, paramount. inside this context, an algorithm considering recovering missing state variable observations inside electricity distribution systems was presented. a proposed method exploits a low rank structure of a state variables using the matrix completion idea behind the method while incorporating prior knowledge inside a form of second order statistics. specifically, a recovery method combines nuclear norm minimization with bayesian estimation. a performance of a new algorithm was compared to a information-theoretic limits and tested trough simulations with the help of real data of an urban low voltage distribution system. a impact of a prior knowledge was analyzed when the mismatched covariance was used and considering the markovian sampling that introduces structure inside a observation pattern. numerical results demonstrate that a proposed algorithm was robust and outperforms existing state of a art algorithms.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
14585,"a present paper extends a thermodynamic dislocation theory developed by langer, bouchbinder, and lookmann to non-uniform plastic deformations. a free energy density as well as a positive definite dissipation function are proposed. a governing equations are derived from a variational equation. as illustration, a problem of plane strain constrained shear of single crystal deforming inside single slip was solved within a proposed theory.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
17368,"inside this paper we focus on a linear algebra theory behind feedforward (fnn) and recurrent (rnn) neural networks. we review backward propagation, including backward propagation through time (bptt). also, we obtain the new exact expression considering hessian, which represents second order effects. we show that considering $t$ time steps a weight gradient should be expressed as the rank-$t$ matrix, while a weight hessian was as the sum of $t^{2}$ kronecker products of rank-$1$ and $w^{t}aw$ matrices, considering some matrix $a$ and weight matrix $w$. also, we show that considering the mini-batch of size $r$, a weight update should be expressed as the rank-$rt$ matrix. finally, we briefly comment on a eigenvalues of a hessian matrix.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17461,"we present component-based simplex architecture (cbsa), the new framework considering assuring a runtime safety of component-based cyber-physical systems (cpss). cbsa integrates assume-guarantee (a-g) reasoning with a core principles of a simplex control architecture to allow component-based cpss to run advanced, uncertified controllers while still providing runtime assurance that a-g contracts and global properties are satisfied. inside cbsa, multiple simplex instances, which should be composed inside the nested, serial or parallel manner, coordinate to assure system-wide properties. combining a-g reasoning and a simplex architecture was the challenging problem that yields significant benefits. by utilizing a-g contracts, we are able to compositionally determine a switching logic considering cbsas, thereby alleviating a state explosion encountered by other approaches. another benefit was that we should use a-g proof rules to decompose a proof of system-wide safety assurance into sub-proofs corresponding to a component-based structure of a system architecture. we also introduce a notion of coordinated switching between simplex instances, the key component of our compositional idea behind the method to reasoning about cbsa switching logic. we illustrate our framework with the component-based control system considering the ground rover. we formally prove that a cbsa considering this system guarantees energy safety (the rover never runs out of power), and collision freedom (the rover never collides with the stationary obstacle). we also consider the cbsa considering a rover that guarantees mission completion: all target destinations visited within the prescribed amount of time.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
4179,"we directly detect dust emission inside an optically-detected, multiply-imaged galaxy lensed by a frontier fields cluster macsj0717.5+3745. we detect two images of a same galaxy at 1.1mm with a aztec camera on a large millimeter telescope leaving no ambiguity inside a counterpart identification. this galaxy, macs071_az9, was at z>4 and a strong lensing model (mu=7.5) allows us to calculate an intrinsic ir luminosity of 9.7e10 lsun and an obscured star formation rate of 14.6 +/- 4.5 msun/yr. a unobscured star formation rate from a uv was only 4.1 +/- 0.3 msun/yr which means a total star formation rate (18.7 +/- 4.5 msun/yr) was dominated (75-80%) by a obscured component. with an intrinsic stellar mass of only 6.9e9msun, macs0717_az9 was one of only the handful of z>4 galaxies at these lower masses that was detected inside dust emission. this galaxy lies close to a estimated star formation sequence at this epoch. however, it does not lie on a dust obscuration relation (irx-beta) considering local starburst galaxies and was instead consistent with a small magellanic cloud (smc) attenuation law. this remarkable lower mass galaxy showing signs of both low metallicity and high dust content may challenge our picture of dust production inside a early universe.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6080,"a standard probabilistic perspective on machine learning gives rise to empirical risk-minimization tasks that are frequently solved by stochastic gradient descent (sgd) and variants thereof. we present the formulation of these tasks as classical inverse or filtering problems and, furthermore, we propose an efficient, gradient-free algorithm considering finding the solution to these problems with the help of ensemble kalman inversion (eki). applications of our idea behind the method include offline and online supervised learning with deep neural networks, as well as graph-based semi-supervised learning. a essence of a eki procedure was an ensemble based approximate gradient descent inside which derivatives are replaced by differences from within a ensemble. we suggest several modifications to a basic method, derived from empirically successful heuristics developed inside a context of sgd. numerical results demonstrate wide applicability and robustness of a proposed algorithm.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5347,"we propose and analyze the variant of a classic polyak-ruppert averaging scheme, broadly used inside stochastic gradient methods. rather than the uniform average of a iterates, we consider the weighted average, with weights decaying inside the geometric fashion. inside a context of linear least squares regression, we show that this averaging scheme has the a same regularizing effect, and indeed was asymptotically equivalent, to ridge regression. inside particular, we derive finite-sample bounds considering a proposed idea behind the method that match a best known results considering regularized stochastic gradient methods.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5490,"this paper presents the novel idea behind the method to estimating a continuous six degree of freedom (6-dof) pose (3d translation and rotation) of an object from the single rgb image. a idea behind the method combines semantic keypoints predicted by the convolutional network (convnet) with the deformable shape model. unlike prior work, we are agnostic to whether a object was textured or textureless, as a convnet learns a optimal representation from a available training image data. furthermore, a idea behind the method should be applied to instance- and class-based pose recovery. empirically, we show that a proposed idea behind the method should accurately recover a 6-dof object pose considering both instance- and class-based scenarios with the cluttered background. considering class-based object pose estimation, state-of-the-art accuracy was shown on a large-scale pascal3d+ dataset.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
4236,"modeling inverse dynamics was crucial considering accurate feedforward robot control. a model computes a necessary joint torques, to perform the desired movement. a highly non-linear inverse function of a dynamical system should be approximated with the help of regression techniques. we propose as regression method the tensor decomposition model that exploits a inherent three-way interaction of positions x velocities x accelerations. most work inside tensor factorization has addressed a decomposition of dense tensors. inside this paper, we build upon a decomposition of sparse tensors, with only small amounts of nonzero entries. a decomposition of sparse tensors has successfully been used inside relational learning, e.g., a modeling of large knowledge graphs. recently, a idea behind the method has been extended to multi-class classification with discrete input variables. representing a data inside high dimensional sparse tensors enables a approximation of complex highly non-linear functions. inside this paper we show how a decomposition of sparse tensors should be applied to regression problems. furthermore, we extend a method to continuous inputs, by learning the mapping from a continuous inputs to a latent representations of a tensor decomposition, with the help of basis functions. we evaluate our proposed model on the dataset with trajectories from the seven degrees of freedom sarcos robot arm. our experimental results show superior performance of a proposed functional tensor model, compared to challenging state-of-the art methods.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
17439,"we give, considering a first time, self-consistent large-$n$ analytical solutions of inhomogeneous condensates inside a quantum ${\mathbb c}p^{n-1}$ model inside a large-$n$ limit. we find the map from the set of gap equations of a ${\mathbb c}p^{n-1}$ model to those of a gross-neveu (gn) model (or a gap equation and a bogoliubov-de gennes equation), which enables us to find a self-consistent solutions. we find that a higgs field of a ${\mathbb c}p^{n-1}$ model was given as the zero mode of solutions of a gn model, and consequently only topologically nontrivial solutions of a gn model yield nontrivial solutions of a ${\mathbb c}p^{n-1}$ model. the stable single soliton was constructed from an anti-kink of a gn model and has the broken (higgs) phase in its core,in which ${\mathbb c}p^{n-1}$ modes are localized,with the symmetric (confining) phase outside. we further find the stable periodic soliton lattice constructed from the real kink crystal inside a gn model,while a ablowitz-kaup-newell-segur hierarchy yields multiple solitons at arbitrary separations.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
9090,"we study a evolution of long-lived controversial debates as manifested on twitter from 2011 to 2016. specifically, we explore how a structure of interactions and content of discussion varies with a level of collective attention, as evidenced by a number of users discussing the topic. spikes inside a volume of users typically correspond to external events that increase a public attention on a topic -- as, considering instance, discussions about `gun control' often erupt after the mass shooting. this work was a first to study a dynamic evolution of polarized online debates at such scale. by employing the wide array of network and content analysis measures, we find consistent evidence that increased collective attention was associated with increased network polarization and network concentration within each side of a debate; and overall more uniform lexicon usage across all users.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
14822,"inside this paper, we present the new method considering egocentric video temporal segmentation based on integrating the statistical mean change detector and agglomerative clustering(ac) within an energy-minimization framework. given a tendency of most ac methods to oversegment video sequences when clustering their frames, we combine a clustering with the concept drift detection technique (adwin) that has rigorous guarantee of performances. adwin serves as the statistical upper bound considering a clustering-based video segmentation. we integrate both techniques inside an energy-minimization framework that serves to disambiguate a decision of both techniques and to complete a segmentation taking into account a temporal continuity of video frames descriptors. we present experiments over egocentric sets of more than 13.000 images acquired with different wearable cameras, showing that our method outperforms state-of-the-art clustering methods.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1392,"we describe an essentially perfect hashing algorithm considering calculating a position of an element inside an ordered list, appropriate considering a construction and manipulation of many-body hamiltonian, sparse matrices. each element of a list corresponds to an integer value whose binary representation reflects a occupation of single-particle basis states considering each element inside a many-body hilbert space. a algorithm replaces conventional methods, such as binary search, considering locating a elements of a ordered list, eliminating a need to store a integer representation considering each element, without increasing a computational complexity. combined with a ""checkerboard"" decomposition of a hamiltonian matrix considering distribution over parallel computing environments, this leads to the substantial savings inside aggregate memory. while a algorithm should be applied broadly to many-body, correlated problems, we demonstrate its utility inside reducing total memory consumption considering the series of fermionic single-band hubbard model calculations on small clusters with progressively larger hilbert space dimension.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
1921,"we derive an exact expression considering a correlation function inside redshift shells including all a relativistic contributions. this expression, which does not rely on a distant-observer or flat-sky approximation, was valid at all scales and includes both local relativistic corrections and integrated contributions, like gravitational lensing. we present two methods to calculate this correlation function, one which makes use of a angular power spectrum c_ell(z1,z2) and the second method which evades a costly calculations of a angular power spectra. a correlation function was then used to define a power spectrum as its fourier transform. inside this work theoretical aspects of this procedure are presented, together with quantitative examples. inside particular, we show that gravitational lensing modifies a multipoles of a correlation function and of a power spectrum by the few percent at redshift z=1 and by up to 30% and more at z=2. we also point out that large-scale relativistic effects and wide-angle corrections generate contributions of a same order of magnitude and have consequently to be treated inside conjunction. these corrections are particularly important at small redshift, z=0.1, where they should reach 10%. this means inside particular that the flat-sky treatment of relativistic effects, with the help of considering example a power spectrum, was not consistent.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16039,"this paper presents the new method --- adversarial advantage actor-critic (adversarial a2c), which significantly improves a efficiency of dialogue policy learning inside task-completion dialogue systems. inspired by generative adversarial networks (gan), we train the discriminator to differentiate responses/actions generated by dialogue agents from responses/actions by experts. then, we incorporate a discriminator as another critic into a advantage actor-critic (a2c) framework, to encourage a dialogue agent to explore state-action within a regions where a agent takes actions similar to those of a experts. experimental results inside the movie-ticket booking domain show that a proposed adversarial a2c should accelerate policy exploration efficiently.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4537,"traffic flow prediction was an important research issue considering solving a traffic congestion problem inside an intelligent transportation system (its). traffic congestion was one of a most serious problems inside the city, which should be predicted inside advance by analyzing traffic flow patterns. such prediction was possible by analyzing a real-time transportation data from correlative roads and vehicles. this article first gives the brief introduction to a transportation data, and surveys a state-of-the-art prediction methods. then, we verify whether or not a prediction performance was able to be improved by fitting actual data to optimize a parameters of a prediction model which was used to predict a traffic flow. such verification was conducted by comparing a optimized time series prediction model with a normal time series prediction model. this means that inside a era of big data, accurate use of a data becomes a focus of studying a traffic flow prediction to solve a congestion problem. finally, experimental results of the case study are provided to verify a existence of such performance improvement, while a research challenges of this data-analytics-based prediction are presented and discussed.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7684,"inside this paper, we present the novel control law considering longitudinal speed control of autonomous vehicles. a key contributions of a proposed work include a design of the control law that reactively integrates a longitudinal surface gradient of road into its operation. inside contrast to a existing works, we found that integrating a path gradient into a control framework improves a speed tracking efficacy. since a control law was implemented over the shrinking domain scheme, it minimizes a integrated error by recomputing a control inputs at every discretized step and consequently provides less reaction time. this makes our control law suitable considering motion planning frameworks that are operating at high frequencies. furthermore, our work was implemented with the help of the generalized vehicle model and should be easily extended to other classes of vehicles. a performance of gradient aware-shrinking domain based controller was implemented and tested on the stock electric vehicle on which the number of sensors are mounted. results from a tests show a robustness of our control law considering speed tracking on the terrain with varying gradient while also considering stringent time constraints imposed by a planning framework.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
18136,"to analyze marine animals behavior, seasonal distribution and abundance, digital imagery should be acquired by visual or lidar camera. depending on a quantity and properties of acquired imagery, a animals are characterized as either features (shape, color, texture, etc.), or dissimilarity matrices derived from different shape analysis methods (shape context, internal distance shape context, etc.). considering both cases, multi-view learning was critical inside integrating more than one set of feature or dissimilarity matrix considering higher classification accuracy. this paper adopts correntropy loss as cost function inside multi-view learning, which has favorable statistical properties considering rejecting noise. considering a case of features, a correntropy loss-based multi-view learning and its entrywise variation are developed based on a multi-view intact space learning algorithm. considering a case of dissimilarity matrices, a robust euclidean embedding algorithm was extended to its multi-view form with a correntropy loss function. results from simulated data and real-world marine animal imagery show that a proposed algorithms should effectively enhance classification rate, as well as suppress noise under different noise conditions.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
6375,"modern technology considering producing extremely bright and coherent x-ray laser pulses provides a possibility to acquire the large number of diffraction patterns from individual biological nanoparticles, including proteins, viruses, and dna. these two-dimensional diffraction patterns should be practically reconstructed and retrieved down to the resolution of the few \angstrom. inside principle, the sufficiently large collection of diffraction patterns will contain a required information considering the full three-dimensional reconstruction of a biomolecule. a computational methodology considering this reconstruction task was still under development and highly resolved reconstructions have not yet been produced. we analyze a expansion-maximization-compression scheme, a current state of a art idea behind the method considering this very challenging application, by isolating different sources of uncertainty. through numerical experiments on synthetic data we evaluate their respective impact. we reach conclusions of relevance considering handling actual experimental data, as well as pointing out certain improvements to a underlying approximation algorithm. we also introduce the practically applicable computational methodology inside a form of bootstrap procedures considering assessing reconstruction uncertainty inside a real data case. we evaluate a sharpness of this idea behind the method and argue that this type of procedure will be critical inside a near future when handling a increasing amount of data.",1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
15147,"image-language matching tasks have recently attracted the lot of attention inside a computer vision field. these tasks include image-sentence matching, i.e., given an image query, retrieving relevant sentences and vice versa, and region-phrase matching or visual grounding, i.e., matching the phrase to relevant regions. this paper investigates two-branch neural networks considering learning a similarity between these two data modalities. we propose two network structures that produce different output representations. a first one, referred to as an embedding network, learns an explicit shared latent embedding space with the maximum-margin ranking loss and novel neighborhood constraints. compared to standard triplet sampling, we perform improved neighborhood sampling that takes neighborhood information into consideration while constructing mini-batches. a second network structure, referred to as the similarity network, fuses a two branches using element-wise product and was trained with regression loss to directly predict the similarity score. extensive experiments show that our networks achieve high accuracies considering phrase localization on a flickr30k entities dataset and considering bi-directional image-sentence retrieval on flickr30k and mscoco datasets.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9724,"recently, mahloujifar and mahmoody (tcc'17) studied attacks against learning algorithms with the help of the special case of valiant's malicious noise, called $p$-tampering, inside which a adversary gets to change any training example with independent probability $p$ but was limited to only choose malicious examples with correct labels. they obtained $p$-tampering attacks that increase a error probability inside a so called targeted poisoning model inside which a adversary's goal was to increase a loss of a trained hypothesis over the particular test example. at a heart of their attack is an efficient algorithm to bias a expected value of any bounded real-output function through $p$-tampering. inside this work, we present new biasing attacks considering increasing a expected value of bounded real-valued functions. our improved biasing attacks, directly imply improved $p$-tampering attacks against learners inside a targeted poisoning model. as the bonus, our attacks come with considerably simpler analysis. we also study a possibility of pac learning under $p$-tampering attacks inside a non-targeted (aka indiscriminate) setting where a adversary's goal was to increase a risk of a generated hypothesis (for the random test example). we show that pac learning was possible under $p$-tampering poisoning attacks essentially whenever it was possible inside a realizable setting without a attacks. we further show that pac learning under ""correct-label"" adversarial noise was not possible inside general, if a adversary could choose a (still limited to only $p$ fraction of) tampered examples that she substitutes with adversarially chosen ones. our formal model considering such ""bounded-budget"" tampering attackers was inspired by a notions of (strong) adaptive corruption inside secure multi-party computation.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5352,"we introduce the phase-field crystal model that creates an array of complex three- and two-dimensional crystal structures using the numerically tractable three-point correlation function. a three-point correlation function was designed inside order to energetically favor a principal interplanar angles of the target crystal structure. this was achieved using an analysis performed by examining a crystal's structure factor. this idea behind the method successfully yields energetically stable simple cubic, diamond cubic, simple hexagonal, graphene layers, and caf$_2$ crystals. to illustrate a ability of a method to yield the particularly complex and technologically important crystal structure, we show how this three-point correlation function method should be used to generate perovskite crystals.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
16295,"autonomous urban driving navigation with complex multi-agent dynamics was under-explored due to a difficulty of learning an optimal driving policy. a traditional modular pipeline heavily relies on hand-designed rules and a pre-processing perception system while a supervised learning-based models are limited by a accessibility of extensive human experience. we present the general and principled controllable imitative reinforcement learning (cirl) idea behind the method which successfully makes a driving agent achieve higher success rates based on only vision inputs inside the high-fidelity car simulator. to alleviate a low exploration efficiency considering large continuous action space that often prohibits a use of classical rl on challenging real tasks, our cirl explores over the reasonably constrained action space guided by encoded experiences that imitate human demonstrations, building upon deep deterministic policy gradient (ddpg). moreover, we propose to specialize adaptive policies and steering-angle reward designs considering different control signals (i.e. follow, straight, turn right, turn left) based on a shared representations to improve a model capability inside tackling with diverse cases. extensive experiments on carla driving benchmark demonstrate that cirl substantially outperforms all previous methods inside terms of a percentage of successfully completed episodes on the variety of goal-directed driving tasks. we also show its superior generalization capability inside unseen environments. to our knowledge, this was a first successful case of a learned driving policy through reinforcement learning inside a high-fidelity simulator, which performs better-than supervised imitation learning.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
2392,"inside this paper, we provide two types of sufficient conditions considering ensuring a quadratic growth conditions of the class of constrained convex symmetric and non-symmetric matrix optimization problems regularized by nonsmooth spectral functions. these sufficient conditions are derived using a study of a $\mathcal{c}^2$-cone reducibility of spectral functions and a metric subregularity of their subdifferentials, respectively. as an application, we demonstrate how quadratic growth conditions are used to guarantee a desirable fast convergence rates of a augmented lagrangian methods (alm) considering solving convex matrix optimization problems. numerical experiments on an easy-to-implement alm applied to a fastest mixing markov chain problem are also presented to illustrate a significance of a obtained results.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
558,"inside this paper, we study a controllability and stabilizability properties of a kolmogorov forward equation of the continuous time markov chain (ctmc) evolving on the finite state space, with the help of a transition rates as a control parameters. firstly, we prove small-time local and global controllability from and to strictly positive equilibrium configurations when a underlying graph was strongly connected. secondly, we show that there always exists the locally exponentially stabilizing decentralized linear (density-)feedback law that takes zero valu at equilibrium and respects a graph structure, provided that a transition rates are allowed to be negative and a desired target density lies inside a interior of a set of probability densities. considering bidirected graphs, that is, graphs where the directed edge inside one direction implies an edge inside a opposite direction, we show that this linear control law should be realized with the help of the decentralized rational feedback law of a form k(x) = a(x) + b(x)f(x)/g(x) that also respects a graph structure and control constraints (positivity and zero at equilibrium). this enables a possibility of with the help of linear matrix inequality (lmi) based tools to algorithmically construct decentralized density feedback controllers considering stabilization of the robotic swarm to the target task distribution with no task-switching at equilibrium, as we demonstrate with several numerical examples.",1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1
5867,"inside this paper we propose the new semi-supervised gan architecture (ss-infogan) considering image synthesis that leverages information from few labels (as little as 0.22%, max. 10% of a dataset) to learn semantically meaningful and controllable data representations where latent variables correspond to label categories. a architecture builds on information maximizing generative adversarial networks (infogan) and was shown to learn both continuous and categorical codes and achieves higher quality of synthetic samples compared to fully unsupervised settings. furthermore, we show that with the help of small amounts of labeled data speeds-up training convergence. a architecture maintains a ability to disentangle latent variables considering which no labels are available. finally, we contribute an information-theoretic reasoning on how introducing semi-supervision increases mutual information between synthetic and real data.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16395,"a connection between multifrequency quasar observational and physical parameters related to accretion processes was still open to debate. inside a last 20 year, eigenvector 1-based approaches developed since a early papers by boroson and green (1992) and sulentic et al. (2000b) have been proven to be the remarkably powerful tool to investigate this issue, and have led to a definition of the quasar ""main sequence"". inside this paper we perform the cladistic analysis on two samples of 215 and 85 low-z quasars (z 0.7) which were studied inside several previous works and which offer the satisfactory coverage of a eigenvector 1-derived main sequence. a data encompass accurate measurements of observational parameters which represent key aspects associated with a structural diversity of quasars. cladistics was able to group sources radiating at higher eddington ratios, as well as to separate radio-quiet (rq) and radio-loud (rl) quasars. a analysis suggests the black hole mass threshold considering powerful radio emission and also properly distinguishes core-dominated and lobe-dominated quasars, inside accordance with a basic tenet of rl unification schemes. considering that black hole mass provides the sort of ""arrow of time"" of nuclear activity, the phylogenetic interpretation becomes possible if cladistic trees are rooted on black hole mass: a ontogeny of black holes was represented by their monotonic increase inside mass. more massive radio-quiet population b sources at low-z become the more evolved counterpart of population the i.e., wind dominated sources to which a ""local"" narrow-line seyfert 1s belong.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14740,"inside this paper, we introduce the novel algorithm considering calculating arbitrary order cumulants of multidimensional data. since a $d^\text{th}$ order cumulant should be presented inside a form of an $d$-dimensional tensor, a algorithm was presented with the help of tensor operations. a algorithm provided inside a paper takes advantage of super-symmetry of cumulant and moment tensors. we show that a proposed algorithm considerably reduces a computational complexity and a computational memory requirement of cumulant calculation as compared with existing algorithms. considering a sizes of interest, a reduction was of a order of $d!$ compared to a naive algorithm.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5038,"a continouity and compactness of embedding operators inside inside sobolev-lions type spaces are derived. by applying this result separability properties of degenerate anisotropic differential operator equations, well-posedeness and strichartz type estimates considering solution of corresponding parabolic problem are established",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12510,"a present work deals with active sampling of graph nodes representing training data considering binary classification. a graph may be given or constructed with the help of similarity measures among nodal features. leveraging a graph considering classification builds on a premise that labels across neighboring nodes are correlated according to the categorical markov random field (mrf). this model was further relaxed to the gaussian (g)mrf with labels taking continuous values - an approximation that not only mitigates a combinatorial complexity of a categorical model, but also offers optimal unbiased soft predictors of a unlabeled nodes. a proposed sampling strategy was based on querying a node whose label disclosure was expected to inflict a largest change on a gmrf, and inside this sense it was a most informative on average. such the strategy subsumes several measures of expected model change, including uncertainty sampling, variance minimization, and sampling based on a $\sigma-$optimality criterion. the simple yet effective heuristic was also introduced considering increasing a exploration capabilities of a sampler, and reducing bias of a resultant classifier, by taking into account a confidence on a model label predictions. a novel sampling strategies are based on quantities that are readily available without a need considering model retraining, rendering them computationally efficient and scalable to large graphs. numerical tests with the help of synthetic and real data demonstrate that a proposed methods achieve accuracy that was comparable or superior to a state-of-the-art even at reduced runtime.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
10640,"domain knowledge should often be encoded inside a structure of the network, such as convolutional layers considering vision, which has been shown to increase generalization and decrease sample complexity, or a number of samples required considering successful learning. inside this study, we ask whether sample complexity should be reduced considering systems where a structure of a domain was unknown beforehand, and a structure and parameters must both be learned from a data. we show that sample complexity reduction through learning structure was possible considering at least two simple cases. inside studying these cases, we also gain insight into how this might be done considering more complex domains.",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16680,"let $x$ be the spherical variety considering the connected reductive group $g$. work of gaitsgory-nadler strongly suggests that a langlands dual group $g^\vee$ of $g$ has the subgroup whose weyl group was a little weyl group of $x$. sakellaridis-venkatesh defined the refined dual group $g^\vee_x$ and verified inside many cases that there exists an isogeny $\phi$ from $g^\vee_x$ to $g^\vee$. inside this paper, we establish a existence of $\phi$ inside full generality. our idea behind the method was purely combinatorial and works (despite a title) considering arbitrary $g$-varieties.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
17276,"optimization with noisy gradients has become ubiquitous inside statistics and machine learning. reparameterization gradients, or gradient estimates computed using a ""reparameterization trick,"" represent the class of noisy gradients often used inside monte carlo variational inference (mcvi). however, when these gradient estimators are too noisy, a optimization procedure should be slow or fail to converge. one way to reduce noise was to use more samples considering a gradient estimate, but this should be computationally expensive. instead, we view a noisy gradient as the random variable, and form an inexpensive approximation of a generating procedure considering a gradient sample. this approximation has high correlation with a noisy gradient by construction, making it the useful control variate considering variance reduction. we demonstrate our idea behind the method on non-conjugate multi-level hierarchical models and the bayesian neural net where we observed gradient variance reductions of multiple orders of magnitude (20-2,000x).",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0
8604,"observational constraints on a abundance of primordial black holes (pbhs) constrain a allowed amplitude of a primordial power spectrum on both a smallest and a largest ranges of scales, covering over 20 decades from $1-10^{20}/ \rm{mpc}$. despite tight constraints on a allowed fraction of pbhs at their time of formation near horizon entry inside a early universe, a corresponding constraints on a primordial power spectrum are quite weak, typically ${\cal p}_\mathcal{r}\lesssim 10^{-2}$ assuming gaussian perturbations. motivated by recent claims that a evaporation of just one pbh would destabilise a higgs vacuum and collapse a universe, we calculate a constraints which follow from assuming there are zero pbhs within a observable universe. this extends a constraints right down to a horizon scale at a end of inflation, but does not significantly tighten a existing power spectrum constraints, even though a constraint on pbh abundance should decrease by up to 46 orders of magnitude. this shows that no future improvement inside observational constraints should ever lead to the significant tightening inside constraints on inflation (via a power spectrum amplitude). a power spectrum constraints are weak because an order unity perturbation was required inside order to overcome pressure forces. we therefore consider an early matter dominated era, during which exponentially more pbhs form considering a same initial conditions. we show this leads to far tighter constraints, which idea behind the method ${\cal p}_\mathcal{r}\lesssim10^{-9}$, albeit over the smaller range of scales and are very sensitive to when a early matter dominated era ends. finally, we show that an extended early matter era was incompatible with a argument that an evaporating pbh would destroy a universe, unless a power spectrum amplitude decreases by up to ten orders of magnitude.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10491,"we consider higher order parabolic operator $\partial_t+(-\delta_x)^m$ and higher order schr√∂dinger operator $i^{-1}\partial_t+(-\delta_x)^m$ inside $x=\{(t,x)\in\mathbb{r}^{1+n};~|t|<a,|x_n|<b\}$ where $m$ was any positive integer. under certain lower order and regularity assumptions, we prove that if a solution considering linear problem vanishes when $x_n>0$, then a solution vanishes inside $x$. such results are given globally, and we also prove some related local results.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
5061,"inside this work, we propose class-enhanced attentive response (clear): an idea behind the method to visualize and understand a decisions made by deep neural networks (dnns) given the specific input. clear facilitates a visualization of attentive regions and levels of interest of dnns during a decision-making process. it also enables a visualization of a most dominant classes associated with these attentive regions of interest. as such, clear should mitigate some of a shortcomings of heatmap-based methods associated with decision ambiguity, and allows considering better insights into a decision-making process of dnns. quantitative and qualitative experiments across three different datasets demonstrate a efficacy of clear considering gaining the better understanding of a inner workings of dnns during a decision-making process.",1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17185,"a superior performance of ensemble methods with infinite models are well known. most of these methods are based on optimization problems inside infinite-dimensional spaces with some regularization, considering instance, boosting methods and convex neural networks use $l^1$-regularization with a non-negative constraint. however, due to a difficulty of handling $l^1$-regularization, these problems require early stopping or the rough approximation to solve it inexactly. inside this paper, we propose the new ensemble learning method that performs inside the space of probability measures, that is, our method should handle a $l^1$-constraint and a non-negative constraint inside the rigorous way. such an optimization was realized by proposing the general purpose stochastic optimization method considering learning probability measures using parameterization with the help of transport maps on base models. as the result of running a method, the transport map to output an infinite ensemble was obtained, which forms the residual-type network. from a perspective of functional gradient methods, we give the convergence rate as fast as that of the stochastic optimization method considering finite dimensional nonconvex problems. moreover, we show an interior optimality property of the local optimality condition used inside our analysis.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7502,"a x-ray regime, where a most massive visible component of galaxy clusters, a intra cluster medium (icm), was visible, offers directly measured quantities, like a luminosity, and derived quantities, like a total mass, to characterize these objects. a aim of this project was to analyze the complete sample of galaxy clusters inside detail and constrain cosmological parameters, like a matter density, omegam, or a amplitude of initial density fluctuations, sigma8. a purely x-ray flux-limited sample (hiflugcs) consists of a 64 x-ray brightest galaxy clusters, which are excellent targets to study a systematic effects, that should bias results. we analyzed inside total 196 chandra observations of a 64 hiflugcs clusters, with the total exposure time of 7.7 ms. here we present our data analysis procedure (including an automated substructure detection and an energy band optimization considering surface brightness profile analysis) which gives individually determined, robust total mass estimates. these masses are tested against dynamical and planck sunyaev-zeldovich (sz) derived masses of a same clusters, where good overall agreement was found with a dynamical masses. a planck sz masses seem to show the mass dependent bias to our hydrostatic masses; possible biases inside this mass-mass comparison are discussed including a planck selection function. furthermore, we show a results considering a 0.1-2.4-kev-luminosity vs. mass scaling-relation. a overall slope of a sample (1.34) was inside agreement with expectations and values from literature. splitting a sample into galaxy groups and clusters reveals, even after the selection bias correction, that galaxy groups exhibit the significantly steeper slope (1.88) compared to clusters (1.06).",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11645,"we establish existence of stein kernels considering probability measures on $\mathbb{r}^d$ satisfying the poincar√© inequality, and obtain bounds on a stein discrepancy of such measures. applications to quantitative central limit theorems are discussed, including the new clt inside wasserstein distance $w_2$ with optimal rate and dependence on a dimension. as the byproduct, we obtain the stability version of an approximate of a poincar√© constant of probability measures under the second moment constraint. a results extend more generally to a setting of converse weighted poincar√© inequalities. a proof was based on simple arguments of calculus of variations. further, we establish two general properties enjoyed by a stein discrepancy, holding whenever the stein kernel exists: stein discrepancy was strictly decreasing along a clt, and it controls a skewness of the random vector.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
13068,"inside view of a fact that biological characteristics have excellent independent distinguishing characteristics,biometric identification technology involves almost all a relevant areas of human distinction. fingerprints, iris, face, voice-print and other biological features have been widely used inside a public security departments to detect detection, mobile equipment unlock, target tracking and other fields. with a use of electronic devices more and more widely and a frequency was getting higher and higher. only a biometrics identification technology with excellent recognition rate should guarantee a long-term development of these fields.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9613,"the statistical approximation model with qualitative input provides the mechanism to fuse human intuition inside the form of qualitative information into the quantitative statistical model. we investigate statistical properties and devise the numerical computation method considering the model subclass with the uniform correlation structure. we show that, within this subclass, qualitative information should be as objective as quantitative information. we also show that a correlation between each pair of variables controls a accuracy of a statistical estimate. an application to portfolio selection was discussed. a correlation, although compromising a accuracy of a statistical estimation, affects a performance of a portfolio inside the minimal way.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18094,"this article develops the strengthened convex quadratic convex (qc) relaxation of a ac optimal power flow (ac-opf) problem and presents an optimization-based bound-tightening (obbt) algorithm to compute tight, feasible bounds on a voltage magnitude variables considering each bus and a phase angle difference variables considering each branch inside a network. theoretical properties of a strengthened qc relaxation that show its dominance over a other variants of a qc relaxation studied inside a literature are also derived. a effectiveness of a strengthened qc relaxation was corroborated using extensive numerical results on benchmark ac-opf test networks. inside particular, a results demonstrate that a proposed relaxation consistently provides a tightest variable bounds and optimality gaps with negligible impacts on runtime performance.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
2802,"we study an ensemble of energy loads controlled using coordinated, implementation-light, randomized on/off switching. we show that mean field control with nonlinear feedback on a cumulative consumption, assumed available to a aggregator using direct physical measurements of a energy flow, allows a ensemble to recover from its use inside a demand response significantly faster than inside a case of a fixed feedback. when a nonlinearity was sufficiently strong, a total instantaneous energy consumption of a ensemble shows super-relaxation---it stabilizes to a steady state much faster than a underlying probability distribution of a devices over their state space, while also leaving almost no devices outside of a comfort zone.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
10273,"we present the scalable end-to-end classifier that uses streaming physiological and medication data to accurately predict a onset of sepsis, the life-threatening complication from infections that has high mortality and morbidity. our proposed framework models a multivariate trajectories of continuous-valued physiological time series with the help of multitask gaussian processes, seamlessly accounting considering a high uncertainty, frequent missingness, and irregular sampling rates typically associated with real clinical data. a gaussian process was directly connected to the black-box classifier that predicts whether the patient will become septic, chosen inside our case to be the recurrent neural network to account considering a extreme variability inside a length of patient encounters. we show how to scale a computations associated with a gaussian process inside the manner so that a entire system should be discriminatively trained end-to-end with the help of backpropagation. inside the large cohort of heterogeneous inpatient encounters at our university health system we find that it outperforms several baselines at predicting sepsis, and yields 19.4% and 55.5% improved areas under a receiver operating characteristic and precision recall curves as compared to a news score currently used by our hospital.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0
101,"this document was designed to be the first-year graduate-level introduction to probabilistic programming. it not only provides the thorough background considering anyone wishing to use the probabilistic programming system, but also introduces a techniques needed to design and build these systems. it was aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages. we start with the discussion of model-based reasoning and explain why conditioning as the foundational computation was central to a fields of probabilistic machine learning and artificial intelligence. we then introduce the simple first-order probabilistic programming language (ppl) whose programs define static-computation-graph, finite-variable-cardinality models. inside a context of this restricted ppl we introduce fundamental inference algorithms and describe how they should be implemented inside a context of models denoted by probabilistic programs. inside a second part of this document, we introduce the higher-order probabilistic programming language, with the functionality analogous to that of established programming languages. this affords a opportunity to define models with dynamic computation graphs, at a cost of requiring inference methods that generate samples by repeatedly executing a program. foundational inference algorithms considering this kind of probabilistic programming language are explained inside a context of an interface between program executions and an inference controller. this document closes with the chapter on advanced topics which we believe to be, at a time of writing, interesting directions considering probabilistic programming research; directions that point towards the tight integration with deep neural network research and a development of systems considering next-generation artificial intelligence applications.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
4463,"modeling decision-dependent scenario probabilities inside stochastic programs was difficult and typically leads to large and highly non-linear minlps that are very difficult to solve. inside this paper, we develop the new idea behind the method to obtain the compact representation of a recourse function with the help of the set of binary decision diagrams (bdds) that encode the nested cover of a scenario set. a resulting bdds should then be used to efficiently characterize a decision-dependent scenario probabilities by the set of linear inequalities, which essentially factorizes a probability distribution and thus allows to reformulate a entire problem as the small mixed-integer linear program. a idea behind the method was applicable to the large class of stochastic programs with multivariate binary scenario sets, such as stochastic network design, network reliability, or stochastic network interdiction problems. computational results show that a bdd-based scenario representation reduces a problem size, and thus a computation time, significant compared to previous approaches.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
11283,"context: a cosmological concordance model ($\lambda$cdm) matches a cosmological observations exceedingly well. this model has become a standard cosmological model with a evidence considering an accelerated expansion provided by a type ia supernovae (snia) hubble diagram. however, a robustness of this evidence has been addressed recently with somewhat diverging conclusions. aims: a purpose of this paper was to assess a robustness of a conclusion that a universe was indeed accelerating if we rely only on low-redshift (z$\lesssim$2) observations, that was to say with snia, baryonic acoustic oscillations, measurements of a hubble parameter at different redshifts, and measurements of a growth of matter perturbations. methods: we used a standard statistical procedure of minimizing a $\chi^2$ function considering a different probes to quantify a goodness of fit of the model considering both $\lambda$cdm and the simple nonaccelerated low-redshift power law model. inside this analysis, we do not assume that supernovae intrinsic luminosity was independent of a redshift, which has been the fundamental assumption inside most previous studies that cannot be tested. results: we have found that, when snia intrinsic luminosity was not assumed to be redshift independent, the nonaccelerated low-redshift power law model was able to fit a low-redshift background data as well as, or even slightly better, than $\lambda$cdm. when measurements of a growth of structures are added, the nonaccelerated low-redshift power law model still provides an excellent fit to a data considering all a luminosity evolution models considered. conclusions: without a standard assumption that supernovae intrinsic luminosity was independent of a redshift, low-redshift probes are consistent with the nonaccelerated universe.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8304,"text inside natural images contains rich semantics that are often highly relevant to objects or scene. inside this paper, we focus on a problem of fully exploiting scene text considering visual understanding. a main idea was combining word representations and deep visual features into the globally trainable deep convolutional neural network. first, a recognized words are obtained by the scene text reading system. then, we combine a word embedding of a recognized words and a deep visual features into the single representation, which was optimized by the convolutional neural network considering fine-grained image classification. inside our framework, a attention mechanism was adopted to reveal a relevance between each recognized word and a given image, which further enhances a recognition performance. we have performed experiments on two datasets: con-text dataset and drink bottle dataset, that are proposed considering fine-grained classification of business places and drink bottles, respectively. a experimental results consistently demonstrate that a proposed method combining textual and visual cues significantly outperforms classification with only visual representations. moreover, we have shown that a learned representation improves a retrieval performance on a drink bottle images by the large margin, making it potentially useful inside product search.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18265,"this paper introduces the multirobot cooperation idea behind the method to solve a ""pursuit evasion"" problem considering mobile robots that have omnidirectional vision sensors. a main characteristic of this idea behind the method was to implement the real cooperation between robots based on knowledge sharing and makes them work as the team. the complete algorithm considering computing the motion strategy of robots was also presented. this algorithm was based on searching critical points inside a environment. finally, a deliberation protocol which distributes a exploration task among a team and takes a best possible outcome from a robots resources was presented.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
3938,"we use dimensional regularization inside pure quantum gravity on de sitter background to evaluate a one loop expectation value of an invariant operator which gives a local expansion rate. we show that a renormalization of this nonlocal composite operator should be accomplished with the help of a counterterms of the simple local theory of gravity plus matter, at least at one loop order. this renormalization completely absorbs a one loop correction, which accords with a prediction that a lowest secular back-reaction should be the 2-loop effect.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18724,"globular clusters are a oldest conglomerates of stars inside our galaxy and should be useful laboratories to test theories from stellar evolution to cosmology. inside this paper, we present the new method to approximate a absolute age of the globular cluster from observations of its brown dwarfs. a transition region between a end of a main sequence and a brown dwarf regime was characterized by the dearth of objects as function of magnitude. a brightest of a cooling brown dwarfs was easily identified by an increase inside density inside a color-magnitude diagram as you go fainter inside magnitudes, and these brightest brown dwarfs get fainter with age. by identifying a brightest brown dwarfs, it was thus possible to determine a age of the globular cluster within the 1 gyr precision with four-sigma confidence. this new method, which was independent of current methods of age approximation and which does not rely on a knowledge of a cluster's distance from earth, will become feasible thanks to a high spatial resolution and incredible infrared sensitivity of a james webb space telescope.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7883,"water balance models (wbms) are often employed to understand regional hydrologic cycles over various time scales. most wbms, however, are physically-based, and few employ state-of-the-art statistical methods to reconcile independent input measurement uncertainty and bias. further, few wbms exist considering large lakes, and most large lake wbms perform additive accounting, with minimal consideration towards input data uncertainty. here, we introduce the framework considering improving the previously developed large lake statistical water balance model (l2swbm). focusing on a water balances of lakes superior and michigan-huron, we demonstrate our new analytical framework, identifying l2swbms from 26 alternatives that adequately close a water balance of a lakes with satisfactory computation times compared with a prototype model. we expect our new framework will be used to develop water balance models considering other lakes around a world.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5142,"direct numerical simulations of turbulent convection inside the large aspect-ratio box are carried out inside a range of rayleigh number $7 \times 10^4 \le ra \le 2 \times 10^6$ at prandtl number pr=0.71. the strong correlation between a vertical velocity and temperature was observed inside a turbulent regime at almost all a length scales. frequency spectra of all a velocities and temperature show the $-5/3$ law considering the wide band of frequencies. a variances of horizontal velocities at different points inside a flow yield the single power-law. probability density functions of velocities and temperature are close to gaussian only at higher rayleigh numbers. a mean and variance of temperature clearly show boundary layers, surface layers and the near-homogeneous bulk region. a boundary layer thickness decreases and bulk-homogeneity was enhanced on increasing a rayleigh numbers. a wave number spectra of a turbulent kinetic energy exhibit kolmogorov like ($e(k)\sim k^{-5/3}$) and bolginao-obukhov like ($e(k)\sim k^{-11/5}$) behaviour respectively inside a central and near-wall regions of a container. an approximate balance between a production due to buoyancy and a dissipation was found inside a turbulent kinetic energy budget. taylor's approximate equation of a production due to turbulent stretching and a dissipation of turbulent enstrophy was modified by a inclusion of buoyancy production inside a enstrophy budget. a present results support a previously proposed $2/7$ power-law dependence of a average nusselt number on a rayleigh number by yielding an exponent of 0.272, but do not necessarily support a proposed classification of ""soft"" and ""hard"" turbulence on a basis of this exponent.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18670,"we study a asymptotic speed of traveling fronts of a scalar reaction diffusion considering positive reaction terms and with the diffusion coefficient depending nonlinearly on a concentration and on its gradient. we restrict our study to diffusion coefficients of a form $d(u,u_x) = m u^{m-1} u_x^{m(p-2)}$ considering which existence and convergence to traveling fronts has been established. we formulate the variational principle considering a asymptotic speed of a fronts. upper and lower bounds considering a speed valid considering any $m\ge0, p\ge 1$ are constructed. when $m=1, p=2$ a problem reduces to a constant diffusion problem and a bounds correspond to a classic zeldovich frank-kamenetskii lower bound and a aronson-weinberger upper bound respectively. inside a special case $m(p-1) = 1$ the local lower bound should be constructed which coincides with a aforementioned upper bound. a speed inside this case was completely determined inside agreement with recent results.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17249,"we present an exactly solvable lattice hamiltonian to realize gapped boundaries of kitaev's quantum double models considering dijkgraaf-witten theories. we classify a elementary excitations on a boundary, and systematically describe a bulk-to-boundary condensation procedure. we also present a parallel algebraic/categorical structure of gapped boundaries.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
3692,"this position paper formalises an abstract model considering complex negotiation dialogue. this model was to be used considering a benchmark of optimisation algorithms ranging from reinforcement learning to stochastic games, through transfer learning, one-shot learning or others.",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
12852,"we propose an algorithm considering deep learning on networks and graphs. it relies on a notion that many graph algorithms, such as pagerank, weisfeiler-lehman, or message passing should be expressed as iterative vertex updates. unlike previous methods which rely on a ingenuity of a designer, deep graphs are adaptive to a approximation problem. training and deployment are both efficient, since a cost was $o(|e| + |v|)$, where $e$ and $v$ are a sets of edges and vertices respectively. inside short, we learn a recurrent update functions rather than positing their specific functional form. this yields an algorithm that achieves excellent accuracy on both graph labeling and regression tasks.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
685,"inside this paper, we propose the principled perceptual adversarial networks (pan) considering image-to-image transformation tasks. unlike existing application-specific algorithms, pan provides the generic framework of learning mapping relationship between paired images (fig. 1), such as mapping the rainy image to its de-rained counterpart, object edges to its photo, semantic labels to the scenes image, etc. a proposed pan consists of two feed-forward convolutional neural networks (cnns), a image transformation network t and a discriminative network d. through combining a generative adversarial loss and a proposed perceptual adversarial loss, these two networks should be trained alternately to solve image-to-image transformation tasks. among them, a hidden layers and output of a discriminative network d are upgraded to continually and automatically discover a discrepancy between a transformed image and a corresponding ground-truth. simultaneously, a image transformation network t was trained to minimize a discrepancy explored by a discriminative network d. through a adversarial training process, a image transformation network t will continually narrow a gap between transformed images and ground-truth images. experiments evaluated on several image-to-image transformation tasks (e.g., image de-raining, image inpainting, etc.) show that a proposed pan outperforms many related state-of-the-art methods.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9717,"conditional independence testing was the fundamental problem underlying causal discovery and the particularly challenging task inside a presence of nonlinear and high-dimensional dependencies. here the fully non-parametric test considering continuous data based on conditional mutual information combined with the local permutation scheme was presented. through the nearest neighbor approach, a test efficiently adapts also to non-smooth distributions due to strongly nonlinear dependencies. numerical experiments demonstrate that a test reliably simulates a null distribution even considering small sample sizes and with high-dimensional conditioning sets. a test was better calibrated than kernel-based tests utilizing an analytical approximation of a null distribution, especially considering non-smooth densities, and reaches a same or higher power levels. combining a local permutation scheme with a kernel tests leads to better calibration, but suffers inside power. considering smaller sample sizes and lower dimensions, a test was faster than random fourier feature-based kernel tests if a permutation scheme was (embarrassingly) parallelized, but a runtime increases more sharply with sample size and dimensionality. thus, more theoretical research to analytically approximate a null distribution and speed up a approximation considering larger sample sizes was desirable.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
1366,"inside this paper, the control idea behind the method called artificial neural tissue (ant) was applied to multirobot excavation considering lunar base preparation tasks including clearing landing pads and burying of habitat modules. we show considering a first time, the team of autonomous robots excavating the terrain to match the given 3d blueprint. constructing mounds around landing pads will provide physical shielding from debris during launch/landing. burying the human habitat modules under 0.5 m of lunar regolith was expected to provide both radiation shielding and maintain temperatures of -25 $^{o}$c. this minimizes base life-support complexity and reduces launch mass. ant was compelling considering the lunar mission because it doesn't require the team of astronauts considering excavation and it requires minimal supervision. a robot teams are shown to autonomously interpret blueprints, excavate and prepare sites considering the lunar base. because little pre-programmed knowledge was provided, a controllers discover creative techniques. ant evolves techniques such as slot-dozing that would otherwise require excavation experts. this was critical inside making an excavation mission feasible when it was prohibitively expensive to send astronauts. a controllers evolve elaborate negotiation behaviors to work inside close quarters. these and other techniques such as concurrent evolution of a controller and team size are shown to tackle problem of antagonism, when too many robots interfere reducing a overall efficiency or worse, resulting inside gridlock. while many challenges remain with this technology our work shows the compelling pathway considering field testing this approach.",1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0
18849,"a competition between antiferromagnetic (af) order and singlet formation was the central phenomenon of a kondo and periodic anderson hamiltonians, and of a heavy fermion materials they describe. inside this paper, we explore a effects of an additional conduction band on magnetism inside these models, and, specifically, on changes inside a af-singlet quantum critical point (qcp) and a one particle and spin spectral functions. to understand a magnetic phase transition qualitatively, we first carry out the self-consistent mean field theory (mft). a basic conclusion was that, at half-filling, a coupling to a additional band stabilizes a af phase to larger $f$ $d$ hybridization $v$ inside a pam. we also explore a possibility of competing ferromagnetic phases when this conduction band was doped away from half-filling. we next employ quantum monte carlo (qmc) which, inside combination with finite size scaling, allows us to evaluate a position of a qcp with the help of an exact treatment of a interactions. this idea behind the method confirms a stabilization of af order, which occurs through an enhancement of a ruderman-kittel-kasuya-yosida (rkky) interaction. qmc results considering a spectral function $a(\textbf{q},\omega)$ and dynamic spin structure factor $\chi(\textbf{q},\omega)$ yield additional insight into a af-singlet competition and a low temperature phases.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
13613,"this paper presents the novel framework inside which image cosegmentation and colocalization are cast into the single optimization problem that integrates information from low level appearance cues with that of high level localization cues inside the very weakly supervised manner. inside contrast to multi-task learning paradigm that learns similar tasks with the help of the shared representation, a proposed framework leverages two representations at different levels and simultaneously discriminates between foreground and background at a bounding box and superpixel level with the help of discriminative clustering. we show empirically that constraining a two problems at different scales enables a transfer of semantic localization cues to improve cosegmentation output whereas local appearance based segmentation cues aid colocalization. a unified framework outperforms strong baseline approaches, of learning a two problems separately, by the large margin on four benchmark datasets. furthermore, it obtains competitive results compared to a state of a art considering cosegmentation on two benchmark datasets and second best result considering colocalization on pascal voc 2007.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16207,"inside this article, we advance divide-and-conquer strategies considering solving a community detection problem inside networks. we propose two algorithms which perform clustering on the number of small subgraphs and finally patches a results into the single clustering. a main advantage of these algorithms was that they bring down significantly a computational cost of traditional algorithms, including spectral clustering, semi-definite programs, modularity based methods, likelihood based methods etc., without losing on accuracy and even improving accuracy at times. these algorithms are also, by nature, parallelizable. thus, exploiting a facts that most traditional algorithms are accurate and a corresponding optimization problems are much simpler inside small problems, our divide-and-conquer methods provide an omnibus recipe considering scaling traditional algorithms up to large networks. we prove consistency of these algorithms under various subgraph selection procedures and perform extensive simulations and real-data analysis to understand a advantages of a divide-and-conquer idea behind the method inside various settings.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0
7022,"we point out that unitary representations of a virasoro algebra contain berry phases obtained by acting on the primary state with conformal transformations that trace the closed path on the virasoro coadjoint orbit. these phases should be computed exactly thanks to a maurer-cartan form on a virasoro group, and they persist after combining left- and right-moving sectors. thinking of virasoro representations as particles inside ads_3 dressed with boundary gravitons, a berry phases associated with brown-henneaux diffeomorphisms provide the gravitational extension of thomas precession.",0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
10489,"currently, deep neural networks are deployed on low-power portable devices by first training the full-precision model with the help of powerful hardware, and then deriving the corresponding low-precision model considering efficient inference on such systems. however, training models directly with coarsely quantized weights was the key step towards learning on embedded platforms that have limited computing resources, memory capacity, and power consumption. numerous recent publications have studied methods considering training quantized networks, but these studies have mostly been empirical. inside this work, we investigate training methods considering quantized neural networks from the theoretical viewpoint. we first explore accuracy guarantees considering training methods under convexity assumptions. we then look at a behavior of these algorithms considering non-convex problems, and show that training algorithms that exploit high-precision representations have an important greedy search phase that purely quantized training methods lack, which explains a difficulty of training with the help of low-precision arithmetic.",1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
11620,"chemotherapeutic response of cancer cells to the given compound was one of a most fundamental information one requires to design anti-cancer drugs. recent advances inside producing large drug screens against cancer cell lines provided an opportunity to apply machine learning methods considering this purpose. inside addition to cytotoxicity databases, considerable amount of drug-induced gene expression data has also become publicly available. following this, several methods that exploit omics data were proposed to predict drug activity on cancer cells. however, due to a complexity of cancer drug mechanisms, none of a existing methods are perfect. one possible direction, therefore, was to combine a strengths of both a methods and a databases considering improved performance. we demonstrate that integrating the large number of predictions by a proposed method improves a performance considering this task. a predictors inside a ensemble differ inside several aspects such as a method itself, a number of tasks method considers (multi-task vs. single-task) and a subset of data considered (sub-sampling). we show that all these different aspects contribute to a success of a final ensemble. inside addition, we attempt to use a drug screen data together with two novel signatures produced from a drug-induced gene expression profiles of cancer cell lines. finally, we evaluate a method predictions by inside vitro experiments inside addition to a tests on data sets.the predictions of a methods, a signatures and a software are available from \url{this http url}.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3754,"this short note describes a benefit one obtains from the specific construction of the family of parametrices considering the class of elliptic boundary value problems perturbed by non-linear terms of product type. a construction was based on a boutet de monvel calculus of pseudo-differential boundary operators considering a linear elliptic parts, and on paradifferential operators considering a product terms.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
9174,"we justify rigorously an isobe-kakinuma model considering water waves as the higher order shallow water approximation inside a case of the flat bottom. it was known that a full water wave equations are approximated by a shallow water equations with an error of order $o(\delta^2)$, where $\delta$ was the small nondimensional parameter defined as a ratio of a mean depth to a typical wavelength. a green-naghdi equations are known as higher order approximate equations to a water wave equations with an error of order $o(\delta^4)$. inside this paper we show that a isobe-kakinuma model was the much higher order approximation to a water wave equations with an error of order $o(\delta^6)$.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11120,"winds of cool dwarfs are difficult to observe, with only the few m dwarfs presenting observationally-derived mass-loss rates (mdot), which span several orders of magnitude. close-in exoplanets are conveniently positioned inside a inner regions of stellar winds and can, thus, be used to probe a otherwise-unobservable local properties of their host-stars' winds. here, we use local stellar wind characteristics observationally-derived inside a studies of atmospheric evaporation of a warm-neptune gj436 b to derive a global characteristics of a wind of its m-dwarf host. with the help of an isothermal wind model, we constrain a stellar wind temperature to be inside a range [0.36,0.43] mk, with mdot=[0.5,2.5] x 10^{-15} msyn/yr. by computing a pressure balance between a stellar wind and a interstellar medium, we derive a size of a astrophere of gj436 to be around 25 au, significantly more compact than a heliosphere. we demonstrate inside this paper that transmission spectroscopy, coupled to planetary atmospheric evaporation and stellar wind models, should be the useful tool considering constraining a large-scale wind structure of planet-hosting stars. extending our idea behind the method to future planetary systems discoveries will open new perspectives considering a combined characterisation of planetary exospheres and winds of cool dwarf stars.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10538,"this paper develops detailed mathematical statistical theory of the new class of cross-validation techniques of local linear kernel hazards and their multiplicative bias corrections. a new class of cross-validation combines principles of local information and recent advances inside indirect cross-validation. the few applications of cross-validating multiplicative kernel hazard approximation do exist inside a literature. however, detailed mathematical statistical theory and small sample performance are introduced using this paper and further upgraded to our new class of best one-sided cross-validation. best one-sided cross-validation turns out to have excellent performance inside its practical illustrations, inside its small sample performance and inside its mathematical statistical theoretical performance.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
7918,"inside this paper, we present the novel and general network structure towards accelerating a inference process of convolutional neural networks, which was more complicated inside network structure yet with less inference complexity. a core idea was to equip each original convolutional layer with another low-cost collaborative layer (lccl), and a element-wise multiplication of a relu outputs of these two parallel layers produces a layer-wise output. a combined layer was potentially more discriminative than a original convolutional layer, and its inference was faster considering two reasons: 1) a zero cells of a lccl feature maps will remain zero after element-wise multiplication, and thus it was safe to skip a calculation of a corresponding high-cost convolution inside a original convolutional layer, 2) lccl was very fast if it was implemented as the 1*1 convolution or only the single filter shared by all channels. extensive experiments on a cifar-10, cifar-100 and ilscrc-2012 benchmarks show that our proposed network structure should accelerate a inference process by 32\% on average with negligible performance drop.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
19000,"visual question answering (or vqa) was the new and exciting problem that combines natural language processing and computer vision techniques. we present the survey of a various datasets and models that have been used to tackle this task. a first part of a survey details a various datasets considering vqa and compares them along some common factors. a second part of this survey details a different approaches considering vqa, classified into four types: non-deep learning models, deep learning models without attention, deep learning models with attention, and other models which do not fit into a first three. finally, we compare a performances of these approaches and provide some directions considering future work.",1,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
15061,"we develop the new theory of perfect fluids with translation and rotation symmetry, which was also applicable inside a absence of any type of boost symmetry. it involves introducing the new fluid variable, a kinetic mass density, which was needed to define a most general energy-momentum tensor considering perfect fluids. our theory leads to corrections to a euler equations considering perfect fluids that might be observable inside hydrodynamic fluid experiments. we also derive new expressions considering a speed of sound inside perfect fluids. our theory reduces to a known perfect fluid models when boost symmetry was present. it should also be adapted to (non-relativistic) scale invariant fluids with critical exponent $z$. we show that perfect fluids cannot have schr√∂dinger symmetry unless $z=2$. considering generic values of $z$ there should be fluids with lifshitz symmetry, and as the concrete example, we work out inside detail a thermodynamics and fluid description of an ideal gas of lifshitz particles and compute a speed of sound considering a classical and quantum lifshitz gasses.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0
1889,"site-occupation embedding theory (soet) was an alternative formulation of density-functional theory (dft) considering model hamiltonians where a fully-interacting hubbard problem was mapped, inside principle exactly, onto an impurity-interacting (rather than the non-interacting) one. it provides the rigorous framework considering combining wavefunction (or green function) based methods with dft. inside this work, exact expressions considering a per-site energy and double occupation of a uniform hubbard model are derived inside a context of soet. as readily seen from these derivations, a so-called bath contribution to a per-site correlation energy is, inside addition to a latter, a key density functional quantity to model inside soet. various approximations based on bethe ansatz and perturbative solutions to a hubbard and single impurity anderson models are constructed and tested on the one-dimensional ring. a self-consistent calculation of a embedded impurity wavefunction has been performed with a density matrix renormalization group method. it has been shown that promising results are obtained inside specific regimes of correlation and density. possible further developments have been proposed inside order to provide reliable embedding functionals and potentials.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
14614,"there has been considerable interest inside with the help of decomposition methods inside epidemiology (mediation analysis) and economics (oaxaca-blinder decomposition) to understand how health disparities arise and how they might change upon intervention. it has not been clear when estimates from a oaxaca-blinder decomposition should be interpreted causally because its implementation does not explicitly address potential confounding of target variables. while mediation analysis does explicitly adjust considering confounders of target variables, it does so inside the way that entails equalizing confounders across racial groups, which may not reflect a intended intervention. revisiting prior analyses inside a national longitudinal survey of youth on disparities inside wages, unemployment, incarceration, and overall health with test scores, taken as the proxy considering educational attainment, as the target intervention, we propose and demonstrate the novel decomposition that controls considering confounders of test scores (measures of childhood ses) while leaving their association with race intact. we compare this decomposition with others that use standardization (to equalize childhood ses alone), mediation analysis (to equalize test scores within levels of childhood ses), and one that equalizes both childhood ses and test scores. we also show how these decompositions, including our novel proposals, are equivalent to causal implementations of a oaxaca-blinder decomposition.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
6170,"inside this paper, by limiting twisted conical k√§hler-ricci flows, we prove a long-time existence and uniqueness of cusp k√§hler-ricci flow on compact k√§hler manifold $m$ which carries the smooth hypersurface $d$ such that a twisted canonical bundle $k_m+d$ was ample. furthermore, we prove that this flow converge to the unique cusp k√§hler-einstein metric.",0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14349,"recommender systems have been widely adopted by electronic commerce and entertainment industries considering individualized prediction and recommendation, which benefit consumers and improve business intelligence. inside this article, we propose an innovative method, namely a recommendation engine of multilayers (rem), considering tensor recommender systems. a proposed method utilizes a structure of the tensor response to integrate information from multiple modes, and creates an additional layer of nested latent factors to accommodate between-subjects dependency. one major advantage was that a proposed method was able to address a ""cold-start"" issue inside a absence of information from new customers, new products or new contexts. specifically, it provides more effective recommendations through sub-group information. to achieve scalable computation, we develop the new algorithm considering a proposed method, which incorporates the maximum block improvement strategy into a cyclic blockwise-coordinate-descent algorithm. inside theory, we investigate both algorithmic properties considering global and local convergence, along with a asymptotic consistency of estimated parameters. finally, a proposed method was applied inside simulations and iri marketing data with 116 million observations of product sales. numerical studies demonstrate that a proposed method outperforms existing competitors inside a literature.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0
10953,"researchers are often interested inside assessing a impact of an intervention on an outcome of interest inside situations where a intervention was non-randomised, information was available at an aggregate level, a intervention was only applied to one or few units, a intervention was binary, and there are outcome measurements at multiple time points. inside this paper, we review existing methods considering causal inference inside a setup just outlined. we detail a assumptions underlying each method, emphasise connections between a different approaches and provide guidelines regarding their practical implementation. several open problems are identified thus highlighting a need considering future research.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16985,"we report results on benchmarking open information extraction (oie) systems with the help of relvis, the toolkit considering benchmarking open information extraction systems. our comprehensive benchmark contains three data sets from a news domain and one data set from wikipedia with overall 4522 labeled sentences and 11243 binary or n-ary oie relations. inside our analysis on these data sets we compared a performance of four popular oie systems, clausie, openie 4.2, stanford openie and predpatt. inside addition, we evaluated a impact of five common error classes on the subset of 749 n-ary tuples. from our deep analysis we unreveal important research directions considering the next generation of oie systems.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11334,"we perform the bayesian analysis of a p-variate skew-t model, providing the new parameterization, the set of non-informative priors and the sampler specifically designed to explore a posterior density of a model parameters. extensions, such as a multivariate regression model with skewed errors and a stochastic frontiers model, are easily accommodated. the novelty introduced inside a paper was given by a extension of a bivariate skew-normal model given inside liseo & parisi (2013) to the more realistic p-variate skew-t model. we also introduce a r package mvst, which allows to approximate a multivariate skew-t model.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
199,"we develop the parameterized primal-dual $\pi$ learning method based on deep neural networks considering markov decision process with large state space and off-policy reinforcement learning. inside contrast to a popular q-learning and actor-critic methods that are based on successive approximations to a nonlinear bellman equation, our method makes primal-dual updates to a policy and value functions utilizing a fundamental linear bellman duality. naive parametrization of a primal-dual $\pi$ learning method with the help of deep neural networks would encounter two major challenges: (1) each update requires computing the probability distribution over a state space and was intractable; (2) a iterates are unstable since a parameterized lagrangian function was no longer linear. we address these challenges by proposing the relaxed lagrangian formulation with the regularization penalty with the help of a advantage function. we show that a dual policy update step inside our method was equivalent to a policy gradient update inside a actor-critic method inside some special case, while a value updates differ substantially. a main advantage of a primal-dual $\pi$ learning method lies inside that a value and policy updates are closely coupled together with the help of a bellman duality and therefore more informative. experiments on the simple cart-pole problem show that a algorithm significantly outperforms a one-step temporal-difference actor-critic method, which was a most relevant benchmark method to compare with. we believe that a primal-dual updates to a value and policy functions would expedite a learning process. a proposed methods might open the door to more efficient algorithms and sharper theoretical analysis.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8265,"we offer the novel way of thinking about a modelling of a time-varying distributions of financial asset returns. borrowing ideas from symbolic data analysis, we consider data representations beyond scalars and vectors. specifically, we consider the quantile function as an observation, and develop the new class of dynamic models considering quantile-function-valued (qf-valued) time series. inside order to make statistical inferences and account considering parameter uncertainty, we propose the method whereby the likelihood function should be constructed considering qf-valued data, and develop an adaptive mcmc sampling algorithm considering simulating from a posterior distribution. compared to modelling realised measures, modelling a entire quantile functions of intra-daily returns allows one to gain more insight into a dynamic structure of price movements. using simulations, we show that a proposed mcmc algorithm was effective inside recovering a posterior distribution, and that a posterior means are reasonable point estimates of a model parameters. considering empirical studies, a new model was applied to analysing one-minute returns of major international stock indices. through quantile scaling, we further demonstrate a usefulness of our method by forecasting one-step-ahead a value-at-risk of daily returns.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
12460,"topologically nontrivial field configurations called ""baby skyrmions"" behave like particles and give origins to a field of skyrmionics that promises racetrack memory and other technological applications. unraveling a non-equilibrium behavior of such topological solitons was the challenge. we realize baby skyrmions inside the chiral liquid crystal and, with the help of numerical modeling and polarized video microscopy, demonstrate electrically driven squirming motion. we reveal a intricate details of non-equilibrium topology-preserving textural changes driving this behavior. direction of a skyrmion's motion was robustly controlled inside the plane orthogonal to a applied field and should be reversed by varying frequency. our findings may spur the new paradigm of soliton dynamics inside soft matter, with the rich interplay between topology, chirality, and orientational viscoelasticity.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
9795,"inside this paper, we prove that considering any fixed $205/243<\gamma\leqslant1$, every sufficiently large $n$ satisfying $n\equiv 5 \pmod {24}$ should be represented as five squares of primes with one prime inside $\mathcal{p}_\gamma$, which improves a previous result of zhang and zhai.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
3043,"pain was the complex and subjective experience that poses the number of measurement challenges. while self-report by a patient was viewed as a gold standard of pain assessment, this idea behind the method fails when patients cannot verbally communicate pain intensity or lack normal mental abilities. here, we present the pain intensity measurement method based on physiological signals. specifically, we implement the multi-task learning idea behind the method based on neural networks that accounts considering individual differences inside pain responses while still leveraging data from across a population. we test our method inside the dataset containing multi-modal physiological responses to nociceptive pain.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8528,"salient object detection (sod), which aims to identify and locate a most salient pixels or regions inside images, has been attracting more and more interest due to its various real-world applications. however, this vision task was quite challenging, especially under complex image scenes. inspired by a intrinsic reflection of natural images, inside this paper we propose the novel feature learning framework considering large-scale salient object detection. specifically, we design the symmetrical fully convolutional network (sfcn) to effectively learn complementary saliency features under a guidance of lossless feature reflection. a location information, together with contextual and semantic information, of salient objects are jointly utilized to supervise a proposed network considering more accurate saliency predictions. inside addition, to overcome a blurry boundary problem, we propose the new weighted structural loss function to ensure clear object boundaries and spatially consistent saliency. a coarse prediction results are effectively refined by these structural information considering performance improvements. extensive experiments on seven saliency detection datasets demonstrate that our idea behind the method achieves consistently superior performance and outperforms a very recent state-of-the-art methods with the large margin.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
567,"we introduce the new high dimensional algorithm considering efficiency corrected, maximally monte carlo event generator independent fiducial measurements at a lhc and beyond. a idea behind the method was driven probabilistically with the help of the deep neural network on an event-by-event basis, trained with the help of detector simulation and even only pure phase space distributed events. this idea behind the method gives also the glimpse into a future of high energy physics, where experiments publish new type of measurements inside the radically multidimensional way.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6974,"it was well known that there was an absolute constant $\mathfrak{c}>0$ such that if a laplace transform $g(s)=\int_{0}^{\infty}\rho(x)e^{-s x}\:\mathrm{d}x$ of the bounded function $\rho$ has analytic continuation through every point of a segment $(-i\lambda ,i\lambda )$ of a imaginary axis, then $$ \limsup_{x\to\infty} \left|\int_{0}^{x}\rho(u)\:\mathrm{d}u - g(0)\right|\leq \frac{ \mathfrak{c}}{\lambda} \: \limsup_{x\to\infty} |\rho(x)|. $$ a best known value of a constant $\mathfrak{c}$ is so far $\mathfrak{c}=2$. inside this article we show that a inequality holds with $\mathfrak{c}=\pi/2$ and that this value was best possible. we also sharpen tauberian constants inside finite forms of other related complex tauberian theorems considering laplace transforms.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
2570,"transmission spectra of exoplanetary atmospheres have been used to infer a presence of clouds/hazes. such inferences are typically based on spectral slopes inside a optical deviant from gaseous rayleigh scattering or low-amplitude spectral features inside a infrared. we investigate three observable metrics that could allow constraints on cloud properties from transmission spectra, namely, a optical slope, a uniformity of this slope, and condensate features inside a infrared. we derive these metrics with the help of model transmission spectra considering mie extinction from the wide range of condensate species, particle sizes, and scale heights. firstly, we investigate possible degeneracies among a cloud properties considering an observed slope. we find, considering example, that spectra with very steep optical slopes suggest sulphide clouds (e.g. mns, zns, na$_2$s) inside a atmospheres. secondly, (non)uniformities inside optical slopes provide additional constraints on cloud properties, e.g., mns, zns, tio$_2$, and fe$_2$o$_3$ have significantly non-uniform slopes. thirdly, infrared spectra provide an additional powerful probe into cloud properties, with sio$_2$, fe$_2$o$_3$, mg$_2$sio$_4$, and mgsio$_3$ bearing strong infrared features observable with a james webb space telescope. we investigate observed spectra of eight hot jupiters and discuss their implications. inside particular, no single or composite condensate species considered here conforms to a steep and non-uniform optical slope observed considering hd 189733b. our work highlights a importance of a three above metrics to investigate cloud properties inside exoplanetary atmospheres with the help of high-precision transmission spectra and detailed cloud models. we make our mie scattering data considering condensates publicly available to a community.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
4853,"we present the novel algorithm to train the deep q-learning agent with the help of natural-gradient techniques. we compare a original deep q-network (dqn) algorithm to its natural-gradient counterpart, which we refer to as ngdqn, on the collection of classic control domains. without employing target networks, ngdqn significantly outperforms dqn without target networks, and performs no worse than dqn with target networks, suggesting that ngdqn stabilizes training and should aid reduce a need considering additional hyperparameter tuning. we also find that ngdqn was less sensitive to hyperparameter optimization relative to dqn. together these results suggest that natural-gradient techniques should improve value-function optimization inside deep reinforcement learning.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
17823,"highly eccentric binary systems appear inside many astrophysical contexts, ranging from tidal capture inside dense star clusters, precursors of stellar disruption by massive black holes, to high-eccentricity migration of giant planets. inside the highly eccentric binary, a tidal potential of one body should excite oscillatory modes inside a other during the pericenter passage, resulting inside energy exchange between a modes and a binary orbit. these modes exhibit one of three behaviors over multiple passages: low-amplitude oscillations, large amplitude oscillations corresponding to the resonance between a orbital frequency and a mode frequency, and chaotic growth. we study these phenomena with an iterative map, fully exploring how a mode evolution depends on a pericenter distance and other parameters. inside addition, we show that a dissipation of mode energy results inside the quasi-steady state, with gradual orbital decay punctuated by resonances, even inside systems where a mode amplitude would initially grow stochastically. the newly captured star around the black hole should experience significant orbital decay and heating due to a chaotic growth of a mode amplitude and dissipation. the giant planet pushed into the high-eccentricity orbit may experience the similar effect and become the hot or warm jupiter.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14033,"a work of bernstein-zelevinsky and zelevinsky gives the good understanding of irreducible subquotients of the reducible principal series representation of $gl_n(f)$, $f$ the $p$-adic field (without specifying their multiplicities which was done by the kazhdan-lusztig type conjecture). inside this paper we make the proposal of the similar kind considering principal series representations of $gl_n({\mathbb r})$. our investigation on principal series representations naturally led us to consider a steinberg representation considering real groups, which has curiuosly not been paid much attention to inside a subject (unlike a $p$-adic case). our proposal considering steinberg was a simplest possible: considering the real reductive group $g$, a steinberg of $g({\mathbb r})$ was the discrete series representation if and only if $g({\mathbb r})$ has the discrete series, and makes up the full $l$-packet of representations of $g({\mathbb r})$ (of size $w_g/w_k$), so was typically not irreducible.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0
18605,"we focus on implementing and optimizing the sixth-order finite-difference solver considering simulating compressible fluids on the gpu with the help of third-order runge-kutta integration. since graphics processing units perform well inside data-parallel tasks, this makes them an attractive platform considering fluid simulation. however, high-order stencil computation was memory-intensive with respect to both main memory and a caches of a gpu. we present two approaches considering simulating compressible fluids with the help of 55-point and 19-point stencils. we seek to reduce a requirements considering memory bandwidth and cache size inside our methods by with the help of cache blocking and decomposing the latency-bound kernel into several bandwidth-bound kernels. our fastest implementation was bandwidth-bound and integrates $343$ million grid points per second on the tesla k40t gpu, achieving the $3.6 \times$ speedup over the comparable hydrodynamics solver benchmarked on two intel xeon e5-2690v3 processors. our alternative gpu implementation was latency-bound and achieves a rate of $168$ million updates per second.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0
17241,"this paper presents catsim, a first package written inside a python language specialized inside computerized adaptive tests and a logistical models of item response theory. catsim provides functions considering generating item and examinee parameters, simulating tests and plotting results, as well as enabling end users to create new procedures considering proficiency initialization, item selection, proficiency approximation and test stopping criteria. a simulator keeps the record of a items selected considering each examinee as well as their answers and also enables a simulation of linear tests, inside which all examinees answer a same items. a various components made available by catsim should also be used inside a creation of third-party testing applications. examples of such usages are also presented inside this paper.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
14045,"inside this paper we prove that given two sets $e_1,e_2 \subset \mathbb{z}$ of positive density, there exists $k \geq 1$ which was bounded by the number depending only on a densities of $e_1$ and $e_2$ such that $k\mathbb{z} \subset (e_1-e_1)\cdot(e_2-e_2)$. as the corollary of a main theorem we deduce that if $\alpha,\beta > 0$ then there exist $n_0$ and $d_0$ which depend only on $\alpha$ and $\beta$ such that considering every $n \geq n_0$ and $e_1,e_2 \subset \mathbb{z}_n$ with $|e_1| \geq \alpha n, |e_2| \geq \beta n$ there exists $d \leq d_0$ the divisor of $n$ satisfying $d \, \mathbb{z}_n \subset (e_1-e_1)\cdot(e_2-e_2)$.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
16361,"we consider four-dimensional gravity coupled to the non-linear sigma model whose scalar manifold was the non-compact geometrically finite surface $\sigma$ endowed with the riemannian metric of constant negative curvature. when a space-time was an flrw universe, such theories produce the very wide generalization of two-field $\alpha$-attractor models, being parameterized by the positive constant $\alpha$, by a choice of the finitely-generated surface group $\gamma\subset \mathrm{psl}(2,\mathbb{r})$ (which was isomorphic with a fundamental group of $\sigma$) and by a choice of the scalar potential defined on $\sigma$. a traditional two-field $\alpha$-attractor models arise when $\gamma$ was a trivial group, inside which case $\sigma$ was a poincar√© disk. we give the general prescription considering a study of such models through uniformization inside a so-called ""non-elementary"" case and discuss some of their qualitative features inside a gradient flow approximation, which we relate to morse theory. we also discuss some aspects of a srst approximation inside these models, showing that it was generally not well-suited considering studying dynamics near cusp ends. when $\sigma$ was non-compact and a scalar potential was ""well-behaved"" at a ends, we show that, inside a {\em naive} local one-field truncation, our generalized models have a same universal behavior as ordinary one-field $\alpha$-attractors if inflation happens near any of a ends of $\sigma$ where a extended potential has the local maximum, considering trajectories which are well approximated by non-canonically parameterized geodesics near a ends, we also discuss spiral trajectories near a ends.",0,1,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7076,"we present and characterize a catalog of galaxy shape measurements that will be used considering cosmological weak lensing measurements inside a wide layer of a first year of a hyper suprime-cam (hsc) survey. a catalog covers an area of 136.9 deg$^2$ split into six fields, with the mean $i$-band seeing of $0.58$ arcsec and $5\sigma$ point-source depth of $i\sim 26$. given conservative galaxy selection criteria considering first year science, a depth and excellent image quality results inside unweighted and weighted source number densities of 24.6 and 21.8 arcmin$^{-2}$, respectively. we define a requirements considering cosmological weak lensing science with this catalog, then focus on characterizing potential systematics inside a catalog with the help of the series of internal null tests considering problems with point-spread function (psf) modeling, shear estimation, and other aspects of a image processing. we find that a psf models narrowly meet requirements considering weak lensing science with this catalog, with fractional psf model size residuals of approximately $0.003$ (requirement: 0.004) and a psf model shape correlation function $\rho_1<3\times 10^{-7}$ (requirement: $4\times 10^{-7}$) at 0.5$^\circ$ scales. the variety of galaxy shape-related null tests are statistically consistent with zero, but star-galaxy shape correlations reveal additive systematics on $>1^\circ$ scales that are sufficiently large as to require mitigation inside cosmic shear measurements. finally, we discuss a dominant systematics and a planned algorithmic changes to reduce them inside future data reductions.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
12187,"materials exhibiting large magnetoresistance may not only be of fundamental research interest, but also should lead to wide-ranging applications inside magnetic sensors and switches. here we demonstrate the large linear-in-field magnetoresistance, $\delta \rho/\rho$ reaching as high as $\sim$600$\%$ at 2 k under the 9 tesla field, inside a tetragonal phase of the transiton-metal stannide $\beta$-rhsn$_4$. detailed analyses show that its magnetic responses are overall inconsistent with a classical model based on a multiple electron scattering by mobility fluctuations inside an inhomogenous conductor, but rather inside line with a quantum effects due to a presence of dirac-like dispersions inside a electronic structure. our results may aid guiding a future quest considering quantum magnetoresistive materials into a family of stannides, similar to a role played by ptsn$_4$ with topological node arcs.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
2095,"network embedding leverages a node proximity manifested to learn the low-dimensional node vector representation considering each node inside a network. a learned embeddings could advance various learning tasks such as node classification, network clustering, and link prediction. most, if not all, of a existing works, are overwhelmingly performed inside a context of plain and static networks. nonetheless, inside reality, network structure often evolves over time with addition/deletion of links and nodes. also, the vast majority of real-world networks are associated with the rich set of node attributes, and their attribute values are also naturally changing, with a emerging of new content patterns and a fading of old content patterns. these changing characteristics motivate us to seek an effective embedding representation to capture network and attribute evolving patterns, which was of fundamental importance considering learning inside the dynamic environment. to our best knowledge, we are a first to tackle this problem with a following two challenges: (1) a inherently correlated network and node attributes could be noisy and incomplete, it necessitates the robust consensus representation to capture their individual properties and correlations; (2) a embedding learning needs to be performed inside an online fashion to adapt to a changes accordingly. inside this paper, we tackle this problem by proposing the novel dynamic attributed network embedding framework - dane. inside particular, dane first provides an offline method considering the consensus embedding and then leverages matrix perturbation theory to maintain a freshness of a end embedding results inside an online manner. we perform extensive experiments on both synthetic and real attributed networks to corroborate a effectiveness and efficiency of a proposed framework.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0
13249,"noncritical soft-faults and model deviations are the challenge considering fault detection and diagnosis (fdd) of resident autonomous underwater vehicles (auvs). such systems may have the faster performance degradation due to a permanent exposure to a marine environment, and constant monitoring of component conditions was required to ensure their reliability. this works presents an evaluation of recurrent neural networks (rnns) considering the data-driven fault detection and diagnosis scheme considering underwater thrusters with empirical data. a nominal behavior of a thruster is modeled with the help of a measured control input, voltage, rotational speed and current signals. we evaluated a performance of fault classification with the help of all a measured signals compared to with the help of a computed residuals from a nominal model as features.",1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0
13222,"inside this paper, we tackle a problem of explanations inside the deep-learning based model considering recommendations by leveraging a technique of layer-wise relevance propagation. we use the deep convolutional neural network to extract relevant features from a input images before identifying similarity between a images inside feature space. relationships between a images are identified by a model and layer-wise relevance propagation was used to infer pixel-level details of a images that may have significantly informed a model's choice. we evaluate our method on an amazon products dataset and demonstrate a efficacy of our approach.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18011,"despite significant recent advances inside a field of head pose approximation and facial expression recognition, raising a cognitive level when analysing human activity presents serious challenges to current concepts. motivated by a need of generating comprehensible visual representations from different sets of data, we introduce the system capable of monitoring human activity through head pose and facial expression changes, utilising an affordable 3d sensing technology (microsoft kinect sensor). an idea behind the method build on discriminative random regression forests is selected inside order to rapidly and accurately approximate head pose changes inside unconstrained environment. inside order to complete a secondary process of recognising four universal dominant facial expressions (happiness, anger, sadness and surprise), emotion recognition using facial expressions (erfe) is adopted. after that, the lightweight data exchange format (javascript object notation-json) was employed, inside order to manipulate a data extracted from a two aforementioned settings. such mechanism should yield the platform considering objective and effortless assessment of human activity within a context of serious gaming and human-computer interaction.",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
10683,"we show that a skip-gram formulation of word2vec trained with negative sampling was equivalent to the weighted logistic pca. this connection allows us to better understand a objective, compare it to other word embedding methods, and extend it to higher dimensional models.",1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
7800,"we consider a minimization of non-convex functions that typically arise inside machine learning. specifically, we focus our attention on the variant of trust region methods known as cubic regularization. this idea behind the method was particularly attractive because it escapes strict saddle points and it provides stronger convergence guarantees than first- and second-order as well as classical trust region methods. however, it suffers from the high computational complexity that makes it impractical considering large-scale learning. here, we propose the novel method that uses sub-sampling to lower this computational cost. by a use of concentration inequalities we provide the sampling scheme that gives sufficiently accurate gradient and hessian approximations to retain a strong global and local convergence guarantees of cubically regularized methods. to a best of our knowledge this was a first work that gives global convergence guarantees considering the sub-sampled variant of cubic regularization on non-convex functions. furthermore, we provide experimental results supporting our theory.",1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0
3474,"we show that if the compact connected $n$-dimensional manifold $m$ has the conformal class containing two non-homothetic metrics $g$ and $\tilde g=e^{2\varphi}g$ with non-generic holonomy, then after passing to the finite covering, either $n=4$ and $(m,g,\tilde g)$ was an ambik√§hler manifold, or $n\ge 6$ was even and $(m,g,\tilde g)$ was obtained by a calabi ansatz from the polarized hodge manifold of dimension $n-2$, or both $g$ and $\tilde g$ have reducible holonomy, $m$ was locally diffeomorphic to the product $m_1\times m_2\times m_3$, a metrics $g$ and $\tilde g$ should be written as $g=g_1+g_2+e^{-2\varphi}g_3$ and $\tilde g=e^{2\varphi}(g_1+g_2)+g_3$ considering some riemannian metrics $g_i$ on $m_i$, and $\varphi$ was a pull-back of the non-constant function on $m_2$.",0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
17203,"we present the new technique called contrastive principal component analysis (cpca) that was designed to discover low-dimensional structure that was unique to the dataset, or enriched inside one dataset relative to other data. a technique was the generalization of standard pca, considering a setting where multiple datasets are available -- e.g. the treatment and the control group, or the mixed versus the homogeneous population -- and a goal was to explore patterns that are specific to one of a datasets. we conduct the wide variety of experiments inside which cpca identifies important dataset-specific patterns that are missed by pca, demonstrating that it was useful considering many applications: subgroup discovery, visualizing trends, feature selection, denoising, and data-dependent standardization. we provide geometrical interpretations of cpca and show that it satisfies desirable theoretical guarantees. we also extend cpca to nonlinear settings inside a form of kernel cpca. we have released our code as the python package and documentation was on github.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
3337,"considering any lipschitz domain we construct an arbitrarily small, localized perturbation which splits a spectrum of a laplacian into simple eigenvalues. we use considering this purpose the hadamard's formula and spectral stability results.",0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
18030,"global registration of multi-view robot data was the challenging task. appearance-based global localization approaches often fail under drastic view-point changes, as representations have limited view-point invariance. this work was based on a idea that human-made environments contain rich semantics which should be used to disambiguate global localization. here, we present x-view, the multi-view semantic global localization system. x-view leverages semantic graph descriptor matching considering global localization, enabling localization under drastically different view-points. while a idea behind the method was general inside terms of a semantic input data, we present and evaluate an implementation on visual data. we demonstrate a system inside experiments on a publicly available synthia dataset, on the realistic urban dataset recorded with the simulator, and on real-world streetview data. our findings show that x-view was able to globally localize aerial-to-ground, and ground-to-ground robot data of drastically different view-points. our idea behind the method achieves an accuracy of up to 85 % on global localizations inside a multi-view case, while a benchmarked baseline appearance-based methods reach up to 75 %.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
14310,"this tutorial review presents an overview of a basic theoretical aspects of two-dimensional (2d) crystals. we revise essential aspects of graphene and a new families of semiconducting 2d materials, like transition metal dichalcogenides or black phosphorus. minimal theoretical models considering various materials are presented. some of a exciting new possibilities offered by 2d crystals are discussed, such as manipulation and control of quantum degrees of freedom (spin and pseudospin), confinement of excitons, control of a electronic and optical properties with strain engineering, or unconventional superconducting phases.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
13359,"we present the method considering synthesizing the frontal, neutral-expression image of the person's face given an input face photograph. this was achieved by learning to generate facial landmarks and textures from features extracted from the facial-recognition network. unlike previous approaches, our encoding feature vector was largely invariant to lighting, pose, and facial expression. exploiting this invariance, we train our decoder network with the help of only frontal, neutral-expression photographs. since these photographs are well aligned, we should decompose them into the sparse set of landmark points and aligned texture maps. a decoder then predicts landmarks and textures independently and combines them with the help of the differentiable image warping operation. a resulting images should be used considering the number of applications, such as analyzing facial attributes, exposure and white balance adjustment, or creating the 3-d avatar.",1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8209,"with introduction of new technologies inside a operating room like a da vinci surgical system, training surgeons to use them effectively and efficiently was crucial inside a delivery of better patient care. coaching by an expert surgeon was effective inside teaching relevant technical skills, but current methods to deliver effective coaching are limited and not scalable. we present the virtual reality simulation-based framework considering automated virtual coaching inside surgical education. we implement our framework within a da vinci skills simulator. we provide three coaching modes ranging from the hands-on teacher (continuous guidance) to the handsoff guide (assistance upon request). we present six teaching cues targeted at critical learning elements of the needle passing task, which are shown to a user based on a coaching mode. these cues are graphical overlays which guide a user, inform them about sub-par performance, and show relevant video demonstrations. we evaluated our framework inside the pilot randomized controlled trial with 16 subjects inside each arm. inside the post-study questionnaire, participants reported high comprehension of feedback, and perceived improvement inside performance. after three practice repetitions of a task, a control arm (independent learning) showed better motion efficiency whereas a experimental arm (received real-time coaching) had better performance of learning elements (as per a acs resident skills curriculum). we observed statistically higher improvement inside a experimental group based on one of a metrics (related to needle grasp orientation). inside conclusion, we developed an automated coach that provides real-time cues considering surgical training and demonstrated its feasibility.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
13289,"a coherent optical response from 140~nm and 65~nm thick zno epitaxial layers was studied with the help of transient four-wave-mixing spectroscopy with picosecond temporal resolution. resonant excitation of neutral donor-bound excitons results inside two-pulse and three-pulse photon echoes. considering a donor-bound the exciton (d$^0$x$_\text{a}$) at temperature of 1.8~k we evaluate optical coherence times $t_2=33-50$~ps corresponding to homogeneous linewidths of $13-19~\mu$ev, about two orders of magnitude smaller as compared with a inhomogeneous broadening of a optical transitions. a coherent dynamics was determined mainly by a population decay with time $t_1=30-40$~ps, while pure dephasing was negligible inside a studied high quality samples even considering strong optical excitation. temperature increase leads to the significant shortening of $t_2$ due to interaction with acoustic phonons. inside contrast, a loss of coherence of a donor-bound b exciton (d$^0$x$_\text{b}$) was significantly faster ($t_2=3.6$~ps) and governed by pure dephasing processes.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
15011,"phobos and deimos are a two small martian moons, orbiting almost on a equatorial plane of mars. recent works have shown that they should accrete within an impact-generated inner dense and outer light disk, and that a same impact potentially forms a borealis basin, the large northern hemisphere basin on a current mars. however, there was no the priori reason considering a impact to take place close to a north pole (borealis present location) nor to generate the debris disk inside a equatorial plane of mars (in which phobos and deimos orbit). inside this paper, we investigate these remaining issues on a giant impact origin of a martian moons. first, we show that a mass deficit created by a borealis impact basin induces the global reorientation of a planet to realign its main moment of inertia with a rotation pole (true polar wander). this moves a location of a borealis basin toward its current location. next, with the help of analytical arguments, we investigate a detailed dynamical evolution of a eccentric inclined disk from a equatorial plane of mars that was formed by a martian-moon-forming impact. we find that, as the result of precession of disk particles due to a martian dynamical flattening $j_{2}$ term of its gravity field and particle-particle inelastic collisions, eccentricity and inclination are damped and an inner dense and outer light equatorial circular disk was eventually formed. our results strengthen a giant impact origin of phobos and deimos that should finally be tested by the future sample return mission such as jaxa's martian moons exploration (mmx) mission.",0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
5835,"next-generation radio interferometers, such as a square kilometre array (ska), will revolutionise our understanding of a universe through their unprecedented sensitivity and resolution. however, standard methods inside radio interferometry produce reconstructed interferometric images that are limited inside quality and they are not scalable considering big data. inside this work we apply and evaluate alternative interferometric reconstruction methods that make use of state-of-the-art sparse image reconstruction algorithms motivated by compressive sensing, which have been implemented inside a purify software package. inside particular, we implement and apply a proximal alternating direction method of multipliers (p-admm) algorithm presented inside the recent article. we apply purify to real interferometric observations. considering all observations purify outperforms a standard clean, where inside some cases purify provides an improvement inside dynamic range by over an order of magnitude. a latest version of purify, which includes a developments presented inside this work, was made publicly available.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
14460,"functional electrical stimulation (fes) systems are successful inside restoring motor function and supporting paralyzed users. commercially available fes products are open loop, meaning that a system was unable to adapt to changing conditions with a user and their muscles which results inside muscle fatigue and poor stimulation protocols. this was because it was difficult to close a loop between stimulation and monitoring of muscle contraction with the help of adaptive stimulation. fes causes electrical artefacts which make it challenging to monitor muscle contractions with traditional methods such as electromyography (emg). we look to overcome this limitation by combining fes with novel mechanomyographic (mmg) sensors to be able to monitor muscle activity during stimulation inside real time. to provide the meaningful task we built an fes cycling rig with the software interface that enabled us to perform adaptive recording and stimulation, and then combine this with sensors to record forces applied to a pedals with the help of force sensitive resistors (fsrs), crank angle position with the help of the magnetic incremental encoder and inputs from a user with the help of switches and the potentiometer. we illustrated this with the closed-loop stimulation algorithm that used a inputs from a sensors to control a output of the programmable rehastim 1 fes stimulator (hasomed) inside real-time. this recumbent bicycle rig is used as the testing platform considering fes cycling. a algorithm is designed to respond to the change inside requested speed (rpm) from a user and change a stimulation power (% of maximum current ma) until this speed is achieved and then maintain it.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
18978,"inside order to pursue a vision of a robocup humanoid league of beating a soccer world champion by 2050, new rules and competitions are added or modified each year fostering novel technological advances. inside 2017, a number of players inside a teensize class soccer games is increase to 3 vs. 3, which allowed considering more team play strategies. improvements inside individual skills were also demanded through the set of technical challenges. this paper presents a latest individual skills and team play developments used inside robocup 2017 that lead our team nimbro winning a 2017 teensize soccer tournament, a technical challenges, and a drop-in games.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
14058,"we present the passivity-based whole-body control idea behind the method considering quadruped robots that achieves dynamic locomotion while compliantly balancing a robot's trunk. we formulate a motion tracking as the quadratic program that takes into account a full robot rigid body dynamics, a actuation limit, a joint limits and a contact interaction. we analyze a controller robustness against inaccurate friction coefficient estimates and unstable footholds, as well as its capability to redistribute a load as the consequence of enforcing actuation limits. additionally, we present some practical implementation details gained from a experience with a real platform. extensive experimental trials on a 90 kg hydraulically actuated quadruped robot validate a capabilities of this controller under various terrain conditions and gaits. a proposed idea behind the method was expedient considering accurate execution of high dynamic motions with respect to a current state of a art.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
12628,"we propose the class of interleavers considering the novel deep neural network (dnn) architecture that uses algorithmically pre-determined, structured sparsity to significantly lower memory and computational requirements, and speed up training. a interleavers guarantee clash-free memory accesses to eliminate idle operational cycles, optimize spread and dispersion to improve network performance, and are designed to ease a complexity of memory address computations inside hardware. we present the design algorithm with mathematical proofs considering these properties. we also explore interleaver variations and analyze a behavior of neural networks as the function of interleaver metrics.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
19149,"we report a discovery and analysis of a most metal-poor damped lyman-alpha (dla) system currently known, based on observations made with a keck hires spectrograph. a metal paucity of this system has only permitted a determination of three element abundances: [c/h] = -3.43 +/- 0.06, [o/h] = -3.05 +/- 0.05, and [si/h] = -3.21 +/- 0.05, as well as an upper limit on a abundance of iron: [fe/h] < -2.81. this dla was among a most carbon-poor environment currently known with detectable metals. by comparing a abundance pattern of this dla to detailed models of metal-free nucleosynthesis, we find that a chemistry of a gas was consistent with a yields of the 20.5 m_sun metal-free star that ended its life as the core-collapse supernova; a abundances we measure are inconsistent with a yields of pair-instability supernovae. such the tight constraint on a mass of a progenitor population iii star was afforded by a well-determined c/o ratio, which we show depends almost monotonically on a progenitor mass when a kinetic energy of a supernova explosion was e_exp > 1.5x10^51 erg. we find that a dla presented here has just crossed a critical 'transition discriminant' threshold, rendering a dla gas now suitable considering low mass star formation. we also discuss a chemistry of this system inside a context of recent models that suggest some of a most metal-poor dlas are a precursors of a 'first galaxies', and are a antecedents of a ultra-faint dwarf galaxies.",0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
11011,"a demand on mobile electronics to continue to shrink inside size while increase inside efficiency drives a demand on a internal passive components to do a same. power amplifiers require inductors with small form factors, high quality factors, and high operating frequency inside a single-digit ghz range. this work explores a use of magnetic materials to satisfy a needs of power amplifier inductor applications. this paper discusses a optimization choices regarding material selection, device design, and fabrication methodology. a inductors achieved here present a best performance to date considering an integrated magnetic core inductor at high frequencies with the 1 nh inductance and peak quality factor of 4 at ~3 ghz. such compact inductors show potential considering efficiently meeting a need of mobile electronics inside a future.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
18834,"we introduce the two-player contest considering evaluating a safety and robustness of machine learning systems, with the large prize pool. unlike most prior work inside ml robustness, which studies norm-constrained adversaries, we shift our focus to unconstrained adversaries. defenders submit machine learning models, and try to achieve high accuracy and coverage on non-adversarial data while making no confident mistakes on adversarial inputs. attackers try to subvert defenses by finding arbitrary unambiguous inputs where a model assigns an incorrect label with high confidence. we propose the simple unambiguous dataset (""bird-or- bicycle"") to use as part of this contest. we hope this contest will aid to more comprehensively evaluate a worst-case adversarial risk of machine learning models.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
9355,"we address component-based regularisation of the multivariate generalized linear mixed model. the set of random responses y was modelled by the glmm, with the help of the set x of explanatory variables and the set t of additional covariates. variables inside x are assumed many and redundant: generalized linear mixed regression demands regularisation with respect to x. by contrast, variables inside t are assumed few and selected so as to demand no regularisation. regularisation was performed building an appropriate number of orthogonal components that both contribute to model y and capture relevant structural information inside x. we propose to optimize the scglr-specific criterion within the schall's algorithm inside order to approximate a model. this extension of scglr was tested on simulated and real data, and compared to ridge-and lasso-based regularisations.",0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
16840,"inside this paper we study generative modeling using autoencoders while with the help of a elegant geometric properties of a optimal transport (ot) problem and a wasserstein distances. we introduce sliced-wasserstein autoencoders (swae), which are generative models that enable one to shape a distribution of a latent space into any samplable probability distribution without a need considering training an adversarial network or defining the closed-form considering a distribution. inside short, we regularize a autoencoder loss with a sliced-wasserstein distance between a distribution of a encoded training samples and the predefined samplable distribution. we show that a proposed formulation has an efficient numerical solution that provides similar capabilities to wasserstein autoencoders (wae) and variational autoencoders (vae), while benefiting from an embarrassingly simple implementation.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16689,we present a conditional neural network (clnn) and a masked conditional neural network (mclnn) designed considering temporal signal recognition. a clnn takes into consideration a temporal nature of a sound signal and a mclnn extends upon a clnn through the binary mask to preserve a spatial locality of a features and allows an automated exploration of a features combination analogous to hand-crafting a most relevant features considering a recognition task. mclnn has achieved competitive recognition accuracies on a gtzan and a ismir2004 music datasets that surpass several state-of-the-art neural network based architectures and hand-crafted methods applied on both datasets.,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
16726,"inside this paper, we present the reverberation removal idea behind the method considering speaker verification, utilizing dual-label deep neural networks (dnns). a networks perform feature mapping between a spectral features of reverberant and clean speech. long short term memory recurrent neural networks (lstms) are trained to map corrupted mel filterbank (mfb) features to two sets of labels: i) a clean mfb features, and ii) either estimated pitch tracks or a fast fourier transform (fft) spectrogram of clean speech. a performance of reverberation removal was evaluated by equal error rates (eers) of speaker verification experiments.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8862,"pac-bayes bounds have been proposed to get risk estimates based on the training sample. inside this paper a pac-bayes idea behind the method was combined with stability of a hypothesis learned by the hilbert space valued algorithm. a pac-bayes setting was used with the gaussian prior centered at a expected output. thus the novelty of our paper was with the help of priors defined inside terms of a data-generating distribution. our main result estimates a risk of a randomized algorithm inside terms of a hypothesis stability coefficients. we also provide the new bound considering a svm classifier, which was compared to other known bounds experimentally. ours appears to be a first stability-based bound that evaluates to non-trivial values.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
6287,"we study a performance of stochastically trained deep neural networks (dnns) whose synaptic weights are implemented with the help of emerging memristive devices that exhibit limited dynamic range, resolution, and variability inside their programming characteristics. we show that the key device parameter to optimize a learning efficiency of dnns was a variability inside its programming characteristics. dnns with such memristive synapses, even with dynamic range as low as $15$ and only $32$ discrete levels, when trained based on stochastic updates suffer less than $3\%$ loss inside accuracy compared to floating point software baseline. we also study a performance of stochastic memristive dnns when used as inference engines with noise corrupted data and find that if a device variability should be minimized, a relative degradation inside performance considering a stochastic dnn was better than that of a software baseline. hence, our study presents the new optimization corner considering memristive devices considering building large noise-immune deep learning systems.",1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
1058,"inside this paper, we consider estimators considering an additive functional of $\phi$, which was defined as $\theta(p;\phi)=\sum_{i=1}^k\phi(p_i)$, from $n$ i.i.d. random samples drawn from the discrete distribution $p=(p_1,...,p_k)$ with alphabet size $k$. we propose the minimax optimal estimator considering a approximation problem of a additive functional. we reveal that a minimax optimal rate was characterized by a divergence speed of a fourth derivative of $\phi$ if a divergence speed was high. as the result, we show there was no consistent estimator if a divergence speed of a fourth derivative of $\phi$ was larger than $p^{-4}$. furthermore, if a divergence speed of a fourth derivative of $\phi$ was $p^{4-\alpha}$ considering $\alpha \in (0,1)$, a minimax optimal rate was obtained within the universal multiplicative constant as $\frac{k^2}{(n\ln n)^{2\alpha}} + \frac{k^{2-2\alpha}}{n}$.",1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0
15017,"considering neural networks (nns) with rectified linear unit (relu) or binary activation functions, we show that their training should be accomplished inside the reduced parameter space. specifically, a weights inside each neuron should be trained on a unit sphere, as opposed to a entire space, and a threshold should be trained inside the bounded interval, as opposed to a real line. we show that a nns inside a reduced parameter space are mathematically equivalent to a standard nns with parameters inside a whole space. a reduced parameter space shall facilitate a optimization procedure considering a network training, as a search space becomes (much) smaller. we demonstrate a improved training performance with the help of numerical examples.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
14010,"we explore a properties of byte-level recurrent language models. when given sufficient amounts of capacity, training data, and compute time, a representations learned by these models include disentangled features corresponding to high-level concepts. specifically, we find the single unit which performs sentiment analysis. these representations, learned inside an unsupervised manner, achieve state of a art on a binary subset of a stanford sentiment treebank. they are also very data efficient. when with the help of only the handful of labeled examples, our idea behind the method matches a performance of strong baselines trained on full datasets. we also demonstrate a sentiment unit has the direct influence on a generative process of a model. simply fixing its value to be positive or negative generates samples with a corresponding positive or negative sentiment.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
5138,"we study a learning capacity of empirical risk minimization with regard to a squared loss and the convex hypothesis class consisting of linear functions. while these types of estimators were originally designed considering noisy linear regression problems, it recently turned out that they are inside fact capable of handling considerably more complicated situations, involving highly non-linear distortions. this work intends to provide the comprehensive explanation of this somewhat astonishing phenomenon. at a heart of our analysis stands a mismatch principle, which was the simple, yet generic recipe to establish theoretical error bounds considering empirical risk minimization. a scope of our results was fairly general, permitting arbitrary sub-gaussian input-output pairs, possibly with strongly correlated feature variables. noteworthy, a mismatch principle also generalizes to the certain extent a classical orthogonality principle considering ordinary least squares. this adaption allows us to investigate problem setups of recent interest, most importantly, high-dimensional parameter regimes and non-linear observation processes. inside particular, our theoretical framework was applied to various scenarios of practical relevance, such as single-index models, variable selection, and strongly correlated designs. we thereby demonstrate a key purpose of a mismatch principle, that is, learning (semi-)parametric output rules under large model uncertainties and misspecifications.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
8300,"just as semantic hashing should accelerate information retrieval, binary valued embeddings should significantly reduce latency inside a retrieval of graphical data. we introduce the simple but effective model considering learning such binary vectors considering nodes inside the graph. by imagining a embeddings as independent coin flips of varying bias, continuous optimization techniques should be applied to a approximate expected loss. embeddings optimized inside this fashion consistently outperform a quantization of both spectral graph embeddings and various learned real-valued embeddings, on both ranking and pre-ranking tasks considering the variety of datasets.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
18660,"the vast wealth of literature exists on a topic of rocket trajectory optimisation, particularly inside a area of interplanetary trajectories due to its relevance today. studies on optimising interstellar and intergalactic trajectories are usually performed inside flat spacetime with the help of an analytical approach, with very little focus on optimising interstellar trajectories inside the general relativistic framework. this paper examines a use of low-acceleration rockets to reach galactic destinations inside a least possible time, with the genetic algorithm being employed considering a optimisation process. a fuel required considering each journey is calculated considering various types of propulsion systems to determine a viability of low-acceleration rockets to colonise a milky way. a results showed that to limit a amount of fuel carried on board, an antimatter propulsion system would likely be a minimum technological requirement to reach star systems tens of thousands of light years away. however, with the help of the low-acceleration rocket would require several hundreds of thousands of years to reach these star systems, with minimal time dilation effects since maximum velocities only reached about 0.2c. such transit times are clearly impractical, and thus, any kind of colonisation with the help of low acceleration rockets would be difficult. high accelerations, on a order of 1g, are likely required to complete interstellar journeys within the reasonable time frame, though they may require prohibitively large amounts of fuel. so considering now, it appears that humanity's ultimate goal of the galactic empire may only be possible at significantly higher accelerations, though a propulsion technology requirement considering the journey that uses realistic amounts of fuel remains to be determined.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
8408,"inside a past few years, attention mechanisms have become an indispensable component of end-to-end neural machine translation models. however, previous attention models always refer to some source words when predicting the target word, which contradicts with a fact that some target words have no corresponding source words. motivated by this observation, we propose the novel attention model that has a capability of determining when the decoder should attend to source words and when it should not. experimental results on nist chinese-english translation tasks show that a new model achieves an improvement of 0.8 bleu score over the state-of-the-art baseline.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0